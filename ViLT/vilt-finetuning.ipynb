{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11783028,"sourceType":"datasetVersion","datasetId":7397846}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finetuning ViLT","metadata":{}},{"cell_type":"markdown","source":"### Setup","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets accelerate peft bitsandbytes torch torchvision pillow scikit-learn tqdm bert-score timeout-decorator\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:01:32.405852Z","iopub.execute_input":"2025-05-14T14:01:32.406173Z","iopub.status.idle":"2025-05-14T14:01:35.856656Z","shell.execute_reply.started":"2025-05-14T14:01:32.406151Z","shell.execute_reply":"2025-05-14T14:01:35.855638Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: timeout-decorator in /usr/local/lib/python3.11/dist-packages (0.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.0.9)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:14.571395Z","iopub.execute_input":"2025-05-14T14:07:14.572212Z","iopub.status.idle":"2025-05-14T14:07:16.092798Z","shell.execute_reply.started":"2025-05-14T14:07:14.572180Z","shell.execute_reply":"2025-05-14T14:07:16.092077Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement csv (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for csv\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os, glob, time\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\n\nfrom transformers import (\n    ViltProcessor,\n    ViltForQuestionAnswering,\n    TrainingArguments,\n    Trainer\n)\nfrom datasets import Dataset, DatasetDict\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom bert_score import score as bertscore_score\nfrom timeout_decorator import timeout, TimeoutError\nimport csv\n\n# Paths for checkpointing predictions & metrics\npred_path        = \"/kaggle/working/vilt_vqa_predictions_baseline.csv\"\nmetrics_path     = \"/kaggle/working/vilt_vqa_metrics_baseline.csv\"\nfinetune_outdir  = \"/kaggle/working/vilt_lora_finetuned\"\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:19.471283Z","iopub.execute_input":"2025-05-14T14:07:19.472063Z","iopub.status.idle":"2025-05-14T14:07:19.480639Z","shell.execute_reply.started":"2025-05-14T14:07:19.472034Z","shell.execute_reply":"2025-05-14T14:07:19.479766Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"ce38015d72ee80819dabf703a40fbf6e26023d69\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:19.843015Z","iopub.execute_input":"2025-05-14T14:07:19.843586Z","iopub.status.idle":"2025-05-14T14:07:19.848473Z","shell.execute_reply.started":"2025-05-14T14:07:19.843561Z","shell.execute_reply":"2025-05-14T14:07:19.847716Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Load & Concatenate Headerless CSVs (S1–S6)","metadata":{}},{"cell_type":"code","source":"base_path = \"/kaggle/input/vr-dataset/dataset_curated\"\nsubfolders = [f\"S{i}\" for i in range(1, 7)]\nall_dfs = []\n\nfor folder in subfolders:\n    csv_path = os.path.join(base_path, folder, f\"{folder}_qa_data.csv\")\n    if os.path.exists(csv_path):\n        df = pd.read_csv(csv_path, header=None, names=[\"image_path\", \"question\", \"answer\"])\n        df[\"image_path\"] = df[\"image_path\"].apply(\n            lambda p: os.path.join(base_path, os.path.normpath(p).split(\"dataset_curated/\")[-1])\n        )\n        df = df[df[\"image_path\"].apply(os.path.exists)].reset_index(drop=True)\n        all_dfs.append(df)\n        print(f\"Loaded {len(df)} examples from {csv_path}\")\n    else:\n        print(f\"CSV not found: {csv_path}\")\n\ndf_all = pd.concat(all_dfs).reset_index(drop=True)\nprint(f\"\\nTotal QA pairs loaded: {len(df_all)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:20.405657Z","iopub.execute_input":"2025-05-14T14:07:20.405950Z","iopub.status.idle":"2025-05-14T14:07:39.664130Z","shell.execute_reply.started":"2025-05-14T14:07:20.405931Z","shell.execute_reply":"2025-05-14T14:07:39.663302Z"}},"outputs":[{"name":"stdout","text":"Loaded 14366 examples from /kaggle/input/vr-dataset/dataset_curated/S1/S1_qa_data.csv\nLoaded 14358 examples from /kaggle/input/vr-dataset/dataset_curated/S2/S2_qa_data.csv\nLoaded 14367 examples from /kaggle/input/vr-dataset/dataset_curated/S3/S3_qa_data.csv\nLoaded 14366 examples from /kaggle/input/vr-dataset/dataset_curated/S4/S4_qa_data.csv\nLoaded 14387 examples from /kaggle/input/vr-dataset/dataset_curated/S5/S5_qa_data.csv\nLoaded 14376 examples from /kaggle/input/vr-dataset/dataset_curated/S6/S6_qa_data.csv\n\nTotal QA pairs loaded: 86220\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### Split into Baseline(first 30%) & Finetune(last 70%)","metadata":{}},{"cell_type":"code","source":"# Compute split indices\nval_split_idx = int(0.2 * len(df_all))\ntrain_split_idx = int(0.8 * len(df_all))  # 60% training, remaining 20% for testing\n\n# Apply the split\nval_df    = df_all.iloc[:val_split_idx].reset_index(drop=True)        # First 20% → Validation\ntrain_df  = df_all.iloc[val_split_idx:train_split_idx].reset_index(drop=True)  # Next 60% → Training\ntest_df   = df_all.iloc[train_split_idx:].reset_index(drop=True)      # Last 20% → Test\n\n# Display the new splits\nprint(f\"Baseline eval samples (20%): {len(val_df)}\")\nprint(f\"LoRA train samples (60%):    {len(train_df)}\")\nprint(f\"Test samples (20%):          {len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:39.665518Z","iopub.execute_input":"2025-05-14T14:07:39.665961Z","iopub.status.idle":"2025-05-14T14:07:39.679945Z","shell.execute_reply.started":"2025-05-14T14:07:39.665935Z","shell.execute_reply":"2025-05-14T14:07:39.679229Z"}},"outputs":[{"name":"stdout","text":"Baseline eval samples (20%): 17244\nLoRA train samples (60%):    51732\nTest samples (20%):          17244\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### Initialize Processor & Model, wrap with LoRA on the two Linear layers","metadata":{}},{"cell_type":"code","source":"pip install --upgrade peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:39.680896Z","iopub.execute_input":"2025-05-14T14:07:39.681455Z","iopub.status.idle":"2025-05-14T14:07:43.041427Z","shell.execute_reply.started":"2025-05-14T14:07:39.681432Z","shell.execute_reply":"2025-05-14T14:07:43.040748Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.31.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.4.26)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from peft import TaskType\nprint([t for t in TaskType])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:43.043250Z","iopub.execute_input":"2025-05-14T14:07:43.043475Z","iopub.status.idle":"2025-05-14T14:07:43.049431Z","shell.execute_reply.started":"2025-05-14T14:07:43.043454Z","shell.execute_reply":"2025-05-14T14:07:43.048869Z"}},"outputs":[{"name":"stdout","text":"[<TaskType.SEQ_CLS: 'SEQ_CLS'>, <TaskType.SEQ_2_SEQ_LM: 'SEQ_2_SEQ_LM'>, <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, <TaskType.TOKEN_CLS: 'TOKEN_CLS'>, <TaskType.QUESTION_ANS: 'QUESTION_ANS'>, <TaskType.FEATURE_EXTRACTION: 'FEATURE_EXTRACTION'>]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Init Processor & Model, Wrap with LoRA, then Monkey‑Patch forward\nprocessor  = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\nbase_model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\nbase_model.to(device)\nbase_model.eval()\n\nlora_config = LoraConfig(\n    task_type=TaskType.QUESTION_ANS,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"classifier.0\", \"classifier.3\"]\n)\n\nmodel = get_peft_model(base_model, lora_config)\nmodel.to(device)\n\n# Monkey‑patch to drop SQuAD args from every forward call\ndef _patched_forward(self, *args, **kwargs):\n    kwargs.pop(\"start_positions\", None)\n    kwargs.pop(\"end_positions\",   None)\n    return super(self.__class__, self).forward(*args, **kwargs)\n\nmodel.forward = _patched_forward.__get__(model, model.__class__)\n\nprint(\"LoRA trainable params:\",\n      sum(p.numel() for p in model.parameters() if p.requires_grad),\n      \"/\", sum(p.numel() for p in model.parameters()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:43.050539Z","iopub.execute_input":"2025-05-14T14:07:43.050757Z","iopub.status.idle":"2025-05-14T14:07:45.674143Z","shell.execute_reply.started":"2025-05-14T14:07:43.050742Z","shell.execute_reply":"2025-05-14T14:07:45.673365Z"}},"outputs":[{"name":"stdout","text":"LoRA trainable params: 55752 / 117644289\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### Training Arguments & Trainer (with step‐wise checkpoints)","metadata":{}},{"cell_type":"code","source":"# Build label2id mapping from the model config\nlabel2id = {lbl.lower(): idx for idx, lbl in base_model.config.id2label.items()}\nnum_labels = len(label2id)\nprint(f\"{num_labels} labels. Example: {list(label2id.items())[:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:45.675057Z","iopub.execute_input":"2025-05-14T14:07:45.675629Z","iopub.status.idle":"2025-05-14T14:07:45.682308Z","shell.execute_reply.started":"2025-05-14T14:07:45.675609Z","shell.execute_reply":"2025-05-14T14:07:45.681716Z"}},"outputs":[{"name":"stdout","text":"3129 labels. Example: [('net', 0), ('pitcher', 1), ('orange', 2), ('yes', 3), ('white', 4)]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### Prepare HuggingFace Datasets & Collator/Metric Functions","metadata":{}},{"cell_type":"code","source":"# Prepare HF Datasets & One‑Hot DataCollator\nfrom datasets import Dataset, DatasetDict\n\ntrain_hf = Dataset.from_pandas(train_df)\nval_hf   = Dataset.from_pandas(val_df)\nds       = DatasetDict({\"train\": train_hf, \"validation\": val_hf})\n\nnum_labels = len(base_model.config.id2label)\n\nclass DataCollatorForVilt:\n    def __init__(self, processor, label2id, num_labels):\n        self.processor = processor\n        self.label2id  = label2id\n        self.num_labels = num_labels\n\n    def __call__(self, features):\n        images    = [Image.open(f[\"image_path\"]).convert(\"RGB\") for f in features]\n        questions = [f[\"question\"] for f in features]\n        answers   = [str(f[\"answer\"]).strip().lower() for f in features]\n\n        # 1) Tokenize → CPU tensors\n        batch = self.processor(\n            images,\n            questions,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True\n        )\n\n        # 2) One‑hot encode labels\n        labels = torch.zeros((len(answers), self.num_labels), dtype=torch.float)\n        for i, ans in enumerate(answers):\n            idx = self.label2id.get(ans, -100)\n            if idx >= 0:\n                labels[i, idx] = 1.0\n\n        batch[\"labels\"] = labels\n        return batch\n\ncollator = DataCollatorForVilt(processor, label2id, num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:45.683256Z","iopub.execute_input":"2025-05-14T14:07:45.683803Z","iopub.status.idle":"2025-05-14T14:07:45.753905Z","shell.execute_reply.started":"2025-05-14T14:07:45.683785Z","shell.execute_reply":"2025-05-14T14:07:45.753318Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Manual evaluation on validation split + save metrics CSV","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # Convert multi-hot vectors to class indices\n    if len(labels.shape) > 1 and labels.shape[1] > 1:\n        labels = labels.argmax(axis=1)   # Converts from one-hot to class IDs\n\n    y_true = labels.tolist()\n    y_pred = preds.argmax(axis=1).tolist()\n    \n    # Calculate accuracy and F1\n    acc = accuracy_score(y_true, y_pred)\n    f1m = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n\n    # BERTScore\n    _, _, F1b = bertscore_score(\n        [base_model.config.id2label[i] for i in y_pred],\n        [base_model.config.id2label[i] for i in y_true],\n        lang=\"en\"\n    )\n    bert = F1b.mean().item()\n\n    return {\n        \"eval_accuracy\": acc,\n        \"eval_f1_macro\": f1m,\n        \"eval_bert_score\": bert\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:45.754744Z","iopub.execute_input":"2025-05-14T14:07:45.755315Z","iopub.status.idle":"2025-05-14T14:07:45.761526Z","shell.execute_reply.started":"2025-05-14T14:07:45.755288Z","shell.execute_reply":"2025-05-14T14:07:45.760789Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Custom Callback to force evaluation every 1000 steps\n# Enhanced Callback for Evaluation & Checkpoint Saving\nfrom transformers import TrainerCallback\n\nclass EvaluationCallback(TrainerCallback):\n    def __init__(self, metrics_csv, pred_csv, processor, model):\n        self.metrics_csv = metrics_csv\n        self.pred_csv    = pred_csv\n        self.processor   = processor\n        self.model       = model\n        \n        # write headers if not exist\n        if not os.path.exists(self.metrics_csv):\n            with open(self.metrics_csv, \"w\", newline=\"\") as f:\n                csv.writer(f).writerow(\n                    [\"step\",\"eval_loss\",\"eval_accuracy\",\"eval_f1_macro\",\"eval_bert_score\"]\n                )\n        if not os.path.exists(self.pred_csv):\n            with open(self.pred_csv, \"w\", newline=\"\") as f:\n                csv.writer(f).writerow(\n                    [\"image_path\",\"question\",\"true_answer\",\"predicted_answer\"]\n                )\n\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % 1000 == 0 and state.global_step > 0:\n            print(f\"\\n[Checkpoint] Running evaluation at step {state.global_step}...\")\n            control.should_evaluate = True\n\n            # === Save Metrics ===\n            metrics = kwargs.get(\"metrics\", {}) or {}\n            row = [\n                state.global_step,\n                metrics.get(\"eval_loss\"),\n                metrics.get(\"eval_accuracy\"),\n                metrics.get(\"eval_f1_macro\"),\n                metrics.get(\"eval_bert_score\")\n            ]\n            with open(self.metrics_csv, \"a\", newline=\"\") as f:\n                csv.writer(f).writerow(row)\n\n            # === Save Predictions ===\n            predictions = []\n            for example in val_df.itertuples(index=False):\n                image = Image.open(example.image_path).convert(\"RGB\")\n                inputs = self.processor(\n                    image,\n                    example.question,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    truncation=True\n                ).to(device)\n\n                with torch.no_grad():\n                    logits = self.model(**inputs).logits\n                    pred_id = logits.argmax(-1).item()\n                    pred_label = base_model.config.id2label[pred_id].split()[0].lower()\n\n                true = str(example.answer).strip().lower()\n                predictions.append([\n                    example.image_path,\n                    example.question,\n                    true,\n                    pred_label\n                ])\n\n            with open(self.pred_csv, \"a\", newline=\"\") as f:\n                csv.writer(f).writerows(predictions)\n\n            print(f\"[Checkpoint] Completed save for step {state.global_step}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:45.762717Z","iopub.execute_input":"2025-05-14T14:07:45.763146Z","iopub.status.idle":"2025-05-14T14:07:45.780715Z","shell.execute_reply.started":"2025-05-14T14:07:45.763128Z","shell.execute_reply":"2025-05-14T14:07:45.779869Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Define TrainingArguments (eval/log/save every 1000 steps)\ntraining_args = TrainingArguments(\n    output_dir=finetune_outdir,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    learning_rate=1e-4,\n    fp16=True,\n    logging_steps=1000,\n    save_steps=1000,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    label_names=[\"labels\"],\n    run_name=\"vilt_lora_finetune_experiment\"\n)\n\n#print(\"Label Names:\", trainer.label_names)   # should output: ['labels']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:45.782450Z","iopub.execute_input":"2025-05-14T14:07:45.782713Z","iopub.status.idle":"2025-05-14T14:07:45.823697Z","shell.execute_reply.started":"2025-05-14T14:07:45.782696Z","shell.execute_reply":"2025-05-14T14:07:45.822912Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Initialize Trainer with Enhanced Callback\neval_callback = EvaluationCallback(\n    metrics_csv=metrics_path,\n    pred_csv=pred_path,\n    processor=processor,\n    model=model\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds[\"train\"],\n    eval_dataset=ds[\"validation\"],\n    data_collator=collator,\n    compute_metrics=compute_metrics,\n    callbacks=[eval_callback]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:45.824517Z","iopub.execute_input":"2025-05-14T14:07:45.824776Z","iopub.status.idle":"2025-05-14T14:07:45.842480Z","shell.execute_reply.started":"2025-05-14T14:07:45.824754Z","shell.execute_reply":"2025-05-14T14:07:45.841696Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Start Fine-Tuning\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T14:07:45.843381Z","iopub.execute_input":"2025-05-14T14:07:45.843653Z","iopub.status.idle":"2025-05-14T19:13:36.638797Z","shell.execute_reply.started":"2025-05-14T14:07:45.843625Z","shell.execute_reply":"2025-05-14T19:13:36.638164Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type dict that is 147544 bytes\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type dict that is 103856 bytes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19398' max='19398' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19398/19398 5:05:49, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Macro</th>\n      <th>Bert Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>2.659000</td>\n      <td>2.406530</td>\n      <td>0.349165</td>\n      <td>0.105772</td>\n      <td>0.976107</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.377400</td>\n      <td>2.258263</td>\n      <td>0.359487</td>\n      <td>0.109688</td>\n      <td>0.976732</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.257000</td>\n      <td>2.192552</td>\n      <td>0.363779</td>\n      <td>0.115421</td>\n      <td>0.977216</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>2.170300</td>\n      <td>2.156758</td>\n      <td>0.366272</td>\n      <td>0.118086</td>\n      <td>0.977448</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.150100</td>\n      <td>2.128710</td>\n      <td>0.369694</td>\n      <td>0.120422</td>\n      <td>0.977403</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>2.098500</td>\n      <td>2.106807</td>\n      <td>0.371723</td>\n      <td>0.124004</td>\n      <td>0.977421</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>2.063000</td>\n      <td>2.089780</td>\n      <td>0.373347</td>\n      <td>0.126154</td>\n      <td>0.977208</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>2.039400</td>\n      <td>2.068769</td>\n      <td>0.377175</td>\n      <td>0.128645</td>\n      <td>0.977526</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>2.034600</td>\n      <td>2.049999</td>\n      <td>0.381060</td>\n      <td>0.132565</td>\n      <td>0.977688</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>2.025400</td>\n      <td>2.031389</td>\n      <td>0.384134</td>\n      <td>0.130654</td>\n      <td>0.977693</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>2.024800</td>\n      <td>2.018382</td>\n      <td>0.384076</td>\n      <td>0.131817</td>\n      <td>0.977592</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>1.998500</td>\n      <td>2.008678</td>\n      <td>0.385119</td>\n      <td>0.132243</td>\n      <td>0.977681</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>2.006700</td>\n      <td>2.001386</td>\n      <td>0.385931</td>\n      <td>0.132812</td>\n      <td>0.977609</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>1.960300</td>\n      <td>1.992965</td>\n      <td>0.385989</td>\n      <td>0.134605</td>\n      <td>0.977554</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>2.013900</td>\n      <td>1.986127</td>\n      <td>0.387439</td>\n      <td>0.134947</td>\n      <td>0.977714</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>1.958900</td>\n      <td>1.982664</td>\n      <td>0.388367</td>\n      <td>0.134245</td>\n      <td>0.977761</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>1.947800</td>\n      <td>1.978922</td>\n      <td>0.388541</td>\n      <td>0.132088</td>\n      <td>0.977721</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>1.957900</td>\n      <td>1.976543</td>\n      <td>0.388425</td>\n      <td>0.132321</td>\n      <td>0.977734</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>1.936800</td>\n      <td>1.976074</td>\n      <td>0.387961</td>\n      <td>0.132508</td>\n      <td>0.977711</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 1000...\n[Checkpoint] Completed save for step 1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd5ae64f29e42e1b62a35cdf9438806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"807396d9fb354aeda389bec1339cec2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21e1dddf304448479fc36daf336e2006"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e630fdab80044ed6afb0162695c93233"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0e720ee9b3845f6952d7db622065341"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94a03f4151454656bac02eddf3aee285"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 2000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 3000...\n[Checkpoint] Completed save for step 3000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 4000...\n[Checkpoint] Completed save for step 4000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 5000...\n[Checkpoint] Completed save for step 5000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 6000...\n[Checkpoint] Completed save for step 6000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 7000...\n[Checkpoint] Completed save for step 7000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 8000...\n[Checkpoint] Completed save for step 8000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 9000...\n[Checkpoint] Completed save for step 9000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 10000...\n[Checkpoint] Completed save for step 10000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 11000...\n[Checkpoint] Completed save for step 11000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 12000...\n[Checkpoint] Completed save for step 12000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 13000...\n[Checkpoint] Completed save for step 13000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 14000...\n[Checkpoint] Completed save for step 14000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 15000...\n[Checkpoint] Completed save for step 15000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 16000...\n[Checkpoint] Completed save for step 16000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 17000...\n[Checkpoint] Completed save for step 17000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 18000...\n[Checkpoint] Completed save for step 18000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n[Checkpoint] Running evaluation at step 19000...\n[Checkpoint] Completed save for step 19000\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=19398, training_loss=2.0845783020076167, metrics={'train_runtime': 18350.2523, 'train_samples_per_second': 8.457, 'train_steps_per_second': 1.057, 'total_flos': 1002824888114736.0, 'train_loss': 2.0845783020076167, 'epoch': 2.9996133920977344})"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"### Save LoRA adapters & processor","metadata":{}},{"cell_type":"code","source":"test_hf  = Dataset.from_pandas(test_df)\nds = DatasetDict({\n    \"train\": train_hf,\n    \"validation\": val_hf,\n    \"test\": test_hf\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T19:49:32.891303Z","iopub.execute_input":"2025-05-14T19:49:32.891984Z","iopub.status.idle":"2025-05-14T19:49:32.911907Z","shell.execute_reply.started":"2025-05-14T19:49:32.891961Z","shell.execute_reply":"2025-05-14T19:49:32.911347Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# 1. Save the fine-tuned model and processor\nmodel.save_pretrained(finetune_outdir)\nprocessor.save_pretrained(finetune_outdir)\nprint(\"Fine-tuned adapters and processor saved.\")\n\n# 2. Evaluate on the Test Set explicitly\nprint(\"\\nRunning Final Test Evaluation...\")\ntest_metrics = trainer.evaluate(eval_dataset=ds[\"test\"])\n\n# 3. Save the test metrics (now separate from validation metrics)\ntest_metrics_path = os.path.join(finetune_outdir, \"vilt_vqa_metrics_test.csv\")\npd.DataFrame([test_metrics]).to_csv(test_metrics_path, index=False)\nprint(f\"Test evaluation completed and saved to {test_metrics_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T19:49:33.774261Z","iopub.execute_input":"2025-05-14T19:49:33.774797Z","iopub.status.idle":"2025-05-14T19:55:30.845434Z","shell.execute_reply.started":"2025-05-14T19:49:33.774776Z","shell.execute_reply":"2025-05-14T19:55:30.844823Z"}},"outputs":[{"name":"stdout","text":"Fine-tuned adapters and processor saved.\n\nRunning Final Test Evaluation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2156' max='2156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2156/2156 05:51]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Test evaluation completed and saved to /kaggle/working/vilt_lora_finetuned/vilt_vqa_metrics_test.csv\n","output_type":"stream"}],"execution_count":38}]}