{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9998453329208878,
  "eval_steps": 500,
  "global_step": 6465,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 1.3009932041168213,
      "learning_rate": 4.995358910891089e-05,
      "loss": 10.3056,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.019365668296814,
      "learning_rate": 4.990202145214521e-05,
      "loss": 10.0721,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 1.0332326889038086,
      "learning_rate": 4.985045379537954e-05,
      "loss": 9.8703,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 0.5590443015098572,
      "learning_rate": 4.979888613861386e-05,
      "loss": 9.6988,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 0.5259720683097839,
      "learning_rate": 4.974731848184819e-05,
      "loss": 9.5528,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 0.5181432962417603,
      "learning_rate": 4.969575082508251e-05,
      "loss": 9.4366,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 1.1109123229980469,
      "learning_rate": 4.964418316831683e-05,
      "loss": 9.3182,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.515997052192688,
      "learning_rate": 4.9592615511551154e-05,
      "loss": 9.2063,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.45821893215179443,
      "learning_rate": 4.954104785478548e-05,
      "loss": 9.0957,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.4271605312824249,
      "learning_rate": 4.94894801980198e-05,
      "loss": 8.982,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 0.4340112507343292,
      "learning_rate": 4.943791254125413e-05,
      "loss": 8.9189,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.4124821126461029,
      "learning_rate": 4.938634488448845e-05,
      "loss": 8.8428,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.3856285512447357,
      "learning_rate": 4.933477722772277e-05,
      "loss": 8.797,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.4301704466342926,
      "learning_rate": 4.92832095709571e-05,
      "loss": 8.7382,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.3532618582248688,
      "learning_rate": 4.923164191419142e-05,
      "loss": 8.6777,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.37639570236206055,
      "learning_rate": 4.918007425742574e-05,
      "loss": 8.6293,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.3652423322200775,
      "learning_rate": 4.912850660066007e-05,
      "loss": 8.6249,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.46085116267204285,
      "learning_rate": 4.907693894389439e-05,
      "loss": 8.5498,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.2962689995765686,
      "learning_rate": 4.902537128712871e-05,
      "loss": 8.5449,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 0.5797005295753479,
      "learning_rate": 4.897380363036304e-05,
      "loss": 8.5128,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 0.3115474283695221,
      "learning_rate": 4.892223597359736e-05,
      "loss": 8.4879,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.3486015796661377,
      "learning_rate": 4.887066831683168e-05,
      "loss": 8.4639,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 0.31876540184020996,
      "learning_rate": 4.881910066006601e-05,
      "loss": 8.4322,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 0.5555785298347473,
      "learning_rate": 4.876753300330033e-05,
      "loss": 8.4374,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 0.48277559876441956,
      "learning_rate": 4.871596534653465e-05,
      "loss": 8.4088,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 0.7364174723625183,
      "learning_rate": 4.866439768976898e-05,
      "loss": 8.3946,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.8368085622787476,
      "learning_rate": 4.86128300330033e-05,
      "loss": 8.3917,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 0.46453332901000977,
      "learning_rate": 4.8561262376237624e-05,
      "loss": 8.3751,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 0.3864535689353943,
      "learning_rate": 4.850969471947195e-05,
      "loss": 8.3395,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 0.34442272782325745,
      "learning_rate": 4.845812706270627e-05,
      "loss": 8.3452,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 1.1528264284133911,
      "learning_rate": 4.84065594059406e-05,
      "loss": 8.3294,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 1.0149487257003784,
      "learning_rate": 4.835499174917492e-05,
      "loss": 8.3085,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.5595150589942932,
      "learning_rate": 4.830342409240924e-05,
      "loss": 8.3096,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.4651087522506714,
      "learning_rate": 4.8251856435643564e-05,
      "loss": 8.3131,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 0.5581487417221069,
      "learning_rate": 4.820028877887789e-05,
      "loss": 8.2927,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 0.8516678810119629,
      "learning_rate": 4.814872112211221e-05,
      "loss": 8.291,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 0.7183502912521362,
      "learning_rate": 4.809715346534654e-05,
      "loss": 8.2851,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 0.32506847381591797,
      "learning_rate": 4.804558580858086e-05,
      "loss": 8.2861,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 0.28630584478378296,
      "learning_rate": 4.799401815181518e-05,
      "loss": 8.2699,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 0.56656414270401,
      "learning_rate": 4.7942450495049504e-05,
      "loss": 8.2745,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 0.7588796019554138,
      "learning_rate": 4.789088283828383e-05,
      "loss": 8.2531,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 1.2718682289123535,
      "learning_rate": 4.783931518151815e-05,
      "loss": 8.2688,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 1.284919261932373,
      "learning_rate": 4.778774752475248e-05,
      "loss": 8.2567,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 0.40990695357322693,
      "learning_rate": 4.77361798679868e-05,
      "loss": 8.2606,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 0.7192785143852234,
      "learning_rate": 4.768461221122112e-05,
      "loss": 8.2543,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 0.35034653544425964,
      "learning_rate": 4.7633044554455444e-05,
      "loss": 8.2649,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.6680278182029724,
      "learning_rate": 4.758147689768977e-05,
      "loss": 8.2398,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 0.38490045070648193,
      "learning_rate": 4.7529909240924094e-05,
      "loss": 8.2357,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 0.7239717245101929,
      "learning_rate": 4.747834158415842e-05,
      "loss": 8.2381,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.6756985187530518,
      "learning_rate": 4.742677392739274e-05,
      "loss": 8.2258,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 0.4035986661911011,
      "learning_rate": 4.7375206270627064e-05,
      "loss": 8.2271,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 0.520331621170044,
      "learning_rate": 4.7323638613861385e-05,
      "loss": 8.2498,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 0.7055450677871704,
      "learning_rate": 4.727207095709571e-05,
      "loss": 8.2434,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.36331188678741455,
      "learning_rate": 4.722050330033004e-05,
      "loss": 8.2475,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 0.2984350919723511,
      "learning_rate": 4.716893564356436e-05,
      "loss": 8.2409,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 0.5074440240859985,
      "learning_rate": 4.711736798679868e-05,
      "loss": 8.2405,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.42889076471328735,
      "learning_rate": 4.7065800330033004e-05,
      "loss": 8.2406,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 0.36390915513038635,
      "learning_rate": 4.7014232673267325e-05,
      "loss": 8.225,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 0.4536621868610382,
      "learning_rate": 4.696266501650165e-05,
      "loss": 8.2305,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 0.3870825171470642,
      "learning_rate": 4.691109735973598e-05,
      "loss": 8.2223,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 0.3985300064086914,
      "learning_rate": 4.68595297029703e-05,
      "loss": 8.2206,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 0.4517325460910797,
      "learning_rate": 4.680796204620462e-05,
      "loss": 8.228,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.8893312215805054,
      "learning_rate": 4.6756394389438944e-05,
      "loss": 8.2335,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 0.49252286553382874,
      "learning_rate": 4.6704826732673265e-05,
      "loss": 8.2187,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.7340846061706543,
      "learning_rate": 4.665325907590759e-05,
      "loss": 8.2145,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.6472063064575195,
      "learning_rate": 4.660169141914192e-05,
      "loss": 8.2288,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 0.7139408588409424,
      "learning_rate": 4.655012376237624e-05,
      "loss": 8.2247,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 1.226208209991455,
      "learning_rate": 4.6498556105610564e-05,
      "loss": 8.2241,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 0.8042271733283997,
      "learning_rate": 4.6446988448844885e-05,
      "loss": 8.231,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 0.7359929084777832,
      "learning_rate": 4.6395420792079206e-05,
      "loss": 8.2079,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 0.3724963068962097,
      "learning_rate": 4.6343853135313534e-05,
      "loss": 8.2172,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 0.42460396885871887,
      "learning_rate": 4.629228547854786e-05,
      "loss": 8.216,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 0.8663830757141113,
      "learning_rate": 4.624071782178218e-05,
      "loss": 8.2321,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 0.6251291632652283,
      "learning_rate": 4.6189150165016504e-05,
      "loss": 8.231,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 0.6948753595352173,
      "learning_rate": 4.6137582508250825e-05,
      "loss": 8.2115,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 0.3679603040218353,
      "learning_rate": 4.6086014851485146e-05,
      "loss": 8.2005,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 0.5393913984298706,
      "learning_rate": 4.6034447194719474e-05,
      "loss": 8.2015,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 0.7875891327857971,
      "learning_rate": 4.59828795379538e-05,
      "loss": 8.2218,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 1.0843207836151123,
      "learning_rate": 4.593131188118812e-05,
      "loss": 8.2303,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 0.3222905695438385,
      "learning_rate": 4.5879744224422444e-05,
      "loss": 8.2282,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 0.5755358338356018,
      "learning_rate": 4.5828176567656765e-05,
      "loss": 8.2057,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.8484311103820801,
      "learning_rate": 4.5776608910891086e-05,
      "loss": 8.217,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.4633706510066986,
      "learning_rate": 4.5725041254125414e-05,
      "loss": 8.2059,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 0.4890466332435608,
      "learning_rate": 4.567347359735974e-05,
      "loss": 8.2156,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 0.42491084337234497,
      "learning_rate": 4.562190594059406e-05,
      "loss": 8.2201,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.43018728494644165,
      "learning_rate": 4.557549504950495e-05,
      "loss": 8.2021,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.7018135190010071,
      "learning_rate": 4.5523927392739274e-05,
      "loss": 8.2171,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 0.665605366230011,
      "learning_rate": 4.54723597359736e-05,
      "loss": 8.1923,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 0.48980820178985596,
      "learning_rate": 4.542079207920792e-05,
      "loss": 8.1944,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 0.6668874621391296,
      "learning_rate": 4.5369224422442244e-05,
      "loss": 8.2138,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.5550077557563782,
      "learning_rate": 4.531765676567657e-05,
      "loss": 8.2192,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 0.5023906826972961,
      "learning_rate": 4.526608910891089e-05,
      "loss": 8.2206,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 0.5319275856018066,
      "learning_rate": 4.5214521452145214e-05,
      "loss": 8.2048,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 0.3944212794303894,
      "learning_rate": 4.516295379537954e-05,
      "loss": 8.2115,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 0.5342583060264587,
      "learning_rate": 4.5111386138613864e-05,
      "loss": 8.1995,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 0.6561295986175537,
      "learning_rate": 4.5059818481848185e-05,
      "loss": 8.1987,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 1.186203956604004,
      "learning_rate": 4.500825082508251e-05,
      "loss": 8.2186,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 0.6538520455360413,
      "learning_rate": 4.4956683168316834e-05,
      "loss": 8.2066,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.446123868227005,
      "learning_rate": 4.4905115511551155e-05,
      "loss": 8.2142,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 0.4712319076061249,
      "learning_rate": 4.485354785478548e-05,
      "loss": 8.2061,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 0.5094484686851501,
      "learning_rate": 4.4801980198019804e-05,
      "loss": 8.2073,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 0.42885255813598633,
      "learning_rate": 4.4750412541254125e-05,
      "loss": 8.1976,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 1.0348752737045288,
      "learning_rate": 4.469884488448845e-05,
      "loss": 8.2119,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 0.9918270707130432,
      "learning_rate": 4.4647277227722774e-05,
      "loss": 8.2072,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 0.3495631515979767,
      "learning_rate": 4.45957095709571e-05,
      "loss": 8.1961,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 0.8824278116226196,
      "learning_rate": 4.454414191419142e-05,
      "loss": 8.212,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 1.350749135017395,
      "learning_rate": 4.4492574257425744e-05,
      "loss": 8.2087,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 1.065460205078125,
      "learning_rate": 4.4441006600660065e-05,
      "loss": 8.1963,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.5962690711021423,
      "learning_rate": 4.438943894389439e-05,
      "loss": 8.2025,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.0668294429779053,
      "learning_rate": 4.4337871287128714e-05,
      "loss": 8.1933,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 0.7562452554702759,
      "learning_rate": 4.428630363036304e-05,
      "loss": 8.212,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 0.4806356132030487,
      "learning_rate": 4.423473597359736e-05,
      "loss": 8.2024,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 0.4920746088027954,
      "learning_rate": 4.4183168316831684e-05,
      "loss": 8.1892,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 0.3985002040863037,
      "learning_rate": 4.4131600660066006e-05,
      "loss": 8.1986,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 0.4058857262134552,
      "learning_rate": 4.4080033003300333e-05,
      "loss": 8.1775,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 0.33125564455986023,
      "learning_rate": 4.4028465346534655e-05,
      "loss": 8.1895,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 0.3962322175502777,
      "learning_rate": 4.397689768976898e-05,
      "loss": 8.1985,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 1.1557269096374512,
      "learning_rate": 4.3925330033003304e-05,
      "loss": 8.1937,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 1.2068843841552734,
      "learning_rate": 4.3873762376237625e-05,
      "loss": 8.1981,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 0.8781382441520691,
      "learning_rate": 4.3822194719471946e-05,
      "loss": 8.2084,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 0.7234132885932922,
      "learning_rate": 4.3770627062706274e-05,
      "loss": 8.1866,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 0.5751306414604187,
      "learning_rate": 4.37190594059406e-05,
      "loss": 8.2006,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 0.4369245171546936,
      "learning_rate": 4.366749174917492e-05,
      "loss": 8.1964,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 0.7975545525550842,
      "learning_rate": 4.3615924092409244e-05,
      "loss": 8.1926,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 0.4619746208190918,
      "learning_rate": 4.3564356435643565e-05,
      "loss": 8.1877,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 0.5739269852638245,
      "learning_rate": 4.3512788778877886e-05,
      "loss": 8.1869,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 0.8290409445762634,
      "learning_rate": 4.3461221122112214e-05,
      "loss": 8.1896,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 0.4183928668498993,
      "learning_rate": 4.340965346534654e-05,
      "loss": 8.1981,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 0.3161006569862366,
      "learning_rate": 4.335808580858086e-05,
      "loss": 8.1997,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 0.4819307327270508,
      "learning_rate": 4.3306518151815184e-05,
      "loss": 8.2041,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 0.7399295568466187,
      "learning_rate": 4.3254950495049505e-05,
      "loss": 8.1913,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 0.6845495700836182,
      "learning_rate": 4.3203382838283827e-05,
      "loss": 8.2072,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 0.7887065410614014,
      "learning_rate": 4.3151815181518154e-05,
      "loss": 8.1953,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 0.6726712584495544,
      "learning_rate": 4.310024752475248e-05,
      "loss": 8.1743,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 0.570452094078064,
      "learning_rate": 4.3048679867986803e-05,
      "loss": 8.1838,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 0.3671797811985016,
      "learning_rate": 4.2997112211221125e-05,
      "loss": 8.1824,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 0.6017147898674011,
      "learning_rate": 4.2945544554455446e-05,
      "loss": 8.174,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.42401963472366333,
      "learning_rate": 4.289397689768977e-05,
      "loss": 8.1852,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.3013074398040771,
      "learning_rate": 4.2842409240924095e-05,
      "loss": 8.2006,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 0.9495974183082581,
      "learning_rate": 4.279084158415842e-05,
      "loss": 8.1716,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 1.1011862754821777,
      "learning_rate": 4.2739273927392744e-05,
      "loss": 8.1992,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 1.3069820404052734,
      "learning_rate": 4.2687706270627065e-05,
      "loss": 8.1982,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 0.4910352826118469,
      "learning_rate": 4.2636138613861386e-05,
      "loss": 8.1955,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 0.8106139302253723,
      "learning_rate": 4.258457095709571e-05,
      "loss": 8.1949,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 1.029158353805542,
      "learning_rate": 4.2533003300330035e-05,
      "loss": 8.1945,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 0.4328593611717224,
      "learning_rate": 4.248143564356436e-05,
      "loss": 8.185,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 0.7992685437202454,
      "learning_rate": 4.2429867986798684e-05,
      "loss": 8.1881,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 0.8257903456687927,
      "learning_rate": 4.2378300330033005e-05,
      "loss": 8.1878,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 0.9643993973731995,
      "learning_rate": 4.2326732673267326e-05,
      "loss": 8.21,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 1.4459971189498901,
      "learning_rate": 4.227516501650165e-05,
      "loss": 8.1776,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 0.49560362100601196,
      "learning_rate": 4.2223597359735975e-05,
      "loss": 8.1931,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 0.8881869912147522,
      "learning_rate": 4.21720297029703e-05,
      "loss": 8.1994,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 0.7250415682792664,
      "learning_rate": 4.2120462046204624e-05,
      "loss": 8.1896,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 0.6604917645454407,
      "learning_rate": 4.2068894389438946e-05,
      "loss": 8.2024,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 0.4466758966445923,
      "learning_rate": 4.201732673267327e-05,
      "loss": 8.1743,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.4067390561103821,
      "learning_rate": 4.196575907590759e-05,
      "loss": 8.1636,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 0.4745461642742157,
      "learning_rate": 4.1914191419141916e-05,
      "loss": 8.2033,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 1.3716013431549072,
      "learning_rate": 4.1862623762376244e-05,
      "loss": 8.1836,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 0.5371493101119995,
      "learning_rate": 4.1811056105610565e-05,
      "loss": 8.1786,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 1.0465422868728638,
      "learning_rate": 4.1759488448844886e-05,
      "loss": 8.1905,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 0.6208813190460205,
      "learning_rate": 4.170792079207921e-05,
      "loss": 8.1862,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 1.3216849565505981,
      "learning_rate": 4.1656353135313535e-05,
      "loss": 8.1853,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 1.4150288105010986,
      "learning_rate": 4.1604785478547856e-05,
      "loss": 8.1642,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 0.740123987197876,
      "learning_rate": 4.1553217821782184e-05,
      "loss": 8.1925,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 0.321842759847641,
      "learning_rate": 4.1501650165016505e-05,
      "loss": 8.189,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 0.5137611031532288,
      "learning_rate": 4.1450082508250826e-05,
      "loss": 8.1815,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 0.4050547778606415,
      "learning_rate": 4.139851485148515e-05,
      "loss": 8.1908,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 0.48005586862564087,
      "learning_rate": 4.1346947194719475e-05,
      "loss": 8.1688,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 0.5765444040298462,
      "learning_rate": 4.1295379537953796e-05,
      "loss": 8.1763,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 1.0549389123916626,
      "learning_rate": 4.1243811881188124e-05,
      "loss": 8.1825,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 0.32078272104263306,
      "learning_rate": 4.1192244224422445e-05,
      "loss": 8.194,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 0.4894616901874542,
      "learning_rate": 4.1140676567656767e-05,
      "loss": 8.1844,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 0.9571215510368347,
      "learning_rate": 4.108910891089109e-05,
      "loss": 8.1774,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 0.40801799297332764,
      "learning_rate": 4.1037541254125416e-05,
      "loss": 8.1852,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 0.9235581755638123,
      "learning_rate": 4.098597359735974e-05,
      "loss": 8.1909,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 0.5904819965362549,
      "learning_rate": 4.0934405940594065e-05,
      "loss": 8.1966,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 1.1319146156311035,
      "learning_rate": 4.0882838283828386e-05,
      "loss": 8.1697,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 1.0986226797103882,
      "learning_rate": 4.083127062706271e-05,
      "loss": 8.1913,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 1.1655359268188477,
      "learning_rate": 4.077970297029703e-05,
      "loss": 8.188,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 0.7333695888519287,
      "learning_rate": 4.0728135313531356e-05,
      "loss": 8.1803,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 1.2834813594818115,
      "learning_rate": 4.067656765676568e-05,
      "loss": 8.1715,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 0.6900805830955505,
      "learning_rate": 4.0625000000000005e-05,
      "loss": 8.196,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 0.48984065651893616,
      "learning_rate": 4.0573432343234326e-05,
      "loss": 8.1861,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 0.37010177969932556,
      "learning_rate": 4.052186468646865e-05,
      "loss": 8.1808,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 0.6773363351821899,
      "learning_rate": 4.0470297029702975e-05,
      "loss": 8.1769,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 0.5194340944290161,
      "learning_rate": 4.0418729372937296e-05,
      "loss": 8.1952,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 0.4094236195087433,
      "learning_rate": 4.036716171617162e-05,
      "loss": 8.205,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 0.43897101283073425,
      "learning_rate": 4.0315594059405945e-05,
      "loss": 8.1825,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.35779088735580444,
      "learning_rate": 4.0264026402640266e-05,
      "loss": 8.1807,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 0.4676540195941925,
      "learning_rate": 4.021245874587459e-05,
      "loss": 8.1912,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 0.3767469823360443,
      "learning_rate": 4.0160891089108915e-05,
      "loss": 8.1846,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 0.46981385350227356,
      "learning_rate": 4.0109323432343237e-05,
      "loss": 8.1888,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.6941916942596436,
      "learning_rate": 4.005775577557756e-05,
      "loss": 8.1863,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 0.4166378974914551,
      "learning_rate": 4.0006188118811886e-05,
      "loss": 8.1911,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 0.4177178740501404,
      "learning_rate": 3.995462046204621e-05,
      "loss": 8.1772,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 0.555965006351471,
      "learning_rate": 3.990305280528053e-05,
      "loss": 8.1722,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 0.471902459859848,
      "learning_rate": 3.9851485148514856e-05,
      "loss": 8.1666,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 0.5693722367286682,
      "learning_rate": 3.979991749174918e-05,
      "loss": 8.1768,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 0.6509510278701782,
      "learning_rate": 3.97483498349835e-05,
      "loss": 8.1864,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 0.433541864156723,
      "learning_rate": 3.9696782178217826e-05,
      "loss": 8.1885,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.3207208216190338,
      "learning_rate": 3.964521452145215e-05,
      "loss": 8.182,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 0.5892543196678162,
      "learning_rate": 3.959364686468647e-05,
      "loss": 8.2003,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 0.5327925682067871,
      "learning_rate": 3.9542079207920796e-05,
      "loss": 8.1804,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 0.6577010154724121,
      "learning_rate": 3.949051155115512e-05,
      "loss": 8.1931,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 1.000326156616211,
      "learning_rate": 3.943894389438944e-05,
      "loss": 8.1874,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 1.3101497888565063,
      "learning_rate": 3.9387376237623766e-05,
      "loss": 8.1816,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 1.3121562004089355,
      "learning_rate": 3.933580858085809e-05,
      "loss": 8.1845,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 0.43563318252563477,
      "learning_rate": 3.9284240924092415e-05,
      "loss": 8.1998,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 0.8871325254440308,
      "learning_rate": 3.9232673267326736e-05,
      "loss": 8.1914,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 0.6624685525894165,
      "learning_rate": 3.918110561056106e-05,
      "loss": 8.1994,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 0.42900368571281433,
      "learning_rate": 3.912953795379538e-05,
      "loss": 8.1835,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 0.6270647644996643,
      "learning_rate": 3.9077970297029707e-05,
      "loss": 8.1779,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 0.7981778979301453,
      "learning_rate": 3.902640264026403e-05,
      "loss": 8.1917,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 0.9898968935012817,
      "learning_rate": 3.8974834983498356e-05,
      "loss": 8.1915,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 0.928290069103241,
      "learning_rate": 3.892326732673268e-05,
      "loss": 8.2009,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 0.8542389869689941,
      "learning_rate": 3.8871699669967e-05,
      "loss": 8.1756,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 0.7735894322395325,
      "learning_rate": 3.882013201320132e-05,
      "loss": 8.18,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 0.4191091060638428,
      "learning_rate": 3.876856435643565e-05,
      "loss": 8.1797,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 0.9207257628440857,
      "learning_rate": 3.871699669966997e-05,
      "loss": 8.1748,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 1.1481151580810547,
      "learning_rate": 3.8665429042904296e-05,
      "loss": 8.1922,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 1.202474594116211,
      "learning_rate": 3.861386138613862e-05,
      "loss": 8.1809,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 0.7902243733406067,
      "learning_rate": 3.856229372937294e-05,
      "loss": 8.1898,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 0.37848255038261414,
      "learning_rate": 3.851072607260726e-05,
      "loss": 8.1854,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 1.0079847574234009,
      "learning_rate": 3.845915841584159e-05,
      "loss": 8.1818,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.7204020023345947,
      "learning_rate": 3.8407590759075915e-05,
      "loss": 8.2035,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 1.0484107732772827,
      "learning_rate": 3.8356023102310236e-05,
      "loss": 8.1752,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 0.42498520016670227,
      "learning_rate": 3.830445544554456e-05,
      "loss": 8.163,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 0.9520195722579956,
      "learning_rate": 3.825288778877888e-05,
      "loss": 8.1722,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 0.28162506222724915,
      "learning_rate": 3.82013201320132e-05,
      "loss": 8.1809,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 0.3972020149230957,
      "learning_rate": 3.814975247524753e-05,
      "loss": 8.1824,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 0.34289854764938354,
      "learning_rate": 3.8098184818481855e-05,
      "loss": 8.17,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.40803593397140503,
      "learning_rate": 3.8046617161716176e-05,
      "loss": 8.1801,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 0.6479795575141907,
      "learning_rate": 3.79950495049505e-05,
      "loss": 8.1766,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 0.7867308259010315,
      "learning_rate": 3.794348184818482e-05,
      "loss": 8.1896,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 0.36888596415519714,
      "learning_rate": 3.789191419141914e-05,
      "loss": 8.1644,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 0.7949223518371582,
      "learning_rate": 3.784034653465347e-05,
      "loss": 8.1959,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 0.3875161111354828,
      "learning_rate": 3.7788778877887796e-05,
      "loss": 8.1813,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 0.4141738712787628,
      "learning_rate": 3.773721122112212e-05,
      "loss": 8.179,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 0.5525515079498291,
      "learning_rate": 3.768564356435644e-05,
      "loss": 8.1708,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 0.34574010968208313,
      "learning_rate": 3.763407590759076e-05,
      "loss": 8.1715,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 0.5597878098487854,
      "learning_rate": 3.758250825082508e-05,
      "loss": 8.1775,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 1.2105799913406372,
      "learning_rate": 3.753094059405941e-05,
      "loss": 8.1895,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 0.601966381072998,
      "learning_rate": 3.7479372937293736e-05,
      "loss": 8.1821,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 0.6268542408943176,
      "learning_rate": 3.742780528052806e-05,
      "loss": 8.1916,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 1.1334913969039917,
      "learning_rate": 3.737623762376238e-05,
      "loss": 8.1857,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.6157822012901306,
      "learning_rate": 3.73246699669967e-05,
      "loss": 8.1871,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.790818989276886,
      "learning_rate": 3.727310231023102e-05,
      "loss": 8.1589,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 0.7334421873092651,
      "learning_rate": 3.722153465346535e-05,
      "loss": 8.1772,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 0.5114063024520874,
      "learning_rate": 3.7169966996699676e-05,
      "loss": 8.1935,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 0.6797065138816833,
      "learning_rate": 3.7118399339934e-05,
      "loss": 8.1745,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.48067954182624817,
      "learning_rate": 3.706683168316832e-05,
      "loss": 8.1928,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 0.4066992402076721,
      "learning_rate": 3.701526402640264e-05,
      "loss": 8.1773,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 0.45492810010910034,
      "learning_rate": 3.696369636963696e-05,
      "loss": 8.1538,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 0.33668041229248047,
      "learning_rate": 3.691212871287129e-05,
      "loss": 8.1829,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 0.3758903741836548,
      "learning_rate": 3.686056105610562e-05,
      "loss": 8.1709,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 0.41328534483909607,
      "learning_rate": 3.680899339933994e-05,
      "loss": 8.1834,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 0.5180423855781555,
      "learning_rate": 3.675742574257426e-05,
      "loss": 8.1837,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 0.7521601915359497,
      "learning_rate": 3.670585808580858e-05,
      "loss": 8.1787,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 0.46507251262664795,
      "learning_rate": 3.66542904290429e-05,
      "loss": 8.1852,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 0.3645488917827606,
      "learning_rate": 3.660272277227723e-05,
      "loss": 8.1854,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 0.4965421259403229,
      "learning_rate": 3.655115511551156e-05,
      "loss": 8.1744,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 0.6513339281082153,
      "learning_rate": 3.649958745874588e-05,
      "loss": 8.1584,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 0.33310481905937195,
      "learning_rate": 3.64480198019802e-05,
      "loss": 8.1617,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 0.3757564127445221,
      "learning_rate": 3.639645214521452e-05,
      "loss": 8.1801,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 0.5331994295120239,
      "learning_rate": 3.634488448844885e-05,
      "loss": 8.1889,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 0.3922751545906067,
      "learning_rate": 3.629331683168317e-05,
      "loss": 8.1751,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 0.4730931222438812,
      "learning_rate": 3.62417491749175e-05,
      "loss": 8.1813,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 0.4514850378036499,
      "learning_rate": 3.619018151815182e-05,
      "loss": 8.1943,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 0.5767418146133423,
      "learning_rate": 3.613861386138614e-05,
      "loss": 8.1777,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 0.64090496301651,
      "learning_rate": 3.608704620462046e-05,
      "loss": 8.1714,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 0.43734458088874817,
      "learning_rate": 3.603547854785479e-05,
      "loss": 8.1789,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 0.3413854241371155,
      "learning_rate": 3.598391089108911e-05,
      "loss": 8.1979,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 0.7636270523071289,
      "learning_rate": 3.593234323432344e-05,
      "loss": 8.1873,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 0.5407769680023193,
      "learning_rate": 3.588077557755776e-05,
      "loss": 8.1716,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.35568028688430786,
      "learning_rate": 3.582920792079208e-05,
      "loss": 8.1872,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 0.7341594099998474,
      "learning_rate": 3.57776402640264e-05,
      "loss": 8.1924,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 0.5697532892227173,
      "learning_rate": 3.572607260726073e-05,
      "loss": 8.1862,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 0.6393995881080627,
      "learning_rate": 3.567450495049505e-05,
      "loss": 8.1788,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.552577555179596,
      "learning_rate": 3.562293729372938e-05,
      "loss": 8.1687,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.3974345326423645,
      "learning_rate": 3.55713696369637e-05,
      "loss": 8.193,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.6061276197433472,
      "learning_rate": 3.551980198019802e-05,
      "loss": 8.1665,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 0.6255852580070496,
      "learning_rate": 3.546823432343234e-05,
      "loss": 8.164,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 0.3569106161594391,
      "learning_rate": 3.541666666666667e-05,
      "loss": 8.2059,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.3049457371234894,
      "learning_rate": 3.536509900990099e-05,
      "loss": 8.179,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.5567184090614319,
      "learning_rate": 3.531353135313532e-05,
      "loss": 8.1835,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 0.8760479688644409,
      "learning_rate": 3.526196369636964e-05,
      "loss": 8.1761,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.5395624041557312,
      "learning_rate": 3.521039603960396e-05,
      "loss": 8.1782,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 0.4962463974952698,
      "learning_rate": 3.515882838283829e-05,
      "loss": 8.176,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 0.4070582091808319,
      "learning_rate": 3.510726072607261e-05,
      "loss": 8.1731,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 0.3263939917087555,
      "learning_rate": 3.505569306930693e-05,
      "loss": 8.185,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 0.7703434228897095,
      "learning_rate": 3.500412541254126e-05,
      "loss": 8.1909,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 0.60872882604599,
      "learning_rate": 3.495255775577558e-05,
      "loss": 8.186,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 0.7756921052932739,
      "learning_rate": 3.49009900990099e-05,
      "loss": 8.1853,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 0.7159290313720703,
      "learning_rate": 3.484942244224423e-05,
      "loss": 8.1698,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 0.3952070474624634,
      "learning_rate": 3.479785478547855e-05,
      "loss": 8.1684,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 0.7423319220542908,
      "learning_rate": 3.474628712871287e-05,
      "loss": 8.1689,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 0.4918156862258911,
      "learning_rate": 3.46947194719472e-05,
      "loss": 8.1752,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 0.4948141574859619,
      "learning_rate": 3.464315181518152e-05,
      "loss": 8.1736,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 0.6276217699050903,
      "learning_rate": 3.459158415841584e-05,
      "loss": 8.1877,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 0.7214821577072144,
      "learning_rate": 3.454001650165017e-05,
      "loss": 8.1863,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.5227526426315308,
      "learning_rate": 3.448844884488449e-05,
      "loss": 8.1552,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.6552598476409912,
      "learning_rate": 3.443688118811881e-05,
      "loss": 8.1795,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.44822707772254944,
      "learning_rate": 3.438531353135314e-05,
      "loss": 8.1756,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 0.32569724321365356,
      "learning_rate": 3.433374587458746e-05,
      "loss": 8.1698,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 0.6209218502044678,
      "learning_rate": 3.428217821782179e-05,
      "loss": 8.1872,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 0.7004386782646179,
      "learning_rate": 3.423061056105611e-05,
      "loss": 8.1873,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 0.48582473397254944,
      "learning_rate": 3.417904290429043e-05,
      "loss": 8.1896,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 0.3923216760158539,
      "learning_rate": 3.412747524752475e-05,
      "loss": 8.1885,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 0.8334612846374512,
      "learning_rate": 3.407590759075908e-05,
      "loss": 8.1643,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 0.4240705072879791,
      "learning_rate": 3.40243399339934e-05,
      "loss": 8.1774,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 0.5117320418357849,
      "learning_rate": 3.397277227722773e-05,
      "loss": 8.1846,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 0.6762744188308716,
      "learning_rate": 3.392120462046205e-05,
      "loss": 8.1671,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 0.8858093023300171,
      "learning_rate": 3.386963696369637e-05,
      "loss": 8.1788,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 0.6012534499168396,
      "learning_rate": 3.381806930693069e-05,
      "loss": 8.172,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 0.5546059608459473,
      "learning_rate": 3.376650165016502e-05,
      "loss": 8.1873,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.3856355547904968,
      "learning_rate": 3.371493399339934e-05,
      "loss": 8.1814,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 0.6124122142791748,
      "learning_rate": 3.366336633663367e-05,
      "loss": 8.1714,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 0.8042996525764465,
      "learning_rate": 3.361179867986799e-05,
      "loss": 8.1764,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.4244207441806793,
      "learning_rate": 3.356023102310231e-05,
      "loss": 8.1693,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 0.46416062116622925,
      "learning_rate": 3.350866336633663e-05,
      "loss": 8.172,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 0.4685104489326477,
      "learning_rate": 3.345709570957096e-05,
      "loss": 8.1758,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.3403065800666809,
      "learning_rate": 3.340552805280528e-05,
      "loss": 8.1723,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 0.41085103154182434,
      "learning_rate": 3.335396039603961e-05,
      "loss": 8.1656,
      "step": 3230
    },
    {
      "epoch": 1.002242672647127,
      "grad_norm": 0.8077384233474731,
      "learning_rate": 3.330239273927393e-05,
      "loss": 8.1684,
      "step": 3240
    },
    {
      "epoch": 1.0053360142293712,
      "grad_norm": 0.47328999638557434,
      "learning_rate": 3.325082508250825e-05,
      "loss": 8.1845,
      "step": 3250
    },
    {
      "epoch": 1.0084293558116155,
      "grad_norm": 0.48247796297073364,
      "learning_rate": 3.319925742574257e-05,
      "loss": 8.1642,
      "step": 3260
    },
    {
      "epoch": 1.0115226973938598,
      "grad_norm": 0.5337053537368774,
      "learning_rate": 3.31476897689769e-05,
      "loss": 8.1594,
      "step": 3270
    },
    {
      "epoch": 1.0146160389761039,
      "grad_norm": 0.4226565957069397,
      "learning_rate": 3.309612211221123e-05,
      "loss": 8.1666,
      "step": 3280
    },
    {
      "epoch": 1.0177093805583481,
      "grad_norm": 0.7151170969009399,
      "learning_rate": 3.304455445544555e-05,
      "loss": 8.1676,
      "step": 3290
    },
    {
      "epoch": 1.0208027221405924,
      "grad_norm": 0.36772558093070984,
      "learning_rate": 3.299298679867987e-05,
      "loss": 8.1845,
      "step": 3300
    },
    {
      "epoch": 1.0238960637228367,
      "grad_norm": 0.43837663531303406,
      "learning_rate": 3.294141914191419e-05,
      "loss": 8.1842,
      "step": 3310
    },
    {
      "epoch": 1.0269894053050808,
      "grad_norm": 0.5833343863487244,
      "learning_rate": 3.288985148514851e-05,
      "loss": 8.1823,
      "step": 3320
    },
    {
      "epoch": 1.030082746887325,
      "grad_norm": 0.701982319355011,
      "learning_rate": 3.283828382838284e-05,
      "loss": 8.1667,
      "step": 3330
    },
    {
      "epoch": 1.0331760884695693,
      "grad_norm": 0.33199334144592285,
      "learning_rate": 3.278671617161717e-05,
      "loss": 8.1814,
      "step": 3340
    },
    {
      "epoch": 1.0362694300518134,
      "grad_norm": 0.31569620966911316,
      "learning_rate": 3.273514851485149e-05,
      "loss": 8.1621,
      "step": 3350
    },
    {
      "epoch": 1.0393627716340577,
      "grad_norm": 0.45697635412216187,
      "learning_rate": 3.268358085808581e-05,
      "loss": 8.1781,
      "step": 3360
    },
    {
      "epoch": 1.042456113216302,
      "grad_norm": 0.4540783166885376,
      "learning_rate": 3.263201320132013e-05,
      "loss": 8.1814,
      "step": 3370
    },
    {
      "epoch": 1.0455494547985462,
      "grad_norm": 0.41054388880729675,
      "learning_rate": 3.258044554455445e-05,
      "loss": 8.1621,
      "step": 3380
    },
    {
      "epoch": 1.0486427963807903,
      "grad_norm": 0.3805939257144928,
      "learning_rate": 3.252887788778878e-05,
      "loss": 8.165,
      "step": 3390
    },
    {
      "epoch": 1.0517361379630346,
      "grad_norm": 0.37846052646636963,
      "learning_rate": 3.247731023102311e-05,
      "loss": 8.1768,
      "step": 3400
    },
    {
      "epoch": 1.0548294795452788,
      "grad_norm": 0.691589891910553,
      "learning_rate": 3.242574257425743e-05,
      "loss": 8.1723,
      "step": 3410
    },
    {
      "epoch": 1.057922821127523,
      "grad_norm": 0.3896739184856415,
      "learning_rate": 3.237417491749175e-05,
      "loss": 8.1602,
      "step": 3420
    },
    {
      "epoch": 1.0610161627097672,
      "grad_norm": 0.4507834315299988,
      "learning_rate": 3.232260726072607e-05,
      "loss": 8.1769,
      "step": 3430
    },
    {
      "epoch": 1.0641095042920115,
      "grad_norm": 0.4127067029476166,
      "learning_rate": 3.2271039603960394e-05,
      "loss": 8.1709,
      "step": 3440
    },
    {
      "epoch": 1.0672028458742557,
      "grad_norm": 0.42008861899375916,
      "learning_rate": 3.221947194719472e-05,
      "loss": 8.184,
      "step": 3450
    },
    {
      "epoch": 1.0702961874564998,
      "grad_norm": 0.6444163918495178,
      "learning_rate": 3.216790429042905e-05,
      "loss": 8.1869,
      "step": 3460
    },
    {
      "epoch": 1.073389529038744,
      "grad_norm": 0.5999700427055359,
      "learning_rate": 3.211633663366337e-05,
      "loss": 8.1684,
      "step": 3470
    },
    {
      "epoch": 1.0764828706209884,
      "grad_norm": 0.38917994499206543,
      "learning_rate": 3.206476897689769e-05,
      "loss": 8.1833,
      "step": 3480
    },
    {
      "epoch": 1.0795762122032326,
      "grad_norm": 0.4304012358188629,
      "learning_rate": 3.201320132013201e-05,
      "loss": 8.195,
      "step": 3490
    },
    {
      "epoch": 1.0826695537854767,
      "grad_norm": 0.4505245089530945,
      "learning_rate": 3.1961633663366334e-05,
      "loss": 8.1646,
      "step": 3500
    },
    {
      "epoch": 1.085762895367721,
      "grad_norm": 0.5217558741569519,
      "learning_rate": 3.191006600660066e-05,
      "loss": 8.164,
      "step": 3510
    },
    {
      "epoch": 1.0888562369499653,
      "grad_norm": 0.4980957806110382,
      "learning_rate": 3.185849834983499e-05,
      "loss": 8.1729,
      "step": 3520
    },
    {
      "epoch": 1.0919495785322093,
      "grad_norm": 0.43257832527160645,
      "learning_rate": 3.180693069306931e-05,
      "loss": 8.1872,
      "step": 3530
    },
    {
      "epoch": 1.0950429201144536,
      "grad_norm": 0.4261259138584137,
      "learning_rate": 3.175536303630363e-05,
      "loss": 8.1776,
      "step": 3540
    },
    {
      "epoch": 1.0981362616966979,
      "grad_norm": 0.3075449764728546,
      "learning_rate": 3.170379537953795e-05,
      "loss": 8.1821,
      "step": 3550
    },
    {
      "epoch": 1.1012296032789421,
      "grad_norm": 0.6443793177604675,
      "learning_rate": 3.1652227722772274e-05,
      "loss": 8.1658,
      "step": 3560
    },
    {
      "epoch": 1.1043229448611862,
      "grad_norm": 0.5471095442771912,
      "learning_rate": 3.16006600660066e-05,
      "loss": 8.1805,
      "step": 3570
    },
    {
      "epoch": 1.1074162864434305,
      "grad_norm": 0.4315832257270813,
      "learning_rate": 3.154909240924093e-05,
      "loss": 8.1674,
      "step": 3580
    },
    {
      "epoch": 1.1105096280256748,
      "grad_norm": 0.41847142577171326,
      "learning_rate": 3.149752475247525e-05,
      "loss": 8.1654,
      "step": 3590
    },
    {
      "epoch": 1.113602969607919,
      "grad_norm": 0.5079861283302307,
      "learning_rate": 3.144595709570957e-05,
      "loss": 8.18,
      "step": 3600
    },
    {
      "epoch": 1.116696311190163,
      "grad_norm": 0.5603570938110352,
      "learning_rate": 3.139438943894389e-05,
      "loss": 8.1774,
      "step": 3610
    },
    {
      "epoch": 1.1197896527724074,
      "grad_norm": 0.6182283163070679,
      "learning_rate": 3.1342821782178214e-05,
      "loss": 8.1701,
      "step": 3620
    },
    {
      "epoch": 1.1228829943546517,
      "grad_norm": 0.4772181212902069,
      "learning_rate": 3.129125412541254e-05,
      "loss": 8.1941,
      "step": 3630
    },
    {
      "epoch": 1.1259763359368957,
      "grad_norm": 0.4240725040435791,
      "learning_rate": 3.123968646864687e-05,
      "loss": 8.1715,
      "step": 3640
    },
    {
      "epoch": 1.12906967751914,
      "grad_norm": 0.36993467807769775,
      "learning_rate": 3.118811881188119e-05,
      "loss": 8.1794,
      "step": 3650
    },
    {
      "epoch": 1.1321630191013843,
      "grad_norm": 0.4038877487182617,
      "learning_rate": 3.113655115511551e-05,
      "loss": 8.1831,
      "step": 3660
    },
    {
      "epoch": 1.1352563606836286,
      "grad_norm": 0.5566087961196899,
      "learning_rate": 3.1084983498349834e-05,
      "loss": 8.1648,
      "step": 3670
    },
    {
      "epoch": 1.1383497022658726,
      "grad_norm": 0.32647904753685,
      "learning_rate": 3.103341584158416e-05,
      "loss": 8.1675,
      "step": 3680
    },
    {
      "epoch": 1.141443043848117,
      "grad_norm": 0.32486364245414734,
      "learning_rate": 3.098184818481848e-05,
      "loss": 8.1613,
      "step": 3690
    },
    {
      "epoch": 1.1445363854303612,
      "grad_norm": 0.49596232175827026,
      "learning_rate": 3.093028052805281e-05,
      "loss": 8.1721,
      "step": 3700
    },
    {
      "epoch": 1.1476297270126055,
      "grad_norm": 0.4125964045524597,
      "learning_rate": 3.087871287128713e-05,
      "loss": 8.159,
      "step": 3710
    },
    {
      "epoch": 1.1507230685948495,
      "grad_norm": 0.4010084867477417,
      "learning_rate": 3.082714521452145e-05,
      "loss": 8.171,
      "step": 3720
    },
    {
      "epoch": 1.1538164101770938,
      "grad_norm": 0.9045507907867432,
      "learning_rate": 3.0775577557755774e-05,
      "loss": 8.1665,
      "step": 3730
    },
    {
      "epoch": 1.156909751759338,
      "grad_norm": 0.5164192914962769,
      "learning_rate": 3.07240099009901e-05,
      "loss": 8.1813,
      "step": 3740
    },
    {
      "epoch": 1.1600030933415821,
      "grad_norm": 0.47241199016571045,
      "learning_rate": 3.067244224422442e-05,
      "loss": 8.1638,
      "step": 3750
    },
    {
      "epoch": 1.1630964349238264,
      "grad_norm": 0.33220043778419495,
      "learning_rate": 3.062087458745875e-05,
      "loss": 8.1773,
      "step": 3760
    },
    {
      "epoch": 1.1661897765060707,
      "grad_norm": 0.48330026865005493,
      "learning_rate": 3.056930693069307e-05,
      "loss": 8.1697,
      "step": 3770
    },
    {
      "epoch": 1.169283118088315,
      "grad_norm": 0.4737001359462738,
      "learning_rate": 3.051773927392739e-05,
      "loss": 8.1682,
      "step": 3780
    },
    {
      "epoch": 1.172376459670559,
      "grad_norm": 0.35414060950279236,
      "learning_rate": 3.0466171617161714e-05,
      "loss": 8.1791,
      "step": 3790
    },
    {
      "epoch": 1.1754698012528033,
      "grad_norm": 0.3959980607032776,
      "learning_rate": 3.0414603960396042e-05,
      "loss": 8.1838,
      "step": 3800
    },
    {
      "epoch": 1.1785631428350476,
      "grad_norm": 0.38094112277030945,
      "learning_rate": 3.0363036303630367e-05,
      "loss": 8.1761,
      "step": 3810
    },
    {
      "epoch": 1.1816564844172919,
      "grad_norm": 0.7425351738929749,
      "learning_rate": 3.0311468646864688e-05,
      "loss": 8.1783,
      "step": 3820
    },
    {
      "epoch": 1.184749825999536,
      "grad_norm": 0.5470761656761169,
      "learning_rate": 3.025990099009901e-05,
      "loss": 8.1574,
      "step": 3830
    },
    {
      "epoch": 1.1878431675817802,
      "grad_norm": 0.49377182126045227,
      "learning_rate": 3.0208333333333334e-05,
      "loss": 8.1774,
      "step": 3840
    },
    {
      "epoch": 1.1909365091640245,
      "grad_norm": 0.4960037171840668,
      "learning_rate": 3.015676567656766e-05,
      "loss": 8.1713,
      "step": 3850
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 0.6575823426246643,
      "learning_rate": 3.0105198019801983e-05,
      "loss": 8.1767,
      "step": 3860
    },
    {
      "epoch": 1.1971231923285128,
      "grad_norm": 0.8213852047920227,
      "learning_rate": 3.0053630363036307e-05,
      "loss": 8.1659,
      "step": 3870
    },
    {
      "epoch": 1.200216533910757,
      "grad_norm": 1.036637544631958,
      "learning_rate": 3.0002062706270628e-05,
      "loss": 8.1837,
      "step": 3880
    },
    {
      "epoch": 1.2033098754930014,
      "grad_norm": 0.8843628168106079,
      "learning_rate": 2.995049504950495e-05,
      "loss": 8.1673,
      "step": 3890
    },
    {
      "epoch": 1.2064032170752454,
      "grad_norm": 0.9577875137329102,
      "learning_rate": 2.9898927392739274e-05,
      "loss": 8.1701,
      "step": 3900
    },
    {
      "epoch": 1.2094965586574897,
      "grad_norm": 0.40400147438049316,
      "learning_rate": 2.9847359735973602e-05,
      "loss": 8.1718,
      "step": 3910
    },
    {
      "epoch": 1.212589900239734,
      "grad_norm": 0.46672436594963074,
      "learning_rate": 2.9795792079207923e-05,
      "loss": 8.1699,
      "step": 3920
    },
    {
      "epoch": 1.2156832418219783,
      "grad_norm": 0.42506060004234314,
      "learning_rate": 2.9744224422442247e-05,
      "loss": 8.1576,
      "step": 3930
    },
    {
      "epoch": 1.2187765834042223,
      "grad_norm": 0.35832977294921875,
      "learning_rate": 2.969265676567657e-05,
      "loss": 8.1839,
      "step": 3940
    },
    {
      "epoch": 1.2218699249864666,
      "grad_norm": 0.39353376626968384,
      "learning_rate": 2.964108910891089e-05,
      "loss": 8.1668,
      "step": 3950
    },
    {
      "epoch": 1.224963266568711,
      "grad_norm": 0.5979236364364624,
      "learning_rate": 2.9589521452145214e-05,
      "loss": 8.172,
      "step": 3960
    },
    {
      "epoch": 1.228056608150955,
      "grad_norm": 0.581454336643219,
      "learning_rate": 2.9537953795379542e-05,
      "loss": 8.1748,
      "step": 3970
    },
    {
      "epoch": 1.2311499497331992,
      "grad_norm": 0.5906539559364319,
      "learning_rate": 2.9486386138613863e-05,
      "loss": 8.1619,
      "step": 3980
    },
    {
      "epoch": 1.2342432913154435,
      "grad_norm": 0.3003133535385132,
      "learning_rate": 2.9434818481848188e-05,
      "loss": 8.1733,
      "step": 3990
    },
    {
      "epoch": 1.2373366328976878,
      "grad_norm": 0.5896344184875488,
      "learning_rate": 2.938325082508251e-05,
      "loss": 8.1766,
      "step": 4000
    },
    {
      "epoch": 1.2404299744799319,
      "grad_norm": 0.8053435683250427,
      "learning_rate": 2.933168316831683e-05,
      "loss": 8.1765,
      "step": 4010
    },
    {
      "epoch": 1.2435233160621761,
      "grad_norm": 0.48820751905441284,
      "learning_rate": 2.9280115511551154e-05,
      "loss": 8.172,
      "step": 4020
    },
    {
      "epoch": 1.2466166576444204,
      "grad_norm": 0.5509219765663147,
      "learning_rate": 2.9228547854785482e-05,
      "loss": 8.1737,
      "step": 4030
    },
    {
      "epoch": 1.2497099992266647,
      "grad_norm": 0.3664526045322418,
      "learning_rate": 2.9176980198019804e-05,
      "loss": 8.1731,
      "step": 4040
    },
    {
      "epoch": 1.2528033408089088,
      "grad_norm": 0.652195155620575,
      "learning_rate": 2.9125412541254128e-05,
      "loss": 8.1598,
      "step": 4050
    },
    {
      "epoch": 1.255896682391153,
      "grad_norm": 0.36292681097984314,
      "learning_rate": 2.907384488448845e-05,
      "loss": 8.1643,
      "step": 4060
    },
    {
      "epoch": 1.2589900239733973,
      "grad_norm": 0.5483609437942505,
      "learning_rate": 2.902227722772277e-05,
      "loss": 8.1609,
      "step": 4070
    },
    {
      "epoch": 1.2620833655556414,
      "grad_norm": 0.4539545476436615,
      "learning_rate": 2.8970709570957098e-05,
      "loss": 8.1936,
      "step": 4080
    },
    {
      "epoch": 1.2651767071378857,
      "grad_norm": 0.7176665663719177,
      "learning_rate": 2.8919141914191423e-05,
      "loss": 8.1885,
      "step": 4090
    },
    {
      "epoch": 1.26827004872013,
      "grad_norm": 0.60526043176651,
      "learning_rate": 2.8867574257425744e-05,
      "loss": 8.1625,
      "step": 4100
    },
    {
      "epoch": 1.2713633903023742,
      "grad_norm": 0.34080544114112854,
      "learning_rate": 2.881600660066007e-05,
      "loss": 8.1593,
      "step": 4110
    },
    {
      "epoch": 1.2744567318846183,
      "grad_norm": 0.4769887328147888,
      "learning_rate": 2.876443894389439e-05,
      "loss": 8.1542,
      "step": 4120
    },
    {
      "epoch": 1.2775500734668626,
      "grad_norm": 0.4628506600856781,
      "learning_rate": 2.871287128712871e-05,
      "loss": 8.1727,
      "step": 4130
    },
    {
      "epoch": 1.2806434150491068,
      "grad_norm": 0.5703310370445251,
      "learning_rate": 2.866130363036304e-05,
      "loss": 8.1746,
      "step": 4140
    },
    {
      "epoch": 1.2837367566313511,
      "grad_norm": 0.6357194781303406,
      "learning_rate": 2.8609735973597363e-05,
      "loss": 8.1739,
      "step": 4150
    },
    {
      "epoch": 1.2868300982135952,
      "grad_norm": 0.5700566172599792,
      "learning_rate": 2.8558168316831684e-05,
      "loss": 8.1792,
      "step": 4160
    },
    {
      "epoch": 1.2899234397958395,
      "grad_norm": 0.4581761062145233,
      "learning_rate": 2.850660066006601e-05,
      "loss": 8.1815,
      "step": 4170
    },
    {
      "epoch": 1.2930167813780837,
      "grad_norm": 0.5910048484802246,
      "learning_rate": 2.845503300330033e-05,
      "loss": 8.1753,
      "step": 4180
    },
    {
      "epoch": 1.2961101229603278,
      "grad_norm": 0.4556485712528229,
      "learning_rate": 2.840346534653465e-05,
      "loss": 8.1627,
      "step": 4190
    },
    {
      "epoch": 1.299203464542572,
      "grad_norm": 0.520637571811676,
      "learning_rate": 2.835189768976898e-05,
      "loss": 8.1652,
      "step": 4200
    },
    {
      "epoch": 1.3022968061248164,
      "grad_norm": 0.7932301163673401,
      "learning_rate": 2.8300330033003303e-05,
      "loss": 8.1753,
      "step": 4210
    },
    {
      "epoch": 1.3053901477070606,
      "grad_norm": 0.510329008102417,
      "learning_rate": 2.8248762376237624e-05,
      "loss": 8.1795,
      "step": 4220
    },
    {
      "epoch": 1.3084834892893047,
      "grad_norm": 0.5799217820167542,
      "learning_rate": 2.819719471947195e-05,
      "loss": 8.1672,
      "step": 4230
    },
    {
      "epoch": 1.311576830871549,
      "grad_norm": 0.42929333448410034,
      "learning_rate": 2.814562706270627e-05,
      "loss": 8.172,
      "step": 4240
    },
    {
      "epoch": 1.3146701724537933,
      "grad_norm": 0.47014281153678894,
      "learning_rate": 2.8094059405940598e-05,
      "loss": 8.1672,
      "step": 4250
    },
    {
      "epoch": 1.3177635140360375,
      "grad_norm": 0.5401324033737183,
      "learning_rate": 2.804249174917492e-05,
      "loss": 8.1576,
      "step": 4260
    },
    {
      "epoch": 1.3208568556182816,
      "grad_norm": 0.6086838245391846,
      "learning_rate": 2.7990924092409244e-05,
      "loss": 8.1733,
      "step": 4270
    },
    {
      "epoch": 1.3239501972005259,
      "grad_norm": 0.523765504360199,
      "learning_rate": 2.7939356435643565e-05,
      "loss": 8.1664,
      "step": 4280
    },
    {
      "epoch": 1.3270435387827701,
      "grad_norm": 0.4795650839805603,
      "learning_rate": 2.788778877887789e-05,
      "loss": 8.1759,
      "step": 4290
    },
    {
      "epoch": 1.3301368803650142,
      "grad_norm": 0.41069427132606506,
      "learning_rate": 2.783622112211221e-05,
      "loss": 8.1645,
      "step": 4300
    },
    {
      "epoch": 1.3332302219472585,
      "grad_norm": 0.7105136513710022,
      "learning_rate": 2.778465346534654e-05,
      "loss": 8.1533,
      "step": 4310
    },
    {
      "epoch": 1.3363235635295028,
      "grad_norm": 0.3954717814922333,
      "learning_rate": 2.773308580858086e-05,
      "loss": 8.1714,
      "step": 4320
    },
    {
      "epoch": 1.339416905111747,
      "grad_norm": 0.40309587121009827,
      "learning_rate": 2.7681518151815184e-05,
      "loss": 8.1758,
      "step": 4330
    },
    {
      "epoch": 1.342510246693991,
      "grad_norm": 0.5005411505699158,
      "learning_rate": 2.7629950495049505e-05,
      "loss": 8.1647,
      "step": 4340
    },
    {
      "epoch": 1.3456035882762354,
      "grad_norm": 0.5643422603607178,
      "learning_rate": 2.757838283828383e-05,
      "loss": 8.1692,
      "step": 4350
    },
    {
      "epoch": 1.3486969298584797,
      "grad_norm": 0.3686229884624481,
      "learning_rate": 2.752681518151815e-05,
      "loss": 8.1782,
      "step": 4360
    },
    {
      "epoch": 1.351790271440724,
      "grad_norm": 0.39017194509506226,
      "learning_rate": 2.747524752475248e-05,
      "loss": 8.1767,
      "step": 4370
    },
    {
      "epoch": 1.354883613022968,
      "grad_norm": 1.9713059663772583,
      "learning_rate": 2.74236798679868e-05,
      "loss": 8.1811,
      "step": 4380
    },
    {
      "epoch": 1.3579769546052123,
      "grad_norm": 0.9738501310348511,
      "learning_rate": 2.7372112211221124e-05,
      "loss": 8.1208,
      "step": 4390
    },
    {
      "epoch": 1.3610702961874566,
      "grad_norm": 0.4568113684654236,
      "learning_rate": 2.7320544554455445e-05,
      "loss": 8.111,
      "step": 4400
    },
    {
      "epoch": 1.3641636377697006,
      "grad_norm": 0.38847458362579346,
      "learning_rate": 2.726897689768977e-05,
      "loss": 8.1151,
      "step": 4410
    },
    {
      "epoch": 1.367256979351945,
      "grad_norm": 0.5352944731712341,
      "learning_rate": 2.721740924092409e-05,
      "loss": 8.1261,
      "step": 4420
    },
    {
      "epoch": 1.3703503209341892,
      "grad_norm": 0.5872432589530945,
      "learning_rate": 2.716584158415842e-05,
      "loss": 8.1348,
      "step": 4430
    },
    {
      "epoch": 1.3734436625164332,
      "grad_norm": 0.4071456789970398,
      "learning_rate": 2.711427392739274e-05,
      "loss": 8.1184,
      "step": 4440
    },
    {
      "epoch": 1.3765370040986775,
      "grad_norm": 0.7157632112503052,
      "learning_rate": 2.7062706270627065e-05,
      "loss": 8.1109,
      "step": 4450
    },
    {
      "epoch": 1.3796303456809218,
      "grad_norm": 0.4722878634929657,
      "learning_rate": 2.7011138613861386e-05,
      "loss": 8.1209,
      "step": 4460
    },
    {
      "epoch": 1.382723687263166,
      "grad_norm": 0.8248410224914551,
      "learning_rate": 2.695957095709571e-05,
      "loss": 8.1181,
      "step": 4470
    },
    {
      "epoch": 1.3858170288454104,
      "grad_norm": 0.4289897084236145,
      "learning_rate": 2.6908003300330038e-05,
      "loss": 8.1126,
      "step": 4480
    },
    {
      "epoch": 1.3889103704276544,
      "grad_norm": 0.7103523015975952,
      "learning_rate": 2.685643564356436e-05,
      "loss": 8.118,
      "step": 4490
    },
    {
      "epoch": 1.3920037120098987,
      "grad_norm": 0.8009370565414429,
      "learning_rate": 2.680486798679868e-05,
      "loss": 8.1052,
      "step": 4500
    },
    {
      "epoch": 1.395097053592143,
      "grad_norm": 0.7330935001373291,
      "learning_rate": 2.6753300330033005e-05,
      "loss": 8.1128,
      "step": 4510
    },
    {
      "epoch": 1.398190395174387,
      "grad_norm": 0.37300264835357666,
      "learning_rate": 2.6701732673267326e-05,
      "loss": 8.1088,
      "step": 4520
    },
    {
      "epoch": 1.4012837367566313,
      "grad_norm": 0.8975840210914612,
      "learning_rate": 2.665016501650165e-05,
      "loss": 8.1295,
      "step": 4530
    },
    {
      "epoch": 1.4043770783388756,
      "grad_norm": 0.9350273013114929,
      "learning_rate": 2.659859735973598e-05,
      "loss": 8.1304,
      "step": 4540
    },
    {
      "epoch": 1.4074704199211197,
      "grad_norm": 0.506894588470459,
      "learning_rate": 2.65470297029703e-05,
      "loss": 8.1072,
      "step": 4550
    },
    {
      "epoch": 1.410563761503364,
      "grad_norm": 0.5046225786209106,
      "learning_rate": 2.649546204620462e-05,
      "loss": 8.127,
      "step": 4560
    },
    {
      "epoch": 1.4136571030856082,
      "grad_norm": 0.5286322236061096,
      "learning_rate": 2.6443894389438945e-05,
      "loss": 8.1294,
      "step": 4570
    },
    {
      "epoch": 1.4167504446678525,
      "grad_norm": 0.4554574489593506,
      "learning_rate": 2.6392326732673266e-05,
      "loss": 8.1147,
      "step": 4580
    },
    {
      "epoch": 1.4198437862500968,
      "grad_norm": 0.7168501615524292,
      "learning_rate": 2.634075907590759e-05,
      "loss": 8.1035,
      "step": 4590
    },
    {
      "epoch": 1.4229371278323408,
      "grad_norm": 0.4060167670249939,
      "learning_rate": 2.628919141914192e-05,
      "loss": 8.099,
      "step": 4600
    },
    {
      "epoch": 1.4260304694145851,
      "grad_norm": 0.643561840057373,
      "learning_rate": 2.623762376237624e-05,
      "loss": 8.1079,
      "step": 4610
    },
    {
      "epoch": 1.4291238109968294,
      "grad_norm": 1.5026196241378784,
      "learning_rate": 2.618605610561056e-05,
      "loss": 8.1154,
      "step": 4620
    },
    {
      "epoch": 1.4322171525790734,
      "grad_norm": 0.5195328593254089,
      "learning_rate": 2.6134488448844886e-05,
      "loss": 8.122,
      "step": 4630
    },
    {
      "epoch": 1.4353104941613177,
      "grad_norm": 0.38001498579978943,
      "learning_rate": 2.6082920792079207e-05,
      "loss": 8.1203,
      "step": 4640
    },
    {
      "epoch": 1.438403835743562,
      "grad_norm": 0.3870541751384735,
      "learning_rate": 2.6031353135313535e-05,
      "loss": 8.122,
      "step": 4650
    },
    {
      "epoch": 1.441497177325806,
      "grad_norm": 0.7285405993461609,
      "learning_rate": 2.597978547854786e-05,
      "loss": 8.1125,
      "step": 4660
    },
    {
      "epoch": 1.4445905189080503,
      "grad_norm": 0.44492441415786743,
      "learning_rate": 2.593337458745875e-05,
      "loss": 8.1232,
      "step": 4670
    },
    {
      "epoch": 1.4476838604902946,
      "grad_norm": 0.4594872295856476,
      "learning_rate": 2.588180693069307e-05,
      "loss": 8.1198,
      "step": 4680
    },
    {
      "epoch": 1.450777202072539,
      "grad_norm": 0.4203779101371765,
      "learning_rate": 2.583023927392739e-05,
      "loss": 8.1086,
      "step": 4690
    },
    {
      "epoch": 1.4538705436547832,
      "grad_norm": 0.5538591742515564,
      "learning_rate": 2.5778671617161716e-05,
      "loss": 8.1195,
      "step": 4700
    },
    {
      "epoch": 1.4569638852370272,
      "grad_norm": 0.4415692985057831,
      "learning_rate": 2.5727103960396043e-05,
      "loss": 8.1188,
      "step": 4710
    },
    {
      "epoch": 1.4600572268192715,
      "grad_norm": 0.48841434717178345,
      "learning_rate": 2.5675536303630365e-05,
      "loss": 8.1225,
      "step": 4720
    },
    {
      "epoch": 1.4631505684015158,
      "grad_norm": 0.6934054493904114,
      "learning_rate": 2.562396864686469e-05,
      "loss": 8.1173,
      "step": 4730
    },
    {
      "epoch": 1.4662439099837599,
      "grad_norm": 0.4883940815925598,
      "learning_rate": 2.557240099009901e-05,
      "loss": 8.1149,
      "step": 4740
    },
    {
      "epoch": 1.4693372515660041,
      "grad_norm": 0.9327614903450012,
      "learning_rate": 2.552083333333333e-05,
      "loss": 8.1145,
      "step": 4750
    },
    {
      "epoch": 1.4724305931482484,
      "grad_norm": 0.6940579414367676,
      "learning_rate": 2.546926567656766e-05,
      "loss": 8.1204,
      "step": 4760
    },
    {
      "epoch": 1.4755239347304925,
      "grad_norm": 0.7890014052391052,
      "learning_rate": 2.5417698019801984e-05,
      "loss": 8.1187,
      "step": 4770
    },
    {
      "epoch": 1.4786172763127368,
      "grad_norm": 1.0575751066207886,
      "learning_rate": 2.5366130363036305e-05,
      "loss": 8.1249,
      "step": 4780
    },
    {
      "epoch": 1.481710617894981,
      "grad_norm": 0.510903000831604,
      "learning_rate": 2.531456270627063e-05,
      "loss": 8.1192,
      "step": 4790
    },
    {
      "epoch": 1.4848039594772253,
      "grad_norm": 0.5671697854995728,
      "learning_rate": 2.526299504950495e-05,
      "loss": 8.1035,
      "step": 4800
    },
    {
      "epoch": 1.4878973010594696,
      "grad_norm": 0.4479960799217224,
      "learning_rate": 2.5211427392739272e-05,
      "loss": 8.1172,
      "step": 4810
    },
    {
      "epoch": 1.4909906426417137,
      "grad_norm": 0.7128333449363708,
      "learning_rate": 2.51598597359736e-05,
      "loss": 8.1147,
      "step": 4820
    },
    {
      "epoch": 1.494083984223958,
      "grad_norm": 0.6942179203033447,
      "learning_rate": 2.5108292079207924e-05,
      "loss": 8.1268,
      "step": 4830
    },
    {
      "epoch": 1.4971773258062022,
      "grad_norm": 0.6661107540130615,
      "learning_rate": 2.5056724422442245e-05,
      "loss": 8.1095,
      "step": 4840
    },
    {
      "epoch": 1.5002706673884463,
      "grad_norm": 0.460071861743927,
      "learning_rate": 2.500515676567657e-05,
      "loss": 8.1038,
      "step": 4850
    },
    {
      "epoch": 1.5033640089706906,
      "grad_norm": 0.42526617646217346,
      "learning_rate": 2.4953589108910894e-05,
      "loss": 8.1043,
      "step": 4860
    },
    {
      "epoch": 1.5064573505529348,
      "grad_norm": 0.35367125272750854,
      "learning_rate": 2.4902021452145215e-05,
      "loss": 8.1151,
      "step": 4870
    },
    {
      "epoch": 1.509550692135179,
      "grad_norm": 0.4556598365306854,
      "learning_rate": 2.485045379537954e-05,
      "loss": 8.1196,
      "step": 4880
    },
    {
      "epoch": 1.5126440337174234,
      "grad_norm": 0.35864385962486267,
      "learning_rate": 2.4798886138613864e-05,
      "loss": 8.1168,
      "step": 4890
    },
    {
      "epoch": 1.5157373752996675,
      "grad_norm": 0.4563215374946594,
      "learning_rate": 2.4747318481848186e-05,
      "loss": 8.1205,
      "step": 4900
    },
    {
      "epoch": 1.5188307168819117,
      "grad_norm": 0.3567292094230652,
      "learning_rate": 2.469575082508251e-05,
      "loss": 8.1109,
      "step": 4910
    },
    {
      "epoch": 1.521924058464156,
      "grad_norm": 0.547868013381958,
      "learning_rate": 2.4644183168316835e-05,
      "loss": 8.1202,
      "step": 4920
    },
    {
      "epoch": 1.5250174000464,
      "grad_norm": 0.3618165850639343,
      "learning_rate": 2.4592615511551156e-05,
      "loss": 8.1003,
      "step": 4930
    },
    {
      "epoch": 1.5281107416286444,
      "grad_norm": 0.5820549726486206,
      "learning_rate": 2.454104785478548e-05,
      "loss": 8.1237,
      "step": 4940
    },
    {
      "epoch": 1.5312040832108886,
      "grad_norm": 0.3476707637310028,
      "learning_rate": 2.4489480198019805e-05,
      "loss": 8.1149,
      "step": 4950
    },
    {
      "epoch": 1.5342974247931327,
      "grad_norm": 0.3444109261035919,
      "learning_rate": 2.4437912541254126e-05,
      "loss": 8.1276,
      "step": 4960
    },
    {
      "epoch": 1.537390766375377,
      "grad_norm": 0.5051685571670532,
      "learning_rate": 2.438634488448845e-05,
      "loss": 8.1171,
      "step": 4970
    },
    {
      "epoch": 1.5404841079576213,
      "grad_norm": 0.31833210587501526,
      "learning_rate": 2.4334777227722775e-05,
      "loss": 8.1234,
      "step": 4980
    },
    {
      "epoch": 1.5435774495398653,
      "grad_norm": 0.5242730379104614,
      "learning_rate": 2.4283209570957096e-05,
      "loss": 8.1187,
      "step": 4990
    },
    {
      "epoch": 1.5466707911221098,
      "grad_norm": 0.3639770448207855,
      "learning_rate": 2.423164191419142e-05,
      "loss": 8.1051,
      "step": 5000
    },
    {
      "epoch": 1.5497641327043539,
      "grad_norm": 0.5012937188148499,
      "learning_rate": 2.4180074257425745e-05,
      "loss": 8.1266,
      "step": 5010
    },
    {
      "epoch": 1.5528574742865981,
      "grad_norm": 0.566142201423645,
      "learning_rate": 2.4128506600660066e-05,
      "loss": 8.0985,
      "step": 5020
    },
    {
      "epoch": 1.5559508158688424,
      "grad_norm": 0.4376172423362732,
      "learning_rate": 2.407693894389439e-05,
      "loss": 8.1166,
      "step": 5030
    },
    {
      "epoch": 1.5590441574510865,
      "grad_norm": 0.4253859519958496,
      "learning_rate": 2.4025371287128715e-05,
      "loss": 8.1095,
      "step": 5040
    },
    {
      "epoch": 1.5621374990333308,
      "grad_norm": 0.5426956415176392,
      "learning_rate": 2.3973803630363036e-05,
      "loss": 8.1045,
      "step": 5050
    },
    {
      "epoch": 1.565230840615575,
      "grad_norm": 0.4816742539405823,
      "learning_rate": 2.392223597359736e-05,
      "loss": 8.128,
      "step": 5060
    },
    {
      "epoch": 1.568324182197819,
      "grad_norm": 0.7413421869277954,
      "learning_rate": 2.3870668316831685e-05,
      "loss": 8.096,
      "step": 5070
    },
    {
      "epoch": 1.5714175237800634,
      "grad_norm": 0.4691101014614105,
      "learning_rate": 2.3819100660066007e-05,
      "loss": 8.115,
      "step": 5080
    },
    {
      "epoch": 1.5745108653623077,
      "grad_norm": 0.352197527885437,
      "learning_rate": 2.376753300330033e-05,
      "loss": 8.1231,
      "step": 5090
    },
    {
      "epoch": 1.5776042069445517,
      "grad_norm": 0.4411577880382538,
      "learning_rate": 2.3715965346534656e-05,
      "loss": 8.122,
      "step": 5100
    },
    {
      "epoch": 1.5806975485267962,
      "grad_norm": 0.6064733266830444,
      "learning_rate": 2.3664397689768977e-05,
      "loss": 8.124,
      "step": 5110
    },
    {
      "epoch": 1.5837908901090403,
      "grad_norm": 0.408006876707077,
      "learning_rate": 2.36128300330033e-05,
      "loss": 8.1087,
      "step": 5120
    },
    {
      "epoch": 1.5868842316912846,
      "grad_norm": 0.32962915301322937,
      "learning_rate": 2.3561262376237626e-05,
      "loss": 8.1074,
      "step": 5130
    },
    {
      "epoch": 1.5899775732735288,
      "grad_norm": 0.3202911913394928,
      "learning_rate": 2.3509694719471947e-05,
      "loss": 8.115,
      "step": 5140
    },
    {
      "epoch": 1.593070914855773,
      "grad_norm": 0.4971058666706085,
      "learning_rate": 2.345812706270627e-05,
      "loss": 8.1295,
      "step": 5150
    },
    {
      "epoch": 1.5961642564380172,
      "grad_norm": 0.5086793303489685,
      "learning_rate": 2.3406559405940596e-05,
      "loss": 8.1132,
      "step": 5160
    },
    {
      "epoch": 1.5992575980202615,
      "grad_norm": 0.32906556129455566,
      "learning_rate": 2.3354991749174917e-05,
      "loss": 8.103,
      "step": 5170
    },
    {
      "epoch": 1.6023509396025055,
      "grad_norm": 0.4223060607910156,
      "learning_rate": 2.330342409240924e-05,
      "loss": 8.1228,
      "step": 5180
    },
    {
      "epoch": 1.6054442811847498,
      "grad_norm": 0.6751744151115417,
      "learning_rate": 2.3251856435643566e-05,
      "loss": 8.1154,
      "step": 5190
    },
    {
      "epoch": 1.608537622766994,
      "grad_norm": 0.39166387915611267,
      "learning_rate": 2.3200288778877887e-05,
      "loss": 8.1114,
      "step": 5200
    },
    {
      "epoch": 1.6116309643492381,
      "grad_norm": 0.42642271518707275,
      "learning_rate": 2.314872112211221e-05,
      "loss": 8.1214,
      "step": 5210
    },
    {
      "epoch": 1.6147243059314826,
      "grad_norm": 0.4147818088531494,
      "learning_rate": 2.3097153465346536e-05,
      "loss": 8.1,
      "step": 5220
    },
    {
      "epoch": 1.6178176475137267,
      "grad_norm": 0.503262996673584,
      "learning_rate": 2.3045585808580857e-05,
      "loss": 8.1227,
      "step": 5230
    },
    {
      "epoch": 1.620910989095971,
      "grad_norm": 0.48748865723609924,
      "learning_rate": 2.2994018151815182e-05,
      "loss": 8.1297,
      "step": 5240
    },
    {
      "epoch": 1.6240043306782153,
      "grad_norm": 0.5096047520637512,
      "learning_rate": 2.2942450495049506e-05,
      "loss": 8.1196,
      "step": 5250
    },
    {
      "epoch": 1.6270976722604593,
      "grad_norm": 0.4548799991607666,
      "learning_rate": 2.289088283828383e-05,
      "loss": 8.1201,
      "step": 5260
    },
    {
      "epoch": 1.6301910138427036,
      "grad_norm": 0.368429958820343,
      "learning_rate": 2.2839315181518152e-05,
      "loss": 8.1179,
      "step": 5270
    },
    {
      "epoch": 1.6332843554249479,
      "grad_norm": 0.6217860579490662,
      "learning_rate": 2.2787747524752477e-05,
      "loss": 8.1352,
      "step": 5280
    },
    {
      "epoch": 1.636377697007192,
      "grad_norm": 0.5135611891746521,
      "learning_rate": 2.27361798679868e-05,
      "loss": 8.108,
      "step": 5290
    },
    {
      "epoch": 1.6394710385894362,
      "grad_norm": 0.3393915593624115,
      "learning_rate": 2.2684612211221122e-05,
      "loss": 8.1147,
      "step": 5300
    },
    {
      "epoch": 1.6425643801716805,
      "grad_norm": 0.5064442753791809,
      "learning_rate": 2.2633044554455447e-05,
      "loss": 8.1132,
      "step": 5310
    },
    {
      "epoch": 1.6456577217539246,
      "grad_norm": 0.36635997891426086,
      "learning_rate": 2.258147689768977e-05,
      "loss": 8.1104,
      "step": 5320
    },
    {
      "epoch": 1.648751063336169,
      "grad_norm": 0.36276236176490784,
      "learning_rate": 2.2529909240924092e-05,
      "loss": 8.1008,
      "step": 5330
    },
    {
      "epoch": 1.6518444049184131,
      "grad_norm": 0.40175968408584595,
      "learning_rate": 2.2478341584158417e-05,
      "loss": 8.1081,
      "step": 5340
    },
    {
      "epoch": 1.6549377465006572,
      "grad_norm": 0.6302782893180847,
      "learning_rate": 2.242677392739274e-05,
      "loss": 8.1139,
      "step": 5350
    },
    {
      "epoch": 1.6580310880829017,
      "grad_norm": 0.4285002648830414,
      "learning_rate": 2.2375206270627062e-05,
      "loss": 8.1078,
      "step": 5360
    },
    {
      "epoch": 1.6611244296651457,
      "grad_norm": 0.4462586045265198,
      "learning_rate": 2.2323638613861387e-05,
      "loss": 8.1204,
      "step": 5370
    },
    {
      "epoch": 1.66421777124739,
      "grad_norm": 0.610527753829956,
      "learning_rate": 2.227207095709571e-05,
      "loss": 8.1234,
      "step": 5380
    },
    {
      "epoch": 1.6673111128296343,
      "grad_norm": 0.45398131012916565,
      "learning_rate": 2.2220503300330033e-05,
      "loss": 8.111,
      "step": 5390
    },
    {
      "epoch": 1.6704044544118783,
      "grad_norm": 0.40342089533805847,
      "learning_rate": 2.2168935643564357e-05,
      "loss": 8.1225,
      "step": 5400
    },
    {
      "epoch": 1.6734977959941226,
      "grad_norm": 0.43942001461982727,
      "learning_rate": 2.211736798679868e-05,
      "loss": 8.102,
      "step": 5410
    },
    {
      "epoch": 1.676591137576367,
      "grad_norm": 0.5081157088279724,
      "learning_rate": 2.2065800330033003e-05,
      "loss": 8.0904,
      "step": 5420
    },
    {
      "epoch": 1.679684479158611,
      "grad_norm": 0.47084468603134155,
      "learning_rate": 2.2014232673267327e-05,
      "loss": 8.0966,
      "step": 5430
    },
    {
      "epoch": 1.6827778207408555,
      "grad_norm": 0.4527207016944885,
      "learning_rate": 2.1962665016501652e-05,
      "loss": 8.1235,
      "step": 5440
    },
    {
      "epoch": 1.6858711623230995,
      "grad_norm": 0.7687830328941345,
      "learning_rate": 2.1911097359735973e-05,
      "loss": 8.1252,
      "step": 5450
    },
    {
      "epoch": 1.6889645039053436,
      "grad_norm": 0.4144943356513977,
      "learning_rate": 2.18595297029703e-05,
      "loss": 8.1204,
      "step": 5460
    },
    {
      "epoch": 1.692057845487588,
      "grad_norm": 0.4927746653556824,
      "learning_rate": 2.1807962046204622e-05,
      "loss": 8.1248,
      "step": 5470
    },
    {
      "epoch": 1.6951511870698321,
      "grad_norm": 0.3922014832496643,
      "learning_rate": 2.1756394389438943e-05,
      "loss": 8.1229,
      "step": 5480
    },
    {
      "epoch": 1.6982445286520764,
      "grad_norm": 0.6617907285690308,
      "learning_rate": 2.170482673267327e-05,
      "loss": 8.1225,
      "step": 5490
    },
    {
      "epoch": 1.7013378702343207,
      "grad_norm": 0.48782527446746826,
      "learning_rate": 2.1653259075907592e-05,
      "loss": 8.1063,
      "step": 5500
    },
    {
      "epoch": 1.7044312118165648,
      "grad_norm": 0.49336710572242737,
      "learning_rate": 2.1601691419141913e-05,
      "loss": 8.1083,
      "step": 5510
    },
    {
      "epoch": 1.707524553398809,
      "grad_norm": 0.41019004583358765,
      "learning_rate": 2.155012376237624e-05,
      "loss": 8.1285,
      "step": 5520
    },
    {
      "epoch": 1.7106178949810533,
      "grad_norm": 0.5004312992095947,
      "learning_rate": 2.1498556105610562e-05,
      "loss": 8.1054,
      "step": 5530
    },
    {
      "epoch": 1.7137112365632974,
      "grad_norm": 0.3607425391674042,
      "learning_rate": 2.1446988448844883e-05,
      "loss": 8.1118,
      "step": 5540
    },
    {
      "epoch": 1.7168045781455419,
      "grad_norm": 0.7158089876174927,
      "learning_rate": 2.139542079207921e-05,
      "loss": 8.1184,
      "step": 5550
    },
    {
      "epoch": 1.719897919727786,
      "grad_norm": 0.5857347249984741,
      "learning_rate": 2.1343853135313532e-05,
      "loss": 8.1087,
      "step": 5560
    },
    {
      "epoch": 1.72299126131003,
      "grad_norm": 0.5431003570556641,
      "learning_rate": 2.1292285478547854e-05,
      "loss": 8.1187,
      "step": 5570
    },
    {
      "epoch": 1.7260846028922745,
      "grad_norm": 0.7388668060302734,
      "learning_rate": 2.124071782178218e-05,
      "loss": 8.118,
      "step": 5580
    },
    {
      "epoch": 1.7291779444745186,
      "grad_norm": 0.5115098357200623,
      "learning_rate": 2.1189150165016503e-05,
      "loss": 8.1076,
      "step": 5590
    },
    {
      "epoch": 1.7322712860567628,
      "grad_norm": 0.3870777189731598,
      "learning_rate": 2.1137582508250824e-05,
      "loss": 8.1324,
      "step": 5600
    },
    {
      "epoch": 1.7353646276390071,
      "grad_norm": 0.417095422744751,
      "learning_rate": 2.108601485148515e-05,
      "loss": 8.1147,
      "step": 5610
    },
    {
      "epoch": 1.7384579692212512,
      "grad_norm": 0.3836570382118225,
      "learning_rate": 2.1034447194719473e-05,
      "loss": 8.1208,
      "step": 5620
    },
    {
      "epoch": 1.7415513108034955,
      "grad_norm": 0.35352763533592224,
      "learning_rate": 2.0988036303630366e-05,
      "loss": 8.1216,
      "step": 5630
    },
    {
      "epoch": 1.7446446523857397,
      "grad_norm": 0.4356909692287445,
      "learning_rate": 2.0936468646864687e-05,
      "loss": 8.1247,
      "step": 5640
    },
    {
      "epoch": 1.7477379939679838,
      "grad_norm": 0.6618097424507141,
      "learning_rate": 2.088490099009901e-05,
      "loss": 8.1133,
      "step": 5650
    },
    {
      "epoch": 1.7508313355502283,
      "grad_norm": 0.521744966506958,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 8.1182,
      "step": 5660
    },
    {
      "epoch": 1.7539246771324724,
      "grad_norm": 0.4473540484905243,
      "learning_rate": 2.0781765676567657e-05,
      "loss": 8.1258,
      "step": 5670
    },
    {
      "epoch": 1.7570180187147164,
      "grad_norm": 0.4443567097187042,
      "learning_rate": 2.073019801980198e-05,
      "loss": 8.128,
      "step": 5680
    },
    {
      "epoch": 1.760111360296961,
      "grad_norm": 0.3990270793437958,
      "learning_rate": 2.0678630363036306e-05,
      "loss": 8.1192,
      "step": 5690
    },
    {
      "epoch": 1.763204701879205,
      "grad_norm": 0.670301616191864,
      "learning_rate": 2.0627062706270627e-05,
      "loss": 8.1132,
      "step": 5700
    },
    {
      "epoch": 1.7662980434614493,
      "grad_norm": 0.3839118182659149,
      "learning_rate": 2.0575495049504952e-05,
      "loss": 8.1256,
      "step": 5710
    },
    {
      "epoch": 1.7693913850436935,
      "grad_norm": 0.6317704319953918,
      "learning_rate": 2.0523927392739276e-05,
      "loss": 8.1177,
      "step": 5720
    },
    {
      "epoch": 1.7724847266259376,
      "grad_norm": 0.40839871764183044,
      "learning_rate": 2.0472359735973597e-05,
      "loss": 8.1081,
      "step": 5730
    },
    {
      "epoch": 1.7755780682081819,
      "grad_norm": 0.3464721739292145,
      "learning_rate": 2.0420792079207922e-05,
      "loss": 8.1192,
      "step": 5740
    },
    {
      "epoch": 1.7786714097904262,
      "grad_norm": 0.3923332393169403,
      "learning_rate": 2.0369224422442247e-05,
      "loss": 8.1022,
      "step": 5750
    },
    {
      "epoch": 1.7817647513726702,
      "grad_norm": 0.5052372217178345,
      "learning_rate": 2.0317656765676568e-05,
      "loss": 8.1076,
      "step": 5760
    },
    {
      "epoch": 1.7848580929549147,
      "grad_norm": 0.3557232916355133,
      "learning_rate": 2.0266089108910892e-05,
      "loss": 8.1173,
      "step": 5770
    },
    {
      "epoch": 1.7879514345371588,
      "grad_norm": 0.42618995904922485,
      "learning_rate": 2.0214521452145217e-05,
      "loss": 8.112,
      "step": 5780
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 0.4043114483356476,
      "learning_rate": 2.0162953795379538e-05,
      "loss": 8.1146,
      "step": 5790
    },
    {
      "epoch": 1.7941381177016473,
      "grad_norm": 0.46768563985824585,
      "learning_rate": 2.0111386138613862e-05,
      "loss": 8.1121,
      "step": 5800
    },
    {
      "epoch": 1.7972314592838914,
      "grad_norm": 0.3837772607803345,
      "learning_rate": 2.0059818481848187e-05,
      "loss": 8.1148,
      "step": 5810
    },
    {
      "epoch": 1.8003248008661357,
      "grad_norm": 0.3837936818599701,
      "learning_rate": 2.0008250825082508e-05,
      "loss": 8.1142,
      "step": 5820
    },
    {
      "epoch": 1.80341814244838,
      "grad_norm": 0.5082842707633972,
      "learning_rate": 1.9956683168316832e-05,
      "loss": 8.1075,
      "step": 5830
    },
    {
      "epoch": 1.806511484030624,
      "grad_norm": 0.32727035880088806,
      "learning_rate": 1.9905115511551157e-05,
      "loss": 8.1033,
      "step": 5840
    },
    {
      "epoch": 1.8096048256128683,
      "grad_norm": 0.34475359320640564,
      "learning_rate": 1.9853547854785478e-05,
      "loss": 8.1126,
      "step": 5850
    },
    {
      "epoch": 1.8126981671951126,
      "grad_norm": 0.45125138759613037,
      "learning_rate": 1.9801980198019803e-05,
      "loss": 8.1237,
      "step": 5860
    },
    {
      "epoch": 1.8157915087773566,
      "grad_norm": 0.7546172142028809,
      "learning_rate": 1.9750412541254127e-05,
      "loss": 8.1001,
      "step": 5870
    },
    {
      "epoch": 1.8188848503596011,
      "grad_norm": 0.7404887080192566,
      "learning_rate": 1.9698844884488448e-05,
      "loss": 8.1251,
      "step": 5880
    },
    {
      "epoch": 1.8219781919418452,
      "grad_norm": 0.6113086342811584,
      "learning_rate": 1.9647277227722773e-05,
      "loss": 8.1219,
      "step": 5890
    },
    {
      "epoch": 1.8250715335240892,
      "grad_norm": 0.5428985357284546,
      "learning_rate": 1.9595709570957097e-05,
      "loss": 8.104,
      "step": 5900
    },
    {
      "epoch": 1.8281648751063337,
      "grad_norm": 0.3699812591075897,
      "learning_rate": 1.954414191419142e-05,
      "loss": 8.1009,
      "step": 5910
    },
    {
      "epoch": 1.8312582166885778,
      "grad_norm": 0.7716334462165833,
      "learning_rate": 1.9492574257425743e-05,
      "loss": 8.1149,
      "step": 5920
    },
    {
      "epoch": 1.834351558270822,
      "grad_norm": 0.4650796949863434,
      "learning_rate": 1.9441006600660067e-05,
      "loss": 8.1132,
      "step": 5930
    },
    {
      "epoch": 1.8374448998530664,
      "grad_norm": 0.4350375235080719,
      "learning_rate": 1.938943894389439e-05,
      "loss": 8.1218,
      "step": 5940
    },
    {
      "epoch": 1.8405382414353104,
      "grad_norm": 0.4420663118362427,
      "learning_rate": 1.9337871287128713e-05,
      "loss": 8.1243,
      "step": 5950
    },
    {
      "epoch": 1.8436315830175547,
      "grad_norm": 0.35412466526031494,
      "learning_rate": 1.9286303630363038e-05,
      "loss": 8.1168,
      "step": 5960
    },
    {
      "epoch": 1.846724924599799,
      "grad_norm": 0.37266162037849426,
      "learning_rate": 1.9234735973597362e-05,
      "loss": 8.1163,
      "step": 5970
    },
    {
      "epoch": 1.849818266182043,
      "grad_norm": 0.4346396028995514,
      "learning_rate": 1.9183168316831683e-05,
      "loss": 8.1027,
      "step": 5980
    },
    {
      "epoch": 1.8529116077642873,
      "grad_norm": 0.3100256621837616,
      "learning_rate": 1.9131600660066008e-05,
      "loss": 8.1096,
      "step": 5990
    },
    {
      "epoch": 1.8560049493465316,
      "grad_norm": 0.5860375165939331,
      "learning_rate": 1.9080033003300332e-05,
      "loss": 8.101,
      "step": 6000
    },
    {
      "epoch": 1.8590982909287757,
      "grad_norm": 0.3119116425514221,
      "learning_rate": 1.9028465346534653e-05,
      "loss": 8.1253,
      "step": 6010
    },
    {
      "epoch": 1.8621916325110202,
      "grad_norm": 0.3956184387207031,
      "learning_rate": 1.8976897689768978e-05,
      "loss": 8.1128,
      "step": 6020
    },
    {
      "epoch": 1.8652849740932642,
      "grad_norm": 0.32189658284187317,
      "learning_rate": 1.8925330033003302e-05,
      "loss": 8.1154,
      "step": 6030
    },
    {
      "epoch": 1.8683783156755085,
      "grad_norm": 0.39368435740470886,
      "learning_rate": 1.8873762376237624e-05,
      "loss": 8.125,
      "step": 6040
    },
    {
      "epoch": 1.8714716572577528,
      "grad_norm": 0.42683467268943787,
      "learning_rate": 1.8822194719471948e-05,
      "loss": 8.1035,
      "step": 6050
    },
    {
      "epoch": 1.8745649988399968,
      "grad_norm": 0.39157307147979736,
      "learning_rate": 1.8770627062706273e-05,
      "loss": 8.1208,
      "step": 6060
    },
    {
      "epoch": 1.8776583404222411,
      "grad_norm": 0.46734827756881714,
      "learning_rate": 1.8719059405940594e-05,
      "loss": 8.1216,
      "step": 6070
    },
    {
      "epoch": 1.8807516820044854,
      "grad_norm": 0.49228182435035706,
      "learning_rate": 1.8667491749174918e-05,
      "loss": 8.1219,
      "step": 6080
    },
    {
      "epoch": 1.8838450235867295,
      "grad_norm": 0.2626582682132721,
      "learning_rate": 1.8615924092409243e-05,
      "loss": 8.1075,
      "step": 6090
    },
    {
      "epoch": 1.8869383651689737,
      "grad_norm": 0.8039379119873047,
      "learning_rate": 1.8564356435643564e-05,
      "loss": 8.0958,
      "step": 6100
    },
    {
      "epoch": 1.890031706751218,
      "grad_norm": 0.3967077434062958,
      "learning_rate": 1.851278877887789e-05,
      "loss": 8.1083,
      "step": 6110
    },
    {
      "epoch": 1.893125048333462,
      "grad_norm": 0.3581739664077759,
      "learning_rate": 1.8461221122112213e-05,
      "loss": 8.12,
      "step": 6120
    },
    {
      "epoch": 1.8962183899157066,
      "grad_norm": 0.3399197459220886,
      "learning_rate": 1.8409653465346534e-05,
      "loss": 8.1008,
      "step": 6130
    },
    {
      "epoch": 1.8993117314979506,
      "grad_norm": 0.4718426465988159,
      "learning_rate": 1.835808580858086e-05,
      "loss": 8.1169,
      "step": 6140
    },
    {
      "epoch": 1.902405073080195,
      "grad_norm": 0.36328548192977905,
      "learning_rate": 1.8306518151815183e-05,
      "loss": 8.1185,
      "step": 6150
    },
    {
      "epoch": 1.9054984146624392,
      "grad_norm": 0.4111133813858032,
      "learning_rate": 1.8254950495049504e-05,
      "loss": 8.1005,
      "step": 6160
    },
    {
      "epoch": 1.9085917562446832,
      "grad_norm": 0.38403478264808655,
      "learning_rate": 1.8203382838283832e-05,
      "loss": 8.1061,
      "step": 6170
    },
    {
      "epoch": 1.9116850978269275,
      "grad_norm": 0.5054445266723633,
      "learning_rate": 1.8151815181518153e-05,
      "loss": 8.1199,
      "step": 6180
    },
    {
      "epoch": 1.9147784394091718,
      "grad_norm": 0.43937262892723083,
      "learning_rate": 1.8100247524752474e-05,
      "loss": 8.1179,
      "step": 6190
    },
    {
      "epoch": 1.9178717809914159,
      "grad_norm": 0.3548332154750824,
      "learning_rate": 1.8048679867986802e-05,
      "loss": 8.1249,
      "step": 6200
    },
    {
      "epoch": 1.9209651225736601,
      "grad_norm": 0.4970510005950928,
      "learning_rate": 1.7997112211221123e-05,
      "loss": 8.1116,
      "step": 6210
    },
    {
      "epoch": 1.9240584641559044,
      "grad_norm": 0.44124549627304077,
      "learning_rate": 1.7945544554455445e-05,
      "loss": 8.1035,
      "step": 6220
    },
    {
      "epoch": 1.9271518057381485,
      "grad_norm": 0.5985711812973022,
      "learning_rate": 1.7893976897689772e-05,
      "loss": 8.116,
      "step": 6230
    },
    {
      "epoch": 1.930245147320393,
      "grad_norm": 0.38676923513412476,
      "learning_rate": 1.7842409240924094e-05,
      "loss": 8.1261,
      "step": 6240
    },
    {
      "epoch": 1.933338488902637,
      "grad_norm": 0.4493061602115631,
      "learning_rate": 1.7790841584158415e-05,
      "loss": 8.0923,
      "step": 6250
    },
    {
      "epoch": 1.9364318304848813,
      "grad_norm": 0.33343610167503357,
      "learning_rate": 1.7739273927392743e-05,
      "loss": 8.1159,
      "step": 6260
    },
    {
      "epoch": 1.9395251720671256,
      "grad_norm": 0.35489287972450256,
      "learning_rate": 1.7687706270627064e-05,
      "loss": 8.1128,
      "step": 6270
    },
    {
      "epoch": 1.9426185136493697,
      "grad_norm": 0.51179438829422,
      "learning_rate": 1.7636138613861385e-05,
      "loss": 8.1278,
      "step": 6280
    },
    {
      "epoch": 1.945711855231614,
      "grad_norm": 0.4336332082748413,
      "learning_rate": 1.7584570957095713e-05,
      "loss": 8.1091,
      "step": 6290
    },
    {
      "epoch": 1.9488051968138582,
      "grad_norm": 0.547326385974884,
      "learning_rate": 1.7533003300330034e-05,
      "loss": 8.1126,
      "step": 6300
    },
    {
      "epoch": 1.9518985383961023,
      "grad_norm": 0.5584889650344849,
      "learning_rate": 1.7481435643564355e-05,
      "loss": 8.115,
      "step": 6310
    },
    {
      "epoch": 1.9549918799783466,
      "grad_norm": 0.33563968539237976,
      "learning_rate": 1.7429867986798683e-05,
      "loss": 8.1008,
      "step": 6320
    },
    {
      "epoch": 1.9580852215605908,
      "grad_norm": 0.5520078539848328,
      "learning_rate": 1.7378300330033004e-05,
      "loss": 8.102,
      "step": 6330
    },
    {
      "epoch": 1.961178563142835,
      "grad_norm": 0.30427733063697815,
      "learning_rate": 1.7326732673267325e-05,
      "loss": 8.1207,
      "step": 6340
    },
    {
      "epoch": 1.9642719047250794,
      "grad_norm": 0.45437195897102356,
      "learning_rate": 1.7275165016501653e-05,
      "loss": 8.1118,
      "step": 6350
    },
    {
      "epoch": 1.9673652463073235,
      "grad_norm": 0.3050922155380249,
      "learning_rate": 1.7223597359735974e-05,
      "loss": 8.1134,
      "step": 6360
    },
    {
      "epoch": 1.9704585878895677,
      "grad_norm": 0.34114310145378113,
      "learning_rate": 1.71720297029703e-05,
      "loss": 8.1211,
      "step": 6370
    },
    {
      "epoch": 1.973551929471812,
      "grad_norm": 0.5685058832168579,
      "learning_rate": 1.7120462046204623e-05,
      "loss": 8.1235,
      "step": 6380
    },
    {
      "epoch": 1.976645271054056,
      "grad_norm": 0.39351707696914673,
      "learning_rate": 1.7068894389438944e-05,
      "loss": 8.1115,
      "step": 6390
    },
    {
      "epoch": 1.9797386126363004,
      "grad_norm": 0.4771071672439575,
      "learning_rate": 1.701732673267327e-05,
      "loss": 8.104,
      "step": 6400
    },
    {
      "epoch": 1.9828319542185446,
      "grad_norm": 0.4005500376224518,
      "learning_rate": 1.6965759075907593e-05,
      "loss": 8.1258,
      "step": 6410
    },
    {
      "epoch": 1.9859252958007887,
      "grad_norm": 0.39303603768348694,
      "learning_rate": 1.6914191419141915e-05,
      "loss": 8.1135,
      "step": 6420
    },
    {
      "epoch": 1.989018637383033,
      "grad_norm": 0.3002622127532959,
      "learning_rate": 1.686262376237624e-05,
      "loss": 8.1179,
      "step": 6430
    },
    {
      "epoch": 1.9921119789652773,
      "grad_norm": 0.36989733576774597,
      "learning_rate": 1.6811056105610564e-05,
      "loss": 8.1142,
      "step": 6440
    },
    {
      "epoch": 1.9952053205475213,
      "grad_norm": 0.47630220651626587,
      "learning_rate": 1.6759488448844885e-05,
      "loss": 8.121,
      "step": 6450
    },
    {
      "epoch": 1.9982986621297658,
      "grad_norm": 0.4460473954677582,
      "learning_rate": 1.670792079207921e-05,
      "loss": 8.1086,
      "step": 6460
    }
  ],
  "logging_steps": 10,
  "max_steps": 9696,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "total_flos": 2.696154044333261e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
