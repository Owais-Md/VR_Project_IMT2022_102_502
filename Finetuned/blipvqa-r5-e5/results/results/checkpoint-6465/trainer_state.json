{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9998453329208878,
  "eval_steps": 500,
  "global_step": 6465,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 2.283881425857544,
      "learning_rate": 4.997524752475248e-05,
      "loss": 10.3057,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.7015577554702759,
      "learning_rate": 4.9944306930693075e-05,
      "loss": 10.0835,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 1.8396601676940918,
      "learning_rate": 4.9913366336633664e-05,
      "loss": 9.8731,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 1.0070555210113525,
      "learning_rate": 4.988242574257426e-05,
      "loss": 9.6963,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 1.0936119556427002,
      "learning_rate": 4.9851485148514855e-05,
      "loss": 9.5427,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 0.9463545083999634,
      "learning_rate": 4.982054455445545e-05,
      "loss": 9.4026,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 1.0548697710037231,
      "learning_rate": 4.978960396039604e-05,
      "loss": 9.2535,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.846869170665741,
      "learning_rate": 4.975866336633664e-05,
      "loss": 9.1537,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.7546383142471313,
      "learning_rate": 4.972772277227723e-05,
      "loss": 9.0376,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.7686005234718323,
      "learning_rate": 4.9696782178217825e-05,
      "loss": 8.9299,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 0.7528889775276184,
      "learning_rate": 4.966584158415842e-05,
      "loss": 8.869,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.7256690859794617,
      "learning_rate": 4.963799504950496e-05,
      "loss": 8.7791,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.6986427903175354,
      "learning_rate": 4.9607054455445546e-05,
      "loss": 8.733,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.6804739236831665,
      "learning_rate": 4.957611386138614e-05,
      "loss": 8.6765,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.7127686142921448,
      "learning_rate": 4.9545173267326736e-05,
      "loss": 8.6174,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.6702990531921387,
      "learning_rate": 4.951423267326733e-05,
      "loss": 8.5714,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.7780759334564209,
      "learning_rate": 4.948329207920792e-05,
      "loss": 8.5676,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.7366856932640076,
      "learning_rate": 4.9452351485148516e-05,
      "loss": 8.497,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.7136861681938171,
      "learning_rate": 4.942141089108911e-05,
      "loss": 8.4928,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 1.917746663093567,
      "learning_rate": 4.9390470297029706e-05,
      "loss": 8.4637,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 1.148116111755371,
      "learning_rate": 4.9359529702970295e-05,
      "loss": 8.4424,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.5712763667106628,
      "learning_rate": 4.93285891089109e-05,
      "loss": 8.4241,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 0.6743816137313843,
      "learning_rate": 4.9297648514851486e-05,
      "loss": 8.3973,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 0.6497937440872192,
      "learning_rate": 4.926670792079208e-05,
      "loss": 8.4058,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 1.2516216039657593,
      "learning_rate": 4.9235767326732677e-05,
      "loss": 8.3807,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 1.2762138843536377,
      "learning_rate": 4.920482673267327e-05,
      "loss": 8.3691,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.6043147444725037,
      "learning_rate": 4.917388613861386e-05,
      "loss": 8.367,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 1.278912901878357,
      "learning_rate": 4.9142945544554456e-05,
      "loss": 8.3501,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 1.0998435020446777,
      "learning_rate": 4.911200495049505e-05,
      "loss": 8.3126,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 1.0249993801116943,
      "learning_rate": 4.908106435643565e-05,
      "loss": 8.3197,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 1.9351379871368408,
      "learning_rate": 4.9050123762376235e-05,
      "loss": 8.307,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 0.9796373844146729,
      "learning_rate": 4.901918316831684e-05,
      "loss": 8.2893,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.837062656879425,
      "learning_rate": 4.8988242574257426e-05,
      "loss": 8.2921,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.7073545455932617,
      "learning_rate": 4.895730198019802e-05,
      "loss": 8.2982,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 1.0989118814468384,
      "learning_rate": 4.892636138613862e-05,
      "loss": 8.2793,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 1.3417258262634277,
      "learning_rate": 4.889542079207921e-05,
      "loss": 8.2782,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 1.1105343103408813,
      "learning_rate": 4.88644801980198e-05,
      "loss": 8.2753,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 1.1419422626495361,
      "learning_rate": 4.8833539603960396e-05,
      "loss": 8.2766,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 1.4925583600997925,
      "learning_rate": 4.880259900990099e-05,
      "loss": 8.2618,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 1.426761507987976,
      "learning_rate": 4.877165841584159e-05,
      "loss": 8.2665,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 1.2589497566223145,
      "learning_rate": 4.8740717821782176e-05,
      "loss": 8.2462,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 3.0927257537841797,
      "learning_rate": 4.870977722772278e-05,
      "loss": 8.2647,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 1.1971580982208252,
      "learning_rate": 4.8678836633663366e-05,
      "loss": 8.2508,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 1.6362168788909912,
      "learning_rate": 4.864789603960396e-05,
      "loss": 8.2566,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 1.0588487386703491,
      "learning_rate": 4.861695544554456e-05,
      "loss": 8.2492,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 0.757152795791626,
      "learning_rate": 4.858601485148515e-05,
      "loss": 8.2621,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.8795488476753235,
      "learning_rate": 4.855507425742574e-05,
      "loss": 8.2369,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 0.695861279964447,
      "learning_rate": 4.852413366336634e-05,
      "loss": 8.2328,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 1.3076118230819702,
      "learning_rate": 4.849319306930693e-05,
      "loss": 8.2352,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.7781069874763489,
      "learning_rate": 4.846225247524753e-05,
      "loss": 8.2228,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 1.1994925737380981,
      "learning_rate": 4.843131188118812e-05,
      "loss": 8.2235,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 1.3085798025131226,
      "learning_rate": 4.840037128712872e-05,
      "loss": 8.2474,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 2.003173589706421,
      "learning_rate": 4.8369430693069314e-05,
      "loss": 8.2414,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.8361700773239136,
      "learning_rate": 4.83384900990099e-05,
      "loss": 8.2453,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 1.8430653810501099,
      "learning_rate": 4.83075495049505e-05,
      "loss": 8.2399,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 1.4458892345428467,
      "learning_rate": 4.827660891089109e-05,
      "loss": 8.2388,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.6414463520050049,
      "learning_rate": 4.824566831683169e-05,
      "loss": 8.2377,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 0.614952802658081,
      "learning_rate": 4.821472772277228e-05,
      "loss": 8.2242,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 2.2713072299957275,
      "learning_rate": 4.818378712871288e-05,
      "loss": 8.2297,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 1.6645755767822266,
      "learning_rate": 4.815284653465347e-05,
      "loss": 8.2216,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 1.319783091545105,
      "learning_rate": 4.812190594059406e-05,
      "loss": 8.22,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 0.7334883213043213,
      "learning_rate": 4.809096534653466e-05,
      "loss": 8.2276,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.5590004324913025,
      "learning_rate": 4.8060024752475254e-05,
      "loss": 8.2329,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 1.0776251554489136,
      "learning_rate": 4.802908415841584e-05,
      "loss": 8.2187,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.783419668674469,
      "learning_rate": 4.799814356435644e-05,
      "loss": 8.214,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.5639674663543701,
      "learning_rate": 4.796720297029703e-05,
      "loss": 8.2288,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 1.6762508153915405,
      "learning_rate": 4.793626237623763e-05,
      "loss": 8.2244,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 1.8065810203552246,
      "learning_rate": 4.790532178217822e-05,
      "loss": 8.2238,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 2.1397507190704346,
      "learning_rate": 4.787438118811882e-05,
      "loss": 8.2311,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 1.9543536901474,
      "learning_rate": 4.784344059405941e-05,
      "loss": 8.2082,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 1.0264626741409302,
      "learning_rate": 4.7812500000000003e-05,
      "loss": 8.218,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 1.7906293869018555,
      "learning_rate": 4.77815594059406e-05,
      "loss": 8.2161,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 0.523497462272644,
      "learning_rate": 4.7750618811881194e-05,
      "loss": 8.2333,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 0.7396768927574158,
      "learning_rate": 4.771967821782178e-05,
      "loss": 8.2318,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 1.0711976289749146,
      "learning_rate": 4.768873762376238e-05,
      "loss": 8.212,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 1.1562745571136475,
      "learning_rate": 4.7657797029702974e-05,
      "loss": 8.2018,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 0.9633840322494507,
      "learning_rate": 4.762685643564357e-05,
      "loss": 8.2027,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 1.9501839876174927,
      "learning_rate": 4.759591584158416e-05,
      "loss": 8.2235,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 0.6358993053436279,
      "learning_rate": 4.756497524752476e-05,
      "loss": 8.2305,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 1.3217599391937256,
      "learning_rate": 4.753403465346535e-05,
      "loss": 8.229,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 1.5870161056518555,
      "learning_rate": 4.7503094059405944e-05,
      "loss": 8.2073,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.7626834511756897,
      "learning_rate": 4.747215346534654e-05,
      "loss": 8.2184,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.9049161672592163,
      "learning_rate": 4.7441212871287135e-05,
      "loss": 8.2069,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 0.767512321472168,
      "learning_rate": 4.741027227722772e-05,
      "loss": 8.2177,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 1.644620418548584,
      "learning_rate": 4.737933168316832e-05,
      "loss": 8.2219,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.6423130035400391,
      "learning_rate": 4.7348391089108914e-05,
      "loss": 8.203,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.9769448041915894,
      "learning_rate": 4.731745049504951e-05,
      "loss": 8.2188,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 0.7573779821395874,
      "learning_rate": 4.72865099009901e-05,
      "loss": 8.1944,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 0.7785887718200684,
      "learning_rate": 4.72555693069307e-05,
      "loss": 8.1956,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 1.4564820528030396,
      "learning_rate": 4.722462871287129e-05,
      "loss": 8.2146,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.9155580997467041,
      "learning_rate": 4.7193688118811884e-05,
      "loss": 8.2203,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 1.7754689455032349,
      "learning_rate": 4.716274752475248e-05,
      "loss": 8.2226,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 1.523375391960144,
      "learning_rate": 4.7131806930693075e-05,
      "loss": 8.2055,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 2.337571620941162,
      "learning_rate": 4.7100866336633663e-05,
      "loss": 8.2128,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 2.301279306411743,
      "learning_rate": 4.706992574257426e-05,
      "loss": 8.2004,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 1.6187915802001953,
      "learning_rate": 4.7038985148514854e-05,
      "loss": 8.1995,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 1.382105827331543,
      "learning_rate": 4.700804455445545e-05,
      "loss": 8.218,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 0.6750116348266602,
      "learning_rate": 4.697710396039604e-05,
      "loss": 8.2066,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.8816543817520142,
      "learning_rate": 4.694616336633664e-05,
      "loss": 8.2152,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 1.4862397909164429,
      "learning_rate": 4.691522277227723e-05,
      "loss": 8.2067,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 2.049319267272949,
      "learning_rate": 4.6884282178217824e-05,
      "loss": 8.2084,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 0.9031298160552979,
      "learning_rate": 4.685334158415842e-05,
      "loss": 8.1988,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 1.8627923727035522,
      "learning_rate": 4.6822400990099015e-05,
      "loss": 8.2123,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 2.3312253952026367,
      "learning_rate": 4.6791460396039604e-05,
      "loss": 8.2085,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 1.4772182703018188,
      "learning_rate": 4.67605198019802e-05,
      "loss": 8.1977,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 0.7699053287506104,
      "learning_rate": 4.6729579207920795e-05,
      "loss": 8.214,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 1.258692741394043,
      "learning_rate": 4.669863861386139e-05,
      "loss": 8.2091,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 1.3253406286239624,
      "learning_rate": 4.666769801980198e-05,
      "loss": 8.1978,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.686968982219696,
      "learning_rate": 4.663675742574258e-05,
      "loss": 8.2043,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.0489264726638794,
      "learning_rate": 4.660581683168317e-05,
      "loss": 8.1951,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 0.6969893574714661,
      "learning_rate": 4.6574876237623765e-05,
      "loss": 8.2131,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 0.8512948155403137,
      "learning_rate": 4.654393564356436e-05,
      "loss": 8.2043,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 1.1636050939559937,
      "learning_rate": 4.6512995049504955e-05,
      "loss": 8.1909,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 2.8269481658935547,
      "learning_rate": 4.6482054455445544e-05,
      "loss": 8.2012,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 1.2299357652664185,
      "learning_rate": 4.645111386138614e-05,
      "loss": 8.1793,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 0.7432317733764648,
      "learning_rate": 4.6420173267326735e-05,
      "loss": 8.1917,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 1.4997855424880981,
      "learning_rate": 4.638923267326733e-05,
      "loss": 8.2003,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 1.17046320438385,
      "learning_rate": 4.635829207920792e-05,
      "loss": 8.194,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 0.8377035856246948,
      "learning_rate": 4.632735148514852e-05,
      "loss": 8.1987,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 1.1011815071105957,
      "learning_rate": 4.629641089108911e-05,
      "loss": 8.2105,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 2.4605493545532227,
      "learning_rate": 4.6265470297029705e-05,
      "loss": 8.1883,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 2.327408790588379,
      "learning_rate": 4.62345297029703e-05,
      "loss": 8.2033,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 0.8229179978370667,
      "learning_rate": 4.6203589108910896e-05,
      "loss": 8.1983,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 0.7746541500091553,
      "learning_rate": 4.6172648514851484e-05,
      "loss": 8.1951,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 1.7976148128509521,
      "learning_rate": 4.614170792079208e-05,
      "loss": 8.1896,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 0.8158743977546692,
      "learning_rate": 4.6110767326732675e-05,
      "loss": 8.1881,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 1.2931594848632812,
      "learning_rate": 4.607982673267327e-05,
      "loss": 8.1896,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 1.3114842176437378,
      "learning_rate": 4.604888613861386e-05,
      "loss": 8.1987,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 1.2831923961639404,
      "learning_rate": 4.601794554455446e-05,
      "loss": 8.1997,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 2.43292236328125,
      "learning_rate": 4.598700495049505e-05,
      "loss": 8.2048,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 1.5269575119018555,
      "learning_rate": 4.5956064356435645e-05,
      "loss": 8.194,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 2.0795676708221436,
      "learning_rate": 4.592512376237624e-05,
      "loss": 8.2075,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 2.4864180088043213,
      "learning_rate": 4.5894183168316836e-05,
      "loss": 8.197,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 1.2086435556411743,
      "learning_rate": 4.5863242574257425e-05,
      "loss": 8.1758,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 1.5092233419418335,
      "learning_rate": 4.583230198019802e-05,
      "loss": 8.1855,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 1.1152379512786865,
      "learning_rate": 4.5801361386138616e-05,
      "loss": 8.1831,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 0.8014148473739624,
      "learning_rate": 4.577042079207921e-05,
      "loss": 8.175,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.7414972186088562,
      "learning_rate": 4.57394801980198e-05,
      "loss": 8.1869,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.085468053817749,
      "learning_rate": 4.57085396039604e-05,
      "loss": 8.2012,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 1.602766752243042,
      "learning_rate": 4.567759900990099e-05,
      "loss": 8.1717,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 0.7542450428009033,
      "learning_rate": 4.5646658415841586e-05,
      "loss": 8.1987,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 0.933017909526825,
      "learning_rate": 4.561571782178218e-05,
      "loss": 8.1989,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 2.079925775527954,
      "learning_rate": 4.5584777227722776e-05,
      "loss": 8.1969,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 1.0393853187561035,
      "learning_rate": 4.5553836633663365e-05,
      "loss": 8.1958,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 0.9792965650558472,
      "learning_rate": 4.552289603960396e-05,
      "loss": 8.1952,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 0.6396719813346863,
      "learning_rate": 4.5491955445544556e-05,
      "loss": 8.1849,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 1.0150868892669678,
      "learning_rate": 4.546101485148515e-05,
      "loss": 8.1889,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 1.6108311414718628,
      "learning_rate": 4.543007425742574e-05,
      "loss": 8.1892,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 1.2107347249984741,
      "learning_rate": 4.539913366336634e-05,
      "loss": 8.2103,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 2.3177919387817383,
      "learning_rate": 4.536819306930693e-05,
      "loss": 8.1775,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 2.523088216781616,
      "learning_rate": 4.5337252475247526e-05,
      "loss": 8.1951,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 1.209039330482483,
      "learning_rate": 4.530631188118812e-05,
      "loss": 8.1994,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 0.9921939969062805,
      "learning_rate": 4.527537128712872e-05,
      "loss": 8.1897,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 1.8710671663284302,
      "learning_rate": 4.524443069306931e-05,
      "loss": 8.2037,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 1.856017827987671,
      "learning_rate": 4.52134900990099e-05,
      "loss": 8.1749,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.9681246876716614,
      "learning_rate": 4.51825495049505e-05,
      "loss": 8.1651,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 1.9962009191513062,
      "learning_rate": 4.515160891089109e-05,
      "loss": 8.2048,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 0.6548168063163757,
      "learning_rate": 4.512066831683169e-05,
      "loss": 8.1847,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 1.0097074508666992,
      "learning_rate": 4.508972772277228e-05,
      "loss": 8.1786,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 3.913090705871582,
      "learning_rate": 4.505878712871288e-05,
      "loss": 8.1914,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 2.477466583251953,
      "learning_rate": 4.5027846534653466e-05,
      "loss": 8.1868,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 1.6980576515197754,
      "learning_rate": 4.499690594059406e-05,
      "loss": 8.1864,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 0.7103257775306702,
      "learning_rate": 4.496596534653466e-05,
      "loss": 8.1642,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 1.0004551410675049,
      "learning_rate": 4.493502475247525e-05,
      "loss": 8.1928,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 0.7323936223983765,
      "learning_rate": 4.490408415841584e-05,
      "loss": 8.19,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 1.083060383796692,
      "learning_rate": 4.487314356435644e-05,
      "loss": 8.1821,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 1.411689281463623,
      "learning_rate": 4.484220297029703e-05,
      "loss": 8.1909,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 1.8345826864242554,
      "learning_rate": 4.481126237623763e-05,
      "loss": 8.1705,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 0.8988075256347656,
      "learning_rate": 4.478032178217822e-05,
      "loss": 8.1757,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 2.0011909008026123,
      "learning_rate": 4.474938118811882e-05,
      "loss": 8.1819,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 1.4574311971664429,
      "learning_rate": 4.471844059405941e-05,
      "loss": 8.1937,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 1.8036798238754272,
      "learning_rate": 4.46875e-05,
      "loss": 8.1856,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 1.1992884874343872,
      "learning_rate": 4.46565594059406e-05,
      "loss": 8.1782,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 1.0265569686889648,
      "learning_rate": 4.462561881188119e-05,
      "loss": 8.1857,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 2.0533766746520996,
      "learning_rate": 4.459467821782178e-05,
      "loss": 8.1917,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 1.6524206399917603,
      "learning_rate": 4.4563737623762384e-05,
      "loss": 8.1967,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 1.3807529211044312,
      "learning_rate": 4.453279702970297e-05,
      "loss": 8.1698,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 2.298870086669922,
      "learning_rate": 4.450185643564357e-05,
      "loss": 8.1933,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 0.7895309329032898,
      "learning_rate": 4.447091584158416e-05,
      "loss": 8.1897,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 0.6869894862174988,
      "learning_rate": 4.443997524752476e-05,
      "loss": 8.181,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 1.9506685733795166,
      "learning_rate": 4.440903465346535e-05,
      "loss": 8.1712,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 1.4609322547912598,
      "learning_rate": 4.437809405940594e-05,
      "loss": 8.1963,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 2.905377149581909,
      "learning_rate": 4.434715346534654e-05,
      "loss": 8.186,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 1.0420995950698853,
      "learning_rate": 4.431621287128713e-05,
      "loss": 8.1818,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 1.0503602027893066,
      "learning_rate": 4.428527227722772e-05,
      "loss": 8.1772,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 1.0495920181274414,
      "learning_rate": 4.4254331683168324e-05,
      "loss": 8.1953,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 1.1845871210098267,
      "learning_rate": 4.422339108910891e-05,
      "loss": 8.2054,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 0.8258082270622253,
      "learning_rate": 4.419245049504951e-05,
      "loss": 8.1825,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.954254686832428,
      "learning_rate": 4.41615099009901e-05,
      "loss": 8.1811,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 0.9058201313018799,
      "learning_rate": 4.41305693069307e-05,
      "loss": 8.1925,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 1.8257588148117065,
      "learning_rate": 4.409962871287129e-05,
      "loss": 8.1849,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 2.6599490642547607,
      "learning_rate": 4.406868811881188e-05,
      "loss": 8.1895,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 3.222787380218506,
      "learning_rate": 4.403774752475248e-05,
      "loss": 8.1875,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 1.5086697340011597,
      "learning_rate": 4.4006806930693073e-05,
      "loss": 8.1931,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 2.020599842071533,
      "learning_rate": 4.397586633663366e-05,
      "loss": 8.1783,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 3.1900675296783447,
      "learning_rate": 4.3944925742574264e-05,
      "loss": 8.1733,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 2.096738576889038,
      "learning_rate": 4.391398514851485e-05,
      "loss": 8.1673,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 1.2224245071411133,
      "learning_rate": 4.388304455445545e-05,
      "loss": 8.1777,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 1.1820451021194458,
      "learning_rate": 4.3852103960396044e-05,
      "loss": 8.1868,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 0.6829479336738586,
      "learning_rate": 4.382116336633664e-05,
      "loss": 8.1895,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.9388048052787781,
      "learning_rate": 4.379022277227723e-05,
      "loss": 8.1835,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 1.8816661834716797,
      "learning_rate": 4.375928217821782e-05,
      "loss": 8.2014,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 2.058828592300415,
      "learning_rate": 4.372834158415842e-05,
      "loss": 8.1804,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 0.6706216931343079,
      "learning_rate": 4.3697400990099014e-05,
      "loss": 8.194,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 1.2952839136123657,
      "learning_rate": 4.36664603960396e-05,
      "loss": 8.1882,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 1.1358423233032227,
      "learning_rate": 4.3635519801980205e-05,
      "loss": 8.1826,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 2.457620620727539,
      "learning_rate": 4.360457920792079e-05,
      "loss": 8.1862,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 0.9240761399269104,
      "learning_rate": 4.357363861386139e-05,
      "loss": 8.2007,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 1.1774667501449585,
      "learning_rate": 4.3542698019801984e-05,
      "loss": 8.192,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 1.1870375871658325,
      "learning_rate": 4.351175742574258e-05,
      "loss": 8.2006,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 0.641690194606781,
      "learning_rate": 4.348081683168317e-05,
      "loss": 8.1851,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 1.9188201427459717,
      "learning_rate": 4.344987623762376e-05,
      "loss": 8.1785,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 1.383307933807373,
      "learning_rate": 4.341893564356436e-05,
      "loss": 8.1925,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 2.127551794052124,
      "learning_rate": 4.3387995049504954e-05,
      "loss": 8.1923,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 1.263771891593933,
      "learning_rate": 4.335705445544554e-05,
      "loss": 8.2007,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 1.2934004068374634,
      "learning_rate": 4.3326113861386145e-05,
      "loss": 8.1754,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 1.2981380224227905,
      "learning_rate": 4.3295173267326733e-05,
      "loss": 8.1811,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 0.7973347902297974,
      "learning_rate": 4.326423267326733e-05,
      "loss": 8.1797,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 2.5965218544006348,
      "learning_rate": 4.3233292079207924e-05,
      "loss": 8.1762,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 1.4685229063034058,
      "learning_rate": 4.320235148514852e-05,
      "loss": 8.1916,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 1.339370846748352,
      "learning_rate": 4.317141089108911e-05,
      "loss": 8.1817,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 1.7123206853866577,
      "learning_rate": 4.3140470297029704e-05,
      "loss": 8.1901,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 1.1629844903945923,
      "learning_rate": 4.31095297029703e-05,
      "loss": 8.1858,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 1.1125881671905518,
      "learning_rate": 4.3078589108910894e-05,
      "loss": 8.1818,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.8006672263145447,
      "learning_rate": 4.304764851485148e-05,
      "loss": 8.2041,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 0.8128392100334167,
      "learning_rate": 4.3016707920792085e-05,
      "loss": 8.1749,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 1.392057180404663,
      "learning_rate": 4.2985767326732674e-05,
      "loss": 8.1642,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 0.7961410284042358,
      "learning_rate": 4.295482673267327e-05,
      "loss": 8.1724,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 0.8821604251861572,
      "learning_rate": 4.2923886138613865e-05,
      "loss": 8.1812,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 2.1213696002960205,
      "learning_rate": 4.289294554455446e-05,
      "loss": 8.1812,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 2.049058198928833,
      "learning_rate": 4.286200495049505e-05,
      "loss": 8.1706,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.8438975811004639,
      "learning_rate": 4.2831064356435644e-05,
      "loss": 8.1814,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 1.5994082689285278,
      "learning_rate": 4.280012376237624e-05,
      "loss": 8.1773,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 1.6760908365249634,
      "learning_rate": 4.2769183168316835e-05,
      "loss": 8.1915,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 1.1761233806610107,
      "learning_rate": 4.273824257425742e-05,
      "loss": 8.1648,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 0.8909958004951477,
      "learning_rate": 4.2707301980198025e-05,
      "loss": 8.1975,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 1.8550859689712524,
      "learning_rate": 4.2676361386138614e-05,
      "loss": 8.1823,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 2.531545877456665,
      "learning_rate": 4.264542079207921e-05,
      "loss": 8.1802,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 1.5944923162460327,
      "learning_rate": 4.2614480198019805e-05,
      "loss": 8.1732,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 1.9887875318527222,
      "learning_rate": 4.25835396039604e-05,
      "loss": 8.1727,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 2.872790575027466,
      "learning_rate": 4.255259900990099e-05,
      "loss": 8.1794,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 0.7231345176696777,
      "learning_rate": 4.2521658415841584e-05,
      "loss": 8.1898,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 1.993692398071289,
      "learning_rate": 4.249071782178218e-05,
      "loss": 8.1832,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 2.00915265083313,
      "learning_rate": 4.2459777227722775e-05,
      "loss": 8.1933,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 1.4420005083084106,
      "learning_rate": 4.2428836633663364e-05,
      "loss": 8.1866,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.6248858571052551,
      "learning_rate": 4.2397896039603966e-05,
      "loss": 8.1862,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.8173538446426392,
      "learning_rate": 4.2366955445544554e-05,
      "loss": 8.1595,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 0.9942618012428284,
      "learning_rate": 4.233601485148515e-05,
      "loss": 8.1785,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 0.8387640118598938,
      "learning_rate": 4.2305074257425745e-05,
      "loss": 8.1929,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 1.4144275188446045,
      "learning_rate": 4.227413366336634e-05,
      "loss": 8.1745,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.7667526006698608,
      "learning_rate": 4.224319306930693e-05,
      "loss": 8.1941,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 1.2658334970474243,
      "learning_rate": 4.2212252475247525e-05,
      "loss": 8.1778,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 1.2555665969848633,
      "learning_rate": 4.218131188118813e-05,
      "loss": 8.1546,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 1.0784926414489746,
      "learning_rate": 4.2150371287128715e-05,
      "loss": 8.1839,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 0.7848606109619141,
      "learning_rate": 4.211943069306931e-05,
      "loss": 8.1716,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 0.8297044634819031,
      "learning_rate": 4.2088490099009906e-05,
      "loss": 8.1839,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 1.2344859838485718,
      "learning_rate": 4.20575495049505e-05,
      "loss": 8.1839,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 1.8767091035842896,
      "learning_rate": 4.202660891089109e-05,
      "loss": 8.18,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 2.172985792160034,
      "learning_rate": 4.1995668316831686e-05,
      "loss": 8.1864,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 0.9894881844520569,
      "learning_rate": 4.196472772277228e-05,
      "loss": 8.1868,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 2.2789270877838135,
      "learning_rate": 4.1933787128712876e-05,
      "loss": 8.1741,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 0.9330989122390747,
      "learning_rate": 4.1902846534653465e-05,
      "loss": 8.1596,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 1.5242124795913696,
      "learning_rate": 4.187190594059407e-05,
      "loss": 8.1624,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 1.4509238004684448,
      "learning_rate": 4.1840965346534656e-05,
      "loss": 8.1812,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 1.110629916191101,
      "learning_rate": 4.181002475247525e-05,
      "loss": 8.1899,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 1.2923803329467773,
      "learning_rate": 4.1779084158415846e-05,
      "loss": 8.1763,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 0.8786554336547852,
      "learning_rate": 4.174814356435644e-05,
      "loss": 8.1813,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 1.402254343032837,
      "learning_rate": 4.171720297029703e-05,
      "loss": 8.1962,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 1.2103198766708374,
      "learning_rate": 4.1686262376237626e-05,
      "loss": 8.1784,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 1.835925579071045,
      "learning_rate": 4.165532178217822e-05,
      "loss": 8.1734,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 1.5465960502624512,
      "learning_rate": 4.1624381188118817e-05,
      "loss": 8.1802,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 1.3195463418960571,
      "learning_rate": 4.1593440594059405e-05,
      "loss": 8.1999,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 1.1356457471847534,
      "learning_rate": 4.156250000000001e-05,
      "loss": 8.1874,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 1.2040010690689087,
      "learning_rate": 4.1531559405940596e-05,
      "loss": 8.1718,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.6268787980079651,
      "learning_rate": 4.150061881188119e-05,
      "loss": 8.1883,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 1.3542238473892212,
      "learning_rate": 4.146967821782179e-05,
      "loss": 8.1929,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 1.0542324781417847,
      "learning_rate": 4.143873762376238e-05,
      "loss": 8.1867,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 0.959769070148468,
      "learning_rate": 4.140779702970297e-05,
      "loss": 8.1805,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.8823913931846619,
      "learning_rate": 4.1376856435643566e-05,
      "loss": 8.1691,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.993774950504303,
      "learning_rate": 4.134591584158416e-05,
      "loss": 8.1943,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.6663392186164856,
      "learning_rate": 4.131497524752476e-05,
      "loss": 8.1682,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 1.050313115119934,
      "learning_rate": 4.1284034653465346e-05,
      "loss": 8.1664,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 0.6561669111251831,
      "learning_rate": 4.125309405940595e-05,
      "loss": 8.2059,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.5854478478431702,
      "learning_rate": 4.1222153465346536e-05,
      "loss": 8.1804,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.6190448999404907,
      "learning_rate": 4.119121287128713e-05,
      "loss": 8.1843,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 0.8077029585838318,
      "learning_rate": 4.116027227722773e-05,
      "loss": 8.1768,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.6319847106933594,
      "learning_rate": 4.112933168316832e-05,
      "loss": 8.1793,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 0.6598100662231445,
      "learning_rate": 4.109839108910891e-05,
      "loss": 8.1773,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 0.6498719453811646,
      "learning_rate": 4.1067450495049506e-05,
      "loss": 8.1743,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 1.3095574378967285,
      "learning_rate": 4.10365099009901e-05,
      "loss": 8.1851,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 2.136505126953125,
      "learning_rate": 4.10055693069307e-05,
      "loss": 8.1921,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 0.8504270315170288,
      "learning_rate": 4.0974628712871286e-05,
      "loss": 8.1865,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 1.7754770517349243,
      "learning_rate": 4.094368811881189e-05,
      "loss": 8.1855,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 1.306828260421753,
      "learning_rate": 4.091274752475248e-05,
      "loss": 8.1704,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 1.1314023733139038,
      "learning_rate": 4.088180693069307e-05,
      "loss": 8.1697,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 2.4816794395446777,
      "learning_rate": 4.085086633663367e-05,
      "loss": 8.1709,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 1.2383394241333008,
      "learning_rate": 4.081992574257426e-05,
      "loss": 8.1762,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 0.8009899258613586,
      "learning_rate": 4.078898514851485e-05,
      "loss": 8.1734,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 1.5692912340164185,
      "learning_rate": 4.075804455445545e-05,
      "loss": 8.188,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 1.0885016918182373,
      "learning_rate": 4.072710396039604e-05,
      "loss": 8.1867,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.7706090807914734,
      "learning_rate": 4.069616336633664e-05,
      "loss": 8.1562,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.8732590675354004,
      "learning_rate": 4.0665222772277226e-05,
      "loss": 8.1804,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.9803152680397034,
      "learning_rate": 4.063428217821783e-05,
      "loss": 8.177,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 0.7919334769248962,
      "learning_rate": 4.060334158415842e-05,
      "loss": 8.1704,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 1.1393951177597046,
      "learning_rate": 4.057240099009901e-05,
      "loss": 8.1884,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 1.2807313203811646,
      "learning_rate": 4.054146039603961e-05,
      "loss": 8.1877,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 1.3348079919815063,
      "learning_rate": 4.05105198019802e-05,
      "loss": 8.1906,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 0.723763644695282,
      "learning_rate": 4.047957920792079e-05,
      "loss": 8.1892,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 0.6793975830078125,
      "learning_rate": 4.044863861386139e-05,
      "loss": 8.1648,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 1.0764813423156738,
      "learning_rate": 4.041769801980198e-05,
      "loss": 8.1786,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 1.1117558479309082,
      "learning_rate": 4.038675742574258e-05,
      "loss": 8.185,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 2.068308115005493,
      "learning_rate": 4.0355816831683166e-05,
      "loss": 8.1686,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 1.702989935874939,
      "learning_rate": 4.032487623762377e-05,
      "loss": 8.18,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 1.5032709836959839,
      "learning_rate": 4.029393564356436e-05,
      "loss": 8.1727,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 1.2828290462493896,
      "learning_rate": 4.026299504950495e-05,
      "loss": 8.1885,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.9744423031806946,
      "learning_rate": 4.023205445544555e-05,
      "loss": 8.1824,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 1.3720958232879639,
      "learning_rate": 4.0201113861386143e-05,
      "loss": 8.1716,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 1.1993892192840576,
      "learning_rate": 4.017017326732673e-05,
      "loss": 8.1774,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.7784874439239502,
      "learning_rate": 4.013923267326733e-05,
      "loss": 8.1701,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 1.4290626049041748,
      "learning_rate": 4.010829207920792e-05,
      "loss": 8.1728,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 1.2606793642044067,
      "learning_rate": 4.007735148514852e-05,
      "loss": 8.1773,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.9906719326972961,
      "learning_rate": 4.004641089108911e-05,
      "loss": 8.1718,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 0.9914515018463135,
      "learning_rate": 4.001547029702971e-05,
      "loss": 8.1653,
      "step": 3230
    },
    {
      "epoch": 1.002242672647127,
      "grad_norm": 1.2213011980056763,
      "learning_rate": 3.99845297029703e-05,
      "loss": 8.1681,
      "step": 3240
    },
    {
      "epoch": 1.0053360142293712,
      "grad_norm": 1.1975106000900269,
      "learning_rate": 3.995358910891089e-05,
      "loss": 8.185,
      "step": 3250
    },
    {
      "epoch": 1.0084293558116155,
      "grad_norm": 1.2800188064575195,
      "learning_rate": 3.992264851485149e-05,
      "loss": 8.1651,
      "step": 3260
    },
    {
      "epoch": 1.0115226973938598,
      "grad_norm": 1.2291233539581299,
      "learning_rate": 3.9891707920792084e-05,
      "loss": 8.1594,
      "step": 3270
    },
    {
      "epoch": 1.0146160389761039,
      "grad_norm": 1.1720836162567139,
      "learning_rate": 3.986076732673267e-05,
      "loss": 8.1664,
      "step": 3280
    },
    {
      "epoch": 1.0177093805583481,
      "grad_norm": 1.567177414894104,
      "learning_rate": 3.982982673267327e-05,
      "loss": 8.1677,
      "step": 3290
    },
    {
      "epoch": 1.0208027221405924,
      "grad_norm": 0.5762407183647156,
      "learning_rate": 3.979888613861386e-05,
      "loss": 8.1839,
      "step": 3300
    },
    {
      "epoch": 1.0238960637228367,
      "grad_norm": 1.0825451612472534,
      "learning_rate": 3.976794554455446e-05,
      "loss": 8.1849,
      "step": 3310
    },
    {
      "epoch": 1.0269894053050808,
      "grad_norm": 0.9254522919654846,
      "learning_rate": 3.973700495049505e-05,
      "loss": 8.1834,
      "step": 3320
    },
    {
      "epoch": 1.030082746887325,
      "grad_norm": 1.2102525234222412,
      "learning_rate": 3.970606435643565e-05,
      "loss": 8.167,
      "step": 3330
    },
    {
      "epoch": 1.0331760884695693,
      "grad_norm": 0.8051912188529968,
      "learning_rate": 3.967512376237624e-05,
      "loss": 8.1812,
      "step": 3340
    },
    {
      "epoch": 1.0362694300518134,
      "grad_norm": 0.5957062244415283,
      "learning_rate": 3.964418316831683e-05,
      "loss": 8.1632,
      "step": 3350
    },
    {
      "epoch": 1.0393627716340577,
      "grad_norm": 0.9674434661865234,
      "learning_rate": 3.961324257425743e-05,
      "loss": 8.1782,
      "step": 3360
    },
    {
      "epoch": 1.042456113216302,
      "grad_norm": 0.8710668683052063,
      "learning_rate": 3.9582301980198024e-05,
      "loss": 8.1824,
      "step": 3370
    },
    {
      "epoch": 1.0455494547985462,
      "grad_norm": 0.7723981738090515,
      "learning_rate": 3.955136138613861e-05,
      "loss": 8.1612,
      "step": 3380
    },
    {
      "epoch": 1.0486427963807903,
      "grad_norm": 0.7678624987602234,
      "learning_rate": 3.952042079207921e-05,
      "loss": 8.1651,
      "step": 3390
    },
    {
      "epoch": 1.0517361379630346,
      "grad_norm": 1.9644544124603271,
      "learning_rate": 3.9489480198019803e-05,
      "loss": 8.1778,
      "step": 3400
    },
    {
      "epoch": 1.0548294795452788,
      "grad_norm": 1.7867558002471924,
      "learning_rate": 3.94585396039604e-05,
      "loss": 8.1736,
      "step": 3410
    },
    {
      "epoch": 1.057922821127523,
      "grad_norm": 0.6512953042984009,
      "learning_rate": 3.942759900990099e-05,
      "loss": 8.1605,
      "step": 3420
    },
    {
      "epoch": 1.0610161627097672,
      "grad_norm": 0.7946992516517639,
      "learning_rate": 3.939665841584159e-05,
      "loss": 8.1768,
      "step": 3430
    },
    {
      "epoch": 1.0641095042920115,
      "grad_norm": 0.8331141471862793,
      "learning_rate": 3.936571782178218e-05,
      "loss": 8.1719,
      "step": 3440
    },
    {
      "epoch": 1.0672028458742557,
      "grad_norm": 0.7313192486763,
      "learning_rate": 3.9334777227722774e-05,
      "loss": 8.1841,
      "step": 3450
    },
    {
      "epoch": 1.0702961874564998,
      "grad_norm": 1.2313882112503052,
      "learning_rate": 3.930383663366337e-05,
      "loss": 8.187,
      "step": 3460
    },
    {
      "epoch": 1.073389529038744,
      "grad_norm": 1.111137866973877,
      "learning_rate": 3.9272896039603964e-05,
      "loss": 8.1685,
      "step": 3470
    },
    {
      "epoch": 1.0764828706209884,
      "grad_norm": 1.3812202215194702,
      "learning_rate": 3.924195544554455e-05,
      "loss": 8.1836,
      "step": 3480
    },
    {
      "epoch": 1.0795762122032326,
      "grad_norm": 1.4169095754623413,
      "learning_rate": 3.921101485148515e-05,
      "loss": 8.1962,
      "step": 3490
    },
    {
      "epoch": 1.0826695537854767,
      "grad_norm": 1.0171616077423096,
      "learning_rate": 3.9180074257425744e-05,
      "loss": 8.1668,
      "step": 3500
    },
    {
      "epoch": 1.085762895367721,
      "grad_norm": 0.7996727824211121,
      "learning_rate": 3.914913366336634e-05,
      "loss": 8.1649,
      "step": 3510
    },
    {
      "epoch": 1.0888562369499653,
      "grad_norm": 1.4300481081008911,
      "learning_rate": 3.911819306930693e-05,
      "loss": 8.1738,
      "step": 3520
    },
    {
      "epoch": 1.0919495785322093,
      "grad_norm": 1.0143523216247559,
      "learning_rate": 3.908725247524753e-05,
      "loss": 8.1888,
      "step": 3530
    },
    {
      "epoch": 1.0950429201144536,
      "grad_norm": 1.0000865459442139,
      "learning_rate": 3.9056311881188125e-05,
      "loss": 8.1781,
      "step": 3540
    },
    {
      "epoch": 1.0981362616966979,
      "grad_norm": 1.3436375856399536,
      "learning_rate": 3.9025371287128714e-05,
      "loss": 8.1825,
      "step": 3550
    },
    {
      "epoch": 1.1012296032789421,
      "grad_norm": 1.0800632238388062,
      "learning_rate": 3.899443069306931e-05,
      "loss": 8.1661,
      "step": 3560
    },
    {
      "epoch": 1.1043229448611862,
      "grad_norm": 1.7473244667053223,
      "learning_rate": 3.8963490099009905e-05,
      "loss": 8.1815,
      "step": 3570
    },
    {
      "epoch": 1.1074162864434305,
      "grad_norm": 1.321324110031128,
      "learning_rate": 3.89325495049505e-05,
      "loss": 8.1679,
      "step": 3580
    },
    {
      "epoch": 1.1105096280256748,
      "grad_norm": 0.7825003862380981,
      "learning_rate": 3.890160891089109e-05,
      "loss": 8.1671,
      "step": 3590
    },
    {
      "epoch": 1.113602969607919,
      "grad_norm": 0.8835910558700562,
      "learning_rate": 3.887066831683169e-05,
      "loss": 8.1803,
      "step": 3600
    },
    {
      "epoch": 1.116696311190163,
      "grad_norm": 0.986366331577301,
      "learning_rate": 3.883972772277228e-05,
      "loss": 8.1777,
      "step": 3610
    },
    {
      "epoch": 1.1197896527724074,
      "grad_norm": 0.8880130052566528,
      "learning_rate": 3.8808787128712875e-05,
      "loss": 8.1713,
      "step": 3620
    },
    {
      "epoch": 1.1228829943546517,
      "grad_norm": 0.9621564745903015,
      "learning_rate": 3.877784653465347e-05,
      "loss": 8.1945,
      "step": 3630
    },
    {
      "epoch": 1.1259763359368957,
      "grad_norm": 1.0133496522903442,
      "learning_rate": 3.8746905940594066e-05,
      "loss": 8.1723,
      "step": 3640
    },
    {
      "epoch": 1.12906967751914,
      "grad_norm": 0.8424130082130432,
      "learning_rate": 3.8715965346534654e-05,
      "loss": 8.1794,
      "step": 3650
    },
    {
      "epoch": 1.1321630191013843,
      "grad_norm": 0.6788762807846069,
      "learning_rate": 3.868502475247525e-05,
      "loss": 8.1831,
      "step": 3660
    },
    {
      "epoch": 1.1352563606836286,
      "grad_norm": 0.9400535225868225,
      "learning_rate": 3.8654084158415845e-05,
      "loss": 8.1648,
      "step": 3670
    },
    {
      "epoch": 1.1383497022658726,
      "grad_norm": 1.319170594215393,
      "learning_rate": 3.862314356435644e-05,
      "loss": 8.1676,
      "step": 3680
    },
    {
      "epoch": 1.141443043848117,
      "grad_norm": 0.5681795477867126,
      "learning_rate": 3.859220297029703e-05,
      "loss": 8.162,
      "step": 3690
    },
    {
      "epoch": 1.1445363854303612,
      "grad_norm": 0.6263239979743958,
      "learning_rate": 3.856126237623763e-05,
      "loss": 8.1723,
      "step": 3700
    },
    {
      "epoch": 1.1476297270126055,
      "grad_norm": 1.3395272493362427,
      "learning_rate": 3.853032178217822e-05,
      "loss": 8.1587,
      "step": 3710
    },
    {
      "epoch": 1.1507230685948495,
      "grad_norm": 0.7281138300895691,
      "learning_rate": 3.8499381188118815e-05,
      "loss": 8.1718,
      "step": 3720
    },
    {
      "epoch": 1.1538164101770938,
      "grad_norm": 1.0737323760986328,
      "learning_rate": 3.846844059405941e-05,
      "loss": 8.167,
      "step": 3730
    },
    {
      "epoch": 1.156909751759338,
      "grad_norm": 0.8106751441955566,
      "learning_rate": 3.8437500000000006e-05,
      "loss": 8.1812,
      "step": 3740
    },
    {
      "epoch": 1.1600030933415821,
      "grad_norm": 1.8575725555419922,
      "learning_rate": 3.8406559405940595e-05,
      "loss": 8.1656,
      "step": 3750
    },
    {
      "epoch": 1.1630964349238264,
      "grad_norm": 0.621423602104187,
      "learning_rate": 3.837561881188119e-05,
      "loss": 8.1779,
      "step": 3760
    },
    {
      "epoch": 1.1661897765060707,
      "grad_norm": 0.6427802443504333,
      "learning_rate": 3.8344678217821785e-05,
      "loss": 8.1687,
      "step": 3770
    },
    {
      "epoch": 1.169283118088315,
      "grad_norm": 0.7275810241699219,
      "learning_rate": 3.831373762376238e-05,
      "loss": 8.1681,
      "step": 3780
    },
    {
      "epoch": 1.172376459670559,
      "grad_norm": 0.5198794007301331,
      "learning_rate": 3.828279702970297e-05,
      "loss": 8.1799,
      "step": 3790
    },
    {
      "epoch": 1.1754698012528033,
      "grad_norm": 1.0406533479690552,
      "learning_rate": 3.825185643564357e-05,
      "loss": 8.1835,
      "step": 3800
    },
    {
      "epoch": 1.1785631428350476,
      "grad_norm": 0.6469219923019409,
      "learning_rate": 3.822091584158416e-05,
      "loss": 8.1766,
      "step": 3810
    },
    {
      "epoch": 1.1816564844172919,
      "grad_norm": 2.0886242389678955,
      "learning_rate": 3.8189975247524755e-05,
      "loss": 8.1786,
      "step": 3820
    },
    {
      "epoch": 1.184749825999536,
      "grad_norm": 0.8466972708702087,
      "learning_rate": 3.815903465346535e-05,
      "loss": 8.1562,
      "step": 3830
    },
    {
      "epoch": 1.1878431675817802,
      "grad_norm": 1.4182608127593994,
      "learning_rate": 3.8128094059405946e-05,
      "loss": 8.1781,
      "step": 3840
    },
    {
      "epoch": 1.1909365091640245,
      "grad_norm": 0.9937980771064758,
      "learning_rate": 3.8097153465346535e-05,
      "loss": 8.1705,
      "step": 3850
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 0.7916375398635864,
      "learning_rate": 3.806621287128713e-05,
      "loss": 8.1759,
      "step": 3860
    },
    {
      "epoch": 1.1971231923285128,
      "grad_norm": 0.8437491059303284,
      "learning_rate": 3.8035272277227726e-05,
      "loss": 8.1652,
      "step": 3870
    },
    {
      "epoch": 1.200216533910757,
      "grad_norm": 1.6076982021331787,
      "learning_rate": 3.800433168316832e-05,
      "loss": 8.1831,
      "step": 3880
    },
    {
      "epoch": 1.2033098754930014,
      "grad_norm": 1.6538817882537842,
      "learning_rate": 3.797339108910891e-05,
      "loss": 8.1673,
      "step": 3890
    },
    {
      "epoch": 1.2064032170752454,
      "grad_norm": 1.3117600679397583,
      "learning_rate": 3.794245049504951e-05,
      "loss": 8.171,
      "step": 3900
    },
    {
      "epoch": 1.2094965586574897,
      "grad_norm": 0.7339670658111572,
      "learning_rate": 3.79115099009901e-05,
      "loss": 8.1719,
      "step": 3910
    },
    {
      "epoch": 1.212589900239734,
      "grad_norm": 1.092329502105713,
      "learning_rate": 3.7880569306930696e-05,
      "loss": 8.1698,
      "step": 3920
    },
    {
      "epoch": 1.2156832418219783,
      "grad_norm": 1.4451707601547241,
      "learning_rate": 3.784962871287129e-05,
      "loss": 8.1575,
      "step": 3930
    },
    {
      "epoch": 1.2187765834042223,
      "grad_norm": 0.7700849771499634,
      "learning_rate": 3.7818688118811887e-05,
      "loss": 8.1833,
      "step": 3940
    },
    {
      "epoch": 1.2218699249864666,
      "grad_norm": 0.6883994936943054,
      "learning_rate": 3.7787747524752475e-05,
      "loss": 8.1667,
      "step": 3950
    },
    {
      "epoch": 1.224963266568711,
      "grad_norm": 1.1646314859390259,
      "learning_rate": 3.775680693069307e-05,
      "loss": 8.1717,
      "step": 3960
    },
    {
      "epoch": 1.228056608150955,
      "grad_norm": 1.423634648323059,
      "learning_rate": 3.7725866336633666e-05,
      "loss": 8.173,
      "step": 3970
    },
    {
      "epoch": 1.2311499497331992,
      "grad_norm": 2.0301990509033203,
      "learning_rate": 3.769492574257426e-05,
      "loss": 8.1608,
      "step": 3980
    },
    {
      "epoch": 1.2342432913154435,
      "grad_norm": 1.3395417928695679,
      "learning_rate": 3.766398514851485e-05,
      "loss": 8.1734,
      "step": 3990
    },
    {
      "epoch": 1.2373366328976878,
      "grad_norm": 0.6827284097671509,
      "learning_rate": 3.763304455445545e-05,
      "loss": 8.1768,
      "step": 4000
    },
    {
      "epoch": 1.2404299744799319,
      "grad_norm": 0.8821004629135132,
      "learning_rate": 3.760210396039604e-05,
      "loss": 8.1767,
      "step": 4010
    },
    {
      "epoch": 1.2435233160621761,
      "grad_norm": 0.6663477420806885,
      "learning_rate": 3.7571163366336636e-05,
      "loss": 8.1713,
      "step": 4020
    },
    {
      "epoch": 1.2466166576444204,
      "grad_norm": 0.6850157380104065,
      "learning_rate": 3.754022277227723e-05,
      "loss": 8.1731,
      "step": 4030
    },
    {
      "epoch": 1.2497099992266647,
      "grad_norm": 0.708370566368103,
      "learning_rate": 3.750928217821783e-05,
      "loss": 8.174,
      "step": 4040
    },
    {
      "epoch": 1.2528033408089088,
      "grad_norm": 0.8563492298126221,
      "learning_rate": 3.7478341584158416e-05,
      "loss": 8.1591,
      "step": 4050
    },
    {
      "epoch": 1.255896682391153,
      "grad_norm": 0.658682644367218,
      "learning_rate": 3.744740099009901e-05,
      "loss": 8.1641,
      "step": 4060
    },
    {
      "epoch": 1.2589900239733973,
      "grad_norm": 0.7775496244430542,
      "learning_rate": 3.7416460396039606e-05,
      "loss": 8.1602,
      "step": 4070
    },
    {
      "epoch": 1.2620833655556414,
      "grad_norm": 1.0847424268722534,
      "learning_rate": 3.73855198019802e-05,
      "loss": 8.1931,
      "step": 4080
    },
    {
      "epoch": 1.2651767071378857,
      "grad_norm": 1.591184139251709,
      "learning_rate": 3.735457920792079e-05,
      "loss": 8.1891,
      "step": 4090
    },
    {
      "epoch": 1.26827004872013,
      "grad_norm": 1.4072344303131104,
      "learning_rate": 3.732363861386139e-05,
      "loss": 8.1626,
      "step": 4100
    },
    {
      "epoch": 1.2713633903023742,
      "grad_norm": 0.8713261485099792,
      "learning_rate": 3.729269801980198e-05,
      "loss": 8.1585,
      "step": 4110
    },
    {
      "epoch": 1.2744567318846183,
      "grad_norm": 0.9413397312164307,
      "learning_rate": 3.7261757425742576e-05,
      "loss": 8.1543,
      "step": 4120
    },
    {
      "epoch": 1.2775500734668626,
      "grad_norm": 1.0500810146331787,
      "learning_rate": 3.723081683168317e-05,
      "loss": 8.1732,
      "step": 4130
    },
    {
      "epoch": 1.2806434150491068,
      "grad_norm": 1.8383299112319946,
      "learning_rate": 3.719987623762377e-05,
      "loss": 8.1751,
      "step": 4140
    },
    {
      "epoch": 1.2837367566313511,
      "grad_norm": 1.2703605890274048,
      "learning_rate": 3.7168935643564356e-05,
      "loss": 8.1746,
      "step": 4150
    },
    {
      "epoch": 1.2868300982135952,
      "grad_norm": 0.8107616901397705,
      "learning_rate": 3.713799504950495e-05,
      "loss": 8.1797,
      "step": 4160
    },
    {
      "epoch": 1.2899234397958395,
      "grad_norm": 1.1127413511276245,
      "learning_rate": 3.710705445544555e-05,
      "loss": 8.1818,
      "step": 4170
    },
    {
      "epoch": 1.2930167813780837,
      "grad_norm": 1.1264545917510986,
      "learning_rate": 3.707611386138614e-05,
      "loss": 8.1755,
      "step": 4180
    },
    {
      "epoch": 1.2961101229603278,
      "grad_norm": 0.8028554320335388,
      "learning_rate": 3.704517326732673e-05,
      "loss": 8.1634,
      "step": 4190
    },
    {
      "epoch": 1.299203464542572,
      "grad_norm": 1.2072283029556274,
      "learning_rate": 3.701423267326733e-05,
      "loss": 8.1653,
      "step": 4200
    },
    {
      "epoch": 1.3022968061248164,
      "grad_norm": 1.267712116241455,
      "learning_rate": 3.698329207920792e-05,
      "loss": 8.175,
      "step": 4210
    },
    {
      "epoch": 1.3053901477070606,
      "grad_norm": 0.8405670523643494,
      "learning_rate": 3.695235148514852e-05,
      "loss": 8.18,
      "step": 4220
    },
    {
      "epoch": 1.3084834892893047,
      "grad_norm": 1.5811591148376465,
      "learning_rate": 3.692141089108911e-05,
      "loss": 8.167,
      "step": 4230
    },
    {
      "epoch": 1.311576830871549,
      "grad_norm": 0.7154774069786072,
      "learning_rate": 3.689047029702971e-05,
      "loss": 8.1731,
      "step": 4240
    },
    {
      "epoch": 1.3146701724537933,
      "grad_norm": 0.5312008261680603,
      "learning_rate": 3.6859529702970296e-05,
      "loss": 8.1673,
      "step": 4250
    },
    {
      "epoch": 1.3177635140360375,
      "grad_norm": 0.9514366388320923,
      "learning_rate": 3.682858910891089e-05,
      "loss": 8.158,
      "step": 4260
    },
    {
      "epoch": 1.3208568556182816,
      "grad_norm": 0.7574382424354553,
      "learning_rate": 3.679764851485149e-05,
      "loss": 8.174,
      "step": 4270
    },
    {
      "epoch": 1.3239501972005259,
      "grad_norm": 0.7057363986968994,
      "learning_rate": 3.676670792079208e-05,
      "loss": 8.167,
      "step": 4280
    },
    {
      "epoch": 1.3270435387827701,
      "grad_norm": 0.7980016469955444,
      "learning_rate": 3.673576732673267e-05,
      "loss": 8.1763,
      "step": 4290
    },
    {
      "epoch": 1.3301368803650142,
      "grad_norm": 0.6745693683624268,
      "learning_rate": 3.670482673267327e-05,
      "loss": 8.1637,
      "step": 4300
    },
    {
      "epoch": 1.3332302219472585,
      "grad_norm": 1.2513898611068726,
      "learning_rate": 3.667388613861386e-05,
      "loss": 8.1529,
      "step": 4310
    },
    {
      "epoch": 1.3363235635295028,
      "grad_norm": 1.2123066186904907,
      "learning_rate": 3.664294554455446e-05,
      "loss": 8.1717,
      "step": 4320
    },
    {
      "epoch": 1.339416905111747,
      "grad_norm": 1.742858648300171,
      "learning_rate": 3.6612004950495046e-05,
      "loss": 8.1762,
      "step": 4330
    },
    {
      "epoch": 1.342510246693991,
      "grad_norm": 0.7529677748680115,
      "learning_rate": 3.658106435643565e-05,
      "loss": 8.1643,
      "step": 4340
    },
    {
      "epoch": 1.3456035882762354,
      "grad_norm": 1.1175521612167358,
      "learning_rate": 3.6550123762376236e-05,
      "loss": 8.1689,
      "step": 4350
    },
    {
      "epoch": 1.3486969298584797,
      "grad_norm": 0.8537497520446777,
      "learning_rate": 3.651918316831683e-05,
      "loss": 8.1774,
      "step": 4360
    },
    {
      "epoch": 1.351790271440724,
      "grad_norm": 0.7117443084716797,
      "learning_rate": 3.648824257425743e-05,
      "loss": 8.178,
      "step": 4370
    },
    {
      "epoch": 1.354883613022968,
      "grad_norm": 0.9373490810394287,
      "learning_rate": 3.645730198019802e-05,
      "loss": 8.1835,
      "step": 4380
    },
    {
      "epoch": 1.3579769546052123,
      "grad_norm": 1.2221544981002808,
      "learning_rate": 3.642636138613861e-05,
      "loss": 8.1704,
      "step": 4390
    },
    {
      "epoch": 1.3610702961874566,
      "grad_norm": 0.8514627814292908,
      "learning_rate": 3.6395420792079213e-05,
      "loss": 8.1624,
      "step": 4400
    },
    {
      "epoch": 1.3641636377697006,
      "grad_norm": 0.6535224318504333,
      "learning_rate": 3.63644801980198e-05,
      "loss": 8.1681,
      "step": 4410
    },
    {
      "epoch": 1.367256979351945,
      "grad_norm": 0.9356518387794495,
      "learning_rate": 3.63335396039604e-05,
      "loss": 8.1797,
      "step": 4420
    },
    {
      "epoch": 1.3703503209341892,
      "grad_norm": 0.5273188948631287,
      "learning_rate": 3.6302599009900986e-05,
      "loss": 8.1891,
      "step": 4430
    },
    {
      "epoch": 1.3734436625164332,
      "grad_norm": 1.5884488821029663,
      "learning_rate": 3.627165841584159e-05,
      "loss": 8.1714,
      "step": 4440
    },
    {
      "epoch": 1.3765370040986775,
      "grad_norm": 0.8049890995025635,
      "learning_rate": 3.624071782178218e-05,
      "loss": 8.1633,
      "step": 4450
    },
    {
      "epoch": 1.3796303456809218,
      "grad_norm": 0.8034205436706543,
      "learning_rate": 3.620977722772277e-05,
      "loss": 8.1739,
      "step": 4460
    },
    {
      "epoch": 1.382723687263166,
      "grad_norm": 0.9518395066261292,
      "learning_rate": 3.617883663366337e-05,
      "loss": 8.1705,
      "step": 4470
    },
    {
      "epoch": 1.3858170288454104,
      "grad_norm": 0.7396552562713623,
      "learning_rate": 3.614789603960396e-05,
      "loss": 8.1652,
      "step": 4480
    },
    {
      "epoch": 1.3889103704276544,
      "grad_norm": 0.6566534638404846,
      "learning_rate": 3.611695544554455e-05,
      "loss": 8.1705,
      "step": 4490
    },
    {
      "epoch": 1.3920037120098987,
      "grad_norm": 0.8734368681907654,
      "learning_rate": 3.6086014851485154e-05,
      "loss": 8.1588,
      "step": 4500
    },
    {
      "epoch": 1.395097053592143,
      "grad_norm": 1.3763115406036377,
      "learning_rate": 3.605507425742574e-05,
      "loss": 8.1649,
      "step": 4510
    },
    {
      "epoch": 1.398190395174387,
      "grad_norm": 0.5389499068260193,
      "learning_rate": 3.602413366336634e-05,
      "loss": 8.1608,
      "step": 4520
    },
    {
      "epoch": 1.4012837367566313,
      "grad_norm": 0.9031329154968262,
      "learning_rate": 3.5993193069306926e-05,
      "loss": 8.1834,
      "step": 4530
    },
    {
      "epoch": 1.4043770783388756,
      "grad_norm": 0.726360559463501,
      "learning_rate": 3.596225247524753e-05,
      "loss": 8.1831,
      "step": 4540
    },
    {
      "epoch": 1.4074704199211197,
      "grad_norm": 0.987770676612854,
      "learning_rate": 3.5931311881188124e-05,
      "loss": 8.1612,
      "step": 4550
    },
    {
      "epoch": 1.410563761503364,
      "grad_norm": 0.9864580631256104,
      "learning_rate": 3.590037128712871e-05,
      "loss": 8.1818,
      "step": 4560
    },
    {
      "epoch": 1.4136571030856082,
      "grad_norm": 0.8496710658073425,
      "learning_rate": 3.5869430693069315e-05,
      "loss": 8.1829,
      "step": 4570
    },
    {
      "epoch": 1.4167504446678525,
      "grad_norm": 1.5568605661392212,
      "learning_rate": 3.58384900990099e-05,
      "loss": 8.1683,
      "step": 4580
    },
    {
      "epoch": 1.4198437862500968,
      "grad_norm": 1.9154937267303467,
      "learning_rate": 3.58075495049505e-05,
      "loss": 8.1567,
      "step": 4590
    },
    {
      "epoch": 1.4229371278323408,
      "grad_norm": 2.359445095062256,
      "learning_rate": 3.5776608910891094e-05,
      "loss": 8.1525,
      "step": 4600
    },
    {
      "epoch": 1.4260304694145851,
      "grad_norm": 1.1888504028320312,
      "learning_rate": 3.574566831683169e-05,
      "loss": 8.1612,
      "step": 4610
    },
    {
      "epoch": 1.4291238109968294,
      "grad_norm": 0.7650144100189209,
      "learning_rate": 3.571472772277228e-05,
      "loss": 8.1683,
      "step": 4620
    },
    {
      "epoch": 1.4322171525790734,
      "grad_norm": 0.773417592048645,
      "learning_rate": 3.5683787128712873e-05,
      "loss": 8.1754,
      "step": 4630
    },
    {
      "epoch": 1.4353104941613177,
      "grad_norm": 0.7749471664428711,
      "learning_rate": 3.565284653465347e-05,
      "loss": 8.173,
      "step": 4640
    },
    {
      "epoch": 1.438403835743562,
      "grad_norm": 0.6494299173355103,
      "learning_rate": 3.5621905940594064e-05,
      "loss": 8.1753,
      "step": 4650
    },
    {
      "epoch": 1.441497177325806,
      "grad_norm": 0.673133134841919,
      "learning_rate": 3.559096534653465e-05,
      "loss": 8.1666,
      "step": 4660
    },
    {
      "epoch": 1.4445905189080503,
      "grad_norm": 0.5829264521598816,
      "learning_rate": 3.5560024752475255e-05,
      "loss": 8.1766,
      "step": 4670
    },
    {
      "epoch": 1.4476838604902946,
      "grad_norm": 0.7950640916824341,
      "learning_rate": 3.5529084158415844e-05,
      "loss": 8.1733,
      "step": 4680
    },
    {
      "epoch": 1.450777202072539,
      "grad_norm": 1.028467059135437,
      "learning_rate": 3.549814356435644e-05,
      "loss": 8.1629,
      "step": 4690
    },
    {
      "epoch": 1.4538705436547832,
      "grad_norm": 0.8920987844467163,
      "learning_rate": 3.5467202970297034e-05,
      "loss": 8.1721,
      "step": 4700
    },
    {
      "epoch": 1.4569638852370272,
      "grad_norm": 1.1075754165649414,
      "learning_rate": 3.543626237623763e-05,
      "loss": 8.1715,
      "step": 4710
    },
    {
      "epoch": 1.4600572268192715,
      "grad_norm": 0.9275581240653992,
      "learning_rate": 3.540532178217822e-05,
      "loss": 8.1765,
      "step": 4720
    },
    {
      "epoch": 1.4631505684015158,
      "grad_norm": 0.6811912059783936,
      "learning_rate": 3.5374381188118814e-05,
      "loss": 8.1715,
      "step": 4730
    },
    {
      "epoch": 1.4662439099837599,
      "grad_norm": 1.149160385131836,
      "learning_rate": 3.534344059405941e-05,
      "loss": 8.1683,
      "step": 4740
    },
    {
      "epoch": 1.4693372515660041,
      "grad_norm": 0.5867406129837036,
      "learning_rate": 3.5312500000000005e-05,
      "loss": 8.1667,
      "step": 4750
    },
    {
      "epoch": 1.4724305931482484,
      "grad_norm": 0.9364137053489685,
      "learning_rate": 3.528155940594059e-05,
      "loss": 8.1744,
      "step": 4760
    },
    {
      "epoch": 1.4755239347304925,
      "grad_norm": 0.628791332244873,
      "learning_rate": 3.5250618811881195e-05,
      "loss": 8.1709,
      "step": 4770
    },
    {
      "epoch": 1.4786172763127368,
      "grad_norm": 0.8404129147529602,
      "learning_rate": 3.5219678217821784e-05,
      "loss": 8.179,
      "step": 4780
    },
    {
      "epoch": 1.481710617894981,
      "grad_norm": 1.3416937589645386,
      "learning_rate": 3.518873762376238e-05,
      "loss": 8.1721,
      "step": 4790
    },
    {
      "epoch": 1.4848039594772253,
      "grad_norm": 0.6549462080001831,
      "learning_rate": 3.515779702970297e-05,
      "loss": 8.156,
      "step": 4800
    },
    {
      "epoch": 1.4878973010594696,
      "grad_norm": 1.0403521060943604,
      "learning_rate": 3.512685643564357e-05,
      "loss": 8.1714,
      "step": 4810
    },
    {
      "epoch": 1.4909906426417137,
      "grad_norm": 0.7095990180969238,
      "learning_rate": 3.509591584158416e-05,
      "loss": 8.168,
      "step": 4820
    },
    {
      "epoch": 1.494083984223958,
      "grad_norm": 0.7644259929656982,
      "learning_rate": 3.5064975247524754e-05,
      "loss": 8.1823,
      "step": 4830
    },
    {
      "epoch": 1.4971773258062022,
      "grad_norm": 0.5413919687271118,
      "learning_rate": 3.503403465346535e-05,
      "loss": 8.1622,
      "step": 4840
    },
    {
      "epoch": 1.5002706673884463,
      "grad_norm": 0.6657000184059143,
      "learning_rate": 3.5003094059405945e-05,
      "loss": 8.1561,
      "step": 4850
    },
    {
      "epoch": 1.5033640089706906,
      "grad_norm": 1.2642089128494263,
      "learning_rate": 3.4972153465346533e-05,
      "loss": 8.1566,
      "step": 4860
    },
    {
      "epoch": 1.5064573505529348,
      "grad_norm": 0.7570921778678894,
      "learning_rate": 3.4941212871287136e-05,
      "loss": 8.1686,
      "step": 4870
    },
    {
      "epoch": 1.509550692135179,
      "grad_norm": 0.6579694151878357,
      "learning_rate": 3.4910272277227724e-05,
      "loss": 8.1738,
      "step": 4880
    },
    {
      "epoch": 1.5126440337174234,
      "grad_norm": 1.0187548398971558,
      "learning_rate": 3.487933168316832e-05,
      "loss": 8.1708,
      "step": 4890
    },
    {
      "epoch": 1.5157373752996675,
      "grad_norm": 0.6494972705841064,
      "learning_rate": 3.484839108910891e-05,
      "loss": 8.1735,
      "step": 4900
    },
    {
      "epoch": 1.5188307168819117,
      "grad_norm": 1.3628088235855103,
      "learning_rate": 3.481745049504951e-05,
      "loss": 8.1639,
      "step": 4910
    },
    {
      "epoch": 1.521924058464156,
      "grad_norm": 1.2158830165863037,
      "learning_rate": 3.47865099009901e-05,
      "loss": 8.1749,
      "step": 4920
    },
    {
      "epoch": 1.5250174000464,
      "grad_norm": 0.6442435383796692,
      "learning_rate": 3.4755569306930694e-05,
      "loss": 8.1531,
      "step": 4930
    },
    {
      "epoch": 1.5281107416286444,
      "grad_norm": 0.6110523343086243,
      "learning_rate": 3.472462871287129e-05,
      "loss": 8.1767,
      "step": 4940
    },
    {
      "epoch": 1.5312040832108886,
      "grad_norm": 1.036623239517212,
      "learning_rate": 3.4693688118811885e-05,
      "loss": 8.1685,
      "step": 4950
    },
    {
      "epoch": 1.5342974247931327,
      "grad_norm": 1.135884404182434,
      "learning_rate": 3.4662747524752474e-05,
      "loss": 8.1831,
      "step": 4960
    },
    {
      "epoch": 1.537390766375377,
      "grad_norm": 0.7258022427558899,
      "learning_rate": 3.4631806930693076e-05,
      "loss": 8.1704,
      "step": 4970
    },
    {
      "epoch": 1.5404841079576213,
      "grad_norm": 0.7321982979774475,
      "learning_rate": 3.4600866336633665e-05,
      "loss": 8.1783,
      "step": 4980
    },
    {
      "epoch": 1.5435774495398653,
      "grad_norm": 1.108925700187683,
      "learning_rate": 3.456992574257426e-05,
      "loss": 8.1712,
      "step": 4990
    },
    {
      "epoch": 1.5466707911221098,
      "grad_norm": 0.5284704566001892,
      "learning_rate": 3.453898514851485e-05,
      "loss": 8.159,
      "step": 5000
    },
    {
      "epoch": 1.5497641327043539,
      "grad_norm": 1.830784797668457,
      "learning_rate": 3.450804455445545e-05,
      "loss": 8.1824,
      "step": 5010
    },
    {
      "epoch": 1.5528574742865981,
      "grad_norm": 1.6082335710525513,
      "learning_rate": 3.447710396039604e-05,
      "loss": 8.1504,
      "step": 5020
    },
    {
      "epoch": 1.5559508158688424,
      "grad_norm": 0.9002389907836914,
      "learning_rate": 3.4446163366336635e-05,
      "loss": 8.169,
      "step": 5030
    },
    {
      "epoch": 1.5590441574510865,
      "grad_norm": 0.7099100351333618,
      "learning_rate": 3.441522277227723e-05,
      "loss": 8.1614,
      "step": 5040
    },
    {
      "epoch": 1.5621374990333308,
      "grad_norm": 0.91901034116745,
      "learning_rate": 3.4384282178217825e-05,
      "loss": 8.1562,
      "step": 5050
    },
    {
      "epoch": 1.565230840615575,
      "grad_norm": 0.7427526116371155,
      "learning_rate": 3.4353341584158414e-05,
      "loss": 8.1818,
      "step": 5060
    },
    {
      "epoch": 1.568324182197819,
      "grad_norm": 0.7421169281005859,
      "learning_rate": 3.4322400990099016e-05,
      "loss": 8.1479,
      "step": 5070
    },
    {
      "epoch": 1.5714175237800634,
      "grad_norm": 0.693516194820404,
      "learning_rate": 3.4291460396039605e-05,
      "loss": 8.1683,
      "step": 5080
    },
    {
      "epoch": 1.5745108653623077,
      "grad_norm": 0.6199151277542114,
      "learning_rate": 3.42605198019802e-05,
      "loss": 8.1772,
      "step": 5090
    },
    {
      "epoch": 1.5776042069445517,
      "grad_norm": 0.8506600856781006,
      "learning_rate": 3.422957920792079e-05,
      "loss": 8.1751,
      "step": 5100
    },
    {
      "epoch": 1.5806975485267962,
      "grad_norm": 0.8408052325248718,
      "learning_rate": 3.419863861386139e-05,
      "loss": 8.1779,
      "step": 5110
    },
    {
      "epoch": 1.5837908901090403,
      "grad_norm": 0.6840189099311829,
      "learning_rate": 3.416769801980198e-05,
      "loss": 8.1611,
      "step": 5120
    },
    {
      "epoch": 1.5868842316912846,
      "grad_norm": 0.624847412109375,
      "learning_rate": 3.4136757425742575e-05,
      "loss": 8.1609,
      "step": 5130
    },
    {
      "epoch": 1.5899775732735288,
      "grad_norm": 0.6341762542724609,
      "learning_rate": 3.410581683168317e-05,
      "loss": 8.1673,
      "step": 5140
    },
    {
      "epoch": 1.593070914855773,
      "grad_norm": 1.453444004058838,
      "learning_rate": 3.4074876237623766e-05,
      "loss": 8.1828,
      "step": 5150
    },
    {
      "epoch": 1.5961642564380172,
      "grad_norm": 1.1536788940429688,
      "learning_rate": 3.4043935643564354e-05,
      "loss": 8.1663,
      "step": 5160
    },
    {
      "epoch": 1.5992575980202615,
      "grad_norm": 0.6205885410308838,
      "learning_rate": 3.401299504950495e-05,
      "loss": 8.1555,
      "step": 5170
    },
    {
      "epoch": 1.6023509396025055,
      "grad_norm": 0.7434297204017639,
      "learning_rate": 3.3982054455445545e-05,
      "loss": 8.1773,
      "step": 5180
    },
    {
      "epoch": 1.6054442811847498,
      "grad_norm": 0.7958365082740784,
      "learning_rate": 3.395111386138614e-05,
      "loss": 8.1703,
      "step": 5190
    },
    {
      "epoch": 1.608537622766994,
      "grad_norm": 1.394105076789856,
      "learning_rate": 3.392017326732673e-05,
      "loss": 8.1645,
      "step": 5200
    },
    {
      "epoch": 1.6116309643492381,
      "grad_norm": 0.7183541059494019,
      "learning_rate": 3.388923267326733e-05,
      "loss": 8.175,
      "step": 5210
    },
    {
      "epoch": 1.6147243059314826,
      "grad_norm": 1.2752171754837036,
      "learning_rate": 3.385829207920792e-05,
      "loss": 8.1518,
      "step": 5220
    },
    {
      "epoch": 1.6178176475137267,
      "grad_norm": 0.8980184197425842,
      "learning_rate": 3.3827351485148515e-05,
      "loss": 8.1762,
      "step": 5230
    },
    {
      "epoch": 1.620910989095971,
      "grad_norm": 0.9900051951408386,
      "learning_rate": 3.379641089108911e-05,
      "loss": 8.1836,
      "step": 5240
    },
    {
      "epoch": 1.6240043306782153,
      "grad_norm": 0.9499492645263672,
      "learning_rate": 3.3765470297029706e-05,
      "loss": 8.1739,
      "step": 5250
    },
    {
      "epoch": 1.6270976722604593,
      "grad_norm": 1.594128131866455,
      "learning_rate": 3.3734529702970295e-05,
      "loss": 8.1754,
      "step": 5260
    },
    {
      "epoch": 1.6301910138427036,
      "grad_norm": 0.9980596303939819,
      "learning_rate": 3.370358910891089e-05,
      "loss": 8.1728,
      "step": 5270
    },
    {
      "epoch": 1.6332843554249479,
      "grad_norm": 1.5016725063323975,
      "learning_rate": 3.3672648514851486e-05,
      "loss": 8.1959,
      "step": 5280
    },
    {
      "epoch": 1.636377697007192,
      "grad_norm": 0.5902643799781799,
      "learning_rate": 3.364170792079208e-05,
      "loss": 8.162,
      "step": 5290
    },
    {
      "epoch": 1.6394710385894362,
      "grad_norm": 0.7083823084831238,
      "learning_rate": 3.361076732673267e-05,
      "loss": 8.1684,
      "step": 5300
    },
    {
      "epoch": 1.6425643801716805,
      "grad_norm": 0.9555543661117554,
      "learning_rate": 3.357982673267327e-05,
      "loss": 8.1653,
      "step": 5310
    },
    {
      "epoch": 1.6456577217539246,
      "grad_norm": 0.49654459953308105,
      "learning_rate": 3.354888613861386e-05,
      "loss": 8.1635,
      "step": 5320
    },
    {
      "epoch": 1.648751063336169,
      "grad_norm": 1.0117039680480957,
      "learning_rate": 3.3517945544554456e-05,
      "loss": 8.1552,
      "step": 5330
    },
    {
      "epoch": 1.6518444049184131,
      "grad_norm": 0.8782178163528442,
      "learning_rate": 3.348700495049505e-05,
      "loss": 8.1592,
      "step": 5340
    },
    {
      "epoch": 1.6549377465006572,
      "grad_norm": 0.7171444296836853,
      "learning_rate": 3.3456064356435646e-05,
      "loss": 8.166,
      "step": 5350
    },
    {
      "epoch": 1.6580310880829017,
      "grad_norm": 0.9967385530471802,
      "learning_rate": 3.3425123762376235e-05,
      "loss": 8.1615,
      "step": 5360
    },
    {
      "epoch": 1.6611244296651457,
      "grad_norm": 0.9756525158882141,
      "learning_rate": 3.339418316831683e-05,
      "loss": 8.1733,
      "step": 5370
    },
    {
      "epoch": 1.66421777124739,
      "grad_norm": 1.3286625146865845,
      "learning_rate": 3.3363242574257426e-05,
      "loss": 8.1769,
      "step": 5380
    },
    {
      "epoch": 1.6673111128296343,
      "grad_norm": 1.2493183612823486,
      "learning_rate": 3.333230198019802e-05,
      "loss": 8.1651,
      "step": 5390
    },
    {
      "epoch": 1.6704044544118783,
      "grad_norm": 0.650644063949585,
      "learning_rate": 3.330136138613861e-05,
      "loss": 8.1739,
      "step": 5400
    },
    {
      "epoch": 1.6734977959941226,
      "grad_norm": 0.6978752017021179,
      "learning_rate": 3.327042079207921e-05,
      "loss": 8.1537,
      "step": 5410
    },
    {
      "epoch": 1.676591137576367,
      "grad_norm": 0.9390348792076111,
      "learning_rate": 3.32394801980198e-05,
      "loss": 8.1416,
      "step": 5420
    },
    {
      "epoch": 1.679684479158611,
      "grad_norm": 0.7109711766242981,
      "learning_rate": 3.3208539603960396e-05,
      "loss": 8.1503,
      "step": 5430
    },
    {
      "epoch": 1.6827778207408555,
      "grad_norm": 1.0785995721817017,
      "learning_rate": 3.317759900990099e-05,
      "loss": 8.1772,
      "step": 5440
    },
    {
      "epoch": 1.6858711623230995,
      "grad_norm": 1.2778241634368896,
      "learning_rate": 3.314665841584159e-05,
      "loss": 8.178,
      "step": 5450
    },
    {
      "epoch": 1.6889645039053436,
      "grad_norm": 0.6923041343688965,
      "learning_rate": 3.3115717821782175e-05,
      "loss": 8.1725,
      "step": 5460
    },
    {
      "epoch": 1.692057845487588,
      "grad_norm": 1.0062930583953857,
      "learning_rate": 3.308477722772277e-05,
      "loss": 8.179,
      "step": 5470
    },
    {
      "epoch": 1.6951511870698321,
      "grad_norm": 1.0806899070739746,
      "learning_rate": 3.3053836633663366e-05,
      "loss": 8.176,
      "step": 5480
    },
    {
      "epoch": 1.6982445286520764,
      "grad_norm": 0.7845777273178101,
      "learning_rate": 3.302289603960396e-05,
      "loss": 8.176,
      "step": 5490
    },
    {
      "epoch": 1.7013378702343207,
      "grad_norm": 0.81512850522995,
      "learning_rate": 3.299195544554455e-05,
      "loss": 8.1601,
      "step": 5500
    },
    {
      "epoch": 1.7044312118165648,
      "grad_norm": 0.7820336818695068,
      "learning_rate": 3.296101485148515e-05,
      "loss": 8.1617,
      "step": 5510
    },
    {
      "epoch": 1.707524553398809,
      "grad_norm": 0.8367799520492554,
      "learning_rate": 3.293007425742574e-05,
      "loss": 8.1832,
      "step": 5520
    },
    {
      "epoch": 1.7106178949810533,
      "grad_norm": 0.9358869194984436,
      "learning_rate": 3.2899133663366336e-05,
      "loss": 8.1586,
      "step": 5530
    },
    {
      "epoch": 1.7137112365632974,
      "grad_norm": 0.6722541451454163,
      "learning_rate": 3.286819306930693e-05,
      "loss": 8.1655,
      "step": 5540
    },
    {
      "epoch": 1.7168045781455419,
      "grad_norm": 0.6855424642562866,
      "learning_rate": 3.283725247524753e-05,
      "loss": 8.1733,
      "step": 5550
    },
    {
      "epoch": 1.719897919727786,
      "grad_norm": 1.004778504371643,
      "learning_rate": 3.280631188118812e-05,
      "loss": 8.161,
      "step": 5560
    },
    {
      "epoch": 1.72299126131003,
      "grad_norm": 0.9205971956253052,
      "learning_rate": 3.277537128712871e-05,
      "loss": 8.1722,
      "step": 5570
    },
    {
      "epoch": 1.7260846028922745,
      "grad_norm": 0.572397768497467,
      "learning_rate": 3.274443069306931e-05,
      "loss": 8.17,
      "step": 5580
    },
    {
      "epoch": 1.7291779444745186,
      "grad_norm": 1.0157551765441895,
      "learning_rate": 3.27134900990099e-05,
      "loss": 8.1613,
      "step": 5590
    },
    {
      "epoch": 1.7322712860567628,
      "grad_norm": 1.26455819606781,
      "learning_rate": 3.26825495049505e-05,
      "loss": 8.1859,
      "step": 5600
    },
    {
      "epoch": 1.7353646276390071,
      "grad_norm": 0.8484792709350586,
      "learning_rate": 3.265160891089109e-05,
      "loss": 8.1688,
      "step": 5610
    },
    {
      "epoch": 1.7384579692212512,
      "grad_norm": 1.1313810348510742,
      "learning_rate": 3.262066831683169e-05,
      "loss": 8.1737,
      "step": 5620
    },
    {
      "epoch": 1.7415513108034955,
      "grad_norm": 0.6978829503059387,
      "learning_rate": 3.259282178217822e-05,
      "loss": 8.1746,
      "step": 5630
    },
    {
      "epoch": 1.7446446523857397,
      "grad_norm": 1.0052316188812256,
      "learning_rate": 3.256188118811881e-05,
      "loss": 8.1788,
      "step": 5640
    },
    {
      "epoch": 1.7477379939679838,
      "grad_norm": 0.8323175311088562,
      "learning_rate": 3.253094059405941e-05,
      "loss": 8.1659,
      "step": 5650
    },
    {
      "epoch": 1.7508313355502283,
      "grad_norm": 0.907998263835907,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 8.1722,
      "step": 5660
    },
    {
      "epoch": 1.7539246771324724,
      "grad_norm": 0.90989089012146,
      "learning_rate": 3.246905940594059e-05,
      "loss": 8.18,
      "step": 5670
    },
    {
      "epoch": 1.7570180187147164,
      "grad_norm": 1.2065491676330566,
      "learning_rate": 3.243811881188119e-05,
      "loss": 8.1813,
      "step": 5680
    },
    {
      "epoch": 1.760111360296961,
      "grad_norm": 0.7596132755279541,
      "learning_rate": 3.240717821782178e-05,
      "loss": 8.1716,
      "step": 5690
    },
    {
      "epoch": 1.763204701879205,
      "grad_norm": 0.8433894515037537,
      "learning_rate": 3.237623762376238e-05,
      "loss": 8.1656,
      "step": 5700
    },
    {
      "epoch": 1.7662980434614493,
      "grad_norm": 0.9066236019134521,
      "learning_rate": 3.234529702970297e-05,
      "loss": 8.1789,
      "step": 5710
    },
    {
      "epoch": 1.7693913850436935,
      "grad_norm": 0.6621925234794617,
      "learning_rate": 3.231435643564357e-05,
      "loss": 8.1674,
      "step": 5720
    },
    {
      "epoch": 1.7724847266259376,
      "grad_norm": 0.7098196744918823,
      "learning_rate": 3.228341584158416e-05,
      "loss": 8.1626,
      "step": 5730
    },
    {
      "epoch": 1.7755780682081819,
      "grad_norm": 0.497574120759964,
      "learning_rate": 3.2252475247524753e-05,
      "loss": 8.173,
      "step": 5740
    },
    {
      "epoch": 1.7786714097904262,
      "grad_norm": 0.7098990678787231,
      "learning_rate": 3.222153465346535e-05,
      "loss": 8.1546,
      "step": 5750
    },
    {
      "epoch": 1.7817647513726702,
      "grad_norm": 0.9132131338119507,
      "learning_rate": 3.2190594059405944e-05,
      "loss": 8.1604,
      "step": 5760
    },
    {
      "epoch": 1.7848580929549147,
      "grad_norm": 0.8173969388008118,
      "learning_rate": 3.215965346534653e-05,
      "loss": 8.1703,
      "step": 5770
    },
    {
      "epoch": 1.7879514345371588,
      "grad_norm": 0.676900327205658,
      "learning_rate": 3.212871287128713e-05,
      "loss": 8.1643,
      "step": 5780
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 0.8337453007698059,
      "learning_rate": 3.2097772277227724e-05,
      "loss": 8.1678,
      "step": 5790
    },
    {
      "epoch": 1.7941381177016473,
      "grad_norm": 0.9127116799354553,
      "learning_rate": 3.206683168316832e-05,
      "loss": 8.1652,
      "step": 5800
    },
    {
      "epoch": 1.7972314592838914,
      "grad_norm": 0.5822204947471619,
      "learning_rate": 3.203589108910891e-05,
      "loss": 8.1677,
      "step": 5810
    },
    {
      "epoch": 1.8003248008661357,
      "grad_norm": 0.7787448167800903,
      "learning_rate": 3.200495049504951e-05,
      "loss": 8.1672,
      "step": 5820
    },
    {
      "epoch": 1.80341814244838,
      "grad_norm": 0.5487079620361328,
      "learning_rate": 3.19740099009901e-05,
      "loss": 8.1581,
      "step": 5830
    },
    {
      "epoch": 1.806511484030624,
      "grad_norm": 0.7901445031166077,
      "learning_rate": 3.1943069306930694e-05,
      "loss": 8.1562,
      "step": 5840
    },
    {
      "epoch": 1.8096048256128683,
      "grad_norm": 0.6084808707237244,
      "learning_rate": 3.191212871287129e-05,
      "loss": 8.1638,
      "step": 5850
    },
    {
      "epoch": 1.8126981671951126,
      "grad_norm": 0.6935718059539795,
      "learning_rate": 3.1881188118811885e-05,
      "loss": 8.1782,
      "step": 5860
    },
    {
      "epoch": 1.8157915087773566,
      "grad_norm": 1.0455889701843262,
      "learning_rate": 3.185024752475247e-05,
      "loss": 8.1525,
      "step": 5870
    },
    {
      "epoch": 1.8188848503596011,
      "grad_norm": 1.3935803174972534,
      "learning_rate": 3.181930693069307e-05,
      "loss": 8.1758,
      "step": 5880
    },
    {
      "epoch": 1.8219781919418452,
      "grad_norm": 1.8674882650375366,
      "learning_rate": 3.1788366336633664e-05,
      "loss": 8.1761,
      "step": 5890
    },
    {
      "epoch": 1.8250715335240892,
      "grad_norm": 1.8687641620635986,
      "learning_rate": 3.175742574257426e-05,
      "loss": 8.1575,
      "step": 5900
    },
    {
      "epoch": 1.8281648751063337,
      "grad_norm": 1.4557090997695923,
      "learning_rate": 3.172648514851485e-05,
      "loss": 8.1539,
      "step": 5910
    },
    {
      "epoch": 1.8312582166885778,
      "grad_norm": 1.0592007637023926,
      "learning_rate": 3.169554455445545e-05,
      "loss": 8.1688,
      "step": 5920
    },
    {
      "epoch": 1.834351558270822,
      "grad_norm": 1.8422770500183105,
      "learning_rate": 3.166460396039604e-05,
      "loss": 8.1682,
      "step": 5930
    },
    {
      "epoch": 1.8374448998530664,
      "grad_norm": 1.6825525760650635,
      "learning_rate": 3.1633663366336634e-05,
      "loss": 8.1757,
      "step": 5940
    },
    {
      "epoch": 1.8405382414353104,
      "grad_norm": 0.7014016509056091,
      "learning_rate": 3.160272277227723e-05,
      "loss": 8.1785,
      "step": 5950
    },
    {
      "epoch": 1.8436315830175547,
      "grad_norm": 0.8315780162811279,
      "learning_rate": 3.1571782178217825e-05,
      "loss": 8.1707,
      "step": 5960
    },
    {
      "epoch": 1.846724924599799,
      "grad_norm": 0.673385739326477,
      "learning_rate": 3.1540841584158413e-05,
      "loss": 8.1694,
      "step": 5970
    },
    {
      "epoch": 1.849818266182043,
      "grad_norm": 0.7577770352363586,
      "learning_rate": 3.150990099009901e-05,
      "loss": 8.156,
      "step": 5980
    },
    {
      "epoch": 1.8529116077642873,
      "grad_norm": 1.0061264038085938,
      "learning_rate": 3.1478960396039604e-05,
      "loss": 8.1631,
      "step": 5990
    },
    {
      "epoch": 1.8560049493465316,
      "grad_norm": 1.0586001873016357,
      "learning_rate": 3.14480198019802e-05,
      "loss": 8.1502,
      "step": 6000
    },
    {
      "epoch": 1.8590982909287757,
      "grad_norm": 0.6203423738479614,
      "learning_rate": 3.141707920792079e-05,
      "loss": 8.1797,
      "step": 6010
    },
    {
      "epoch": 1.8621916325110202,
      "grad_norm": 0.9820427894592285,
      "learning_rate": 3.138613861386139e-05,
      "loss": 8.1648,
      "step": 6020
    },
    {
      "epoch": 1.8652849740932642,
      "grad_norm": 0.8698956966400146,
      "learning_rate": 3.135519801980198e-05,
      "loss": 8.1695,
      "step": 6030
    },
    {
      "epoch": 1.8683783156755085,
      "grad_norm": 1.0272184610366821,
      "learning_rate": 3.1324257425742574e-05,
      "loss": 8.1791,
      "step": 6040
    },
    {
      "epoch": 1.8714716572577528,
      "grad_norm": 0.8092179894447327,
      "learning_rate": 3.129331683168317e-05,
      "loss": 8.1563,
      "step": 6050
    },
    {
      "epoch": 1.8745649988399968,
      "grad_norm": 0.8402934670448303,
      "learning_rate": 3.1262376237623765e-05,
      "loss": 8.1743,
      "step": 6060
    },
    {
      "epoch": 1.8776583404222411,
      "grad_norm": 1.0219727754592896,
      "learning_rate": 3.123143564356436e-05,
      "loss": 8.1753,
      "step": 6070
    },
    {
      "epoch": 1.8807516820044854,
      "grad_norm": 0.9831115007400513,
      "learning_rate": 3.120049504950495e-05,
      "loss": 8.1766,
      "step": 6080
    },
    {
      "epoch": 1.8838450235867295,
      "grad_norm": 0.42784062027931213,
      "learning_rate": 3.116955445544555e-05,
      "loss": 8.1617,
      "step": 6090
    },
    {
      "epoch": 1.8869383651689737,
      "grad_norm": 0.6545125842094421,
      "learning_rate": 3.113861386138614e-05,
      "loss": 8.1467,
      "step": 6100
    },
    {
      "epoch": 1.890031706751218,
      "grad_norm": 0.6908999085426331,
      "learning_rate": 3.1107673267326735e-05,
      "loss": 8.1599,
      "step": 6110
    },
    {
      "epoch": 1.893125048333462,
      "grad_norm": 0.546354353427887,
      "learning_rate": 3.107673267326733e-05,
      "loss": 8.1741,
      "step": 6120
    },
    {
      "epoch": 1.8962183899157066,
      "grad_norm": 0.6718780994415283,
      "learning_rate": 3.1045792079207926e-05,
      "loss": 8.1536,
      "step": 6130
    },
    {
      "epoch": 1.8993117314979506,
      "grad_norm": 0.8059448003768921,
      "learning_rate": 3.1014851485148515e-05,
      "loss": 8.1708,
      "step": 6140
    },
    {
      "epoch": 1.902405073080195,
      "grad_norm": 0.6182971596717834,
      "learning_rate": 3.098391089108911e-05,
      "loss": 8.1726,
      "step": 6150
    },
    {
      "epoch": 1.9054984146624392,
      "grad_norm": 0.7981633543968201,
      "learning_rate": 3.0952970297029706e-05,
      "loss": 8.1542,
      "step": 6160
    },
    {
      "epoch": 1.9085917562446832,
      "grad_norm": 0.6761003136634827,
      "learning_rate": 3.09220297029703e-05,
      "loss": 8.1601,
      "step": 6170
    },
    {
      "epoch": 1.9116850978269275,
      "grad_norm": 0.79880291223526,
      "learning_rate": 3.089108910891089e-05,
      "loss": 8.1725,
      "step": 6180
    },
    {
      "epoch": 1.9147784394091718,
      "grad_norm": 0.6769899725914001,
      "learning_rate": 3.086014851485149e-05,
      "loss": 8.1708,
      "step": 6190
    },
    {
      "epoch": 1.9178717809914159,
      "grad_norm": 0.5824911594390869,
      "learning_rate": 3.082920792079208e-05,
      "loss": 8.1796,
      "step": 6200
    },
    {
      "epoch": 1.9209651225736601,
      "grad_norm": 0.7731926441192627,
      "learning_rate": 3.0798267326732676e-05,
      "loss": 8.1647,
      "step": 6210
    },
    {
      "epoch": 1.9240584641559044,
      "grad_norm": 0.6705347895622253,
      "learning_rate": 3.076732673267327e-05,
      "loss": 8.1553,
      "step": 6220
    },
    {
      "epoch": 1.9271518057381485,
      "grad_norm": 0.8091216087341309,
      "learning_rate": 3.0736386138613866e-05,
      "loss": 8.1697,
      "step": 6230
    },
    {
      "epoch": 1.930245147320393,
      "grad_norm": 0.8407959938049316,
      "learning_rate": 3.0705445544554455e-05,
      "loss": 8.1808,
      "step": 6240
    },
    {
      "epoch": 1.933338488902637,
      "grad_norm": 0.7498672604560852,
      "learning_rate": 3.067450495049505e-05,
      "loss": 8.1439,
      "step": 6250
    },
    {
      "epoch": 1.9364318304848813,
      "grad_norm": 1.1580160856246948,
      "learning_rate": 3.0643564356435646e-05,
      "loss": 8.1694,
      "step": 6260
    },
    {
      "epoch": 1.9395251720671256,
      "grad_norm": 0.732685923576355,
      "learning_rate": 3.061262376237624e-05,
      "loss": 8.166,
      "step": 6270
    },
    {
      "epoch": 1.9426185136493697,
      "grad_norm": 0.8561883568763733,
      "learning_rate": 3.058168316831683e-05,
      "loss": 8.182,
      "step": 6280
    },
    {
      "epoch": 1.945711855231614,
      "grad_norm": 0.5598315596580505,
      "learning_rate": 3.055074257425743e-05,
      "loss": 8.1616,
      "step": 6290
    },
    {
      "epoch": 1.9488051968138582,
      "grad_norm": 0.6322380304336548,
      "learning_rate": 3.051980198019802e-05,
      "loss": 8.1677,
      "step": 6300
    },
    {
      "epoch": 1.9518985383961023,
      "grad_norm": 0.722793459892273,
      "learning_rate": 3.0488861386138616e-05,
      "loss": 8.166,
      "step": 6310
    },
    {
      "epoch": 1.9549918799783466,
      "grad_norm": 0.9101493954658508,
      "learning_rate": 3.0457920792079208e-05,
      "loss": 8.1549,
      "step": 6320
    },
    {
      "epoch": 1.9580852215605908,
      "grad_norm": 1.1809587478637695,
      "learning_rate": 3.0426980198019807e-05,
      "loss": 8.1551,
      "step": 6330
    },
    {
      "epoch": 1.961178563142835,
      "grad_norm": 0.9287338852882385,
      "learning_rate": 3.0396039603960395e-05,
      "loss": 8.1741,
      "step": 6340
    },
    {
      "epoch": 1.9642719047250794,
      "grad_norm": 0.6922351121902466,
      "learning_rate": 3.0365099009900994e-05,
      "loss": 8.1644,
      "step": 6350
    },
    {
      "epoch": 1.9673652463073235,
      "grad_norm": 0.626541018486023,
      "learning_rate": 3.0334158415841586e-05,
      "loss": 8.1686,
      "step": 6360
    },
    {
      "epoch": 1.9704585878895677,
      "grad_norm": 0.8416675925254822,
      "learning_rate": 3.030321782178218e-05,
      "loss": 8.1739,
      "step": 6370
    },
    {
      "epoch": 1.973551929471812,
      "grad_norm": 0.8686550259590149,
      "learning_rate": 3.0272277227722774e-05,
      "loss": 8.176,
      "step": 6380
    },
    {
      "epoch": 1.976645271054056,
      "grad_norm": 0.6437850594520569,
      "learning_rate": 3.024133663366337e-05,
      "loss": 8.1645,
      "step": 6390
    },
    {
      "epoch": 1.9797386126363004,
      "grad_norm": 0.7487887144088745,
      "learning_rate": 3.021039603960396e-05,
      "loss": 8.158,
      "step": 6400
    },
    {
      "epoch": 1.9828319542185446,
      "grad_norm": 0.7612968683242798,
      "learning_rate": 3.0179455445544556e-05,
      "loss": 8.1778,
      "step": 6410
    },
    {
      "epoch": 1.9859252958007887,
      "grad_norm": 1.4509085416793823,
      "learning_rate": 3.014851485148515e-05,
      "loss": 8.1678,
      "step": 6420
    },
    {
      "epoch": 1.989018637383033,
      "grad_norm": 0.44345802068710327,
      "learning_rate": 3.0117574257425747e-05,
      "loss": 8.1723,
      "step": 6430
    },
    {
      "epoch": 1.9921119789652773,
      "grad_norm": 0.5903739333152771,
      "learning_rate": 3.0086633663366336e-05,
      "loss": 8.1681,
      "step": 6440
    },
    {
      "epoch": 1.9952053205475213,
      "grad_norm": 0.8666324615478516,
      "learning_rate": 3.0055693069306934e-05,
      "loss": 8.1741,
      "step": 6450
    },
    {
      "epoch": 1.9982986621297658,
      "grad_norm": 0.654120683670044,
      "learning_rate": 3.0024752475247526e-05,
      "loss": 8.1606,
      "step": 6460
    }
  ],
  "logging_steps": 10,
  "max_steps": 16160,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 2.6832674412355584e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
