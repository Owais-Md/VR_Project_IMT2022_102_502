{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9997679993813317,
  "eval_steps": 500,
  "global_step": 3232,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 2.283881425857544,
      "learning_rate": 4.997524752475248e-05,
      "loss": 10.3057,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.7015577554702759,
      "learning_rate": 4.9944306930693075e-05,
      "loss": 10.0835,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 1.8396601676940918,
      "learning_rate": 4.9913366336633664e-05,
      "loss": 9.8731,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 1.0070555210113525,
      "learning_rate": 4.988242574257426e-05,
      "loss": 9.6963,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 1.0936119556427002,
      "learning_rate": 4.9851485148514855e-05,
      "loss": 9.5427,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 0.9463545083999634,
      "learning_rate": 4.982054455445545e-05,
      "loss": 9.4026,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 1.0548697710037231,
      "learning_rate": 4.978960396039604e-05,
      "loss": 9.2535,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.846869170665741,
      "learning_rate": 4.975866336633664e-05,
      "loss": 9.1537,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.7546383142471313,
      "learning_rate": 4.972772277227723e-05,
      "loss": 9.0376,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.7686005234718323,
      "learning_rate": 4.9696782178217825e-05,
      "loss": 8.9299,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 0.7528889775276184,
      "learning_rate": 4.966584158415842e-05,
      "loss": 8.869,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.7256690859794617,
      "learning_rate": 4.963799504950496e-05,
      "loss": 8.7791,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.6986427903175354,
      "learning_rate": 4.9607054455445546e-05,
      "loss": 8.733,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.6804739236831665,
      "learning_rate": 4.957611386138614e-05,
      "loss": 8.6765,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.7127686142921448,
      "learning_rate": 4.9545173267326736e-05,
      "loss": 8.6174,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.6702990531921387,
      "learning_rate": 4.951423267326733e-05,
      "loss": 8.5714,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.7780759334564209,
      "learning_rate": 4.948329207920792e-05,
      "loss": 8.5676,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.7366856932640076,
      "learning_rate": 4.9452351485148516e-05,
      "loss": 8.497,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.7136861681938171,
      "learning_rate": 4.942141089108911e-05,
      "loss": 8.4928,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 1.917746663093567,
      "learning_rate": 4.9390470297029706e-05,
      "loss": 8.4637,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 1.148116111755371,
      "learning_rate": 4.9359529702970295e-05,
      "loss": 8.4424,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.5712763667106628,
      "learning_rate": 4.93285891089109e-05,
      "loss": 8.4241,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 0.6743816137313843,
      "learning_rate": 4.9297648514851486e-05,
      "loss": 8.3973,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 0.6497937440872192,
      "learning_rate": 4.926670792079208e-05,
      "loss": 8.4058,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 1.2516216039657593,
      "learning_rate": 4.9235767326732677e-05,
      "loss": 8.3807,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 1.2762138843536377,
      "learning_rate": 4.920482673267327e-05,
      "loss": 8.3691,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.6043147444725037,
      "learning_rate": 4.917388613861386e-05,
      "loss": 8.367,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 1.278912901878357,
      "learning_rate": 4.9142945544554456e-05,
      "loss": 8.3501,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 1.0998435020446777,
      "learning_rate": 4.911200495049505e-05,
      "loss": 8.3126,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 1.0249993801116943,
      "learning_rate": 4.908106435643565e-05,
      "loss": 8.3197,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 1.9351379871368408,
      "learning_rate": 4.9050123762376235e-05,
      "loss": 8.307,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 0.9796373844146729,
      "learning_rate": 4.901918316831684e-05,
      "loss": 8.2893,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.837062656879425,
      "learning_rate": 4.8988242574257426e-05,
      "loss": 8.2921,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.7073545455932617,
      "learning_rate": 4.895730198019802e-05,
      "loss": 8.2982,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 1.0989118814468384,
      "learning_rate": 4.892636138613862e-05,
      "loss": 8.2793,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 1.3417258262634277,
      "learning_rate": 4.889542079207921e-05,
      "loss": 8.2782,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 1.1105343103408813,
      "learning_rate": 4.88644801980198e-05,
      "loss": 8.2753,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 1.1419422626495361,
      "learning_rate": 4.8833539603960396e-05,
      "loss": 8.2766,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 1.4925583600997925,
      "learning_rate": 4.880259900990099e-05,
      "loss": 8.2618,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 1.426761507987976,
      "learning_rate": 4.877165841584159e-05,
      "loss": 8.2665,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 1.2589497566223145,
      "learning_rate": 4.8740717821782176e-05,
      "loss": 8.2462,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 3.0927257537841797,
      "learning_rate": 4.870977722772278e-05,
      "loss": 8.2647,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 1.1971580982208252,
      "learning_rate": 4.8678836633663366e-05,
      "loss": 8.2508,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 1.6362168788909912,
      "learning_rate": 4.864789603960396e-05,
      "loss": 8.2566,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 1.0588487386703491,
      "learning_rate": 4.861695544554456e-05,
      "loss": 8.2492,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 0.757152795791626,
      "learning_rate": 4.858601485148515e-05,
      "loss": 8.2621,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.8795488476753235,
      "learning_rate": 4.855507425742574e-05,
      "loss": 8.2369,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 0.695861279964447,
      "learning_rate": 4.852413366336634e-05,
      "loss": 8.2328,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 1.3076118230819702,
      "learning_rate": 4.849319306930693e-05,
      "loss": 8.2352,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.7781069874763489,
      "learning_rate": 4.846225247524753e-05,
      "loss": 8.2228,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 1.1994925737380981,
      "learning_rate": 4.843131188118812e-05,
      "loss": 8.2235,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 1.3085798025131226,
      "learning_rate": 4.840037128712872e-05,
      "loss": 8.2474,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 2.003173589706421,
      "learning_rate": 4.8369430693069314e-05,
      "loss": 8.2414,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.8361700773239136,
      "learning_rate": 4.83384900990099e-05,
      "loss": 8.2453,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 1.8430653810501099,
      "learning_rate": 4.83075495049505e-05,
      "loss": 8.2399,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 1.4458892345428467,
      "learning_rate": 4.827660891089109e-05,
      "loss": 8.2388,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.6414463520050049,
      "learning_rate": 4.824566831683169e-05,
      "loss": 8.2377,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 0.614952802658081,
      "learning_rate": 4.821472772277228e-05,
      "loss": 8.2242,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 2.2713072299957275,
      "learning_rate": 4.818378712871288e-05,
      "loss": 8.2297,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 1.6645755767822266,
      "learning_rate": 4.815284653465347e-05,
      "loss": 8.2216,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 1.319783091545105,
      "learning_rate": 4.812190594059406e-05,
      "loss": 8.22,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 0.7334883213043213,
      "learning_rate": 4.809096534653466e-05,
      "loss": 8.2276,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.5590004324913025,
      "learning_rate": 4.8060024752475254e-05,
      "loss": 8.2329,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 1.0776251554489136,
      "learning_rate": 4.802908415841584e-05,
      "loss": 8.2187,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.783419668674469,
      "learning_rate": 4.799814356435644e-05,
      "loss": 8.214,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.5639674663543701,
      "learning_rate": 4.796720297029703e-05,
      "loss": 8.2288,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 1.6762508153915405,
      "learning_rate": 4.793626237623763e-05,
      "loss": 8.2244,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 1.8065810203552246,
      "learning_rate": 4.790532178217822e-05,
      "loss": 8.2238,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 2.1397507190704346,
      "learning_rate": 4.787438118811882e-05,
      "loss": 8.2311,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 1.9543536901474,
      "learning_rate": 4.784344059405941e-05,
      "loss": 8.2082,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 1.0264626741409302,
      "learning_rate": 4.7812500000000003e-05,
      "loss": 8.218,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 1.7906293869018555,
      "learning_rate": 4.77815594059406e-05,
      "loss": 8.2161,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 0.523497462272644,
      "learning_rate": 4.7750618811881194e-05,
      "loss": 8.2333,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 0.7396768927574158,
      "learning_rate": 4.771967821782178e-05,
      "loss": 8.2318,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 1.0711976289749146,
      "learning_rate": 4.768873762376238e-05,
      "loss": 8.212,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 1.1562745571136475,
      "learning_rate": 4.7657797029702974e-05,
      "loss": 8.2018,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 0.9633840322494507,
      "learning_rate": 4.762685643564357e-05,
      "loss": 8.2027,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 1.9501839876174927,
      "learning_rate": 4.759591584158416e-05,
      "loss": 8.2235,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 0.6358993053436279,
      "learning_rate": 4.756497524752476e-05,
      "loss": 8.2305,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 1.3217599391937256,
      "learning_rate": 4.753403465346535e-05,
      "loss": 8.229,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 1.5870161056518555,
      "learning_rate": 4.7503094059405944e-05,
      "loss": 8.2073,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.7626834511756897,
      "learning_rate": 4.747215346534654e-05,
      "loss": 8.2184,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.9049161672592163,
      "learning_rate": 4.7441212871287135e-05,
      "loss": 8.2069,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 0.767512321472168,
      "learning_rate": 4.741027227722772e-05,
      "loss": 8.2177,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 1.644620418548584,
      "learning_rate": 4.737933168316832e-05,
      "loss": 8.2219,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.6423130035400391,
      "learning_rate": 4.7348391089108914e-05,
      "loss": 8.203,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.9769448041915894,
      "learning_rate": 4.731745049504951e-05,
      "loss": 8.2188,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 0.7573779821395874,
      "learning_rate": 4.72865099009901e-05,
      "loss": 8.1944,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 0.7785887718200684,
      "learning_rate": 4.72555693069307e-05,
      "loss": 8.1956,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 1.4564820528030396,
      "learning_rate": 4.722462871287129e-05,
      "loss": 8.2146,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.9155580997467041,
      "learning_rate": 4.7193688118811884e-05,
      "loss": 8.2203,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 1.7754689455032349,
      "learning_rate": 4.716274752475248e-05,
      "loss": 8.2226,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 1.523375391960144,
      "learning_rate": 4.7131806930693075e-05,
      "loss": 8.2055,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 2.337571620941162,
      "learning_rate": 4.7100866336633663e-05,
      "loss": 8.2128,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 2.301279306411743,
      "learning_rate": 4.706992574257426e-05,
      "loss": 8.2004,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 1.6187915802001953,
      "learning_rate": 4.7038985148514854e-05,
      "loss": 8.1995,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 1.382105827331543,
      "learning_rate": 4.700804455445545e-05,
      "loss": 8.218,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 0.6750116348266602,
      "learning_rate": 4.697710396039604e-05,
      "loss": 8.2066,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.8816543817520142,
      "learning_rate": 4.694616336633664e-05,
      "loss": 8.2152,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 1.4862397909164429,
      "learning_rate": 4.691522277227723e-05,
      "loss": 8.2067,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 2.049319267272949,
      "learning_rate": 4.6884282178217824e-05,
      "loss": 8.2084,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 0.9031298160552979,
      "learning_rate": 4.685334158415842e-05,
      "loss": 8.1988,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 1.8627923727035522,
      "learning_rate": 4.6822400990099015e-05,
      "loss": 8.2123,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 2.3312253952026367,
      "learning_rate": 4.6791460396039604e-05,
      "loss": 8.2085,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 1.4772182703018188,
      "learning_rate": 4.67605198019802e-05,
      "loss": 8.1977,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 0.7699053287506104,
      "learning_rate": 4.6729579207920795e-05,
      "loss": 8.214,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 1.258692741394043,
      "learning_rate": 4.669863861386139e-05,
      "loss": 8.2091,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 1.3253406286239624,
      "learning_rate": 4.666769801980198e-05,
      "loss": 8.1978,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.686968982219696,
      "learning_rate": 4.663675742574258e-05,
      "loss": 8.2043,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.0489264726638794,
      "learning_rate": 4.660581683168317e-05,
      "loss": 8.1951,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 0.6969893574714661,
      "learning_rate": 4.6574876237623765e-05,
      "loss": 8.2131,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 0.8512948155403137,
      "learning_rate": 4.654393564356436e-05,
      "loss": 8.2043,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 1.1636050939559937,
      "learning_rate": 4.6512995049504955e-05,
      "loss": 8.1909,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 2.8269481658935547,
      "learning_rate": 4.6482054455445544e-05,
      "loss": 8.2012,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 1.2299357652664185,
      "learning_rate": 4.645111386138614e-05,
      "loss": 8.1793,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 0.7432317733764648,
      "learning_rate": 4.6420173267326735e-05,
      "loss": 8.1917,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 1.4997855424880981,
      "learning_rate": 4.638923267326733e-05,
      "loss": 8.2003,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 1.17046320438385,
      "learning_rate": 4.635829207920792e-05,
      "loss": 8.194,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 0.8377035856246948,
      "learning_rate": 4.632735148514852e-05,
      "loss": 8.1987,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 1.1011815071105957,
      "learning_rate": 4.629641089108911e-05,
      "loss": 8.2105,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 2.4605493545532227,
      "learning_rate": 4.6265470297029705e-05,
      "loss": 8.1883,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 2.327408790588379,
      "learning_rate": 4.62345297029703e-05,
      "loss": 8.2033,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 0.8229179978370667,
      "learning_rate": 4.6203589108910896e-05,
      "loss": 8.1983,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 0.7746541500091553,
      "learning_rate": 4.6172648514851484e-05,
      "loss": 8.1951,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 1.7976148128509521,
      "learning_rate": 4.614170792079208e-05,
      "loss": 8.1896,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 0.8158743977546692,
      "learning_rate": 4.6110767326732675e-05,
      "loss": 8.1881,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 1.2931594848632812,
      "learning_rate": 4.607982673267327e-05,
      "loss": 8.1896,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 1.3114842176437378,
      "learning_rate": 4.604888613861386e-05,
      "loss": 8.1987,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 1.2831923961639404,
      "learning_rate": 4.601794554455446e-05,
      "loss": 8.1997,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 2.43292236328125,
      "learning_rate": 4.598700495049505e-05,
      "loss": 8.2048,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 1.5269575119018555,
      "learning_rate": 4.5956064356435645e-05,
      "loss": 8.194,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 2.0795676708221436,
      "learning_rate": 4.592512376237624e-05,
      "loss": 8.2075,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 2.4864180088043213,
      "learning_rate": 4.5894183168316836e-05,
      "loss": 8.197,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 1.2086435556411743,
      "learning_rate": 4.5863242574257425e-05,
      "loss": 8.1758,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 1.5092233419418335,
      "learning_rate": 4.583230198019802e-05,
      "loss": 8.1855,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 1.1152379512786865,
      "learning_rate": 4.5801361386138616e-05,
      "loss": 8.1831,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 0.8014148473739624,
      "learning_rate": 4.577042079207921e-05,
      "loss": 8.175,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.7414972186088562,
      "learning_rate": 4.57394801980198e-05,
      "loss": 8.1869,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.085468053817749,
      "learning_rate": 4.57085396039604e-05,
      "loss": 8.2012,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 1.602766752243042,
      "learning_rate": 4.567759900990099e-05,
      "loss": 8.1717,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 0.7542450428009033,
      "learning_rate": 4.5646658415841586e-05,
      "loss": 8.1987,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 0.933017909526825,
      "learning_rate": 4.561571782178218e-05,
      "loss": 8.1989,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 2.079925775527954,
      "learning_rate": 4.5584777227722776e-05,
      "loss": 8.1969,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 1.0393853187561035,
      "learning_rate": 4.5553836633663365e-05,
      "loss": 8.1958,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 0.9792965650558472,
      "learning_rate": 4.552289603960396e-05,
      "loss": 8.1952,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 0.6396719813346863,
      "learning_rate": 4.5491955445544556e-05,
      "loss": 8.1849,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 1.0150868892669678,
      "learning_rate": 4.546101485148515e-05,
      "loss": 8.1889,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 1.6108311414718628,
      "learning_rate": 4.543007425742574e-05,
      "loss": 8.1892,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 1.2107347249984741,
      "learning_rate": 4.539913366336634e-05,
      "loss": 8.2103,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 2.3177919387817383,
      "learning_rate": 4.536819306930693e-05,
      "loss": 8.1775,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 2.523088216781616,
      "learning_rate": 4.5337252475247526e-05,
      "loss": 8.1951,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 1.209039330482483,
      "learning_rate": 4.530631188118812e-05,
      "loss": 8.1994,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 0.9921939969062805,
      "learning_rate": 4.527537128712872e-05,
      "loss": 8.1897,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 1.8710671663284302,
      "learning_rate": 4.524443069306931e-05,
      "loss": 8.2037,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 1.856017827987671,
      "learning_rate": 4.52134900990099e-05,
      "loss": 8.1749,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.9681246876716614,
      "learning_rate": 4.51825495049505e-05,
      "loss": 8.1651,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 1.9962009191513062,
      "learning_rate": 4.515160891089109e-05,
      "loss": 8.2048,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 0.6548168063163757,
      "learning_rate": 4.512066831683169e-05,
      "loss": 8.1847,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 1.0097074508666992,
      "learning_rate": 4.508972772277228e-05,
      "loss": 8.1786,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 3.913090705871582,
      "learning_rate": 4.505878712871288e-05,
      "loss": 8.1914,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 2.477466583251953,
      "learning_rate": 4.5027846534653466e-05,
      "loss": 8.1868,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 1.6980576515197754,
      "learning_rate": 4.499690594059406e-05,
      "loss": 8.1864,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 0.7103257775306702,
      "learning_rate": 4.496596534653466e-05,
      "loss": 8.1642,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 1.0004551410675049,
      "learning_rate": 4.493502475247525e-05,
      "loss": 8.1928,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 0.7323936223983765,
      "learning_rate": 4.490408415841584e-05,
      "loss": 8.19,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 1.083060383796692,
      "learning_rate": 4.487314356435644e-05,
      "loss": 8.1821,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 1.411689281463623,
      "learning_rate": 4.484220297029703e-05,
      "loss": 8.1909,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 1.8345826864242554,
      "learning_rate": 4.481126237623763e-05,
      "loss": 8.1705,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 0.8988075256347656,
      "learning_rate": 4.478032178217822e-05,
      "loss": 8.1757,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 2.0011909008026123,
      "learning_rate": 4.474938118811882e-05,
      "loss": 8.1819,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 1.4574311971664429,
      "learning_rate": 4.471844059405941e-05,
      "loss": 8.1937,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 1.8036798238754272,
      "learning_rate": 4.46875e-05,
      "loss": 8.1856,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 1.1992884874343872,
      "learning_rate": 4.46565594059406e-05,
      "loss": 8.1782,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 1.0265569686889648,
      "learning_rate": 4.462561881188119e-05,
      "loss": 8.1857,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 2.0533766746520996,
      "learning_rate": 4.459467821782178e-05,
      "loss": 8.1917,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 1.6524206399917603,
      "learning_rate": 4.4563737623762384e-05,
      "loss": 8.1967,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 1.3807529211044312,
      "learning_rate": 4.453279702970297e-05,
      "loss": 8.1698,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 2.298870086669922,
      "learning_rate": 4.450185643564357e-05,
      "loss": 8.1933,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 0.7895309329032898,
      "learning_rate": 4.447091584158416e-05,
      "loss": 8.1897,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 0.6869894862174988,
      "learning_rate": 4.443997524752476e-05,
      "loss": 8.181,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 1.9506685733795166,
      "learning_rate": 4.440903465346535e-05,
      "loss": 8.1712,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 1.4609322547912598,
      "learning_rate": 4.437809405940594e-05,
      "loss": 8.1963,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 2.905377149581909,
      "learning_rate": 4.434715346534654e-05,
      "loss": 8.186,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 1.0420995950698853,
      "learning_rate": 4.431621287128713e-05,
      "loss": 8.1818,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 1.0503602027893066,
      "learning_rate": 4.428527227722772e-05,
      "loss": 8.1772,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 1.0495920181274414,
      "learning_rate": 4.4254331683168324e-05,
      "loss": 8.1953,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 1.1845871210098267,
      "learning_rate": 4.422339108910891e-05,
      "loss": 8.2054,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 0.8258082270622253,
      "learning_rate": 4.419245049504951e-05,
      "loss": 8.1825,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.954254686832428,
      "learning_rate": 4.41615099009901e-05,
      "loss": 8.1811,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 0.9058201313018799,
      "learning_rate": 4.41305693069307e-05,
      "loss": 8.1925,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 1.8257588148117065,
      "learning_rate": 4.409962871287129e-05,
      "loss": 8.1849,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 2.6599490642547607,
      "learning_rate": 4.406868811881188e-05,
      "loss": 8.1895,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 3.222787380218506,
      "learning_rate": 4.403774752475248e-05,
      "loss": 8.1875,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 1.5086697340011597,
      "learning_rate": 4.4006806930693073e-05,
      "loss": 8.1931,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 2.020599842071533,
      "learning_rate": 4.397586633663366e-05,
      "loss": 8.1783,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 3.1900675296783447,
      "learning_rate": 4.3944925742574264e-05,
      "loss": 8.1733,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 2.096738576889038,
      "learning_rate": 4.391398514851485e-05,
      "loss": 8.1673,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 1.2224245071411133,
      "learning_rate": 4.388304455445545e-05,
      "loss": 8.1777,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 1.1820451021194458,
      "learning_rate": 4.3852103960396044e-05,
      "loss": 8.1868,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 0.6829479336738586,
      "learning_rate": 4.382116336633664e-05,
      "loss": 8.1895,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.9388048052787781,
      "learning_rate": 4.379022277227723e-05,
      "loss": 8.1835,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 1.8816661834716797,
      "learning_rate": 4.375928217821782e-05,
      "loss": 8.2014,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 2.058828592300415,
      "learning_rate": 4.372834158415842e-05,
      "loss": 8.1804,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 0.6706216931343079,
      "learning_rate": 4.3697400990099014e-05,
      "loss": 8.194,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 1.2952839136123657,
      "learning_rate": 4.36664603960396e-05,
      "loss": 8.1882,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 1.1358423233032227,
      "learning_rate": 4.3635519801980205e-05,
      "loss": 8.1826,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 2.457620620727539,
      "learning_rate": 4.360457920792079e-05,
      "loss": 8.1862,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 0.9240761399269104,
      "learning_rate": 4.357363861386139e-05,
      "loss": 8.2007,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 1.1774667501449585,
      "learning_rate": 4.3542698019801984e-05,
      "loss": 8.192,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 1.1870375871658325,
      "learning_rate": 4.351175742574258e-05,
      "loss": 8.2006,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 0.641690194606781,
      "learning_rate": 4.348081683168317e-05,
      "loss": 8.1851,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 1.9188201427459717,
      "learning_rate": 4.344987623762376e-05,
      "loss": 8.1785,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 1.383307933807373,
      "learning_rate": 4.341893564356436e-05,
      "loss": 8.1925,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 2.127551794052124,
      "learning_rate": 4.3387995049504954e-05,
      "loss": 8.1923,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 1.263771891593933,
      "learning_rate": 4.335705445544554e-05,
      "loss": 8.2007,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 1.2934004068374634,
      "learning_rate": 4.3326113861386145e-05,
      "loss": 8.1754,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 1.2981380224227905,
      "learning_rate": 4.3295173267326733e-05,
      "loss": 8.1811,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 0.7973347902297974,
      "learning_rate": 4.326423267326733e-05,
      "loss": 8.1797,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 2.5965218544006348,
      "learning_rate": 4.3233292079207924e-05,
      "loss": 8.1762,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 1.4685229063034058,
      "learning_rate": 4.320235148514852e-05,
      "loss": 8.1916,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 1.339370846748352,
      "learning_rate": 4.317141089108911e-05,
      "loss": 8.1817,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 1.7123206853866577,
      "learning_rate": 4.3140470297029704e-05,
      "loss": 8.1901,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 1.1629844903945923,
      "learning_rate": 4.31095297029703e-05,
      "loss": 8.1858,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 1.1125881671905518,
      "learning_rate": 4.3078589108910894e-05,
      "loss": 8.1818,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.8006672263145447,
      "learning_rate": 4.304764851485148e-05,
      "loss": 8.2041,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 0.8128392100334167,
      "learning_rate": 4.3016707920792085e-05,
      "loss": 8.1749,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 1.392057180404663,
      "learning_rate": 4.2985767326732674e-05,
      "loss": 8.1642,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 0.7961410284042358,
      "learning_rate": 4.295482673267327e-05,
      "loss": 8.1724,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 0.8821604251861572,
      "learning_rate": 4.2923886138613865e-05,
      "loss": 8.1812,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 2.1213696002960205,
      "learning_rate": 4.289294554455446e-05,
      "loss": 8.1812,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 2.049058198928833,
      "learning_rate": 4.286200495049505e-05,
      "loss": 8.1706,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.8438975811004639,
      "learning_rate": 4.2831064356435644e-05,
      "loss": 8.1814,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 1.5994082689285278,
      "learning_rate": 4.280012376237624e-05,
      "loss": 8.1773,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 1.6760908365249634,
      "learning_rate": 4.2769183168316835e-05,
      "loss": 8.1915,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 1.1761233806610107,
      "learning_rate": 4.273824257425742e-05,
      "loss": 8.1648,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 0.8909958004951477,
      "learning_rate": 4.2707301980198025e-05,
      "loss": 8.1975,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 1.8550859689712524,
      "learning_rate": 4.2676361386138614e-05,
      "loss": 8.1823,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 2.531545877456665,
      "learning_rate": 4.264542079207921e-05,
      "loss": 8.1802,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 1.5944923162460327,
      "learning_rate": 4.2614480198019805e-05,
      "loss": 8.1732,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 1.9887875318527222,
      "learning_rate": 4.25835396039604e-05,
      "loss": 8.1727,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 2.872790575027466,
      "learning_rate": 4.255259900990099e-05,
      "loss": 8.1794,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 0.7231345176696777,
      "learning_rate": 4.2521658415841584e-05,
      "loss": 8.1898,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 1.993692398071289,
      "learning_rate": 4.249071782178218e-05,
      "loss": 8.1832,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 2.00915265083313,
      "learning_rate": 4.2459777227722775e-05,
      "loss": 8.1933,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 1.4420005083084106,
      "learning_rate": 4.2428836633663364e-05,
      "loss": 8.1866,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.6248858571052551,
      "learning_rate": 4.2397896039603966e-05,
      "loss": 8.1862,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.8173538446426392,
      "learning_rate": 4.2366955445544554e-05,
      "loss": 8.1595,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 0.9942618012428284,
      "learning_rate": 4.233601485148515e-05,
      "loss": 8.1785,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 0.8387640118598938,
      "learning_rate": 4.2305074257425745e-05,
      "loss": 8.1929,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 1.4144275188446045,
      "learning_rate": 4.227413366336634e-05,
      "loss": 8.1745,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.7667526006698608,
      "learning_rate": 4.224319306930693e-05,
      "loss": 8.1941,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 1.2658334970474243,
      "learning_rate": 4.2212252475247525e-05,
      "loss": 8.1778,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 1.2555665969848633,
      "learning_rate": 4.218131188118813e-05,
      "loss": 8.1546,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 1.0784926414489746,
      "learning_rate": 4.2150371287128715e-05,
      "loss": 8.1839,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 0.7848606109619141,
      "learning_rate": 4.211943069306931e-05,
      "loss": 8.1716,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 0.8297044634819031,
      "learning_rate": 4.2088490099009906e-05,
      "loss": 8.1839,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 1.2344859838485718,
      "learning_rate": 4.20575495049505e-05,
      "loss": 8.1839,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 1.8767091035842896,
      "learning_rate": 4.202660891089109e-05,
      "loss": 8.18,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 2.172985792160034,
      "learning_rate": 4.1995668316831686e-05,
      "loss": 8.1864,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 0.9894881844520569,
      "learning_rate": 4.196472772277228e-05,
      "loss": 8.1868,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 2.2789270877838135,
      "learning_rate": 4.1933787128712876e-05,
      "loss": 8.1741,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 0.9330989122390747,
      "learning_rate": 4.1902846534653465e-05,
      "loss": 8.1596,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 1.5242124795913696,
      "learning_rate": 4.187190594059407e-05,
      "loss": 8.1624,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 1.4509238004684448,
      "learning_rate": 4.1840965346534656e-05,
      "loss": 8.1812,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 1.110629916191101,
      "learning_rate": 4.181002475247525e-05,
      "loss": 8.1899,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 1.2923803329467773,
      "learning_rate": 4.1779084158415846e-05,
      "loss": 8.1763,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 0.8786554336547852,
      "learning_rate": 4.174814356435644e-05,
      "loss": 8.1813,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 1.402254343032837,
      "learning_rate": 4.171720297029703e-05,
      "loss": 8.1962,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 1.2103198766708374,
      "learning_rate": 4.1686262376237626e-05,
      "loss": 8.1784,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 1.835925579071045,
      "learning_rate": 4.165532178217822e-05,
      "loss": 8.1734,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 1.5465960502624512,
      "learning_rate": 4.1624381188118817e-05,
      "loss": 8.1802,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 1.3195463418960571,
      "learning_rate": 4.1593440594059405e-05,
      "loss": 8.1999,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 1.1356457471847534,
      "learning_rate": 4.156250000000001e-05,
      "loss": 8.1874,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 1.2040010690689087,
      "learning_rate": 4.1531559405940596e-05,
      "loss": 8.1718,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.6268787980079651,
      "learning_rate": 4.150061881188119e-05,
      "loss": 8.1883,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 1.3542238473892212,
      "learning_rate": 4.146967821782179e-05,
      "loss": 8.1929,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 1.0542324781417847,
      "learning_rate": 4.143873762376238e-05,
      "loss": 8.1867,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 0.959769070148468,
      "learning_rate": 4.140779702970297e-05,
      "loss": 8.1805,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.8823913931846619,
      "learning_rate": 4.1376856435643566e-05,
      "loss": 8.1691,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.993774950504303,
      "learning_rate": 4.134591584158416e-05,
      "loss": 8.1943,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.6663392186164856,
      "learning_rate": 4.131497524752476e-05,
      "loss": 8.1682,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 1.050313115119934,
      "learning_rate": 4.1284034653465346e-05,
      "loss": 8.1664,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 0.6561669111251831,
      "learning_rate": 4.125309405940595e-05,
      "loss": 8.2059,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.5854478478431702,
      "learning_rate": 4.1222153465346536e-05,
      "loss": 8.1804,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.6190448999404907,
      "learning_rate": 4.119121287128713e-05,
      "loss": 8.1843,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 0.8077029585838318,
      "learning_rate": 4.116027227722773e-05,
      "loss": 8.1768,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.6319847106933594,
      "learning_rate": 4.112933168316832e-05,
      "loss": 8.1793,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 0.6598100662231445,
      "learning_rate": 4.109839108910891e-05,
      "loss": 8.1773,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 0.6498719453811646,
      "learning_rate": 4.1067450495049506e-05,
      "loss": 8.1743,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 1.3095574378967285,
      "learning_rate": 4.10365099009901e-05,
      "loss": 8.1851,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 2.136505126953125,
      "learning_rate": 4.10055693069307e-05,
      "loss": 8.1921,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 0.8504270315170288,
      "learning_rate": 4.0974628712871286e-05,
      "loss": 8.1865,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 1.7754770517349243,
      "learning_rate": 4.094368811881189e-05,
      "loss": 8.1855,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 1.306828260421753,
      "learning_rate": 4.091274752475248e-05,
      "loss": 8.1704,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 1.1314023733139038,
      "learning_rate": 4.088180693069307e-05,
      "loss": 8.1697,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 2.4816794395446777,
      "learning_rate": 4.085086633663367e-05,
      "loss": 8.1709,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 1.2383394241333008,
      "learning_rate": 4.081992574257426e-05,
      "loss": 8.1762,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 0.8009899258613586,
      "learning_rate": 4.078898514851485e-05,
      "loss": 8.1734,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 1.5692912340164185,
      "learning_rate": 4.075804455445545e-05,
      "loss": 8.188,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 1.0885016918182373,
      "learning_rate": 4.072710396039604e-05,
      "loss": 8.1867,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.7706090807914734,
      "learning_rate": 4.069616336633664e-05,
      "loss": 8.1562,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.8732590675354004,
      "learning_rate": 4.0665222772277226e-05,
      "loss": 8.1804,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.9803152680397034,
      "learning_rate": 4.063428217821783e-05,
      "loss": 8.177,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 0.7919334769248962,
      "learning_rate": 4.060334158415842e-05,
      "loss": 8.1704,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 1.1393951177597046,
      "learning_rate": 4.057240099009901e-05,
      "loss": 8.1884,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 1.2807313203811646,
      "learning_rate": 4.054146039603961e-05,
      "loss": 8.1877,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 1.3348079919815063,
      "learning_rate": 4.05105198019802e-05,
      "loss": 8.1906,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 0.723763644695282,
      "learning_rate": 4.047957920792079e-05,
      "loss": 8.1892,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 0.6793975830078125,
      "learning_rate": 4.044863861386139e-05,
      "loss": 8.1648,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 1.0764813423156738,
      "learning_rate": 4.041769801980198e-05,
      "loss": 8.1786,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 1.1117558479309082,
      "learning_rate": 4.038675742574258e-05,
      "loss": 8.185,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 2.068308115005493,
      "learning_rate": 4.0355816831683166e-05,
      "loss": 8.1686,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 1.702989935874939,
      "learning_rate": 4.032487623762377e-05,
      "loss": 8.18,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 1.5032709836959839,
      "learning_rate": 4.029393564356436e-05,
      "loss": 8.1727,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 1.2828290462493896,
      "learning_rate": 4.026299504950495e-05,
      "loss": 8.1885,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.9744423031806946,
      "learning_rate": 4.023205445544555e-05,
      "loss": 8.1824,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 1.3720958232879639,
      "learning_rate": 4.0201113861386143e-05,
      "loss": 8.1716,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 1.1993892192840576,
      "learning_rate": 4.017017326732673e-05,
      "loss": 8.1774,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.7784874439239502,
      "learning_rate": 4.013923267326733e-05,
      "loss": 8.1701,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 1.4290626049041748,
      "learning_rate": 4.010829207920792e-05,
      "loss": 8.1728,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 1.2606793642044067,
      "learning_rate": 4.007735148514852e-05,
      "loss": 8.1773,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.9906719326972961,
      "learning_rate": 4.004641089108911e-05,
      "loss": 8.1718,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 0.9914515018463135,
      "learning_rate": 4.001547029702971e-05,
      "loss": 8.1653,
      "step": 3230
    }
  ],
  "logging_steps": 10,
  "max_steps": 16160,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 1.3416337206177792e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
