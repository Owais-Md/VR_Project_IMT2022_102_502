{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.999845332920888,
  "eval_steps": 500,
  "global_step": 19396,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 2.2978525161743164,
      "learning_rate": 4.9982319660537485e-05,
      "loss": 10.3112,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.697009563446045,
      "learning_rate": 4.9962429278642154e-05,
      "loss": 10.1044,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 1.9493228197097778,
      "learning_rate": 4.9940328854314005e-05,
      "loss": 9.9147,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 1.2537775039672852,
      "learning_rate": 4.9918228429985856e-05,
      "loss": 9.7383,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 0.9784691333770752,
      "learning_rate": 4.9896128005657714e-05,
      "loss": 9.5668,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 1.3054183721542358,
      "learning_rate": 4.9874027581329565e-05,
      "loss": 9.4386,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 1.3254427909851074,
      "learning_rate": 4.9851927157001416e-05,
      "loss": 9.3105,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.8787651062011719,
      "learning_rate": 4.982982673267327e-05,
      "loss": 9.1853,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.7771647572517395,
      "learning_rate": 4.980772630834512e-05,
      "loss": 9.0639,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.7927806377410889,
      "learning_rate": 4.9785625884016976e-05,
      "loss": 8.9485,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 19.9823055267334,
      "learning_rate": 4.976352545968883e-05,
      "loss": 8.8797,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.7020925879478455,
      "learning_rate": 4.9741425035360684e-05,
      "loss": 8.7873,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.6891294717788696,
      "learning_rate": 4.9719324611032535e-05,
      "loss": 8.7412,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.6724461317062378,
      "learning_rate": 4.9697224186704386e-05,
      "loss": 8.6824,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.6063492298126221,
      "learning_rate": 4.9675123762376244e-05,
      "loss": 8.6219,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.649882435798645,
      "learning_rate": 4.9653023338048095e-05,
      "loss": 8.576,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.8346493244171143,
      "learning_rate": 4.9630922913719946e-05,
      "loss": 8.5714,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.7668482661247253,
      "learning_rate": 4.96088224893918e-05,
      "loss": 8.4979,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.81424480676651,
      "learning_rate": 4.9586722065063654e-05,
      "loss": 8.4936,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 1.412393569946289,
      "learning_rate": 4.9564621640735505e-05,
      "loss": 8.4625,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 0.9642157554626465,
      "learning_rate": 4.9542521216407356e-05,
      "loss": 8.439,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.5858935713768005,
      "learning_rate": 4.952042079207921e-05,
      "loss": 8.4209,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 1.2992138862609863,
      "learning_rate": 4.949832036775106e-05,
      "loss": 8.3929,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 1.6960211992263794,
      "learning_rate": 4.9476219943422916e-05,
      "loss": 8.4018,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 1.0526018142700195,
      "learning_rate": 4.9454119519094774e-05,
      "loss": 8.374,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 0.9247509241104126,
      "learning_rate": 4.9432019094766625e-05,
      "loss": 8.3619,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.650263249874115,
      "learning_rate": 4.9409918670438475e-05,
      "loss": 8.3572,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 0.8367137908935547,
      "learning_rate": 4.9387818246110326e-05,
      "loss": 8.3374,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 0.5976635813713074,
      "learning_rate": 4.9365717821782184e-05,
      "loss": 8.3021,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 1.133556604385376,
      "learning_rate": 4.9343617397454035e-05,
      "loss": 8.3094,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 2.212977409362793,
      "learning_rate": 4.9321516973125886e-05,
      "loss": 8.2983,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 0.8591291308403015,
      "learning_rate": 4.929941654879774e-05,
      "loss": 8.254,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.7119815349578857,
      "learning_rate": 4.9277316124469595e-05,
      "loss": 8.2402,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.860245406627655,
      "learning_rate": 4.9255215700141446e-05,
      "loss": 8.245,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 1.2802064418792725,
      "learning_rate": 4.9233115275813297e-05,
      "loss": 8.2269,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 0.6481074690818787,
      "learning_rate": 4.921101485148515e-05,
      "loss": 8.2231,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 1.1971083879470825,
      "learning_rate": 4.9188914427157e-05,
      "loss": 8.2189,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 1.7580838203430176,
      "learning_rate": 4.9166814002828856e-05,
      "loss": 8.2223,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 1.2845189571380615,
      "learning_rate": 4.9144713578500714e-05,
      "loss": 8.2078,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 2.1474101543426514,
      "learning_rate": 4.9122613154172565e-05,
      "loss": 8.2149,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 0.6936149001121521,
      "learning_rate": 4.9100512729844416e-05,
      "loss": 8.1938,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 2.1909961700439453,
      "learning_rate": 4.907841230551627e-05,
      "loss": 8.2125,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 0.7333984971046448,
      "learning_rate": 4.9056311881188124e-05,
      "loss": 8.1978,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 1.0266189575195312,
      "learning_rate": 4.9034211456859975e-05,
      "loss": 8.2031,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 0.7094117403030396,
      "learning_rate": 4.9012111032531826e-05,
      "loss": 8.1962,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 1.4269354343414307,
      "learning_rate": 4.899001060820368e-05,
      "loss": 8.2066,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.6357672810554504,
      "learning_rate": 4.8967910183875535e-05,
      "loss": 8.1813,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 1.524726390838623,
      "learning_rate": 4.8945809759547386e-05,
      "loss": 8.1788,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 1.4942302703857422,
      "learning_rate": 4.892370933521924e-05,
      "loss": 8.1809,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.7273604273796082,
      "learning_rate": 4.890160891089109e-05,
      "loss": 8.1683,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 1.1210496425628662,
      "learning_rate": 4.8879508486562946e-05,
      "loss": 8.1691,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 1.029540777206421,
      "learning_rate": 4.8857408062234796e-05,
      "loss": 8.1916,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 1.556471347808838,
      "learning_rate": 4.8835307637906654e-05,
      "loss": 8.1873,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.7839841246604919,
      "learning_rate": 4.8813207213578505e-05,
      "loss": 8.1909,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 2.0269980430603027,
      "learning_rate": 4.8791106789250356e-05,
      "loss": 8.1842,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 1.3354445695877075,
      "learning_rate": 4.876900636492221e-05,
      "loss": 8.1838,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.6603084206581116,
      "learning_rate": 4.8746905940594065e-05,
      "loss": 8.1838,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 1.0205258131027222,
      "learning_rate": 4.8724805516265916e-05,
      "loss": 8.1684,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 0.6809381246566772,
      "learning_rate": 4.870270509193777e-05,
      "loss": 8.1736,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 0.7379045486450195,
      "learning_rate": 4.868060466760962e-05,
      "loss": 8.1663,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 1.0289238691329956,
      "learning_rate": 4.8658504243281475e-05,
      "loss": 8.1652,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 1.4722477197647095,
      "learning_rate": 4.8636403818953326e-05,
      "loss": 8.1719,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.985119640827179,
      "learning_rate": 4.861430339462518e-05,
      "loss": 8.1764,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 2.193415880203247,
      "learning_rate": 4.859220297029703e-05,
      "loss": 8.1621,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.7924662828445435,
      "learning_rate": 4.8570102545968886e-05,
      "loss": 8.1579,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.7754663228988647,
      "learning_rate": 4.854800212164074e-05,
      "loss": 8.1731,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 1.1253646612167358,
      "learning_rate": 4.8525901697312595e-05,
      "loss": 8.1679,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 1.7341458797454834,
      "learning_rate": 4.8503801272984445e-05,
      "loss": 8.1671,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 1.9493780136108398,
      "learning_rate": 4.8481700848656296e-05,
      "loss": 8.1735,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 3.367149829864502,
      "learning_rate": 4.845960042432815e-05,
      "loss": 8.1516,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 1.8500995635986328,
      "learning_rate": 4.8437500000000005e-05,
      "loss": 8.1614,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 0.8552808165550232,
      "learning_rate": 4.8415399575671856e-05,
      "loss": 8.1589,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 1.2967503070831299,
      "learning_rate": 4.839329915134371e-05,
      "loss": 8.1755,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 1.5053879022598267,
      "learning_rate": 4.837119872701556e-05,
      "loss": 8.1735,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 0.7907457947731018,
      "learning_rate": 4.8349098302687416e-05,
      "loss": 8.1565,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 0.8512012958526611,
      "learning_rate": 4.8326997878359267e-05,
      "loss": 8.1454,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 0.8684555292129517,
      "learning_rate": 4.830489745403112e-05,
      "loss": 8.1468,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 2.083754301071167,
      "learning_rate": 4.8282797029702975e-05,
      "loss": 8.1667,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 0.9785476922988892,
      "learning_rate": 4.8260696605374826e-05,
      "loss": 8.1745,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 0.8621866703033447,
      "learning_rate": 4.823859618104668e-05,
      "loss": 8.1734,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 1.065608024597168,
      "learning_rate": 4.8216495756718535e-05,
      "loss": 8.1508,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.7921701669692993,
      "learning_rate": 4.8194395332390386e-05,
      "loss": 8.1613,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.9583138823509216,
      "learning_rate": 4.817229490806224e-05,
      "loss": 8.1486,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 1.781218409538269,
      "learning_rate": 4.815019448373409e-05,
      "loss": 8.1601,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 1.411521315574646,
      "learning_rate": 4.8128094059405945e-05,
      "loss": 8.166,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.6862319707870483,
      "learning_rate": 4.8105993635077796e-05,
      "loss": 8.1452,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.910425066947937,
      "learning_rate": 4.808389321074965e-05,
      "loss": 8.162,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 1.6667001247406006,
      "learning_rate": 4.80617927864215e-05,
      "loss": 8.1382,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 1.0555663108825684,
      "learning_rate": 4.803969236209335e-05,
      "loss": 8.1397,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 1.4642082452774048,
      "learning_rate": 4.801759193776521e-05,
      "loss": 8.159,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.7943904399871826,
      "learning_rate": 4.799549151343706e-05,
      "loss": 8.1628,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 1.0083155632019043,
      "learning_rate": 4.7973391089108916e-05,
      "loss": 8.1653,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 0.8918737173080444,
      "learning_rate": 4.7951290664780766e-05,
      "loss": 8.1491,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 1.3468626737594604,
      "learning_rate": 4.792919024045262e-05,
      "loss": 8.1569,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 1.7926127910614014,
      "learning_rate": 4.7907089816124475e-05,
      "loss": 8.145,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 2.6235406398773193,
      "learning_rate": 4.7884989391796326e-05,
      "loss": 8.1438,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 1.613265037536621,
      "learning_rate": 4.786288896746818e-05,
      "loss": 8.1618,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 1.0502805709838867,
      "learning_rate": 4.784078854314003e-05,
      "loss": 8.1507,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.9201352596282959,
      "learning_rate": 4.7818688118811886e-05,
      "loss": 8.1586,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 0.9506177306175232,
      "learning_rate": 4.779658769448374e-05,
      "loss": 8.1499,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 0.8562129139900208,
      "learning_rate": 4.777448727015559e-05,
      "loss": 8.1545,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 1.137046456336975,
      "learning_rate": 4.775238684582744e-05,
      "loss": 8.1428,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 4.624128341674805,
      "learning_rate": 4.773028642149929e-05,
      "loss": 8.159,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 0.9169142246246338,
      "learning_rate": 4.770818599717115e-05,
      "loss": 8.1528,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 4.3146209716796875,
      "learning_rate": 4.7686085572843005e-05,
      "loss": 8.1421,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 1.1886427402496338,
      "learning_rate": 4.7663985148514856e-05,
      "loss": 8.1575,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 1.0226562023162842,
      "learning_rate": 4.764188472418671e-05,
      "loss": 8.1538,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 0.9351243376731873,
      "learning_rate": 4.761978429985856e-05,
      "loss": 8.1418,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.6706920862197876,
      "learning_rate": 4.7597683875530415e-05,
      "loss": 8.1471,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.1914315223693848,
      "learning_rate": 4.7575583451202266e-05,
      "loss": 8.1389,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 0.7992492914199829,
      "learning_rate": 4.755348302687412e-05,
      "loss": 8.1572,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 1.4357860088348389,
      "learning_rate": 4.753138260254597e-05,
      "loss": 8.147,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 0.9896734952926636,
      "learning_rate": 4.7509282178217826e-05,
      "loss": 8.1361,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 0.7115346193313599,
      "learning_rate": 4.748718175388968e-05,
      "loss": 8.1448,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 1.3110510110855103,
      "learning_rate": 4.746508132956153e-05,
      "loss": 8.1237,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 1.0065312385559082,
      "learning_rate": 4.744298090523338e-05,
      "loss": 8.135,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 1.3407121896743774,
      "learning_rate": 4.742088048090523e-05,
      "loss": 8.1458,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 1.2935097217559814,
      "learning_rate": 4.7398780056577094e-05,
      "loss": 8.1379,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 0.9102542996406555,
      "learning_rate": 4.7376679632248945e-05,
      "loss": 8.1432,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 2.4544589519500732,
      "learning_rate": 4.7354579207920796e-05,
      "loss": 8.1546,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 0.9030328989028931,
      "learning_rate": 4.733247878359265e-05,
      "loss": 8.1316,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 0.8985339403152466,
      "learning_rate": 4.73103783592645e-05,
      "loss": 8.1446,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 1.6634750366210938,
      "learning_rate": 4.7288277934936356e-05,
      "loss": 8.1418,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 3.069864511489868,
      "learning_rate": 4.726617751060821e-05,
      "loss": 8.1398,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 0.7385414838790894,
      "learning_rate": 4.724407708628006e-05,
      "loss": 8.1339,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 0.7566729187965393,
      "learning_rate": 4.722197666195191e-05,
      "loss": 8.1308,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 2.357363224029541,
      "learning_rate": 4.7199876237623766e-05,
      "loss": 8.1348,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 0.7281991839408875,
      "learning_rate": 4.717777581329562e-05,
      "loss": 8.1437,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 0.8826242685317993,
      "learning_rate": 4.715567538896747e-05,
      "loss": 8.1458,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 0.9293636083602905,
      "learning_rate": 4.713357496463932e-05,
      "loss": 8.1498,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 2.375598907470703,
      "learning_rate": 4.711147454031117e-05,
      "loss": 8.1371,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 1.2174696922302246,
      "learning_rate": 4.7089374115983035e-05,
      "loss": 8.1515,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 1.1991961002349854,
      "learning_rate": 4.7067273691654886e-05,
      "loss": 8.1401,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 0.7864306569099426,
      "learning_rate": 4.7045173267326737e-05,
      "loss": 8.1199,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 0.7330999374389648,
      "learning_rate": 4.702307284299859e-05,
      "loss": 8.1305,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 0.8905574083328247,
      "learning_rate": 4.700097241867044e-05,
      "loss": 8.1283,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 1.4793035984039307,
      "learning_rate": 4.6978871994342296e-05,
      "loss": 8.1219,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.665165901184082,
      "learning_rate": 4.695677157001415e-05,
      "loss": 8.131,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.6748032569885254,
      "learning_rate": 4.6934671145686e-05,
      "loss": 8.1446,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 0.8435137271881104,
      "learning_rate": 4.691257072135785e-05,
      "loss": 8.1182,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 0.8907887935638428,
      "learning_rate": 4.689047029702971e-05,
      "loss": 8.1445,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 0.9058837294578552,
      "learning_rate": 4.686836987270156e-05,
      "loss": 8.1434,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 2.731689214706421,
      "learning_rate": 4.684626944837341e-05,
      "loss": 8.143,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 1.3596062660217285,
      "learning_rate": 4.682416902404526e-05,
      "loss": 8.1403,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 1.5356475114822388,
      "learning_rate": 4.680206859971712e-05,
      "loss": 8.14,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 2.6440606117248535,
      "learning_rate": 4.6779968175388975e-05,
      "loss": 8.1299,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 1.2428972721099854,
      "learning_rate": 4.6757867751060826e-05,
      "loss": 8.134,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 2.447598695755005,
      "learning_rate": 4.673576732673268e-05,
      "loss": 8.1356,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 2.3701610565185547,
      "learning_rate": 4.671366690240453e-05,
      "loss": 8.1562,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 0.5955577492713928,
      "learning_rate": 4.669156647807638e-05,
      "loss": 8.1211,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 1.9649672508239746,
      "learning_rate": 4.6669466053748236e-05,
      "loss": 8.1387,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 1.5700713396072388,
      "learning_rate": 4.664736562942009e-05,
      "loss": 8.144,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 1.2700107097625732,
      "learning_rate": 4.662526520509194e-05,
      "loss": 8.1341,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 1.2104922533035278,
      "learning_rate": 4.660316478076379e-05,
      "loss": 8.1475,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 1.0661829710006714,
      "learning_rate": 4.658106435643565e-05,
      "loss": 8.1214,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.8409711122512817,
      "learning_rate": 4.65589639321075e-05,
      "loss": 8.1108,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 3.221015214920044,
      "learning_rate": 4.653686350777935e-05,
      "loss": 8.1492,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 1.6262084245681763,
      "learning_rate": 4.6514763083451207e-05,
      "loss": 8.1294,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 2.6076228618621826,
      "learning_rate": 4.649266265912306e-05,
      "loss": 8.1244,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 0.7324378490447998,
      "learning_rate": 4.6470562234794915e-05,
      "loss": 8.1377,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 0.8694437742233276,
      "learning_rate": 4.6448461810466766e-05,
      "loss": 8.1318,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 1.190567135810852,
      "learning_rate": 4.642636138613862e-05,
      "loss": 8.1313,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 1.044485092163086,
      "learning_rate": 4.640426096181047e-05,
      "loss": 8.1099,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 1.5426740646362305,
      "learning_rate": 4.638216053748232e-05,
      "loss": 8.1393,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 1.1623259782791138,
      "learning_rate": 4.636006011315418e-05,
      "loss": 8.1356,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 1.1647775173187256,
      "learning_rate": 4.633795968882603e-05,
      "loss": 8.1274,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 1.2641135454177856,
      "learning_rate": 4.631585926449788e-05,
      "loss": 8.1376,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 1.3211780786514282,
      "learning_rate": 4.629375884016973e-05,
      "loss": 8.116,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 1.3174850940704346,
      "learning_rate": 4.627165841584159e-05,
      "loss": 8.1222,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 1.4946551322937012,
      "learning_rate": 4.624955799151344e-05,
      "loss": 8.1274,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 1.0001411437988281,
      "learning_rate": 4.622745756718529e-05,
      "loss": 8.1387,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 1.0746123790740967,
      "learning_rate": 4.620535714285715e-05,
      "loss": 8.1302,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 1.3485393524169922,
      "learning_rate": 4.6183256718529e-05,
      "loss": 8.1223,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 1.9648051261901855,
      "learning_rate": 4.6161156294200856e-05,
      "loss": 8.1313,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 1.2787649631500244,
      "learning_rate": 4.6139055869872707e-05,
      "loss": 8.1354,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 0.7873222231864929,
      "learning_rate": 4.611695544554456e-05,
      "loss": 8.1405,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 2.2081856727600098,
      "learning_rate": 4.609485502121641e-05,
      "loss": 8.1167,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 0.6028438806533813,
      "learning_rate": 4.607275459688826e-05,
      "loss": 8.1369,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 0.9986103773117065,
      "learning_rate": 4.605065417256012e-05,
      "loss": 8.1335,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 0.6742298603057861,
      "learning_rate": 4.602855374823197e-05,
      "loss": 8.1261,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 1.2286006212234497,
      "learning_rate": 4.600645332390382e-05,
      "loss": 8.1172,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 0.8139893412590027,
      "learning_rate": 4.598435289957567e-05,
      "loss": 8.1406,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 1.3433475494384766,
      "learning_rate": 4.596225247524753e-05,
      "loss": 8.1309,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 1.5777674913406372,
      "learning_rate": 4.594015205091938e-05,
      "loss": 8.1259,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 0.9981272220611572,
      "learning_rate": 4.5918051626591236e-05,
      "loss": 8.1216,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 0.880254864692688,
      "learning_rate": 4.589595120226309e-05,
      "loss": 8.1399,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 1.2760629653930664,
      "learning_rate": 4.587385077793494e-05,
      "loss": 8.15,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 1.6959831714630127,
      "learning_rate": 4.5851750353606796e-05,
      "loss": 8.1277,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.7466061115264893,
      "learning_rate": 4.582964992927865e-05,
      "loss": 8.1268,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 1.1226886510849,
      "learning_rate": 4.58075495049505e-05,
      "loss": 8.1372,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 1.5224499702453613,
      "learning_rate": 4.578544908062235e-05,
      "loss": 8.13,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 1.4019792079925537,
      "learning_rate": 4.57633486562942e-05,
      "loss": 8.1344,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.8918095827102661,
      "learning_rate": 4.574124823196606e-05,
      "loss": 8.1311,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 0.9084849953651428,
      "learning_rate": 4.571914780763791e-05,
      "loss": 8.1366,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 0.7314757704734802,
      "learning_rate": 4.569704738330976e-05,
      "loss": 8.1226,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 0.8405730128288269,
      "learning_rate": 4.567494695898161e-05,
      "loss": 8.1186,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 0.7289599776268005,
      "learning_rate": 4.565284653465347e-05,
      "loss": 8.1128,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 2.0056517124176025,
      "learning_rate": 4.5630746110325326e-05,
      "loss": 8.1228,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 1.4175626039505005,
      "learning_rate": 4.5608645685997177e-05,
      "loss": 8.1313,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 1.7403383255004883,
      "learning_rate": 4.558654526166903e-05,
      "loss": 8.1341,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.9728478789329529,
      "learning_rate": 4.556444483734088e-05,
      "loss": 8.1278,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 0.9657310843467712,
      "learning_rate": 4.5542344413012736e-05,
      "loss": 8.1455,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 0.7016928791999817,
      "learning_rate": 4.552024398868459e-05,
      "loss": 8.1257,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 1.750995397567749,
      "learning_rate": 4.549814356435644e-05,
      "loss": 8.1391,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 3.0268523693084717,
      "learning_rate": 4.547604314002829e-05,
      "loss": 8.1331,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 2.257249116897583,
      "learning_rate": 4.545394271570014e-05,
      "loss": 8.1268,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 0.7347825169563293,
      "learning_rate": 4.5431842291372e-05,
      "loss": 8.1315,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 2.116239070892334,
      "learning_rate": 4.540974186704385e-05,
      "loss": 8.1446,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 2.6621463298797607,
      "learning_rate": 4.53876414427157e-05,
      "loss": 8.1376,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 0.8156794309616089,
      "learning_rate": 4.536554101838755e-05,
      "loss": 8.1435,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 1.779827356338501,
      "learning_rate": 4.534344059405941e-05,
      "loss": 8.1286,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 3.6931838989257812,
      "learning_rate": 4.5321340169731266e-05,
      "loss": 8.1238,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 1.4945954084396362,
      "learning_rate": 4.529923974540312e-05,
      "loss": 8.1367,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 1.838985800743103,
      "learning_rate": 4.527713932107497e-05,
      "loss": 8.1363,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 1.0356295108795166,
      "learning_rate": 4.525503889674682e-05,
      "loss": 8.1453,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 2.1280863285064697,
      "learning_rate": 4.5232938472418677e-05,
      "loss": 8.1204,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 2.179682731628418,
      "learning_rate": 4.521083804809053e-05,
      "loss": 8.1256,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 1.5666074752807617,
      "learning_rate": 4.518873762376238e-05,
      "loss": 8.1255,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 1.3898165225982666,
      "learning_rate": 4.516663719943423e-05,
      "loss": 8.1194,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 1.6152321100234985,
      "learning_rate": 4.514453677510608e-05,
      "loss": 8.1377,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 0.9872780442237854,
      "learning_rate": 4.512243635077794e-05,
      "loss": 8.1251,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 1.3864965438842773,
      "learning_rate": 4.510033592644979e-05,
      "loss": 8.1347,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 1.6805392503738403,
      "learning_rate": 4.507823550212164e-05,
      "loss": 8.1309,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 0.8944875597953796,
      "learning_rate": 4.505613507779349e-05,
      "loss": 8.1284,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.7924079895019531,
      "learning_rate": 4.503403465346535e-05,
      "loss": 8.1483,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 2.8641250133514404,
      "learning_rate": 4.5011934229137206e-05,
      "loss": 8.1206,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 2.6894214153289795,
      "learning_rate": 4.498983380480906e-05,
      "loss": 8.1099,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 1.2986316680908203,
      "learning_rate": 4.496773338048091e-05,
      "loss": 8.1174,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 0.7564579248428345,
      "learning_rate": 4.494563295615276e-05,
      "loss": 8.1263,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 2.1276535987854004,
      "learning_rate": 4.492353253182462e-05,
      "loss": 8.1272,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 1.3789678812026978,
      "learning_rate": 4.490143210749647e-05,
      "loss": 8.1159,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.9833824634552002,
      "learning_rate": 4.487933168316832e-05,
      "loss": 8.1261,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 1.6973011493682861,
      "learning_rate": 4.485723125884017e-05,
      "loss": 8.1218,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 1.2579853534698486,
      "learning_rate": 4.483513083451202e-05,
      "loss": 8.1356,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 1.0678609609603882,
      "learning_rate": 4.481303041018388e-05,
      "loss": 8.1115,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 1.0080513954162598,
      "learning_rate": 4.479092998585573e-05,
      "loss": 8.1416,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 1.728085994720459,
      "learning_rate": 4.476882956152758e-05,
      "loss": 8.1256,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 1.0013184547424316,
      "learning_rate": 4.474672913719944e-05,
      "loss": 8.1265,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 1.2561790943145752,
      "learning_rate": 4.472462871287129e-05,
      "loss": 8.1165,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 2.669976234436035,
      "learning_rate": 4.470252828854315e-05,
      "loss": 8.117,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 1.0632567405700684,
      "learning_rate": 4.4680427864215e-05,
      "loss": 8.1222,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 2.326310396194458,
      "learning_rate": 4.465832743988685e-05,
      "loss": 8.1339,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 1.9265451431274414,
      "learning_rate": 4.46362270155587e-05,
      "loss": 8.1283,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 1.9307504892349243,
      "learning_rate": 4.461412659123056e-05,
      "loss": 8.1387,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 1.3953375816345215,
      "learning_rate": 4.459202616690241e-05,
      "loss": 8.1301,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.7817894220352173,
      "learning_rate": 4.456992574257426e-05,
      "loss": 8.1312,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.9156811237335205,
      "learning_rate": 4.454782531824611e-05,
      "loss": 8.1052,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 1.400242567062378,
      "learning_rate": 4.452572489391796e-05,
      "loss": 8.1224,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 1.0398952960968018,
      "learning_rate": 4.450362446958982e-05,
      "loss": 8.1375,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 0.8242036700248718,
      "learning_rate": 4.448152404526167e-05,
      "loss": 8.1193,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.8784144520759583,
      "learning_rate": 4.445942362093352e-05,
      "loss": 8.1386,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 0.7130635976791382,
      "learning_rate": 4.443732319660538e-05,
      "loss": 8.122,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 0.9169226288795471,
      "learning_rate": 4.441522277227723e-05,
      "loss": 8.0999,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 0.8026123642921448,
      "learning_rate": 4.439312234794909e-05,
      "loss": 8.1268,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 3.0320956707000732,
      "learning_rate": 4.437102192362094e-05,
      "loss": 8.1179,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 1.7157331705093384,
      "learning_rate": 4.434892149929279e-05,
      "loss": 8.1289,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 1.0207972526550293,
      "learning_rate": 4.432682107496464e-05,
      "loss": 8.1293,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 1.2355436086654663,
      "learning_rate": 4.43047206506365e-05,
      "loss": 8.1244,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 0.8576319813728333,
      "learning_rate": 4.428262022630835e-05,
      "loss": 8.1309,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 1.5930135250091553,
      "learning_rate": 4.42605198019802e-05,
      "loss": 8.1319,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 1.2188942432403564,
      "learning_rate": 4.423841937765205e-05,
      "loss": 8.1204,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 1.5238261222839355,
      "learning_rate": 4.42163189533239e-05,
      "loss": 8.1038,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 1.3246231079101562,
      "learning_rate": 4.419421852899576e-05,
      "loss": 8.1085,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 2.379167318344116,
      "learning_rate": 4.417211810466761e-05,
      "loss": 8.1276,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 1.0612999200820923,
      "learning_rate": 4.415001768033947e-05,
      "loss": 8.1354,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 1.0679903030395508,
      "learning_rate": 4.412791725601132e-05,
      "loss": 8.1209,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 1.2253437042236328,
      "learning_rate": 4.410581683168317e-05,
      "loss": 8.1256,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 1.4744584560394287,
      "learning_rate": 4.408371640735503e-05,
      "loss": 8.141,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 0.7224366664886475,
      "learning_rate": 4.406161598302688e-05,
      "loss": 8.1234,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 1.8724217414855957,
      "learning_rate": 4.403951555869873e-05,
      "loss": 8.1192,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 0.7011707425117493,
      "learning_rate": 4.401741513437058e-05,
      "loss": 8.1254,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 0.8190743923187256,
      "learning_rate": 4.399531471004244e-05,
      "loss": 8.1438,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 1.3879348039627075,
      "learning_rate": 4.397321428571429e-05,
      "loss": 8.1342,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 1.4845677614212036,
      "learning_rate": 4.395111386138614e-05,
      "loss": 8.1168,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.7718964219093323,
      "learning_rate": 4.392901343705799e-05,
      "loss": 8.132,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 0.7378916144371033,
      "learning_rate": 4.390691301272984e-05,
      "loss": 8.1368,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 0.7976837754249573,
      "learning_rate": 4.38848125884017e-05,
      "loss": 8.1308,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 1.2536346912384033,
      "learning_rate": 4.386271216407356e-05,
      "loss": 8.1242,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.9537434577941895,
      "learning_rate": 4.384061173974541e-05,
      "loss": 8.1134,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.9349040389060974,
      "learning_rate": 4.381851131541726e-05,
      "loss": 8.1374,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.7789999842643738,
      "learning_rate": 4.379641089108911e-05,
      "loss": 8.1113,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 1.2549035549163818,
      "learning_rate": 4.377431046676097e-05,
      "loss": 8.1103,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 1.6388391256332397,
      "learning_rate": 4.375221004243282e-05,
      "loss": 8.1508,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.7326608300209045,
      "learning_rate": 4.373010961810467e-05,
      "loss": 8.1253,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.9195551872253418,
      "learning_rate": 4.370800919377652e-05,
      "loss": 8.1298,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 1.2265956401824951,
      "learning_rate": 4.368590876944838e-05,
      "loss": 8.1217,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.8913390040397644,
      "learning_rate": 4.366380834512023e-05,
      "loss": 8.1237,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 1.0213226079940796,
      "learning_rate": 4.364170792079208e-05,
      "loss": 8.1215,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 1.399215817451477,
      "learning_rate": 4.361960749646393e-05,
      "loss": 8.12,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 2.366387128829956,
      "learning_rate": 4.359750707213578e-05,
      "loss": 8.132,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 1.5653551816940308,
      "learning_rate": 4.357540664780764e-05,
      "loss": 8.1367,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 2.0236313343048096,
      "learning_rate": 4.35533062234795e-05,
      "loss": 8.1309,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 2.4536983966827393,
      "learning_rate": 4.353120579915135e-05,
      "loss": 8.1306,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 0.7204288840293884,
      "learning_rate": 4.35091053748232e-05,
      "loss": 8.1167,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 0.8028860688209534,
      "learning_rate": 4.348700495049505e-05,
      "loss": 8.1145,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 1.3860328197479248,
      "learning_rate": 4.346490452616691e-05,
      "loss": 8.1142,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 0.9409781694412231,
      "learning_rate": 4.344280410183876e-05,
      "loss": 8.1198,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 2.06516695022583,
      "learning_rate": 4.342070367751061e-05,
      "loss": 8.1193,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 1.0741915702819824,
      "learning_rate": 4.339860325318246e-05,
      "loss": 8.1338,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 1.0689867734909058,
      "learning_rate": 4.337650282885432e-05,
      "loss": 8.13,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.6213433146476746,
      "learning_rate": 4.335440240452617e-05,
      "loss": 8.1026,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.8502789735794067,
      "learning_rate": 4.333230198019802e-05,
      "loss": 8.1249,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.8461669087409973,
      "learning_rate": 4.331020155586987e-05,
      "loss": 8.1218,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 1.2638654708862305,
      "learning_rate": 4.328810113154172e-05,
      "loss": 8.1162,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 1.817523717880249,
      "learning_rate": 4.326600070721358e-05,
      "loss": 8.1324,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 1.7943769693374634,
      "learning_rate": 4.324390028288544e-05,
      "loss": 8.1317,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 1.5107828378677368,
      "learning_rate": 4.322179985855729e-05,
      "loss": 8.1342,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 1.071122407913208,
      "learning_rate": 4.319969943422914e-05,
      "loss": 8.1342,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 1.332771897315979,
      "learning_rate": 4.317759900990099e-05,
      "loss": 8.1103,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 1.1416230201721191,
      "learning_rate": 4.315549858557285e-05,
      "loss": 8.1236,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 0.8420162200927734,
      "learning_rate": 4.31333981612447e-05,
      "loss": 8.1285,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 2.129894256591797,
      "learning_rate": 4.311129773691655e-05,
      "loss": 8.113,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 0.8761067986488342,
      "learning_rate": 4.30891973125884e-05,
      "loss": 8.1245,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 1.02657949924469,
      "learning_rate": 4.306709688826026e-05,
      "loss": 8.1165,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 0.9194222092628479,
      "learning_rate": 4.304499646393211e-05,
      "loss": 8.1331,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.9798198938369751,
      "learning_rate": 4.302289603960396e-05,
      "loss": 8.127,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 0.6078124046325684,
      "learning_rate": 4.300079561527581e-05,
      "loss": 8.1163,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 0.9838354587554932,
      "learning_rate": 4.297869519094767e-05,
      "loss": 8.1217,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.9231829047203064,
      "learning_rate": 4.295659476661952e-05,
      "loss": 8.116,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 1.1258717775344849,
      "learning_rate": 4.293449434229138e-05,
      "loss": 8.1171,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 1.7347180843353271,
      "learning_rate": 4.291239391796323e-05,
      "loss": 8.1217,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.7764060497283936,
      "learning_rate": 4.289029349363508e-05,
      "loss": 8.1162,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 1.1703591346740723,
      "learning_rate": 4.286819306930693e-05,
      "loss": 8.1115,
      "step": 3230
    },
    {
      "epoch": 1.002242672647127,
      "grad_norm": 1.196240782737732,
      "learning_rate": 4.284609264497879e-05,
      "loss": 8.1123,
      "step": 3240
    },
    {
      "epoch": 1.0053360142293712,
      "grad_norm": 0.6723086833953857,
      "learning_rate": 4.282399222065064e-05,
      "loss": 8.129,
      "step": 3250
    },
    {
      "epoch": 1.0084293558116155,
      "grad_norm": 0.9803570508956909,
      "learning_rate": 4.280189179632249e-05,
      "loss": 8.1097,
      "step": 3260
    },
    {
      "epoch": 1.0115226973938598,
      "grad_norm": 0.9006723761558533,
      "learning_rate": 4.277979137199434e-05,
      "loss": 8.1046,
      "step": 3270
    },
    {
      "epoch": 1.0146160389761039,
      "grad_norm": 1.476979374885559,
      "learning_rate": 4.27576909476662e-05,
      "loss": 8.1122,
      "step": 3280
    },
    {
      "epoch": 1.0177093805583481,
      "grad_norm": 0.8841773271560669,
      "learning_rate": 4.273559052333805e-05,
      "loss": 8.1124,
      "step": 3290
    },
    {
      "epoch": 1.0208027221405924,
      "grad_norm": 0.9440211057662964,
      "learning_rate": 4.27134900990099e-05,
      "loss": 8.128,
      "step": 3300
    },
    {
      "epoch": 1.0238960637228367,
      "grad_norm": 0.9217091798782349,
      "learning_rate": 4.269138967468175e-05,
      "loss": 8.1291,
      "step": 3310
    },
    {
      "epoch": 1.0269894053050808,
      "grad_norm": 1.0334994792938232,
      "learning_rate": 4.266928925035361e-05,
      "loss": 8.1276,
      "step": 3320
    },
    {
      "epoch": 1.030082746887325,
      "grad_norm": 0.8725922107696533,
      "learning_rate": 4.264718882602546e-05,
      "loss": 8.1128,
      "step": 3330
    },
    {
      "epoch": 1.0331760884695693,
      "grad_norm": 0.8615626096725464,
      "learning_rate": 4.262508840169732e-05,
      "loss": 8.1265,
      "step": 3340
    },
    {
      "epoch": 1.0362694300518134,
      "grad_norm": 1.0167025327682495,
      "learning_rate": 4.260298797736917e-05,
      "loss": 8.109,
      "step": 3350
    },
    {
      "epoch": 1.0393627716340577,
      "grad_norm": 1.2046356201171875,
      "learning_rate": 4.258088755304102e-05,
      "loss": 8.1251,
      "step": 3360
    },
    {
      "epoch": 1.042456113216302,
      "grad_norm": 0.7053648829460144,
      "learning_rate": 4.255878712871287e-05,
      "loss": 8.1268,
      "step": 3370
    },
    {
      "epoch": 1.0455494547985462,
      "grad_norm": 0.7233771681785583,
      "learning_rate": 4.253668670438473e-05,
      "loss": 8.1078,
      "step": 3380
    },
    {
      "epoch": 1.0486427963807903,
      "grad_norm": 0.6681994795799255,
      "learning_rate": 4.251458628005658e-05,
      "loss": 8.1095,
      "step": 3390
    },
    {
      "epoch": 1.0517361379630346,
      "grad_norm": 0.8064314126968384,
      "learning_rate": 4.249248585572843e-05,
      "loss": 8.122,
      "step": 3400
    },
    {
      "epoch": 1.0548294795452788,
      "grad_norm": 1.7897396087646484,
      "learning_rate": 4.247038543140028e-05,
      "loss": 8.1182,
      "step": 3410
    },
    {
      "epoch": 1.057922821127523,
      "grad_norm": 0.9416254162788391,
      "learning_rate": 4.244828500707214e-05,
      "loss": 8.1054,
      "step": 3420
    },
    {
      "epoch": 1.0610161627097672,
      "grad_norm": 1.1845312118530273,
      "learning_rate": 4.242618458274399e-05,
      "loss": 8.1231,
      "step": 3430
    },
    {
      "epoch": 1.0641095042920115,
      "grad_norm": 1.0002448558807373,
      "learning_rate": 4.240408415841584e-05,
      "loss": 8.1166,
      "step": 3440
    },
    {
      "epoch": 1.0672028458742557,
      "grad_norm": 1.26829993724823,
      "learning_rate": 4.23819837340877e-05,
      "loss": 8.1306,
      "step": 3450
    },
    {
      "epoch": 1.0702961874564998,
      "grad_norm": 0.8291583061218262,
      "learning_rate": 4.235988330975955e-05,
      "loss": 8.1312,
      "step": 3460
    },
    {
      "epoch": 1.073389529038744,
      "grad_norm": 1.0712348222732544,
      "learning_rate": 4.23377828854314e-05,
      "loss": 8.1141,
      "step": 3470
    },
    {
      "epoch": 1.0764828706209884,
      "grad_norm": 0.7974317669868469,
      "learning_rate": 4.231568246110326e-05,
      "loss": 8.1273,
      "step": 3480
    },
    {
      "epoch": 1.0795762122032326,
      "grad_norm": 0.7962043285369873,
      "learning_rate": 4.229358203677511e-05,
      "loss": 8.142,
      "step": 3490
    },
    {
      "epoch": 1.0826695537854767,
      "grad_norm": 0.7399154305458069,
      "learning_rate": 4.227148161244696e-05,
      "loss": 8.1102,
      "step": 3500
    },
    {
      "epoch": 1.085762895367721,
      "grad_norm": 0.807029128074646,
      "learning_rate": 4.224938118811881e-05,
      "loss": 8.1096,
      "step": 3510
    },
    {
      "epoch": 1.0888562369499653,
      "grad_norm": 0.8599138259887695,
      "learning_rate": 4.222728076379067e-05,
      "loss": 8.119,
      "step": 3520
    },
    {
      "epoch": 1.0919495785322093,
      "grad_norm": 0.8368381857872009,
      "learning_rate": 4.220518033946252e-05,
      "loss": 8.1338,
      "step": 3530
    },
    {
      "epoch": 1.0950429201144536,
      "grad_norm": 1.4200211763381958,
      "learning_rate": 4.218307991513437e-05,
      "loss": 8.1225,
      "step": 3540
    },
    {
      "epoch": 1.0981362616966979,
      "grad_norm": 0.8795577883720398,
      "learning_rate": 4.216097949080622e-05,
      "loss": 8.1274,
      "step": 3550
    },
    {
      "epoch": 1.1012296032789421,
      "grad_norm": 0.6865503787994385,
      "learning_rate": 4.213887906647808e-05,
      "loss": 8.1112,
      "step": 3560
    },
    {
      "epoch": 1.1043229448611862,
      "grad_norm": 1.285470724105835,
      "learning_rate": 4.211677864214993e-05,
      "loss": 8.1272,
      "step": 3570
    },
    {
      "epoch": 1.1074162864434305,
      "grad_norm": 1.0266700983047485,
      "learning_rate": 4.209467821782179e-05,
      "loss": 8.1126,
      "step": 3580
    },
    {
      "epoch": 1.1105096280256748,
      "grad_norm": 1.6995761394500732,
      "learning_rate": 4.207257779349364e-05,
      "loss": 8.1124,
      "step": 3590
    },
    {
      "epoch": 1.113602969607919,
      "grad_norm": 0.841712236404419,
      "learning_rate": 4.205047736916549e-05,
      "loss": 8.125,
      "step": 3600
    },
    {
      "epoch": 1.116696311190163,
      "grad_norm": 0.7503789663314819,
      "learning_rate": 4.202837694483734e-05,
      "loss": 8.1223,
      "step": 3610
    },
    {
      "epoch": 1.1197896527724074,
      "grad_norm": 0.9557420611381531,
      "learning_rate": 4.20062765205092e-05,
      "loss": 8.1153,
      "step": 3620
    },
    {
      "epoch": 1.1228829943546517,
      "grad_norm": 0.7093141674995422,
      "learning_rate": 4.198417609618105e-05,
      "loss": 8.1388,
      "step": 3630
    },
    {
      "epoch": 1.1259763359368957,
      "grad_norm": 0.7262153625488281,
      "learning_rate": 4.19620756718529e-05,
      "loss": 8.1167,
      "step": 3640
    },
    {
      "epoch": 1.12906967751914,
      "grad_norm": 0.7020951509475708,
      "learning_rate": 4.193997524752475e-05,
      "loss": 8.1244,
      "step": 3650
    },
    {
      "epoch": 1.1321630191013843,
      "grad_norm": 1.3000520467758179,
      "learning_rate": 4.191787482319661e-05,
      "loss": 8.1277,
      "step": 3660
    },
    {
      "epoch": 1.1352563606836286,
      "grad_norm": 0.5666297674179077,
      "learning_rate": 4.189577439886846e-05,
      "loss": 8.1104,
      "step": 3670
    },
    {
      "epoch": 1.1383497022658726,
      "grad_norm": 1.1555589437484741,
      "learning_rate": 4.187367397454031e-05,
      "loss": 8.1115,
      "step": 3680
    },
    {
      "epoch": 1.141443043848117,
      "grad_norm": 0.8192570209503174,
      "learning_rate": 4.185157355021216e-05,
      "loss": 8.1068,
      "step": 3690
    },
    {
      "epoch": 1.1445363854303612,
      "grad_norm": 0.8242233395576477,
      "learning_rate": 4.182947312588402e-05,
      "loss": 8.1175,
      "step": 3700
    },
    {
      "epoch": 1.1476297270126055,
      "grad_norm": 1.2689039707183838,
      "learning_rate": 4.180737270155587e-05,
      "loss": 8.1058,
      "step": 3710
    },
    {
      "epoch": 1.1507230685948495,
      "grad_norm": 1.6071594953536987,
      "learning_rate": 4.178527227722773e-05,
      "loss": 8.1172,
      "step": 3720
    },
    {
      "epoch": 1.1538164101770938,
      "grad_norm": 1.1766440868377686,
      "learning_rate": 4.176317185289958e-05,
      "loss": 8.1127,
      "step": 3730
    },
    {
      "epoch": 1.156909751759338,
      "grad_norm": 1.276888370513916,
      "learning_rate": 4.174107142857143e-05,
      "loss": 8.1253,
      "step": 3740
    },
    {
      "epoch": 1.1600030933415821,
      "grad_norm": 0.7621655464172363,
      "learning_rate": 4.171897100424328e-05,
      "loss": 8.11,
      "step": 3750
    },
    {
      "epoch": 1.1630964349238264,
      "grad_norm": 0.8809208869934082,
      "learning_rate": 4.169687057991514e-05,
      "loss": 8.1232,
      "step": 3760
    },
    {
      "epoch": 1.1661897765060707,
      "grad_norm": 0.8856102824211121,
      "learning_rate": 4.167477015558699e-05,
      "loss": 8.1147,
      "step": 3770
    },
    {
      "epoch": 1.169283118088315,
      "grad_norm": 0.7884340882301331,
      "learning_rate": 4.165266973125884e-05,
      "loss": 8.1138,
      "step": 3780
    },
    {
      "epoch": 1.172376459670559,
      "grad_norm": 0.6883849501609802,
      "learning_rate": 4.163056930693069e-05,
      "loss": 8.1246,
      "step": 3790
    },
    {
      "epoch": 1.1754698012528033,
      "grad_norm": 0.6409220695495605,
      "learning_rate": 4.160846888260255e-05,
      "loss": 8.1282,
      "step": 3800
    },
    {
      "epoch": 1.1785631428350476,
      "grad_norm": 0.958209216594696,
      "learning_rate": 4.15863684582744e-05,
      "loss": 8.1206,
      "step": 3810
    },
    {
      "epoch": 1.1816564844172919,
      "grad_norm": 0.7926703095436096,
      "learning_rate": 4.156426803394625e-05,
      "loss": 8.1229,
      "step": 3820
    },
    {
      "epoch": 1.184749825999536,
      "grad_norm": 0.8342359662055969,
      "learning_rate": 4.15421676096181e-05,
      "loss": 8.101,
      "step": 3830
    },
    {
      "epoch": 1.1878431675817802,
      "grad_norm": 1.3700296878814697,
      "learning_rate": 4.152006718528996e-05,
      "loss": 8.1235,
      "step": 3840
    },
    {
      "epoch": 1.1909365091640245,
      "grad_norm": 1.2245920896530151,
      "learning_rate": 4.149796676096182e-05,
      "loss": 8.1154,
      "step": 3850
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 0.8439896106719971,
      "learning_rate": 4.147586633663367e-05,
      "loss": 8.1216,
      "step": 3860
    },
    {
      "epoch": 1.1971231923285128,
      "grad_norm": 1.4020590782165527,
      "learning_rate": 4.145376591230552e-05,
      "loss": 8.1103,
      "step": 3870
    },
    {
      "epoch": 1.200216533910757,
      "grad_norm": 0.6946353912353516,
      "learning_rate": 4.143166548797737e-05,
      "loss": 8.1273,
      "step": 3880
    },
    {
      "epoch": 1.2033098754930014,
      "grad_norm": 1.4112876653671265,
      "learning_rate": 4.140956506364922e-05,
      "loss": 8.1156,
      "step": 3890
    },
    {
      "epoch": 1.2064032170752454,
      "grad_norm": 1.538412094116211,
      "learning_rate": 4.138746463932108e-05,
      "loss": 8.1164,
      "step": 3900
    },
    {
      "epoch": 1.2094965586574897,
      "grad_norm": 0.8160387277603149,
      "learning_rate": 4.136536421499293e-05,
      "loss": 8.1171,
      "step": 3910
    },
    {
      "epoch": 1.212589900239734,
      "grad_norm": 0.8632400035858154,
      "learning_rate": 4.134326379066478e-05,
      "loss": 8.1156,
      "step": 3920
    },
    {
      "epoch": 1.2156832418219783,
      "grad_norm": 1.040489673614502,
      "learning_rate": 4.132116336633663e-05,
      "loss": 8.1033,
      "step": 3930
    },
    {
      "epoch": 1.2187765834042223,
      "grad_norm": 1.193398356437683,
      "learning_rate": 4.129906294200849e-05,
      "loss": 8.128,
      "step": 3940
    },
    {
      "epoch": 1.2218699249864666,
      "grad_norm": 0.8503872752189636,
      "learning_rate": 4.127696251768034e-05,
      "loss": 8.1126,
      "step": 3950
    },
    {
      "epoch": 1.224963266568711,
      "grad_norm": 2.0096020698547363,
      "learning_rate": 4.125486209335219e-05,
      "loss": 8.1267,
      "step": 3960
    },
    {
      "epoch": 1.228056608150955,
      "grad_norm": 2.299232244491577,
      "learning_rate": 4.123276166902404e-05,
      "loss": 8.1184,
      "step": 3970
    },
    {
      "epoch": 1.2311499497331992,
      "grad_norm": 0.7290074229240417,
      "learning_rate": 4.12106612446959e-05,
      "loss": 8.106,
      "step": 3980
    },
    {
      "epoch": 1.2342432913154435,
      "grad_norm": 1.5019081830978394,
      "learning_rate": 4.118856082036776e-05,
      "loss": 8.1194,
      "step": 3990
    },
    {
      "epoch": 1.2373366328976878,
      "grad_norm": 0.9979866743087769,
      "learning_rate": 4.116646039603961e-05,
      "loss": 8.1205,
      "step": 4000
    },
    {
      "epoch": 1.2404299744799319,
      "grad_norm": 0.748677670955658,
      "learning_rate": 4.114435997171146e-05,
      "loss": 8.1205,
      "step": 4010
    },
    {
      "epoch": 1.2435233160621761,
      "grad_norm": 0.5963840484619141,
      "learning_rate": 4.112225954738331e-05,
      "loss": 8.1159,
      "step": 4020
    },
    {
      "epoch": 1.2466166576444204,
      "grad_norm": 1.075423002243042,
      "learning_rate": 4.110015912305516e-05,
      "loss": 8.1272,
      "step": 4030
    },
    {
      "epoch": 1.2497099992266647,
      "grad_norm": 0.8496195673942566,
      "learning_rate": 4.107805869872702e-05,
      "loss": 8.1193,
      "step": 4040
    },
    {
      "epoch": 1.2528033408089088,
      "grad_norm": 0.7476891279220581,
      "learning_rate": 4.105595827439887e-05,
      "loss": 8.1056,
      "step": 4050
    },
    {
      "epoch": 1.255896682391153,
      "grad_norm": 1.019012212753296,
      "learning_rate": 4.103385785007072e-05,
      "loss": 8.1105,
      "step": 4060
    },
    {
      "epoch": 1.2589900239733973,
      "grad_norm": 0.9022393226623535,
      "learning_rate": 4.101175742574257e-05,
      "loss": 8.1065,
      "step": 4070
    },
    {
      "epoch": 1.2620833655556414,
      "grad_norm": 0.8938788771629333,
      "learning_rate": 4.098965700141443e-05,
      "loss": 8.1397,
      "step": 4080
    },
    {
      "epoch": 1.2651767071378857,
      "grad_norm": 0.9741756319999695,
      "learning_rate": 4.096755657708628e-05,
      "loss": 8.1337,
      "step": 4090
    },
    {
      "epoch": 1.26827004872013,
      "grad_norm": 1.0345206260681152,
      "learning_rate": 4.094545615275813e-05,
      "loss": 8.1068,
      "step": 4100
    },
    {
      "epoch": 1.2713633903023742,
      "grad_norm": 0.7918864488601685,
      "learning_rate": 4.092335572842998e-05,
      "loss": 8.104,
      "step": 4110
    },
    {
      "epoch": 1.2744567318846183,
      "grad_norm": 0.8275113701820374,
      "learning_rate": 4.090125530410184e-05,
      "loss": 8.1012,
      "step": 4120
    },
    {
      "epoch": 1.2775500734668626,
      "grad_norm": 0.8178954124450684,
      "learning_rate": 4.08791548797737e-05,
      "loss": 8.1174,
      "step": 4130
    },
    {
      "epoch": 1.2806434150491068,
      "grad_norm": 0.8959318399429321,
      "learning_rate": 4.085705445544555e-05,
      "loss": 8.1188,
      "step": 4140
    },
    {
      "epoch": 1.2837367566313511,
      "grad_norm": 1.0194681882858276,
      "learning_rate": 4.08349540311174e-05,
      "loss": 8.1197,
      "step": 4150
    },
    {
      "epoch": 1.2868300982135952,
      "grad_norm": 0.8763304948806763,
      "learning_rate": 4.081285360678925e-05,
      "loss": 8.1239,
      "step": 4160
    },
    {
      "epoch": 1.2899234397958395,
      "grad_norm": 0.9870306253433228,
      "learning_rate": 4.07907531824611e-05,
      "loss": 8.1268,
      "step": 4170
    },
    {
      "epoch": 1.2930167813780837,
      "grad_norm": 1.064742922782898,
      "learning_rate": 4.076865275813296e-05,
      "loss": 8.1194,
      "step": 4180
    },
    {
      "epoch": 1.2961101229603278,
      "grad_norm": 0.6587199568748474,
      "learning_rate": 4.074655233380481e-05,
      "loss": 8.1078,
      "step": 4190
    },
    {
      "epoch": 1.299203464542572,
      "grad_norm": 0.9830871224403381,
      "learning_rate": 4.072445190947666e-05,
      "loss": 8.1107,
      "step": 4200
    },
    {
      "epoch": 1.3022968061248164,
      "grad_norm": 1.1235051155090332,
      "learning_rate": 4.070235148514851e-05,
      "loss": 8.1202,
      "step": 4210
    },
    {
      "epoch": 1.3053901477070606,
      "grad_norm": 0.8844292759895325,
      "learning_rate": 4.068025106082037e-05,
      "loss": 8.125,
      "step": 4220
    },
    {
      "epoch": 1.3084834892893047,
      "grad_norm": 0.8180374503135681,
      "learning_rate": 4.0660360678925034e-05,
      "loss": 8.1132,
      "step": 4230
    },
    {
      "epoch": 1.311576830871549,
      "grad_norm": 0.7194439172744751,
      "learning_rate": 4.0638260254596885e-05,
      "loss": 8.1183,
      "step": 4240
    },
    {
      "epoch": 1.3146701724537933,
      "grad_norm": 0.6383553743362427,
      "learning_rate": 4.061615983026874e-05,
      "loss": 8.1124,
      "step": 4250
    },
    {
      "epoch": 1.3177635140360375,
      "grad_norm": 1.5439708232879639,
      "learning_rate": 4.05940594059406e-05,
      "loss": 8.1049,
      "step": 4260
    },
    {
      "epoch": 1.3208568556182816,
      "grad_norm": 0.8215919137001038,
      "learning_rate": 4.057195898161245e-05,
      "loss": 8.1187,
      "step": 4270
    },
    {
      "epoch": 1.3239501972005259,
      "grad_norm": 1.3486407995224,
      "learning_rate": 4.05498585572843e-05,
      "loss": 8.1113,
      "step": 4280
    },
    {
      "epoch": 1.3270435387827701,
      "grad_norm": 0.7948507070541382,
      "learning_rate": 4.052775813295615e-05,
      "loss": 8.1193,
      "step": 4290
    },
    {
      "epoch": 1.3301368803650142,
      "grad_norm": 0.8348109126091003,
      "learning_rate": 4.050565770862801e-05,
      "loss": 8.1097,
      "step": 4300
    },
    {
      "epoch": 1.3332302219472585,
      "grad_norm": 1.6734665632247925,
      "learning_rate": 4.048355728429986e-05,
      "loss": 8.1013,
      "step": 4310
    },
    {
      "epoch": 1.3363235635295028,
      "grad_norm": 1.6595321893692017,
      "learning_rate": 4.046145685997171e-05,
      "loss": 8.1157,
      "step": 4320
    },
    {
      "epoch": 1.339416905111747,
      "grad_norm": 1.155346155166626,
      "learning_rate": 4.0439356435643564e-05,
      "loss": 8.1204,
      "step": 4330
    },
    {
      "epoch": 1.342510246693991,
      "grad_norm": 1.04776930809021,
      "learning_rate": 4.041725601131542e-05,
      "loss": 8.1105,
      "step": 4340
    },
    {
      "epoch": 1.3456035882762354,
      "grad_norm": 0.9083101153373718,
      "learning_rate": 4.039515558698727e-05,
      "loss": 8.1142,
      "step": 4350
    },
    {
      "epoch": 1.3486969298584797,
      "grad_norm": 0.7897149920463562,
      "learning_rate": 4.037305516265912e-05,
      "loss": 8.1239,
      "step": 4360
    },
    {
      "epoch": 1.351790271440724,
      "grad_norm": 0.9427816867828369,
      "learning_rate": 4.0350954738330974e-05,
      "loss": 8.1215,
      "step": 4370
    },
    {
      "epoch": 1.354883613022968,
      "grad_norm": 0.8597368597984314,
      "learning_rate": 4.0328854314002825e-05,
      "loss": 8.129,
      "step": 4380
    },
    {
      "epoch": 1.3579769546052123,
      "grad_norm": 0.7657512426376343,
      "learning_rate": 4.030675388967468e-05,
      "loss": 8.1156,
      "step": 4390
    },
    {
      "epoch": 1.3610702961874566,
      "grad_norm": 1.1744600534439087,
      "learning_rate": 4.028465346534654e-05,
      "loss": 8.1072,
      "step": 4400
    },
    {
      "epoch": 1.3641636377697006,
      "grad_norm": 0.7139284014701843,
      "learning_rate": 4.026255304101839e-05,
      "loss": 8.1136,
      "step": 4410
    },
    {
      "epoch": 1.367256979351945,
      "grad_norm": 0.9807798266410828,
      "learning_rate": 4.024045261669024e-05,
      "loss": 8.124,
      "step": 4420
    },
    {
      "epoch": 1.3703503209341892,
      "grad_norm": 0.5589386224746704,
      "learning_rate": 4.0218352192362093e-05,
      "loss": 8.1331,
      "step": 4430
    },
    {
      "epoch": 1.3734436625164332,
      "grad_norm": 0.8512092232704163,
      "learning_rate": 4.019625176803395e-05,
      "loss": 8.1161,
      "step": 4440
    },
    {
      "epoch": 1.3765370040986775,
      "grad_norm": 0.8046604990959167,
      "learning_rate": 4.01741513437058e-05,
      "loss": 8.1099,
      "step": 4450
    },
    {
      "epoch": 1.3796303456809218,
      "grad_norm": 0.9923661947250366,
      "learning_rate": 4.015205091937765e-05,
      "loss": 8.1189,
      "step": 4460
    },
    {
      "epoch": 1.382723687263166,
      "grad_norm": 1.1229900121688843,
      "learning_rate": 4.0129950495049504e-05,
      "loss": 8.117,
      "step": 4470
    },
    {
      "epoch": 1.3858170288454104,
      "grad_norm": 1.0397707223892212,
      "learning_rate": 4.010785007072136e-05,
      "loss": 8.1115,
      "step": 4480
    },
    {
      "epoch": 1.3889103704276544,
      "grad_norm": 1.102938175201416,
      "learning_rate": 4.008574964639321e-05,
      "loss": 8.1159,
      "step": 4490
    },
    {
      "epoch": 1.3920037120098987,
      "grad_norm": 0.8214883208274841,
      "learning_rate": 4.0063649222065064e-05,
      "loss": 8.1035,
      "step": 4500
    },
    {
      "epoch": 1.395097053592143,
      "grad_norm": 0.5585210919380188,
      "learning_rate": 4.0041548797736915e-05,
      "loss": 8.1115,
      "step": 4510
    },
    {
      "epoch": 1.398190395174387,
      "grad_norm": 0.7796432971954346,
      "learning_rate": 4.0019448373408765e-05,
      "loss": 8.1078,
      "step": 4520
    },
    {
      "epoch": 1.4012837367566313,
      "grad_norm": 1.1832889318466187,
      "learning_rate": 3.999734794908063e-05,
      "loss": 8.1274,
      "step": 4530
    },
    {
      "epoch": 1.4043770783388756,
      "grad_norm": 0.934551477432251,
      "learning_rate": 3.997524752475248e-05,
      "loss": 8.1287,
      "step": 4540
    },
    {
      "epoch": 1.4074704199211197,
      "grad_norm": 0.9180008172988892,
      "learning_rate": 3.995314710042433e-05,
      "loss": 8.105,
      "step": 4550
    },
    {
      "epoch": 1.410563761503364,
      "grad_norm": 1.7560917139053345,
      "learning_rate": 3.993104667609618e-05,
      "loss": 8.1259,
      "step": 4560
    },
    {
      "epoch": 1.4136571030856082,
      "grad_norm": 0.662970244884491,
      "learning_rate": 3.9908946251768034e-05,
      "loss": 8.129,
      "step": 4570
    },
    {
      "epoch": 1.4167504446678525,
      "grad_norm": 0.7053908109664917,
      "learning_rate": 3.988684582743989e-05,
      "loss": 8.1133,
      "step": 4580
    },
    {
      "epoch": 1.4198437862500968,
      "grad_norm": 0.7108381986618042,
      "learning_rate": 3.986474540311174e-05,
      "loss": 8.1014,
      "step": 4590
    },
    {
      "epoch": 1.4229371278323408,
      "grad_norm": 0.5228309035301208,
      "learning_rate": 3.984264497878359e-05,
      "loss": 8.0972,
      "step": 4600
    },
    {
      "epoch": 1.4260304694145851,
      "grad_norm": 1.6444404125213623,
      "learning_rate": 3.9820544554455444e-05,
      "loss": 8.1057,
      "step": 4610
    },
    {
      "epoch": 1.4291238109968294,
      "grad_norm": 1.3708868026733398,
      "learning_rate": 3.97984441301273e-05,
      "loss": 8.1136,
      "step": 4620
    },
    {
      "epoch": 1.4322171525790734,
      "grad_norm": 0.8128147125244141,
      "learning_rate": 3.977634370579915e-05,
      "loss": 8.1201,
      "step": 4630
    },
    {
      "epoch": 1.4353104941613177,
      "grad_norm": 0.6589893102645874,
      "learning_rate": 3.9754243281471004e-05,
      "loss": 8.1182,
      "step": 4640
    },
    {
      "epoch": 1.438403835743562,
      "grad_norm": 0.6600621342658997,
      "learning_rate": 3.9732142857142855e-05,
      "loss": 8.121,
      "step": 4650
    },
    {
      "epoch": 1.441497177325806,
      "grad_norm": 0.7272864580154419,
      "learning_rate": 3.971004243281471e-05,
      "loss": 8.1113,
      "step": 4660
    },
    {
      "epoch": 1.4445905189080503,
      "grad_norm": 0.5461433529853821,
      "learning_rate": 3.968794200848657e-05,
      "loss": 8.1205,
      "step": 4670
    },
    {
      "epoch": 1.4476838604902946,
      "grad_norm": 1.0180320739746094,
      "learning_rate": 3.966584158415842e-05,
      "loss": 8.1157,
      "step": 4680
    },
    {
      "epoch": 1.450777202072539,
      "grad_norm": 0.6926457285881042,
      "learning_rate": 3.964374115983027e-05,
      "loss": 8.108,
      "step": 4690
    },
    {
      "epoch": 1.4538705436547832,
      "grad_norm": 0.7072666883468628,
      "learning_rate": 3.962164073550212e-05,
      "loss": 8.1183,
      "step": 4700
    },
    {
      "epoch": 1.4569638852370272,
      "grad_norm": 0.7059903740882874,
      "learning_rate": 3.9599540311173974e-05,
      "loss": 8.1171,
      "step": 4710
    },
    {
      "epoch": 1.4600572268192715,
      "grad_norm": 0.957748293876648,
      "learning_rate": 3.957743988684583e-05,
      "loss": 8.1209,
      "step": 4720
    },
    {
      "epoch": 1.4631505684015158,
      "grad_norm": 1.2396856546401978,
      "learning_rate": 3.955533946251768e-05,
      "loss": 8.1166,
      "step": 4730
    },
    {
      "epoch": 1.4662439099837599,
      "grad_norm": 0.7316197752952576,
      "learning_rate": 3.9533239038189534e-05,
      "loss": 8.1132,
      "step": 4740
    },
    {
      "epoch": 1.4693372515660041,
      "grad_norm": 1.0521752834320068,
      "learning_rate": 3.9511138613861385e-05,
      "loss": 8.1125,
      "step": 4750
    },
    {
      "epoch": 1.4724305931482484,
      "grad_norm": 0.9110093116760254,
      "learning_rate": 3.948903818953324e-05,
      "loss": 8.1194,
      "step": 4760
    },
    {
      "epoch": 1.4755239347304925,
      "grad_norm": 1.4379953145980835,
      "learning_rate": 3.946693776520509e-05,
      "loss": 8.1168,
      "step": 4770
    },
    {
      "epoch": 1.4786172763127368,
      "grad_norm": 2.6326723098754883,
      "learning_rate": 3.9444837340876944e-05,
      "loss": 8.1228,
      "step": 4780
    },
    {
      "epoch": 1.481710617894981,
      "grad_norm": 0.9093007445335388,
      "learning_rate": 3.9422736916548795e-05,
      "loss": 8.1172,
      "step": 4790
    },
    {
      "epoch": 1.4848039594772253,
      "grad_norm": 1.3808073997497559,
      "learning_rate": 3.940063649222065e-05,
      "loss": 8.1097,
      "step": 4800
    },
    {
      "epoch": 1.4878973010594696,
      "grad_norm": 1.156030297279358,
      "learning_rate": 3.937853606789251e-05,
      "loss": 8.1163,
      "step": 4810
    },
    {
      "epoch": 1.4909906426417137,
      "grad_norm": 0.951022207736969,
      "learning_rate": 3.935643564356436e-05,
      "loss": 8.1125,
      "step": 4820
    },
    {
      "epoch": 1.494083984223958,
      "grad_norm": 1.0353246927261353,
      "learning_rate": 3.933433521923621e-05,
      "loss": 8.1266,
      "step": 4830
    },
    {
      "epoch": 1.4971773258062022,
      "grad_norm": 1.5673549175262451,
      "learning_rate": 3.9312234794908063e-05,
      "loss": 8.1071,
      "step": 4840
    },
    {
      "epoch": 1.5002706673884463,
      "grad_norm": 0.8361724019050598,
      "learning_rate": 3.9290134370579914e-05,
      "loss": 8.1019,
      "step": 4850
    },
    {
      "epoch": 1.5033640089706906,
      "grad_norm": 1.9008229970932007,
      "learning_rate": 3.926803394625177e-05,
      "loss": 8.1018,
      "step": 4860
    },
    {
      "epoch": 1.5064573505529348,
      "grad_norm": 1.5485745668411255,
      "learning_rate": 3.924593352192362e-05,
      "loss": 8.1134,
      "step": 4870
    },
    {
      "epoch": 1.509550692135179,
      "grad_norm": 1.1073501110076904,
      "learning_rate": 3.9223833097595474e-05,
      "loss": 8.1189,
      "step": 4880
    },
    {
      "epoch": 1.5126440337174234,
      "grad_norm": 0.9330412149429321,
      "learning_rate": 3.9201732673267325e-05,
      "loss": 8.1145,
      "step": 4890
    },
    {
      "epoch": 1.5157373752996675,
      "grad_norm": 1.2551467418670654,
      "learning_rate": 3.917963224893918e-05,
      "loss": 8.1187,
      "step": 4900
    },
    {
      "epoch": 1.5188307168819117,
      "grad_norm": 0.8344804048538208,
      "learning_rate": 3.9157531824611034e-05,
      "loss": 8.1093,
      "step": 4910
    },
    {
      "epoch": 1.521924058464156,
      "grad_norm": 0.6430570483207703,
      "learning_rate": 3.9135431400282885e-05,
      "loss": 8.1202,
      "step": 4920
    },
    {
      "epoch": 1.5250174000464,
      "grad_norm": 0.8347640037536621,
      "learning_rate": 3.911333097595474e-05,
      "loss": 8.0982,
      "step": 4930
    },
    {
      "epoch": 1.5281107416286444,
      "grad_norm": 0.781800389289856,
      "learning_rate": 3.909123055162659e-05,
      "loss": 8.1216,
      "step": 4940
    },
    {
      "epoch": 1.5312040832108886,
      "grad_norm": 0.928343653678894,
      "learning_rate": 3.906913012729845e-05,
      "loss": 8.1129,
      "step": 4950
    },
    {
      "epoch": 1.5342974247931327,
      "grad_norm": 1.2262492179870605,
      "learning_rate": 3.90470297029703e-05,
      "loss": 8.1268,
      "step": 4960
    },
    {
      "epoch": 1.537390766375377,
      "grad_norm": 1.014028549194336,
      "learning_rate": 3.902492927864215e-05,
      "loss": 8.1158,
      "step": 4970
    },
    {
      "epoch": 1.5404841079576213,
      "grad_norm": 0.7858170866966248,
      "learning_rate": 3.9002828854314004e-05,
      "loss": 8.1221,
      "step": 4980
    },
    {
      "epoch": 1.5435774495398653,
      "grad_norm": 0.6599904894828796,
      "learning_rate": 3.8980728429985855e-05,
      "loss": 8.1171,
      "step": 4990
    },
    {
      "epoch": 1.5466707911221098,
      "grad_norm": 1.3178737163543701,
      "learning_rate": 3.895862800565771e-05,
      "loss": 8.1041,
      "step": 5000
    },
    {
      "epoch": 1.5497641327043539,
      "grad_norm": 1.6083639860153198,
      "learning_rate": 3.8936527581329563e-05,
      "loss": 8.1254,
      "step": 5010
    },
    {
      "epoch": 1.5528574742865981,
      "grad_norm": 1.5805290937423706,
      "learning_rate": 3.8914427157001414e-05,
      "loss": 8.0958,
      "step": 5020
    },
    {
      "epoch": 1.5559508158688424,
      "grad_norm": 1.2158139944076538,
      "learning_rate": 3.8892326732673265e-05,
      "loss": 8.1145,
      "step": 5030
    },
    {
      "epoch": 1.5590441574510865,
      "grad_norm": 0.7423880696296692,
      "learning_rate": 3.887022630834512e-05,
      "loss": 8.1075,
      "step": 5040
    },
    {
      "epoch": 1.5621374990333308,
      "grad_norm": 1.2053617238998413,
      "learning_rate": 3.8848125884016974e-05,
      "loss": 8.1019,
      "step": 5050
    },
    {
      "epoch": 1.565230840615575,
      "grad_norm": 0.5607873201370239,
      "learning_rate": 3.882602545968883e-05,
      "loss": 8.1265,
      "step": 5060
    },
    {
      "epoch": 1.568324182197819,
      "grad_norm": 0.7237718105316162,
      "learning_rate": 3.880392503536068e-05,
      "loss": 8.0937,
      "step": 5070
    },
    {
      "epoch": 1.5714175237800634,
      "grad_norm": 1.6400437355041504,
      "learning_rate": 3.8781824611032534e-05,
      "loss": 8.1132,
      "step": 5080
    },
    {
      "epoch": 1.5745108653623077,
      "grad_norm": 0.618736982345581,
      "learning_rate": 3.875972418670439e-05,
      "loss": 8.1222,
      "step": 5090
    },
    {
      "epoch": 1.5776042069445517,
      "grad_norm": 1.4394365549087524,
      "learning_rate": 3.873762376237624e-05,
      "loss": 8.1201,
      "step": 5100
    },
    {
      "epoch": 1.5806975485267962,
      "grad_norm": 1.4025369882583618,
      "learning_rate": 3.871552333804809e-05,
      "loss": 8.1226,
      "step": 5110
    },
    {
      "epoch": 1.5837908901090403,
      "grad_norm": 0.745320737361908,
      "learning_rate": 3.8693422913719944e-05,
      "loss": 8.1075,
      "step": 5120
    },
    {
      "epoch": 1.5868842316912846,
      "grad_norm": 0.6909487247467041,
      "learning_rate": 3.8671322489391795e-05,
      "loss": 8.1062,
      "step": 5130
    },
    {
      "epoch": 1.5899775732735288,
      "grad_norm": 1.0641508102416992,
      "learning_rate": 3.864922206506365e-05,
      "loss": 8.1132,
      "step": 5140
    },
    {
      "epoch": 1.593070914855773,
      "grad_norm": 1.2837423086166382,
      "learning_rate": 3.8627121640735504e-05,
      "loss": 8.1281,
      "step": 5150
    },
    {
      "epoch": 1.5961642564380172,
      "grad_norm": 0.9686927795410156,
      "learning_rate": 3.8605021216407355e-05,
      "loss": 8.111,
      "step": 5160
    },
    {
      "epoch": 1.5992575980202615,
      "grad_norm": 0.5163270235061646,
      "learning_rate": 3.8582920792079206e-05,
      "loss": 8.1001,
      "step": 5170
    },
    {
      "epoch": 1.6023509396025055,
      "grad_norm": 0.7700867652893066,
      "learning_rate": 3.856082036775106e-05,
      "loss": 8.1198,
      "step": 5180
    },
    {
      "epoch": 1.6054442811847498,
      "grad_norm": 1.0453859567642212,
      "learning_rate": 3.8538719943422914e-05,
      "loss": 8.114,
      "step": 5190
    },
    {
      "epoch": 1.608537622766994,
      "grad_norm": 1.1160624027252197,
      "learning_rate": 3.851661951909477e-05,
      "loss": 8.1098,
      "step": 5200
    },
    {
      "epoch": 1.6116309643492381,
      "grad_norm": 0.7706289887428284,
      "learning_rate": 3.849451909476662e-05,
      "loss": 8.1194,
      "step": 5210
    },
    {
      "epoch": 1.6147243059314826,
      "grad_norm": 1.5528191328048706,
      "learning_rate": 3.8472418670438474e-05,
      "loss": 8.0974,
      "step": 5220
    },
    {
      "epoch": 1.6178176475137267,
      "grad_norm": 1.085829257965088,
      "learning_rate": 3.845031824611033e-05,
      "loss": 8.1216,
      "step": 5230
    },
    {
      "epoch": 1.620910989095971,
      "grad_norm": 1.0217398405075073,
      "learning_rate": 3.842821782178218e-05,
      "loss": 8.1287,
      "step": 5240
    },
    {
      "epoch": 1.6240043306782153,
      "grad_norm": 1.269791603088379,
      "learning_rate": 3.8406117397454033e-05,
      "loss": 8.1177,
      "step": 5250
    },
    {
      "epoch": 1.6270976722604593,
      "grad_norm": 0.6634871363639832,
      "learning_rate": 3.8384016973125884e-05,
      "loss": 8.1193,
      "step": 5260
    },
    {
      "epoch": 1.6301910138427036,
      "grad_norm": 0.7631880044937134,
      "learning_rate": 3.8361916548797735e-05,
      "loss": 8.1177,
      "step": 5270
    },
    {
      "epoch": 1.6332843554249479,
      "grad_norm": 1.3291696310043335,
      "learning_rate": 3.833981612446959e-05,
      "loss": 8.1398,
      "step": 5280
    },
    {
      "epoch": 1.636377697007192,
      "grad_norm": 0.8691796660423279,
      "learning_rate": 3.8317715700141444e-05,
      "loss": 8.1067,
      "step": 5290
    },
    {
      "epoch": 1.6394710385894362,
      "grad_norm": 0.931352436542511,
      "learning_rate": 3.8295615275813295e-05,
      "loss": 8.1137,
      "step": 5300
    },
    {
      "epoch": 1.6425643801716805,
      "grad_norm": 0.9599955081939697,
      "learning_rate": 3.8273514851485146e-05,
      "loss": 8.1128,
      "step": 5310
    },
    {
      "epoch": 1.6456577217539246,
      "grad_norm": 0.7459596395492554,
      "learning_rate": 3.8251414427157004e-05,
      "loss": 8.109,
      "step": 5320
    },
    {
      "epoch": 1.648751063336169,
      "grad_norm": 0.8625908493995667,
      "learning_rate": 3.822931400282886e-05,
      "loss": 8.0995,
      "step": 5330
    },
    {
      "epoch": 1.6518444049184131,
      "grad_norm": 1.3661296367645264,
      "learning_rate": 3.820721357850071e-05,
      "loss": 8.1041,
      "step": 5340
    },
    {
      "epoch": 1.6549377465006572,
      "grad_norm": 1.1211706399917603,
      "learning_rate": 3.818511315417256e-05,
      "loss": 8.1119,
      "step": 5350
    },
    {
      "epoch": 1.6580310880829017,
      "grad_norm": 0.6514363288879395,
      "learning_rate": 3.8163012729844414e-05,
      "loss": 8.1057,
      "step": 5360
    },
    {
      "epoch": 1.6611244296651457,
      "grad_norm": 0.8911955952644348,
      "learning_rate": 3.814091230551627e-05,
      "loss": 8.1198,
      "step": 5370
    },
    {
      "epoch": 1.66421777124739,
      "grad_norm": 1.1355024576187134,
      "learning_rate": 3.811881188118812e-05,
      "loss": 8.1219,
      "step": 5380
    },
    {
      "epoch": 1.6673111128296343,
      "grad_norm": 0.9677493572235107,
      "learning_rate": 3.8096711456859974e-05,
      "loss": 8.1104,
      "step": 5390
    },
    {
      "epoch": 1.6704044544118783,
      "grad_norm": 0.7535940408706665,
      "learning_rate": 3.8074611032531825e-05,
      "loss": 8.1196,
      "step": 5400
    },
    {
      "epoch": 1.6734977959941226,
      "grad_norm": 0.5847275257110596,
      "learning_rate": 3.8052510608203676e-05,
      "loss": 8.1004,
      "step": 5410
    },
    {
      "epoch": 1.676591137576367,
      "grad_norm": 0.885806143283844,
      "learning_rate": 3.8030410183875533e-05,
      "loss": 8.0886,
      "step": 5420
    },
    {
      "epoch": 1.679684479158611,
      "grad_norm": 1.359310269355774,
      "learning_rate": 3.8008309759547384e-05,
      "loss": 8.0951,
      "step": 5430
    },
    {
      "epoch": 1.6827778207408555,
      "grad_norm": 2.5270769596099854,
      "learning_rate": 3.7986209335219235e-05,
      "loss": 8.1231,
      "step": 5440
    },
    {
      "epoch": 1.6858711623230995,
      "grad_norm": 1.284864068031311,
      "learning_rate": 3.7964108910891086e-05,
      "loss": 8.1244,
      "step": 5450
    },
    {
      "epoch": 1.6889645039053436,
      "grad_norm": 1.3578400611877441,
      "learning_rate": 3.7942008486562944e-05,
      "loss": 8.1189,
      "step": 5460
    },
    {
      "epoch": 1.692057845487588,
      "grad_norm": 2.665391683578491,
      "learning_rate": 3.79199080622348e-05,
      "loss": 8.1234,
      "step": 5470
    },
    {
      "epoch": 1.6951511870698321,
      "grad_norm": 0.6308792233467102,
      "learning_rate": 3.789780763790665e-05,
      "loss": 8.1219,
      "step": 5480
    },
    {
      "epoch": 1.6982445286520764,
      "grad_norm": 1.0213325023651123,
      "learning_rate": 3.7875707213578504e-05,
      "loss": 8.1212,
      "step": 5490
    },
    {
      "epoch": 1.7013378702343207,
      "grad_norm": 1.4457077980041504,
      "learning_rate": 3.7853606789250355e-05,
      "loss": 8.1045,
      "step": 5500
    },
    {
      "epoch": 1.7044312118165648,
      "grad_norm": 0.6500237584114075,
      "learning_rate": 3.783150636492221e-05,
      "loss": 8.1071,
      "step": 5510
    },
    {
      "epoch": 1.707524553398809,
      "grad_norm": 0.9573272466659546,
      "learning_rate": 3.780940594059406e-05,
      "loss": 8.1274,
      "step": 5520
    },
    {
      "epoch": 1.7106178949810533,
      "grad_norm": 0.8521197438240051,
      "learning_rate": 3.7787305516265914e-05,
      "loss": 8.1037,
      "step": 5530
    },
    {
      "epoch": 1.7137112365632974,
      "grad_norm": 0.679905891418457,
      "learning_rate": 3.7765205091937765e-05,
      "loss": 8.1103,
      "step": 5540
    },
    {
      "epoch": 1.7168045781455419,
      "grad_norm": 0.9739455580711365,
      "learning_rate": 3.7743104667609616e-05,
      "loss": 8.1163,
      "step": 5550
    },
    {
      "epoch": 1.719897919727786,
      "grad_norm": 1.4107309579849243,
      "learning_rate": 3.7721004243281474e-05,
      "loss": 8.1068,
      "step": 5560
    },
    {
      "epoch": 1.72299126131003,
      "grad_norm": 0.9260146617889404,
      "learning_rate": 3.7698903818953325e-05,
      "loss": 8.1167,
      "step": 5570
    },
    {
      "epoch": 1.7260846028922745,
      "grad_norm": 0.6201887130737305,
      "learning_rate": 3.7676803394625176e-05,
      "loss": 8.1159,
      "step": 5580
    },
    {
      "epoch": 1.7291779444745186,
      "grad_norm": 0.917417049407959,
      "learning_rate": 3.7654702970297027e-05,
      "loss": 8.106,
      "step": 5590
    },
    {
      "epoch": 1.7322712860567628,
      "grad_norm": 1.094301462173462,
      "learning_rate": 3.7632602545968884e-05,
      "loss": 8.1308,
      "step": 5600
    },
    {
      "epoch": 1.7353646276390071,
      "grad_norm": 0.8865593075752258,
      "learning_rate": 3.7612712164073554e-05,
      "loss": 8.1141,
      "step": 5610
    },
    {
      "epoch": 1.7384579692212512,
      "grad_norm": 2.1721909046173096,
      "learning_rate": 3.7590611739745405e-05,
      "loss": 8.1198,
      "step": 5620
    },
    {
      "epoch": 1.7415513108034955,
      "grad_norm": 2.3732712268829346,
      "learning_rate": 3.756851131541726e-05,
      "loss": 8.1197,
      "step": 5630
    },
    {
      "epoch": 1.7446446523857397,
      "grad_norm": 1.7584190368652344,
      "learning_rate": 3.7546410891089114e-05,
      "loss": 8.1233,
      "step": 5640
    },
    {
      "epoch": 1.7477379939679838,
      "grad_norm": 0.9037502408027649,
      "learning_rate": 3.7524310466760965e-05,
      "loss": 8.1121,
      "step": 5650
    },
    {
      "epoch": 1.7508313355502283,
      "grad_norm": 1.0689423084259033,
      "learning_rate": 3.7502210042432816e-05,
      "loss": 8.1178,
      "step": 5660
    },
    {
      "epoch": 1.7539246771324724,
      "grad_norm": 0.8617199063301086,
      "learning_rate": 3.748010961810467e-05,
      "loss": 8.1235,
      "step": 5670
    },
    {
      "epoch": 1.7570180187147164,
      "grad_norm": 0.687280535697937,
      "learning_rate": 3.7458009193776524e-05,
      "loss": 8.1259,
      "step": 5680
    },
    {
      "epoch": 1.760111360296961,
      "grad_norm": 0.6172989010810852,
      "learning_rate": 3.7435908769448375e-05,
      "loss": 8.1163,
      "step": 5690
    },
    {
      "epoch": 1.763204701879205,
      "grad_norm": 0.7951784133911133,
      "learning_rate": 3.7413808345120226e-05,
      "loss": 8.1104,
      "step": 5700
    },
    {
      "epoch": 1.7662980434614493,
      "grad_norm": 1.39143705368042,
      "learning_rate": 3.739170792079208e-05,
      "loss": 8.1237,
      "step": 5710
    },
    {
      "epoch": 1.7693913850436935,
      "grad_norm": 0.688704252243042,
      "learning_rate": 3.7369607496463935e-05,
      "loss": 8.1073,
      "step": 5720
    },
    {
      "epoch": 1.7724847266259376,
      "grad_norm": 0.890242338180542,
      "learning_rate": 3.7347507072135786e-05,
      "loss": 8.1066,
      "step": 5730
    },
    {
      "epoch": 1.7755780682081819,
      "grad_norm": 0.5613893866539001,
      "learning_rate": 3.732540664780764e-05,
      "loss": 8.1179,
      "step": 5740
    },
    {
      "epoch": 1.7786714097904262,
      "grad_norm": 1.1916487216949463,
      "learning_rate": 3.7303306223479495e-05,
      "loss": 8.101,
      "step": 5750
    },
    {
      "epoch": 1.7817647513726702,
      "grad_norm": 0.8538577556610107,
      "learning_rate": 3.7281205799151345e-05,
      "loss": 8.1059,
      "step": 5760
    },
    {
      "epoch": 1.7848580929549147,
      "grad_norm": 0.7381499409675598,
      "learning_rate": 3.72591053748232e-05,
      "loss": 8.1159,
      "step": 5770
    },
    {
      "epoch": 1.7879514345371588,
      "grad_norm": 0.6222342848777771,
      "learning_rate": 3.7237004950495054e-05,
      "loss": 8.1096,
      "step": 5780
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 0.6624096035957336,
      "learning_rate": 3.7214904526166905e-05,
      "loss": 8.1123,
      "step": 5790
    },
    {
      "epoch": 1.7941381177016473,
      "grad_norm": 1.1382503509521484,
      "learning_rate": 3.7192804101838756e-05,
      "loss": 8.111,
      "step": 5800
    },
    {
      "epoch": 1.7972314592838914,
      "grad_norm": 0.8287107348442078,
      "learning_rate": 3.717070367751061e-05,
      "loss": 8.1129,
      "step": 5810
    },
    {
      "epoch": 1.8003248008661357,
      "grad_norm": 0.6545336842536926,
      "learning_rate": 3.7148603253182465e-05,
      "loss": 8.1124,
      "step": 5820
    },
    {
      "epoch": 1.80341814244838,
      "grad_norm": 1.1369034051895142,
      "learning_rate": 3.7126502828854316e-05,
      "loss": 8.1055,
      "step": 5830
    },
    {
      "epoch": 1.806511484030624,
      "grad_norm": 1.0850688219070435,
      "learning_rate": 3.7104402404526167e-05,
      "loss": 8.1012,
      "step": 5840
    },
    {
      "epoch": 1.8096048256128683,
      "grad_norm": 0.6212559938430786,
      "learning_rate": 3.708230198019802e-05,
      "loss": 8.1106,
      "step": 5850
    },
    {
      "epoch": 1.8126981671951126,
      "grad_norm": 1.202318549156189,
      "learning_rate": 3.7060201555869875e-05,
      "loss": 8.1227,
      "step": 5860
    },
    {
      "epoch": 1.8157915087773566,
      "grad_norm": 1.0532689094543457,
      "learning_rate": 3.7038101131541726e-05,
      "loss": 8.0981,
      "step": 5870
    },
    {
      "epoch": 1.8188848503596011,
      "grad_norm": 1.1592659950256348,
      "learning_rate": 3.7016000707213584e-05,
      "loss": 8.1151,
      "step": 5880
    },
    {
      "epoch": 1.8219781919418452,
      "grad_norm": 0.48596423864364624,
      "learning_rate": 3.6993900282885435e-05,
      "loss": 8.1205,
      "step": 5890
    },
    {
      "epoch": 1.8250715335240892,
      "grad_norm": 0.6388185024261475,
      "learning_rate": 3.6971799858557286e-05,
      "loss": 8.1025,
      "step": 5900
    },
    {
      "epoch": 1.8281648751063337,
      "grad_norm": 0.7377865314483643,
      "learning_rate": 3.6949699434229143e-05,
      "loss": 8.0985,
      "step": 5910
    },
    {
      "epoch": 1.8312582166885778,
      "grad_norm": 0.7783490419387817,
      "learning_rate": 3.6927599009900994e-05,
      "loss": 8.1144,
      "step": 5920
    },
    {
      "epoch": 1.834351558270822,
      "grad_norm": 0.6981886029243469,
      "learning_rate": 3.6905498585572845e-05,
      "loss": 8.1129,
      "step": 5930
    },
    {
      "epoch": 1.8374448998530664,
      "grad_norm": 1.4069945812225342,
      "learning_rate": 3.6883398161244696e-05,
      "loss": 8.121,
      "step": 5940
    },
    {
      "epoch": 1.8405382414353104,
      "grad_norm": 0.6732660531997681,
      "learning_rate": 3.686129773691655e-05,
      "loss": 8.1234,
      "step": 5950
    },
    {
      "epoch": 1.8436315830175547,
      "grad_norm": 0.8745042681694031,
      "learning_rate": 3.6839197312588405e-05,
      "loss": 8.1142,
      "step": 5960
    },
    {
      "epoch": 1.846724924599799,
      "grad_norm": 0.6202366352081299,
      "learning_rate": 3.6817096888260256e-05,
      "loss": 8.1145,
      "step": 5970
    },
    {
      "epoch": 1.849818266182043,
      "grad_norm": 0.8213227391242981,
      "learning_rate": 3.679499646393211e-05,
      "loss": 8.1017,
      "step": 5980
    },
    {
      "epoch": 1.8529116077642873,
      "grad_norm": 0.5994757413864136,
      "learning_rate": 3.677289603960396e-05,
      "loss": 8.1097,
      "step": 5990
    },
    {
      "epoch": 1.8560049493465316,
      "grad_norm": 1.21660315990448,
      "learning_rate": 3.6750795615275816e-05,
      "loss": 8.0982,
      "step": 6000
    },
    {
      "epoch": 1.8590982909287757,
      "grad_norm": 0.48843103647232056,
      "learning_rate": 3.672869519094767e-05,
      "loss": 8.1246,
      "step": 6010
    },
    {
      "epoch": 1.8621916325110202,
      "grad_norm": 0.7663419246673584,
      "learning_rate": 3.6706594766619524e-05,
      "loss": 8.1105,
      "step": 6020
    },
    {
      "epoch": 1.8652849740932642,
      "grad_norm": 0.4496191740036011,
      "learning_rate": 3.6684494342291375e-05,
      "loss": 8.1142,
      "step": 6030
    },
    {
      "epoch": 1.8683783156755085,
      "grad_norm": 0.770578920841217,
      "learning_rate": 3.6662393917963226e-05,
      "loss": 8.1236,
      "step": 6040
    },
    {
      "epoch": 1.8714716572577528,
      "grad_norm": 1.0419385433197021,
      "learning_rate": 3.6640293493635084e-05,
      "loss": 8.1019,
      "step": 6050
    },
    {
      "epoch": 1.8745649988399968,
      "grad_norm": 0.9988660216331482,
      "learning_rate": 3.6618193069306935e-05,
      "loss": 8.119,
      "step": 6060
    },
    {
      "epoch": 1.8776583404222411,
      "grad_norm": 0.7319515347480774,
      "learning_rate": 3.6596092644978786e-05,
      "loss": 8.1195,
      "step": 6070
    },
    {
      "epoch": 1.8807516820044854,
      "grad_norm": 0.826828122138977,
      "learning_rate": 3.657399222065064e-05,
      "loss": 8.1205,
      "step": 6080
    },
    {
      "epoch": 1.8838450235867295,
      "grad_norm": 0.4903888702392578,
      "learning_rate": 3.655189179632249e-05,
      "loss": 8.107,
      "step": 6090
    },
    {
      "epoch": 1.8869383651689737,
      "grad_norm": 0.757673442363739,
      "learning_rate": 3.6529791371994345e-05,
      "loss": 8.0938,
      "step": 6100
    },
    {
      "epoch": 1.890031706751218,
      "grad_norm": 0.9070502519607544,
      "learning_rate": 3.6507690947666196e-05,
      "loss": 8.1064,
      "step": 6110
    },
    {
      "epoch": 1.893125048333462,
      "grad_norm": 0.5928594470024109,
      "learning_rate": 3.648559052333805e-05,
      "loss": 8.1188,
      "step": 6120
    },
    {
      "epoch": 1.8962183899157066,
      "grad_norm": 0.6357729434967041,
      "learning_rate": 3.64634900990099e-05,
      "loss": 8.0994,
      "step": 6130
    },
    {
      "epoch": 1.8993117314979506,
      "grad_norm": 0.7452264428138733,
      "learning_rate": 3.6441389674681756e-05,
      "loss": 8.1156,
      "step": 6140
    },
    {
      "epoch": 1.902405073080195,
      "grad_norm": 0.7934433817863464,
      "learning_rate": 3.6419289250353614e-05,
      "loss": 8.1179,
      "step": 6150
    },
    {
      "epoch": 1.9054984146624392,
      "grad_norm": 1.0771790742874146,
      "learning_rate": 3.6397188826025465e-05,
      "loss": 8.099,
      "step": 6160
    },
    {
      "epoch": 1.9085917562446832,
      "grad_norm": 0.8761550784111023,
      "learning_rate": 3.6375088401697315e-05,
      "loss": 8.1045,
      "step": 6170
    },
    {
      "epoch": 1.9116850978269275,
      "grad_norm": 0.6440709829330444,
      "learning_rate": 3.6352987977369166e-05,
      "loss": 8.1169,
      "step": 6180
    },
    {
      "epoch": 1.9147784394091718,
      "grad_norm": 0.9463909268379211,
      "learning_rate": 3.6330887553041024e-05,
      "loss": 8.1157,
      "step": 6190
    },
    {
      "epoch": 1.9178717809914159,
      "grad_norm": 0.8317248225212097,
      "learning_rate": 3.6308787128712875e-05,
      "loss": 8.1227,
      "step": 6200
    },
    {
      "epoch": 1.9209651225736601,
      "grad_norm": 0.7556623816490173,
      "learning_rate": 3.6286686704384726e-05,
      "loss": 8.1101,
      "step": 6210
    },
    {
      "epoch": 1.9240584641559044,
      "grad_norm": 1.0234439373016357,
      "learning_rate": 3.626458628005658e-05,
      "loss": 8.1006,
      "step": 6220
    },
    {
      "epoch": 1.9271518057381485,
      "grad_norm": 0.7974225878715515,
      "learning_rate": 3.624248585572843e-05,
      "loss": 8.1144,
      "step": 6230
    },
    {
      "epoch": 1.930245147320393,
      "grad_norm": 1.123907446861267,
      "learning_rate": 3.6220385431400286e-05,
      "loss": 8.1249,
      "step": 6240
    },
    {
      "epoch": 1.933338488902637,
      "grad_norm": 1.000357985496521,
      "learning_rate": 3.6198285007072137e-05,
      "loss": 8.0901,
      "step": 6250
    },
    {
      "epoch": 1.9364318304848813,
      "grad_norm": 1.2129359245300293,
      "learning_rate": 3.617618458274399e-05,
      "loss": 8.1148,
      "step": 6260
    },
    {
      "epoch": 1.9395251720671256,
      "grad_norm": 0.7437799572944641,
      "learning_rate": 3.615408415841584e-05,
      "loss": 8.1119,
      "step": 6270
    },
    {
      "epoch": 1.9426185136493697,
      "grad_norm": 1.0087193250656128,
      "learning_rate": 3.6131983734087696e-05,
      "loss": 8.1252,
      "step": 6280
    },
    {
      "epoch": 1.945711855231614,
      "grad_norm": 1.2354352474212646,
      "learning_rate": 3.6109883309759554e-05,
      "loss": 8.1075,
      "step": 6290
    },
    {
      "epoch": 1.9488051968138582,
      "grad_norm": 1.126196265220642,
      "learning_rate": 3.6087782885431405e-05,
      "loss": 8.1118,
      "step": 6300
    },
    {
      "epoch": 1.9518985383961023,
      "grad_norm": 0.8466517329216003,
      "learning_rate": 3.6065682461103256e-05,
      "loss": 8.1117,
      "step": 6310
    },
    {
      "epoch": 1.9549918799783466,
      "grad_norm": 0.7501287460327148,
      "learning_rate": 3.604358203677511e-05,
      "loss": 8.0991,
      "step": 6320
    },
    {
      "epoch": 1.9580852215605908,
      "grad_norm": 1.0328483581542969,
      "learning_rate": 3.6021481612446964e-05,
      "loss": 8.1006,
      "step": 6330
    },
    {
      "epoch": 1.961178563142835,
      "grad_norm": 1.1136817932128906,
      "learning_rate": 3.5999381188118815e-05,
      "loss": 8.1184,
      "step": 6340
    },
    {
      "epoch": 1.9642719047250794,
      "grad_norm": 1.0115580558776855,
      "learning_rate": 3.5977280763790666e-05,
      "loss": 8.1086,
      "step": 6350
    },
    {
      "epoch": 1.9673652463073235,
      "grad_norm": 0.8790600895881653,
      "learning_rate": 3.595518033946252e-05,
      "loss": 8.1123,
      "step": 6360
    },
    {
      "epoch": 1.9704585878895677,
      "grad_norm": 0.6049520969390869,
      "learning_rate": 3.593307991513437e-05,
      "loss": 8.1193,
      "step": 6370
    },
    {
      "epoch": 1.973551929471812,
      "grad_norm": 1.1578625440597534,
      "learning_rate": 3.5910979490806226e-05,
      "loss": 8.1196,
      "step": 6380
    },
    {
      "epoch": 1.976645271054056,
      "grad_norm": 0.6569232940673828,
      "learning_rate": 3.588887906647808e-05,
      "loss": 8.11,
      "step": 6390
    },
    {
      "epoch": 1.9797386126363004,
      "grad_norm": 0.8159201741218567,
      "learning_rate": 3.586677864214993e-05,
      "loss": 8.1017,
      "step": 6400
    },
    {
      "epoch": 1.9828319542185446,
      "grad_norm": 0.8416740298271179,
      "learning_rate": 3.5844678217821786e-05,
      "loss": 8.1226,
      "step": 6410
    },
    {
      "epoch": 1.9859252958007887,
      "grad_norm": 0.7170424461364746,
      "learning_rate": 3.5822577793493636e-05,
      "loss": 8.1122,
      "step": 6420
    },
    {
      "epoch": 1.989018637383033,
      "grad_norm": 0.48569872975349426,
      "learning_rate": 3.5800477369165494e-05,
      "loss": 8.1162,
      "step": 6430
    },
    {
      "epoch": 1.9921119789652773,
      "grad_norm": 0.9472678303718567,
      "learning_rate": 3.5778376944837345e-05,
      "loss": 8.1127,
      "step": 6440
    },
    {
      "epoch": 1.9952053205475213,
      "grad_norm": 0.8315709829330444,
      "learning_rate": 3.5756276520509196e-05,
      "loss": 8.1198,
      "step": 6450
    },
    {
      "epoch": 1.9982986621297658,
      "grad_norm": 0.8758796453475952,
      "learning_rate": 3.573417609618105e-05,
      "loss": 8.1063,
      "step": 6460
    },
    {
      "epoch": 2.00139200371201,
      "grad_norm": 1.0007539987564087,
      "learning_rate": 3.5712075671852905e-05,
      "loss": 8.1037,
      "step": 6470
    },
    {
      "epoch": 2.004485345294254,
      "grad_norm": 0.8656543493270874,
      "learning_rate": 3.5689975247524756e-05,
      "loss": 8.1169,
      "step": 6480
    },
    {
      "epoch": 2.0075786868764984,
      "grad_norm": 0.5615689754486084,
      "learning_rate": 3.566787482319661e-05,
      "loss": 8.1057,
      "step": 6490
    },
    {
      "epoch": 2.0106720284587425,
      "grad_norm": 0.669521689414978,
      "learning_rate": 3.564577439886846e-05,
      "loss": 8.1122,
      "step": 6500
    },
    {
      "epoch": 2.013765370040987,
      "grad_norm": 0.8987202644348145,
      "learning_rate": 3.562367397454031e-05,
      "loss": 8.1,
      "step": 6510
    },
    {
      "epoch": 2.016858711623231,
      "grad_norm": 0.8017703890800476,
      "learning_rate": 3.5601573550212166e-05,
      "loss": 8.1146,
      "step": 6520
    },
    {
      "epoch": 2.019952053205475,
      "grad_norm": 1.0373128652572632,
      "learning_rate": 3.557947312588402e-05,
      "loss": 8.1044,
      "step": 6530
    },
    {
      "epoch": 2.0230453947877196,
      "grad_norm": 1.2457104921340942,
      "learning_rate": 3.555737270155587e-05,
      "loss": 8.1045,
      "step": 6540
    },
    {
      "epoch": 2.0261387363699637,
      "grad_norm": 0.6558496952056885,
      "learning_rate": 3.5535272277227726e-05,
      "loss": 8.1138,
      "step": 6550
    },
    {
      "epoch": 2.0292320779522077,
      "grad_norm": 0.5642615556716919,
      "learning_rate": 3.551317185289958e-05,
      "loss": 8.107,
      "step": 6560
    },
    {
      "epoch": 2.0323254195344522,
      "grad_norm": 0.7868329882621765,
      "learning_rate": 3.5491071428571435e-05,
      "loss": 8.1063,
      "step": 6570
    },
    {
      "epoch": 2.0354187611166963,
      "grad_norm": 0.7656639814376831,
      "learning_rate": 3.5468971004243285e-05,
      "loss": 8.1114,
      "step": 6580
    },
    {
      "epoch": 2.0385121026989403,
      "grad_norm": 0.6155396103858948,
      "learning_rate": 3.5446870579915136e-05,
      "loss": 8.1103,
      "step": 6590
    },
    {
      "epoch": 2.041605444281185,
      "grad_norm": 0.8541555404663086,
      "learning_rate": 3.542477015558699e-05,
      "loss": 8.1207,
      "step": 6600
    },
    {
      "epoch": 2.044698785863429,
      "grad_norm": 0.9370625019073486,
      "learning_rate": 3.5402669731258845e-05,
      "loss": 8.1017,
      "step": 6610
    },
    {
      "epoch": 2.0477921274456734,
      "grad_norm": 0.7069550156593323,
      "learning_rate": 3.5380569306930696e-05,
      "loss": 8.1038,
      "step": 6620
    },
    {
      "epoch": 2.0508854690279175,
      "grad_norm": 0.9374760389328003,
      "learning_rate": 3.535846888260255e-05,
      "loss": 8.1098,
      "step": 6630
    },
    {
      "epoch": 2.0539788106101615,
      "grad_norm": 0.8458296060562134,
      "learning_rate": 3.53363684582744e-05,
      "loss": 8.1001,
      "step": 6640
    },
    {
      "epoch": 2.057072152192406,
      "grad_norm": 0.8135137557983398,
      "learning_rate": 3.531426803394625e-05,
      "loss": 8.1168,
      "step": 6650
    },
    {
      "epoch": 2.06016549377465,
      "grad_norm": 0.9663774967193604,
      "learning_rate": 3.5292167609618107e-05,
      "loss": 8.1215,
      "step": 6660
    },
    {
      "epoch": 2.063258835356894,
      "grad_norm": 1.0900077819824219,
      "learning_rate": 3.527006718528996e-05,
      "loss": 8.0952,
      "step": 6670
    },
    {
      "epoch": 2.0663521769391386,
      "grad_norm": 0.9920023679733276,
      "learning_rate": 3.5247966760961815e-05,
      "loss": 8.0943,
      "step": 6680
    },
    {
      "epoch": 2.0694455185213827,
      "grad_norm": 1.369418740272522,
      "learning_rate": 3.5225866336633666e-05,
      "loss": 8.1113,
      "step": 6690
    },
    {
      "epoch": 2.0725388601036268,
      "grad_norm": 1.2087135314941406,
      "learning_rate": 3.520376591230552e-05,
      "loss": 8.1073,
      "step": 6700
    },
    {
      "epoch": 2.0756322016858713,
      "grad_norm": 1.1333025693893433,
      "learning_rate": 3.5181665487977375e-05,
      "loss": 8.107,
      "step": 6710
    },
    {
      "epoch": 2.0787255432681153,
      "grad_norm": 0.5594799518585205,
      "learning_rate": 3.5159565063649226e-05,
      "loss": 8.0972,
      "step": 6720
    },
    {
      "epoch": 2.0818188848503594,
      "grad_norm": 0.7939359545707703,
      "learning_rate": 3.513746463932108e-05,
      "loss": 8.0981,
      "step": 6730
    },
    {
      "epoch": 2.084912226432604,
      "grad_norm": 0.7826473712921143,
      "learning_rate": 3.511536421499293e-05,
      "loss": 8.128,
      "step": 6740
    },
    {
      "epoch": 2.088005568014848,
      "grad_norm": 0.6012619137763977,
      "learning_rate": 3.5093263790664785e-05,
      "loss": 8.095,
      "step": 6750
    },
    {
      "epoch": 2.0910989095970924,
      "grad_norm": 0.8687459230422974,
      "learning_rate": 3.5071163366336636e-05,
      "loss": 8.1158,
      "step": 6760
    },
    {
      "epoch": 2.0941922511793365,
      "grad_norm": 1.0301141738891602,
      "learning_rate": 3.504906294200849e-05,
      "loss": 8.1157,
      "step": 6770
    },
    {
      "epoch": 2.0972855927615806,
      "grad_norm": 0.6876010894775391,
      "learning_rate": 3.502696251768034e-05,
      "loss": 8.1098,
      "step": 6780
    },
    {
      "epoch": 2.100378934343825,
      "grad_norm": 0.6319012641906738,
      "learning_rate": 3.500486209335219e-05,
      "loss": 8.1159,
      "step": 6790
    },
    {
      "epoch": 2.103472275926069,
      "grad_norm": 0.6446672677993774,
      "learning_rate": 3.498276166902405e-05,
      "loss": 8.1058,
      "step": 6800
    },
    {
      "epoch": 2.106565617508313,
      "grad_norm": 0.9121010899543762,
      "learning_rate": 3.4960661244695905e-05,
      "loss": 8.1111,
      "step": 6810
    },
    {
      "epoch": 2.1096589590905577,
      "grad_norm": 0.7679120302200317,
      "learning_rate": 3.4938560820367756e-05,
      "loss": 8.1159,
      "step": 6820
    },
    {
      "epoch": 2.1127523006728017,
      "grad_norm": 0.7495049238204956,
      "learning_rate": 3.4916460396039607e-05,
      "loss": 8.109,
      "step": 6830
    },
    {
      "epoch": 2.115845642255046,
      "grad_norm": 0.7522687911987305,
      "learning_rate": 3.489435997171146e-05,
      "loss": 8.1117,
      "step": 6840
    },
    {
      "epoch": 2.1189389838372903,
      "grad_norm": 0.8810567259788513,
      "learning_rate": 3.4872259547383315e-05,
      "loss": 8.1076,
      "step": 6850
    },
    {
      "epoch": 2.1220323254195343,
      "grad_norm": 0.7428545951843262,
      "learning_rate": 3.4850159123055166e-05,
      "loss": 8.1087,
      "step": 6860
    },
    {
      "epoch": 2.125125667001779,
      "grad_norm": 0.703344464302063,
      "learning_rate": 3.482805869872702e-05,
      "loss": 8.1083,
      "step": 6870
    },
    {
      "epoch": 2.128219008584023,
      "grad_norm": 1.0406299829483032,
      "learning_rate": 3.480595827439887e-05,
      "loss": 8.1129,
      "step": 6880
    },
    {
      "epoch": 2.131312350166267,
      "grad_norm": 0.752870500087738,
      "learning_rate": 3.4783857850070726e-05,
      "loss": 8.1042,
      "step": 6890
    },
    {
      "epoch": 2.1344056917485115,
      "grad_norm": 0.6443193554878235,
      "learning_rate": 3.476175742574258e-05,
      "loss": 8.1156,
      "step": 6900
    },
    {
      "epoch": 2.1374990333307555,
      "grad_norm": 0.6838558316230774,
      "learning_rate": 3.473965700141443e-05,
      "loss": 8.1162,
      "step": 6910
    },
    {
      "epoch": 2.1405923749129996,
      "grad_norm": 0.6239073872566223,
      "learning_rate": 3.471755657708628e-05,
      "loss": 8.1017,
      "step": 6920
    },
    {
      "epoch": 2.143685716495244,
      "grad_norm": 0.6637186408042908,
      "learning_rate": 3.469545615275813e-05,
      "loss": 8.1147,
      "step": 6930
    },
    {
      "epoch": 2.146779058077488,
      "grad_norm": 0.5597219467163086,
      "learning_rate": 3.467335572842999e-05,
      "loss": 8.1049,
      "step": 6940
    },
    {
      "epoch": 2.149872399659732,
      "grad_norm": 0.6399242877960205,
      "learning_rate": 3.4651255304101845e-05,
      "loss": 8.1031,
      "step": 6950
    },
    {
      "epoch": 2.1529657412419767,
      "grad_norm": 1.1416271924972534,
      "learning_rate": 3.4629154879773696e-05,
      "loss": 8.107,
      "step": 6960
    },
    {
      "epoch": 2.1560590828242208,
      "grad_norm": 0.9163448810577393,
      "learning_rate": 3.460705445544555e-05,
      "loss": 8.0966,
      "step": 6970
    },
    {
      "epoch": 2.1591524244064653,
      "grad_norm": 0.7773393988609314,
      "learning_rate": 3.45849540311174e-05,
      "loss": 8.104,
      "step": 6980
    },
    {
      "epoch": 2.1622457659887093,
      "grad_norm": 0.5174259543418884,
      "learning_rate": 3.4562853606789255e-05,
      "loss": 8.1139,
      "step": 6990
    },
    {
      "epoch": 2.1653391075709534,
      "grad_norm": 1.4004563093185425,
      "learning_rate": 3.4540753182461106e-05,
      "loss": 8.112,
      "step": 7000
    },
    {
      "epoch": 2.168432449153198,
      "grad_norm": 1.0067520141601562,
      "learning_rate": 3.451865275813296e-05,
      "loss": 8.1162,
      "step": 7010
    },
    {
      "epoch": 2.171525790735442,
      "grad_norm": 0.8072924017906189,
      "learning_rate": 3.449655233380481e-05,
      "loss": 8.1182,
      "step": 7020
    },
    {
      "epoch": 2.174619132317686,
      "grad_norm": 0.5735112428665161,
      "learning_rate": 3.4474451909476666e-05,
      "loss": 8.0978,
      "step": 7030
    },
    {
      "epoch": 2.1777124738999305,
      "grad_norm": 1.1015483140945435,
      "learning_rate": 3.445235148514852e-05,
      "loss": 8.1214,
      "step": 7040
    },
    {
      "epoch": 2.1808058154821746,
      "grad_norm": 0.9034416079521179,
      "learning_rate": 3.443025106082037e-05,
      "loss": 8.1156,
      "step": 7050
    },
    {
      "epoch": 2.1838991570644186,
      "grad_norm": 0.7700001001358032,
      "learning_rate": 3.440815063649222e-05,
      "loss": 8.1123,
      "step": 7060
    },
    {
      "epoch": 2.186992498646663,
      "grad_norm": 0.8236660361289978,
      "learning_rate": 3.438605021216407e-05,
      "loss": 8.0989,
      "step": 7070
    },
    {
      "epoch": 2.190085840228907,
      "grad_norm": 0.5504794716835022,
      "learning_rate": 3.436394978783593e-05,
      "loss": 8.0951,
      "step": 7080
    },
    {
      "epoch": 2.1931791818111517,
      "grad_norm": 1.2962919473648071,
      "learning_rate": 3.4341849363507785e-05,
      "loss": 8.1137,
      "step": 7090
    },
    {
      "epoch": 2.1962725233933957,
      "grad_norm": 1.162819266319275,
      "learning_rate": 3.4319748939179636e-05,
      "loss": 8.0999,
      "step": 7100
    },
    {
      "epoch": 2.19936586497564,
      "grad_norm": 0.6030256152153015,
      "learning_rate": 3.429764851485149e-05,
      "loss": 8.1104,
      "step": 7110
    },
    {
      "epoch": 2.2024592065578843,
      "grad_norm": 0.9995957016944885,
      "learning_rate": 3.427554809052334e-05,
      "loss": 8.0951,
      "step": 7120
    },
    {
      "epoch": 2.2055525481401284,
      "grad_norm": 0.9046604037284851,
      "learning_rate": 3.4253447666195196e-05,
      "loss": 8.1152,
      "step": 7130
    },
    {
      "epoch": 2.2086458897223724,
      "grad_norm": 0.7797784805297852,
      "learning_rate": 3.423134724186705e-05,
      "loss": 8.0983,
      "step": 7140
    },
    {
      "epoch": 2.211739231304617,
      "grad_norm": 0.7534307241439819,
      "learning_rate": 3.42092468175389e-05,
      "loss": 8.1089,
      "step": 7150
    },
    {
      "epoch": 2.214832572886861,
      "grad_norm": 1.0686620473861694,
      "learning_rate": 3.418714639321075e-05,
      "loss": 8.0987,
      "step": 7160
    },
    {
      "epoch": 2.217925914469105,
      "grad_norm": 0.9020822644233704,
      "learning_rate": 3.4165045968882606e-05,
      "loss": 8.1097,
      "step": 7170
    },
    {
      "epoch": 2.2210192560513495,
      "grad_norm": 1.0647459030151367,
      "learning_rate": 3.414294554455446e-05,
      "loss": 8.1077,
      "step": 7180
    },
    {
      "epoch": 2.2241125976335936,
      "grad_norm": 0.8828995227813721,
      "learning_rate": 3.412084512022631e-05,
      "loss": 8.0981,
      "step": 7190
    },
    {
      "epoch": 2.227205939215838,
      "grad_norm": 1.0085227489471436,
      "learning_rate": 3.409874469589816e-05,
      "loss": 8.1115,
      "step": 7200
    },
    {
      "epoch": 2.230299280798082,
      "grad_norm": 1.5114398002624512,
      "learning_rate": 3.407664427157002e-05,
      "loss": 8.0966,
      "step": 7210
    },
    {
      "epoch": 2.233392622380326,
      "grad_norm": 0.7035461068153381,
      "learning_rate": 3.405454384724187e-05,
      "loss": 8.1034,
      "step": 7220
    },
    {
      "epoch": 2.2364859639625707,
      "grad_norm": 0.9180713891983032,
      "learning_rate": 3.4032443422913726e-05,
      "loss": 8.1241,
      "step": 7230
    },
    {
      "epoch": 2.2395793055448148,
      "grad_norm": 0.8001329898834229,
      "learning_rate": 3.4010342998585577e-05,
      "loss": 8.1032,
      "step": 7240
    },
    {
      "epoch": 2.242672647127059,
      "grad_norm": 0.6048594117164612,
      "learning_rate": 3.398824257425743e-05,
      "loss": 8.0986,
      "step": 7250
    },
    {
      "epoch": 2.2457659887093033,
      "grad_norm": 0.6743848323822021,
      "learning_rate": 3.396614214992928e-05,
      "loss": 8.1076,
      "step": 7260
    },
    {
      "epoch": 2.2488593302915474,
      "grad_norm": 0.6874709129333496,
      "learning_rate": 3.3944041725601136e-05,
      "loss": 8.1171,
      "step": 7270
    },
    {
      "epoch": 2.2519526718737914,
      "grad_norm": 1.0765892267227173,
      "learning_rate": 3.392194130127299e-05,
      "loss": 8.1083,
      "step": 7280
    },
    {
      "epoch": 2.255046013456036,
      "grad_norm": 0.8502073884010315,
      "learning_rate": 3.389984087694484e-05,
      "loss": 8.1095,
      "step": 7290
    },
    {
      "epoch": 2.25813935503828,
      "grad_norm": 1.6911770105361938,
      "learning_rate": 3.387774045261669e-05,
      "loss": 8.1235,
      "step": 7300
    },
    {
      "epoch": 2.2612326966205245,
      "grad_norm": 2.313249111175537,
      "learning_rate": 3.385564002828855e-05,
      "loss": 8.0983,
      "step": 7310
    },
    {
      "epoch": 2.2643260382027686,
      "grad_norm": 1.0674614906311035,
      "learning_rate": 3.38335396039604e-05,
      "loss": 8.1068,
      "step": 7320
    },
    {
      "epoch": 2.2674193797850126,
      "grad_norm": 1.7403515577316284,
      "learning_rate": 3.381143917963225e-05,
      "loss": 8.1267,
      "step": 7330
    },
    {
      "epoch": 2.270512721367257,
      "grad_norm": 2.1093876361846924,
      "learning_rate": 3.37893387553041e-05,
      "loss": 8.0998,
      "step": 7340
    },
    {
      "epoch": 2.273606062949501,
      "grad_norm": 1.8482344150543213,
      "learning_rate": 3.376723833097596e-05,
      "loss": 8.1192,
      "step": 7350
    },
    {
      "epoch": 2.2766994045317452,
      "grad_norm": 2.4280407428741455,
      "learning_rate": 3.374513790664781e-05,
      "loss": 8.1001,
      "step": 7360
    },
    {
      "epoch": 2.2797927461139897,
      "grad_norm": 1.7819243669509888,
      "learning_rate": 3.3723037482319666e-05,
      "loss": 8.1028,
      "step": 7370
    },
    {
      "epoch": 2.282886087696234,
      "grad_norm": 1.0094128847122192,
      "learning_rate": 3.370093705799152e-05,
      "loss": 8.1176,
      "step": 7380
    },
    {
      "epoch": 2.285979429278478,
      "grad_norm": 0.8377143144607544,
      "learning_rate": 3.367883663366337e-05,
      "loss": 8.1097,
      "step": 7390
    },
    {
      "epoch": 2.2890727708607224,
      "grad_norm": 0.5556678175926208,
      "learning_rate": 3.365673620933522e-05,
      "loss": 8.1112,
      "step": 7400
    },
    {
      "epoch": 2.2921661124429664,
      "grad_norm": 0.6424263715744019,
      "learning_rate": 3.3634635785007076e-05,
      "loss": 8.1178,
      "step": 7410
    },
    {
      "epoch": 2.295259454025211,
      "grad_norm": 0.7304377555847168,
      "learning_rate": 3.361253536067893e-05,
      "loss": 8.1072,
      "step": 7420
    },
    {
      "epoch": 2.298352795607455,
      "grad_norm": 0.6802080869674683,
      "learning_rate": 3.359043493635078e-05,
      "loss": 8.0901,
      "step": 7430
    },
    {
      "epoch": 2.301446137189699,
      "grad_norm": 0.982870876789093,
      "learning_rate": 3.356833451202263e-05,
      "loss": 8.1184,
      "step": 7440
    },
    {
      "epoch": 2.3045394787719435,
      "grad_norm": 0.7983734607696533,
      "learning_rate": 3.354623408769448e-05,
      "loss": 8.1126,
      "step": 7450
    },
    {
      "epoch": 2.3076328203541876,
      "grad_norm": 0.6771348714828491,
      "learning_rate": 3.352413366336634e-05,
      "loss": 8.0988,
      "step": 7460
    },
    {
      "epoch": 2.3107261619364317,
      "grad_norm": 0.7076992988586426,
      "learning_rate": 3.350203323903819e-05,
      "loss": 8.108,
      "step": 7470
    },
    {
      "epoch": 2.313819503518676,
      "grad_norm": 1.0335328578948975,
      "learning_rate": 3.347993281471005e-05,
      "loss": 8.0945,
      "step": 7480
    },
    {
      "epoch": 2.31691284510092,
      "grad_norm": 0.9147815108299255,
      "learning_rate": 3.34578323903819e-05,
      "loss": 8.1067,
      "step": 7490
    },
    {
      "epoch": 2.3200061866831643,
      "grad_norm": 0.6106426119804382,
      "learning_rate": 3.343573196605375e-05,
      "loss": 8.1179,
      "step": 7500
    },
    {
      "epoch": 2.3230995282654088,
      "grad_norm": 0.7255522012710571,
      "learning_rate": 3.3413631541725606e-05,
      "loss": 8.1008,
      "step": 7510
    },
    {
      "epoch": 2.326192869847653,
      "grad_norm": 0.6398424506187439,
      "learning_rate": 3.339153111739746e-05,
      "loss": 8.1036,
      "step": 7520
    },
    {
      "epoch": 2.3292862114298973,
      "grad_norm": 0.7270287871360779,
      "learning_rate": 3.336943069306931e-05,
      "loss": 8.1115,
      "step": 7530
    },
    {
      "epoch": 2.3323795530121414,
      "grad_norm": 1.3434518575668335,
      "learning_rate": 3.334733026874116e-05,
      "loss": 8.0963,
      "step": 7540
    },
    {
      "epoch": 2.3354728945943855,
      "grad_norm": 0.8692969679832458,
      "learning_rate": 3.332522984441302e-05,
      "loss": 8.1117,
      "step": 7550
    },
    {
      "epoch": 2.33856623617663,
      "grad_norm": 0.6519240736961365,
      "learning_rate": 3.330312942008487e-05,
      "loss": 8.1044,
      "step": 7560
    },
    {
      "epoch": 2.341659577758874,
      "grad_norm": 0.8035667538642883,
      "learning_rate": 3.328102899575672e-05,
      "loss": 8.1098,
      "step": 7570
    },
    {
      "epoch": 2.344752919341118,
      "grad_norm": 0.7302557229995728,
      "learning_rate": 3.325892857142857e-05,
      "loss": 8.1276,
      "step": 7580
    },
    {
      "epoch": 2.3478462609233626,
      "grad_norm": 0.8394455313682556,
      "learning_rate": 3.323682814710042e-05,
      "loss": 8.097,
      "step": 7590
    },
    {
      "epoch": 2.3509396025056066,
      "grad_norm": 0.8157474398612976,
      "learning_rate": 3.321472772277228e-05,
      "loss": 8.1098,
      "step": 7600
    },
    {
      "epoch": 2.3540329440878507,
      "grad_norm": 0.6307093501091003,
      "learning_rate": 3.3192627298444136e-05,
      "loss": 8.1105,
      "step": 7610
    },
    {
      "epoch": 2.357126285670095,
      "grad_norm": 0.7378803491592407,
      "learning_rate": 3.317052687411599e-05,
      "loss": 8.0956,
      "step": 7620
    },
    {
      "epoch": 2.3602196272523392,
      "grad_norm": 0.6132755875587463,
      "learning_rate": 3.314842644978784e-05,
      "loss": 8.1186,
      "step": 7630
    },
    {
      "epoch": 2.3633129688345837,
      "grad_norm": 1.782410979270935,
      "learning_rate": 3.312632602545969e-05,
      "loss": 8.1124,
      "step": 7640
    },
    {
      "epoch": 2.366406310416828,
      "grad_norm": 1.033473253250122,
      "learning_rate": 3.3104225601131547e-05,
      "loss": 8.1078,
      "step": 7650
    },
    {
      "epoch": 2.369499651999072,
      "grad_norm": 1.1419384479522705,
      "learning_rate": 3.30821251768034e-05,
      "loss": 8.1032,
      "step": 7660
    },
    {
      "epoch": 2.3725929935813164,
      "grad_norm": 0.9021451473236084,
      "learning_rate": 3.306002475247525e-05,
      "loss": 8.1203,
      "step": 7670
    },
    {
      "epoch": 2.3756863351635604,
      "grad_norm": 1.115128993988037,
      "learning_rate": 3.30379243281471e-05,
      "loss": 8.1416,
      "step": 7680
    },
    {
      "epoch": 2.3787796767458045,
      "grad_norm": 1.12799072265625,
      "learning_rate": 3.301582390381896e-05,
      "loss": 8.1044,
      "step": 7690
    },
    {
      "epoch": 2.381873018328049,
      "grad_norm": 1.0197303295135498,
      "learning_rate": 3.299372347949081e-05,
      "loss": 8.1003,
      "step": 7700
    },
    {
      "epoch": 2.384966359910293,
      "grad_norm": 0.834733784198761,
      "learning_rate": 3.297162305516266e-05,
      "loss": 8.1109,
      "step": 7710
    },
    {
      "epoch": 2.388059701492537,
      "grad_norm": 0.926567792892456,
      "learning_rate": 3.294952263083451e-05,
      "loss": 8.118,
      "step": 7720
    },
    {
      "epoch": 2.3911530430747816,
      "grad_norm": 1.0582120418548584,
      "learning_rate": 3.292742220650636e-05,
      "loss": 8.1065,
      "step": 7730
    },
    {
      "epoch": 2.3942463846570257,
      "grad_norm": 0.5994278788566589,
      "learning_rate": 3.290532178217822e-05,
      "loss": 8.1055,
      "step": 7740
    },
    {
      "epoch": 2.39733972623927,
      "grad_norm": 1.3775322437286377,
      "learning_rate": 3.2883221357850076e-05,
      "loss": 8.1136,
      "step": 7750
    },
    {
      "epoch": 2.400433067821514,
      "grad_norm": 1.0181688070297241,
      "learning_rate": 3.286112093352193e-05,
      "loss": 8.1103,
      "step": 7760
    },
    {
      "epoch": 2.4035264094037583,
      "grad_norm": 0.7116814255714417,
      "learning_rate": 3.283902050919378e-05,
      "loss": 8.1153,
      "step": 7770
    },
    {
      "epoch": 2.406619750986003,
      "grad_norm": 0.8103683590888977,
      "learning_rate": 3.281692008486563e-05,
      "loss": 8.1021,
      "step": 7780
    },
    {
      "epoch": 2.409713092568247,
      "grad_norm": 0.902049720287323,
      "learning_rate": 3.279481966053749e-05,
      "loss": 8.1123,
      "step": 7790
    },
    {
      "epoch": 2.412806434150491,
      "grad_norm": 1.3356956243515015,
      "learning_rate": 3.277271923620934e-05,
      "loss": 8.1081,
      "step": 7800
    },
    {
      "epoch": 2.4158997757327354,
      "grad_norm": 1.5610414743423462,
      "learning_rate": 3.275061881188119e-05,
      "loss": 8.1147,
      "step": 7810
    },
    {
      "epoch": 2.4189931173149795,
      "grad_norm": 0.9040111303329468,
      "learning_rate": 3.272851838755304e-05,
      "loss": 8.1084,
      "step": 7820
    },
    {
      "epoch": 2.4220864588972235,
      "grad_norm": 0.8800963163375854,
      "learning_rate": 3.27064179632249e-05,
      "loss": 8.0994,
      "step": 7830
    },
    {
      "epoch": 2.425179800479468,
      "grad_norm": 1.0713331699371338,
      "learning_rate": 3.268431753889675e-05,
      "loss": 8.105,
      "step": 7840
    },
    {
      "epoch": 2.428273142061712,
      "grad_norm": 0.6551599502563477,
      "learning_rate": 3.26622171145686e-05,
      "loss": 8.1157,
      "step": 7850
    },
    {
      "epoch": 2.4313664836439566,
      "grad_norm": 1.2914223670959473,
      "learning_rate": 3.264011669024045e-05,
      "loss": 8.1127,
      "step": 7860
    },
    {
      "epoch": 2.4344598252262006,
      "grad_norm": 1.1945642232894897,
      "learning_rate": 3.26180162659123e-05,
      "loss": 8.1118,
      "step": 7870
    },
    {
      "epoch": 2.4375531668084447,
      "grad_norm": 1.3108043670654297,
      "learning_rate": 3.2595915841584166e-05,
      "loss": 8.0951,
      "step": 7880
    },
    {
      "epoch": 2.440646508390689,
      "grad_norm": 1.1306266784667969,
      "learning_rate": 3.257381541725602e-05,
      "loss": 8.1114,
      "step": 7890
    },
    {
      "epoch": 2.4437398499729333,
      "grad_norm": 0.9698882102966309,
      "learning_rate": 3.255171499292787e-05,
      "loss": 8.1097,
      "step": 7900
    },
    {
      "epoch": 2.4468331915551773,
      "grad_norm": 0.9795451760292053,
      "learning_rate": 3.252961456859972e-05,
      "loss": 8.1275,
      "step": 7910
    },
    {
      "epoch": 2.449926533137422,
      "grad_norm": 0.7491248250007629,
      "learning_rate": 3.250751414427157e-05,
      "loss": 8.1055,
      "step": 7920
    },
    {
      "epoch": 2.453019874719666,
      "grad_norm": 1.0926135778427124,
      "learning_rate": 3.248541371994343e-05,
      "loss": 8.12,
      "step": 7930
    },
    {
      "epoch": 2.45611321630191,
      "grad_norm": 0.6629694700241089,
      "learning_rate": 3.246331329561528e-05,
      "loss": 8.1012,
      "step": 7940
    },
    {
      "epoch": 2.4592065578841544,
      "grad_norm": 0.6353484988212585,
      "learning_rate": 3.244121287128713e-05,
      "loss": 8.1127,
      "step": 7950
    },
    {
      "epoch": 2.4622998994663985,
      "grad_norm": 0.7175458669662476,
      "learning_rate": 3.241911244695898e-05,
      "loss": 8.1063,
      "step": 7960
    },
    {
      "epoch": 2.465393241048643,
      "grad_norm": 1.1927061080932617,
      "learning_rate": 3.239701202263084e-05,
      "loss": 8.1297,
      "step": 7970
    },
    {
      "epoch": 2.468486582630887,
      "grad_norm": 0.7836518883705139,
      "learning_rate": 3.237491159830269e-05,
      "loss": 8.1122,
      "step": 7980
    },
    {
      "epoch": 2.471579924213131,
      "grad_norm": 0.5218590497970581,
      "learning_rate": 3.235281117397454e-05,
      "loss": 8.1046,
      "step": 7990
    },
    {
      "epoch": 2.4746732657953756,
      "grad_norm": 0.792202889919281,
      "learning_rate": 3.233071074964639e-05,
      "loss": 8.1042,
      "step": 8000
    },
    {
      "epoch": 2.4777666073776197,
      "grad_norm": 0.702460765838623,
      "learning_rate": 3.230861032531825e-05,
      "loss": 8.1055,
      "step": 8010
    },
    {
      "epoch": 2.4808599489598637,
      "grad_norm": 0.8208922147750854,
      "learning_rate": 3.2286509900990106e-05,
      "loss": 8.1083,
      "step": 8020
    },
    {
      "epoch": 2.4839532905421082,
      "grad_norm": 0.8189626932144165,
      "learning_rate": 3.226440947666196e-05,
      "loss": 8.1138,
      "step": 8030
    },
    {
      "epoch": 2.4870466321243523,
      "grad_norm": 0.7804985642433167,
      "learning_rate": 3.224230905233381e-05,
      "loss": 8.1128,
      "step": 8040
    },
    {
      "epoch": 2.4901399737065963,
      "grad_norm": NaN,
      "learning_rate": 3.222241867043848e-05,
      "loss": 8.1188,
      "step": 8050
    },
    {
      "epoch": 2.493233315288841,
      "grad_norm": 0.7697494029998779,
      "learning_rate": 3.220031824611033e-05,
      "loss": 8.1176,
      "step": 8060
    },
    {
      "epoch": 2.496326656871085,
      "grad_norm": 0.8394335508346558,
      "learning_rate": 3.217821782178218e-05,
      "loss": 8.1076,
      "step": 8070
    },
    {
      "epoch": 2.4994199984533294,
      "grad_norm": 0.7376328110694885,
      "learning_rate": 3.215611739745403e-05,
      "loss": 8.1141,
      "step": 8080
    },
    {
      "epoch": 2.5025133400355735,
      "grad_norm": 0.7855545282363892,
      "learning_rate": 3.213401697312589e-05,
      "loss": 8.1062,
      "step": 8090
    },
    {
      "epoch": 2.5056066816178175,
      "grad_norm": 0.6221078634262085,
      "learning_rate": 3.211191654879774e-05,
      "loss": 8.1123,
      "step": 8100
    },
    {
      "epoch": 2.508700023200062,
      "grad_norm": 0.7163888216018677,
      "learning_rate": 3.208981612446959e-05,
      "loss": 8.1165,
      "step": 8110
    },
    {
      "epoch": 2.511793364782306,
      "grad_norm": 1.189736008644104,
      "learning_rate": 3.206771570014144e-05,
      "loss": 8.1213,
      "step": 8120
    },
    {
      "epoch": 2.51488670636455,
      "grad_norm": 0.9522821307182312,
      "learning_rate": 3.204561527581329e-05,
      "loss": 8.0977,
      "step": 8130
    },
    {
      "epoch": 2.5179800479467946,
      "grad_norm": 0.7873066663742065,
      "learning_rate": 3.202351485148515e-05,
      "loss": 8.1067,
      "step": 8140
    },
    {
      "epoch": 2.5210733895290387,
      "grad_norm": 0.7745828628540039,
      "learning_rate": 3.2001414427157e-05,
      "loss": 8.0973,
      "step": 8150
    },
    {
      "epoch": 2.5241667311112828,
      "grad_norm": 0.7857110500335693,
      "learning_rate": 3.197931400282886e-05,
      "loss": 8.1026,
      "step": 8160
    },
    {
      "epoch": 2.5272600726935273,
      "grad_norm": 0.5915932059288025,
      "learning_rate": 3.195721357850071e-05,
      "loss": 8.1088,
      "step": 8170
    },
    {
      "epoch": 2.5303534142757713,
      "grad_norm": 1.2239477634429932,
      "learning_rate": 3.193511315417256e-05,
      "loss": 8.1229,
      "step": 8180
    },
    {
      "epoch": 2.533446755858016,
      "grad_norm": 1.1027289628982544,
      "learning_rate": 3.191301272984442e-05,
      "loss": 8.0968,
      "step": 8190
    },
    {
      "epoch": 2.53654009744026,
      "grad_norm": 0.84235680103302,
      "learning_rate": 3.189091230551627e-05,
      "loss": 8.0915,
      "step": 8200
    },
    {
      "epoch": 2.539633439022504,
      "grad_norm": 1.046024203300476,
      "learning_rate": 3.186881188118812e-05,
      "loss": 8.1074,
      "step": 8210
    },
    {
      "epoch": 2.5427267806047484,
      "grad_norm": 0.8541591763496399,
      "learning_rate": 3.184671145685997e-05,
      "loss": 8.1011,
      "step": 8220
    },
    {
      "epoch": 2.5458201221869925,
      "grad_norm": 0.7309040427207947,
      "learning_rate": 3.182461103253183e-05,
      "loss": 8.1174,
      "step": 8230
    },
    {
      "epoch": 2.5489134637692366,
      "grad_norm": 0.9606079459190369,
      "learning_rate": 3.180251060820368e-05,
      "loss": 8.0986,
      "step": 8240
    },
    {
      "epoch": 2.552006805351481,
      "grad_norm": 0.7950613498687744,
      "learning_rate": 3.178041018387553e-05,
      "loss": 8.0949,
      "step": 8250
    },
    {
      "epoch": 2.555100146933725,
      "grad_norm": 0.5533953309059143,
      "learning_rate": 3.175830975954738e-05,
      "loss": 8.0918,
      "step": 8260
    },
    {
      "epoch": 2.558193488515969,
      "grad_norm": 0.6102727651596069,
      "learning_rate": 3.173620933521923e-05,
      "loss": 8.1015,
      "step": 8270
    },
    {
      "epoch": 2.5612868300982137,
      "grad_norm": 0.5869930386543274,
      "learning_rate": 3.171410891089109e-05,
      "loss": 8.0907,
      "step": 8280
    },
    {
      "epoch": 2.5643801716804577,
      "grad_norm": 0.6928898692131042,
      "learning_rate": 3.169200848656295e-05,
      "loss": 8.1008,
      "step": 8290
    },
    {
      "epoch": 2.5674735132627022,
      "grad_norm": 0.9670068621635437,
      "learning_rate": 3.16699080622348e-05,
      "loss": 8.1147,
      "step": 8300
    },
    {
      "epoch": 2.5705668548449463,
      "grad_norm": 0.9791125655174255,
      "learning_rate": 3.164780763790665e-05,
      "loss": 8.1094,
      "step": 8310
    },
    {
      "epoch": 2.5736601964271903,
      "grad_norm": 0.8992400765419006,
      "learning_rate": 3.16257072135785e-05,
      "loss": 8.0969,
      "step": 8320
    },
    {
      "epoch": 2.576753538009435,
      "grad_norm": 0.8083611726760864,
      "learning_rate": 3.160360678925036e-05,
      "loss": 8.1059,
      "step": 8330
    },
    {
      "epoch": 2.579846879591679,
      "grad_norm": 1.174232840538025,
      "learning_rate": 3.158150636492221e-05,
      "loss": 8.1054,
      "step": 8340
    },
    {
      "epoch": 2.582940221173923,
      "grad_norm": 0.889126181602478,
      "learning_rate": 3.155940594059406e-05,
      "loss": 8.1165,
      "step": 8350
    },
    {
      "epoch": 2.5860335627561675,
      "grad_norm": 0.7267313599586487,
      "learning_rate": 3.153730551626591e-05,
      "loss": 8.1073,
      "step": 8360
    },
    {
      "epoch": 2.5891269043384115,
      "grad_norm": 1.0108970403671265,
      "learning_rate": 3.151520509193777e-05,
      "loss": 8.1125,
      "step": 8370
    },
    {
      "epoch": 2.5922202459206556,
      "grad_norm": 0.6237018704414368,
      "learning_rate": 3.149310466760962e-05,
      "loss": 8.1142,
      "step": 8380
    },
    {
      "epoch": 2.5953135875029,
      "grad_norm": 0.8178033232688904,
      "learning_rate": 3.147100424328147e-05,
      "loss": 8.1088,
      "step": 8390
    },
    {
      "epoch": 2.598406929085144,
      "grad_norm": 0.9345575571060181,
      "learning_rate": 3.144890381895332e-05,
      "loss": 8.1038,
      "step": 8400
    },
    {
      "epoch": 2.6015002706673886,
      "grad_norm": 0.8388963937759399,
      "learning_rate": 3.142680339462517e-05,
      "loss": 8.0982,
      "step": 8410
    },
    {
      "epoch": 2.6045936122496327,
      "grad_norm": 0.9106768369674683,
      "learning_rate": 3.140470297029703e-05,
      "loss": 8.0855,
      "step": 8420
    },
    {
      "epoch": 2.6076869538318768,
      "grad_norm": 0.5219316482543945,
      "learning_rate": 3.138260254596889e-05,
      "loss": 8.1185,
      "step": 8430
    },
    {
      "epoch": 2.6107802954141213,
      "grad_norm": 0.7657433152198792,
      "learning_rate": 3.136050212164074e-05,
      "loss": 8.1072,
      "step": 8440
    },
    {
      "epoch": 2.6138736369963653,
      "grad_norm": 1.067562222480774,
      "learning_rate": 3.133840169731259e-05,
      "loss": 8.1014,
      "step": 8450
    },
    {
      "epoch": 2.6169669785786094,
      "grad_norm": 0.8140122294425964,
      "learning_rate": 3.131630127298444e-05,
      "loss": 8.1128,
      "step": 8460
    },
    {
      "epoch": 2.620060320160854,
      "grad_norm": 0.804144024848938,
      "learning_rate": 3.12942008486563e-05,
      "loss": 8.1205,
      "step": 8470
    },
    {
      "epoch": 2.623153661743098,
      "grad_norm": 0.7900106906890869,
      "learning_rate": 3.127210042432815e-05,
      "loss": 8.0945,
      "step": 8480
    },
    {
      "epoch": 2.626247003325342,
      "grad_norm": 0.9645407795906067,
      "learning_rate": 3.125e-05,
      "loss": 8.1026,
      "step": 8490
    },
    {
      "epoch": 2.6293403449075865,
      "grad_norm": 0.7588421702384949,
      "learning_rate": 3.122789957567185e-05,
      "loss": 8.1119,
      "step": 8500
    },
    {
      "epoch": 2.6324336864898306,
      "grad_norm": 0.8346685171127319,
      "learning_rate": 3.120579915134371e-05,
      "loss": 8.0961,
      "step": 8510
    },
    {
      "epoch": 2.635527028072075,
      "grad_norm": 1.0086463689804077,
      "learning_rate": 3.118369872701556e-05,
      "loss": 8.1164,
      "step": 8520
    },
    {
      "epoch": 2.638620369654319,
      "grad_norm": 1.4347095489501953,
      "learning_rate": 3.116159830268741e-05,
      "loss": 8.1053,
      "step": 8530
    },
    {
      "epoch": 2.641713711236563,
      "grad_norm": 0.5833174586296082,
      "learning_rate": 3.113949787835926e-05,
      "loss": 8.0913,
      "step": 8540
    },
    {
      "epoch": 2.6448070528188077,
      "grad_norm": 0.8336417078971863,
      "learning_rate": 3.111739745403111e-05,
      "loss": 8.1015,
      "step": 8550
    },
    {
      "epoch": 2.6479003944010517,
      "grad_norm": 0.734184741973877,
      "learning_rate": 3.109529702970298e-05,
      "loss": 8.1216,
      "step": 8560
    },
    {
      "epoch": 2.650993735983296,
      "grad_norm": 0.5296395421028137,
      "learning_rate": 3.107319660537483e-05,
      "loss": 8.1094,
      "step": 8570
    },
    {
      "epoch": 2.6540870775655403,
      "grad_norm": 0.4169941842556,
      "learning_rate": 3.105109618104668e-05,
      "loss": 8.11,
      "step": 8580
    },
    {
      "epoch": 2.6571804191477844,
      "grad_norm": 0.7747315168380737,
      "learning_rate": 3.102899575671853e-05,
      "loss": 8.1138,
      "step": 8590
    },
    {
      "epoch": 2.6602737607300284,
      "grad_norm": 0.6991413235664368,
      "learning_rate": 3.100689533239038e-05,
      "loss": 8.0857,
      "step": 8600
    },
    {
      "epoch": 2.663367102312273,
      "grad_norm": 0.9075675010681152,
      "learning_rate": 3.098479490806224e-05,
      "loss": 8.1037,
      "step": 8610
    },
    {
      "epoch": 2.666460443894517,
      "grad_norm": 1.363296389579773,
      "learning_rate": 3.096269448373409e-05,
      "loss": 8.0972,
      "step": 8620
    },
    {
      "epoch": 2.6695537854767615,
      "grad_norm": 0.905543327331543,
      "learning_rate": 3.094059405940594e-05,
      "loss": 8.1,
      "step": 8630
    },
    {
      "epoch": 2.6726471270590055,
      "grad_norm": 0.5974816679954529,
      "learning_rate": 3.091849363507779e-05,
      "loss": 8.0904,
      "step": 8640
    },
    {
      "epoch": 2.6757404686412496,
      "grad_norm": 0.7393059730529785,
      "learning_rate": 3.089639321074965e-05,
      "loss": 8.1147,
      "step": 8650
    },
    {
      "epoch": 2.678833810223494,
      "grad_norm": 1.3015791177749634,
      "learning_rate": 3.08742927864215e-05,
      "loss": 8.1212,
      "step": 8660
    },
    {
      "epoch": 2.681927151805738,
      "grad_norm": 0.637285053730011,
      "learning_rate": 3.085219236209335e-05,
      "loss": 8.1117,
      "step": 8670
    },
    {
      "epoch": 2.685020493387982,
      "grad_norm": 0.6907744407653809,
      "learning_rate": 3.08300919377652e-05,
      "loss": 8.0938,
      "step": 8680
    },
    {
      "epoch": 2.6881138349702267,
      "grad_norm": 0.8641349673271179,
      "learning_rate": 3.0807991513437053e-05,
      "loss": 8.107,
      "step": 8690
    },
    {
      "epoch": 2.6912071765524708,
      "grad_norm": 0.6408829092979431,
      "learning_rate": 3.078589108910892e-05,
      "loss": 8.0934,
      "step": 8700
    },
    {
      "epoch": 2.694300518134715,
      "grad_norm": 0.7601438164710999,
      "learning_rate": 3.076379066478077e-05,
      "loss": 8.11,
      "step": 8710
    },
    {
      "epoch": 2.6973938597169593,
      "grad_norm": 1.263532280921936,
      "learning_rate": 3.074169024045262e-05,
      "loss": 8.1236,
      "step": 8720
    },
    {
      "epoch": 2.7004872012992034,
      "grad_norm": 0.7138783931732178,
      "learning_rate": 3.071958981612447e-05,
      "loss": 8.1209,
      "step": 8730
    },
    {
      "epoch": 2.703580542881448,
      "grad_norm": 1.109820008277893,
      "learning_rate": 3.069748939179632e-05,
      "loss": 8.1003,
      "step": 8740
    },
    {
      "epoch": 2.706673884463692,
      "grad_norm": 1.1382349729537964,
      "learning_rate": 3.067538896746818e-05,
      "loss": 8.0829,
      "step": 8750
    },
    {
      "epoch": 2.709767226045936,
      "grad_norm": 0.6802732348442078,
      "learning_rate": 3.065328854314003e-05,
      "loss": 8.1164,
      "step": 8760
    },
    {
      "epoch": 2.7128605676281805,
      "grad_norm": 0.8587148785591125,
      "learning_rate": 3.063118811881188e-05,
      "loss": 8.1103,
      "step": 8770
    },
    {
      "epoch": 2.7159539092104246,
      "grad_norm": 0.6482588052749634,
      "learning_rate": 3.060908769448373e-05,
      "loss": 8.1161,
      "step": 8780
    },
    {
      "epoch": 2.7190472507926686,
      "grad_norm": 0.9727070927619934,
      "learning_rate": 3.058698727015559e-05,
      "loss": 8.1261,
      "step": 8790
    },
    {
      "epoch": 2.722140592374913,
      "grad_norm": 0.9787530899047852,
      "learning_rate": 3.056488684582744e-05,
      "loss": 8.1158,
      "step": 8800
    },
    {
      "epoch": 2.725233933957157,
      "grad_norm": 1.090343952178955,
      "learning_rate": 3.054278642149929e-05,
      "loss": 8.0904,
      "step": 8810
    },
    {
      "epoch": 2.7283272755394012,
      "grad_norm": 1.2675690650939941,
      "learning_rate": 3.052068599717114e-05,
      "loss": 8.1049,
      "step": 8820
    },
    {
      "epoch": 2.7314206171216457,
      "grad_norm": 1.2774320840835571,
      "learning_rate": 3.0498585572843004e-05,
      "loss": 8.1215,
      "step": 8830
    },
    {
      "epoch": 2.73451395870389,
      "grad_norm": 0.5210721492767334,
      "learning_rate": 3.0476485148514855e-05,
      "loss": 8.0958,
      "step": 8840
    },
    {
      "epoch": 2.7376073002861343,
      "grad_norm": 0.6900545954704285,
      "learning_rate": 3.045438472418671e-05,
      "loss": 8.116,
      "step": 8850
    },
    {
      "epoch": 2.7407006418683784,
      "grad_norm": 0.772678017616272,
      "learning_rate": 3.043228429985856e-05,
      "loss": 8.0975,
      "step": 8860
    },
    {
      "epoch": 2.7437939834506224,
      "grad_norm": 0.8305589556694031,
      "learning_rate": 3.041018387553041e-05,
      "loss": 8.1115,
      "step": 8870
    },
    {
      "epoch": 2.7468873250328665,
      "grad_norm": 0.7291635870933533,
      "learning_rate": 3.0388083451202265e-05,
      "loss": 8.1119,
      "step": 8880
    },
    {
      "epoch": 2.749980666615111,
      "grad_norm": 0.6218641996383667,
      "learning_rate": 3.0365983026874116e-05,
      "loss": 8.0935,
      "step": 8890
    },
    {
      "epoch": 2.753074008197355,
      "grad_norm": 0.593895435333252,
      "learning_rate": 3.034388260254597e-05,
      "loss": 8.0982,
      "step": 8900
    },
    {
      "epoch": 2.7561673497795995,
      "grad_norm": 0.730521023273468,
      "learning_rate": 3.032178217821782e-05,
      "loss": 8.105,
      "step": 8910
    },
    {
      "epoch": 2.7592606913618436,
      "grad_norm": 0.812873899936676,
      "learning_rate": 3.0299681753889676e-05,
      "loss": 8.1115,
      "step": 8920
    },
    {
      "epoch": 2.7623540329440877,
      "grad_norm": 0.6386334300041199,
      "learning_rate": 3.0277581329561527e-05,
      "loss": 8.1087,
      "step": 8930
    },
    {
      "epoch": 2.765447374526332,
      "grad_norm": 0.7302212715148926,
      "learning_rate": 3.025548090523338e-05,
      "loss": 8.1043,
      "step": 8940
    },
    {
      "epoch": 2.768540716108576,
      "grad_norm": 1.023606300354004,
      "learning_rate": 3.0233380480905232e-05,
      "loss": 8.0989,
      "step": 8950
    },
    {
      "epoch": 2.7716340576908207,
      "grad_norm": 0.5995789170265198,
      "learning_rate": 3.021128005657709e-05,
      "loss": 8.0908,
      "step": 8960
    },
    {
      "epoch": 2.7747273992730648,
      "grad_norm": 0.9647005796432495,
      "learning_rate": 3.0189179632248944e-05,
      "loss": 8.1252,
      "step": 8970
    },
    {
      "epoch": 2.777820740855309,
      "grad_norm": 1.0539631843566895,
      "learning_rate": 3.0167079207920795e-05,
      "loss": 8.1174,
      "step": 8980
    },
    {
      "epoch": 2.780914082437553,
      "grad_norm": 0.6901851296424866,
      "learning_rate": 3.014497878359265e-05,
      "loss": 8.1152,
      "step": 8990
    },
    {
      "epoch": 2.7840074240197974,
      "grad_norm": 0.6861261129379272,
      "learning_rate": 3.01228783592645e-05,
      "loss": 8.1167,
      "step": 9000
    },
    {
      "epoch": 2.7871007656020415,
      "grad_norm": 0.8329610228538513,
      "learning_rate": 3.010077793493635e-05,
      "loss": 8.1064,
      "step": 9010
    },
    {
      "epoch": 2.790194107184286,
      "grad_norm": 0.9155378341674805,
      "learning_rate": 3.0078677510608206e-05,
      "loss": 8.1052,
      "step": 9020
    },
    {
      "epoch": 2.79328744876653,
      "grad_norm": 1.3295714855194092,
      "learning_rate": 3.0056577086280057e-05,
      "loss": 8.107,
      "step": 9030
    },
    {
      "epoch": 2.796380790348774,
      "grad_norm": 0.7985524535179138,
      "learning_rate": 3.003447666195191e-05,
      "loss": 8.1109,
      "step": 9040
    },
    {
      "epoch": 2.7994741319310186,
      "grad_norm": 0.7160539031028748,
      "learning_rate": 3.0012376237623762e-05,
      "loss": 8.1112,
      "step": 9050
    },
    {
      "epoch": 2.8025674735132626,
      "grad_norm": 0.9186545610427856,
      "learning_rate": 2.9990275813295616e-05,
      "loss": 8.1152,
      "step": 9060
    },
    {
      "epoch": 2.805660815095507,
      "grad_norm": 0.7360554337501526,
      "learning_rate": 2.9968175388967467e-05,
      "loss": 8.1115,
      "step": 9070
    },
    {
      "epoch": 2.808754156677751,
      "grad_norm": 0.6553969383239746,
      "learning_rate": 2.994607496463932e-05,
      "loss": 8.1118,
      "step": 9080
    },
    {
      "epoch": 2.8118474982599952,
      "grad_norm": 0.5571410655975342,
      "learning_rate": 2.9923974540311172e-05,
      "loss": 8.1047,
      "step": 9090
    },
    {
      "epoch": 2.8149408398422393,
      "grad_norm": 0.6194122433662415,
      "learning_rate": 2.990187411598303e-05,
      "loss": 8.1029,
      "step": 9100
    },
    {
      "epoch": 2.818034181424484,
      "grad_norm": 0.5988227128982544,
      "learning_rate": 2.9879773691654885e-05,
      "loss": 8.0884,
      "step": 9110
    },
    {
      "epoch": 2.821127523006728,
      "grad_norm": 0.6157711744308472,
      "learning_rate": 2.9857673267326735e-05,
      "loss": 8.1192,
      "step": 9120
    },
    {
      "epoch": 2.8242208645889724,
      "grad_norm": 0.8260595798492432,
      "learning_rate": 2.983557284299859e-05,
      "loss": 8.1044,
      "step": 9130
    },
    {
      "epoch": 2.8273142061712164,
      "grad_norm": 0.7173621654510498,
      "learning_rate": 2.981347241867044e-05,
      "loss": 8.1026,
      "step": 9140
    },
    {
      "epoch": 2.8304075477534605,
      "grad_norm": 1.073249340057373,
      "learning_rate": 2.9791371994342292e-05,
      "loss": 8.0891,
      "step": 9150
    },
    {
      "epoch": 2.833500889335705,
      "grad_norm": 0.6968765258789062,
      "learning_rate": 2.9769271570014146e-05,
      "loss": 8.0983,
      "step": 9160
    },
    {
      "epoch": 2.836594230917949,
      "grad_norm": 0.6450964212417603,
      "learning_rate": 2.9747171145685997e-05,
      "loss": 8.0982,
      "step": 9170
    },
    {
      "epoch": 2.8396875725001935,
      "grad_norm": 0.7721308469772339,
      "learning_rate": 2.972507072135785e-05,
      "loss": 8.1229,
      "step": 9180
    },
    {
      "epoch": 2.8427809140824376,
      "grad_norm": 0.7427786588668823,
      "learning_rate": 2.9702970297029702e-05,
      "loss": 8.1041,
      "step": 9190
    },
    {
      "epoch": 2.8458742556646817,
      "grad_norm": 0.6238769888877869,
      "learning_rate": 2.9680869872701557e-05,
      "loss": 8.1088,
      "step": 9200
    },
    {
      "epoch": 2.8489675972469257,
      "grad_norm": 0.8641448616981506,
      "learning_rate": 2.9658769448373408e-05,
      "loss": 8.1061,
      "step": 9210
    },
    {
      "epoch": 2.8520609388291702,
      "grad_norm": 0.9350304007530212,
      "learning_rate": 2.9636669024045262e-05,
      "loss": 8.116,
      "step": 9220
    },
    {
      "epoch": 2.8551542804114143,
      "grad_norm": 1.1103359460830688,
      "learning_rate": 2.961456859971712e-05,
      "loss": 8.1294,
      "step": 9230
    },
    {
      "epoch": 2.858247621993659,
      "grad_norm": 0.7019891738891602,
      "learning_rate": 2.959246817538897e-05,
      "loss": 8.1149,
      "step": 9240
    },
    {
      "epoch": 2.861340963575903,
      "grad_norm": 0.7045094966888428,
      "learning_rate": 2.9570367751060825e-05,
      "loss": 8.107,
      "step": 9250
    },
    {
      "epoch": 2.864434305158147,
      "grad_norm": 0.7139843106269836,
      "learning_rate": 2.9548267326732676e-05,
      "loss": 8.0928,
      "step": 9260
    },
    {
      "epoch": 2.8675276467403914,
      "grad_norm": 0.6711528897285461,
      "learning_rate": 2.9526166902404527e-05,
      "loss": 8.1008,
      "step": 9270
    },
    {
      "epoch": 2.8706209883226355,
      "grad_norm": 0.6146473288536072,
      "learning_rate": 2.950406647807638e-05,
      "loss": 8.1174,
      "step": 9280
    },
    {
      "epoch": 2.87371432990488,
      "grad_norm": 1.5169636011123657,
      "learning_rate": 2.9481966053748232e-05,
      "loss": 8.1184,
      "step": 9290
    },
    {
      "epoch": 2.876807671487124,
      "grad_norm": 0.872775137424469,
      "learning_rate": 2.9459865629420086e-05,
      "loss": 8.1163,
      "step": 9300
    },
    {
      "epoch": 2.879901013069368,
      "grad_norm": 1.0280133485794067,
      "learning_rate": 2.9437765205091937e-05,
      "loss": 8.1069,
      "step": 9310
    },
    {
      "epoch": 2.882994354651612,
      "grad_norm": 0.6314733028411865,
      "learning_rate": 2.941566478076379e-05,
      "loss": 8.1043,
      "step": 9320
    },
    {
      "epoch": 2.8860876962338566,
      "grad_norm": 0.677172064781189,
      "learning_rate": 2.9393564356435643e-05,
      "loss": 8.1198,
      "step": 9330
    },
    {
      "epoch": 2.8891810378161007,
      "grad_norm": 0.9150173664093018,
      "learning_rate": 2.9371463932107497e-05,
      "loss": 8.1165,
      "step": 9340
    },
    {
      "epoch": 2.892274379398345,
      "grad_norm": 0.7769370675086975,
      "learning_rate": 2.9349363507779348e-05,
      "loss": 8.1093,
      "step": 9350
    },
    {
      "epoch": 2.8953677209805893,
      "grad_norm": 0.5955384373664856,
      "learning_rate": 2.9327263083451206e-05,
      "loss": 8.0871,
      "step": 9360
    },
    {
      "epoch": 2.8984610625628333,
      "grad_norm": 0.9734750390052795,
      "learning_rate": 2.930516265912306e-05,
      "loss": 8.1023,
      "step": 9370
    },
    {
      "epoch": 2.901554404145078,
      "grad_norm": 0.7921080589294434,
      "learning_rate": 2.928306223479491e-05,
      "loss": 8.0942,
      "step": 9380
    },
    {
      "epoch": 2.904647745727322,
      "grad_norm": 0.7161248326301575,
      "learning_rate": 2.9260961810466765e-05,
      "loss": 8.1005,
      "step": 9390
    },
    {
      "epoch": 2.9077410873095664,
      "grad_norm": 0.7211789488792419,
      "learning_rate": 2.9238861386138616e-05,
      "loss": 8.1121,
      "step": 9400
    },
    {
      "epoch": 2.9108344288918104,
      "grad_norm": 0.6164160370826721,
      "learning_rate": 2.9216760961810467e-05,
      "loss": 8.0947,
      "step": 9410
    },
    {
      "epoch": 2.9139277704740545,
      "grad_norm": 0.7180536389350891,
      "learning_rate": 2.919466053748232e-05,
      "loss": 8.1094,
      "step": 9420
    },
    {
      "epoch": 2.9170211120562985,
      "grad_norm": 0.5280080437660217,
      "learning_rate": 2.9172560113154172e-05,
      "loss": 8.1158,
      "step": 9430
    },
    {
      "epoch": 2.920114453638543,
      "grad_norm": 0.7820165753364563,
      "learning_rate": 2.9150459688826027e-05,
      "loss": 8.1146,
      "step": 9440
    },
    {
      "epoch": 2.923207795220787,
      "grad_norm": 1.354386806488037,
      "learning_rate": 2.9128359264497878e-05,
      "loss": 8.1099,
      "step": 9450
    },
    {
      "epoch": 2.9263011368030316,
      "grad_norm": 0.6691245436668396,
      "learning_rate": 2.9106258840169732e-05,
      "loss": 8.1101,
      "step": 9460
    },
    {
      "epoch": 2.9293944783852757,
      "grad_norm": 0.9536430239677429,
      "learning_rate": 2.9084158415841583e-05,
      "loss": 8.106,
      "step": 9470
    },
    {
      "epoch": 2.9324878199675197,
      "grad_norm": 0.6749069094657898,
      "learning_rate": 2.9062057991513437e-05,
      "loss": 8.13,
      "step": 9480
    },
    {
      "epoch": 2.9355811615497642,
      "grad_norm": 0.8197126984596252,
      "learning_rate": 2.9039957567185288e-05,
      "loss": 8.1024,
      "step": 9490
    },
    {
      "epoch": 2.9386745031320083,
      "grad_norm": 0.8701388835906982,
      "learning_rate": 2.9017857142857146e-05,
      "loss": 8.1054,
      "step": 9500
    },
    {
      "epoch": 2.941767844714253,
      "grad_norm": 0.8511384129524231,
      "learning_rate": 2.8995756718529e-05,
      "loss": 8.1021,
      "step": 9510
    },
    {
      "epoch": 2.944861186296497,
      "grad_norm": 0.6534299254417419,
      "learning_rate": 2.897365629420085e-05,
      "loss": 8.1098,
      "step": 9520
    },
    {
      "epoch": 2.947954527878741,
      "grad_norm": 0.7964046001434326,
      "learning_rate": 2.8951555869872705e-05,
      "loss": 8.0982,
      "step": 9530
    },
    {
      "epoch": 2.951047869460985,
      "grad_norm": 0.7299574017524719,
      "learning_rate": 2.8929455445544556e-05,
      "loss": 8.1013,
      "step": 9540
    },
    {
      "epoch": 2.9541412110432295,
      "grad_norm": 0.9422321915626526,
      "learning_rate": 2.8907355021216407e-05,
      "loss": 8.1146,
      "step": 9550
    },
    {
      "epoch": 2.9572345526254735,
      "grad_norm": 0.7960830926895142,
      "learning_rate": 2.8885254596888262e-05,
      "loss": 8.1241,
      "step": 9560
    },
    {
      "epoch": 2.960327894207718,
      "grad_norm": 0.5942158102989197,
      "learning_rate": 2.8863154172560113e-05,
      "loss": 8.1029,
      "step": 9570
    },
    {
      "epoch": 2.963421235789962,
      "grad_norm": 0.879065990447998,
      "learning_rate": 2.8841053748231967e-05,
      "loss": 8.096,
      "step": 9580
    },
    {
      "epoch": 2.966514577372206,
      "grad_norm": 0.7971692681312561,
      "learning_rate": 2.8818953323903818e-05,
      "loss": 8.1248,
      "step": 9590
    },
    {
      "epoch": 2.9696079189544506,
      "grad_norm": 1.0028879642486572,
      "learning_rate": 2.8796852899575672e-05,
      "loss": 8.1105,
      "step": 9600
    },
    {
      "epoch": 2.9727012605366947,
      "grad_norm": 0.6132903695106506,
      "learning_rate": 2.8774752475247523e-05,
      "loss": 8.1125,
      "step": 9610
    },
    {
      "epoch": 2.975794602118939,
      "grad_norm": 0.8034408092498779,
      "learning_rate": 2.8752652050919378e-05,
      "loss": 8.109,
      "step": 9620
    },
    {
      "epoch": 2.9788879437011833,
      "grad_norm": 0.7330671548843384,
      "learning_rate": 2.8730551626591235e-05,
      "loss": 8.1117,
      "step": 9630
    },
    {
      "epoch": 2.9819812852834273,
      "grad_norm": 1.3551102876663208,
      "learning_rate": 2.8708451202263086e-05,
      "loss": 8.0958,
      "step": 9640
    },
    {
      "epoch": 2.9850746268656714,
      "grad_norm": 0.8165993690490723,
      "learning_rate": 2.868635077793494e-05,
      "loss": 8.1108,
      "step": 9650
    },
    {
      "epoch": 2.988167968447916,
      "grad_norm": 0.8879673480987549,
      "learning_rate": 2.866425035360679e-05,
      "loss": 8.1201,
      "step": 9660
    },
    {
      "epoch": 2.99126131003016,
      "grad_norm": 0.7033306360244751,
      "learning_rate": 2.8642149929278646e-05,
      "loss": 8.1094,
      "step": 9670
    },
    {
      "epoch": 2.9943546516124044,
      "grad_norm": 1.238077998161316,
      "learning_rate": 2.8620049504950497e-05,
      "loss": 8.0959,
      "step": 9680
    },
    {
      "epoch": 2.9974479931946485,
      "grad_norm": 0.6576830744743347,
      "learning_rate": 2.8597949080622348e-05,
      "loss": 8.1195,
      "step": 9690
    },
    {
      "epoch": 3.0005413347768926,
      "grad_norm": 0.8184911012649536,
      "learning_rate": 2.8575848656294202e-05,
      "loss": 8.0814,
      "step": 9700
    },
    {
      "epoch": 3.003634676359137,
      "grad_norm": 0.6919069290161133,
      "learning_rate": 2.8553748231966053e-05,
      "loss": 8.114,
      "step": 9710
    },
    {
      "epoch": 3.006728017941381,
      "grad_norm": 0.8699197173118591,
      "learning_rate": 2.8531647807637907e-05,
      "loss": 8.1104,
      "step": 9720
    },
    {
      "epoch": 3.0098213595236256,
      "grad_norm": 0.7390061020851135,
      "learning_rate": 2.8509547383309758e-05,
      "loss": 8.1011,
      "step": 9730
    },
    {
      "epoch": 3.0129147011058697,
      "grad_norm": 0.8009206056594849,
      "learning_rate": 2.8487446958981613e-05,
      "loss": 8.1083,
      "step": 9740
    },
    {
      "epoch": 3.0160080426881137,
      "grad_norm": 0.7796114087104797,
      "learning_rate": 2.8465346534653464e-05,
      "loss": 8.0899,
      "step": 9750
    },
    {
      "epoch": 3.0191013842703582,
      "grad_norm": 0.7079329490661621,
      "learning_rate": 2.844324611032532e-05,
      "loss": 8.1022,
      "step": 9760
    },
    {
      "epoch": 3.0221947258526023,
      "grad_norm": 0.6712028980255127,
      "learning_rate": 2.8421145685997176e-05,
      "loss": 8.0885,
      "step": 9770
    },
    {
      "epoch": 3.0252880674348464,
      "grad_norm": 0.7117624878883362,
      "learning_rate": 2.8399045261669027e-05,
      "loss": 8.1149,
      "step": 9780
    },
    {
      "epoch": 3.028381409017091,
      "grad_norm": 0.6632695198059082,
      "learning_rate": 2.837694483734088e-05,
      "loss": 8.1138,
      "step": 9790
    },
    {
      "epoch": 3.031474750599335,
      "grad_norm": 0.8488229513168335,
      "learning_rate": 2.8354844413012732e-05,
      "loss": 8.1052,
      "step": 9800
    },
    {
      "epoch": 3.034568092181579,
      "grad_norm": 0.5635408759117126,
      "learning_rate": 2.8332743988684586e-05,
      "loss": 8.0935,
      "step": 9810
    },
    {
      "epoch": 3.0376614337638235,
      "grad_norm": 0.7147914171218872,
      "learning_rate": 2.8310643564356437e-05,
      "loss": 8.1116,
      "step": 9820
    },
    {
      "epoch": 3.0407547753460675,
      "grad_norm": 0.6911981701850891,
      "learning_rate": 2.8288543140028288e-05,
      "loss": 8.1079,
      "step": 9830
    },
    {
      "epoch": 3.043848116928312,
      "grad_norm": 0.5810366868972778,
      "learning_rate": 2.8266442715700142e-05,
      "loss": 8.1064,
      "step": 9840
    },
    {
      "epoch": 3.046941458510556,
      "grad_norm": 0.6798393130302429,
      "learning_rate": 2.8244342291371993e-05,
      "loss": 8.0978,
      "step": 9850
    },
    {
      "epoch": 3.0500348000928,
      "grad_norm": 0.8289865851402283,
      "learning_rate": 2.8222241867043848e-05,
      "loss": 8.1073,
      "step": 9860
    },
    {
      "epoch": 3.0531281416750446,
      "grad_norm": 0.6055102944374084,
      "learning_rate": 2.82001414427157e-05,
      "loss": 8.0968,
      "step": 9870
    },
    {
      "epoch": 3.0562214832572887,
      "grad_norm": 0.7936039566993713,
      "learning_rate": 2.8178041018387553e-05,
      "loss": 8.1044,
      "step": 9880
    },
    {
      "epoch": 3.0593148248395328,
      "grad_norm": 0.9882813692092896,
      "learning_rate": 2.8155940594059404e-05,
      "loss": 8.1015,
      "step": 9890
    },
    {
      "epoch": 3.0624081664217773,
      "grad_norm": 0.7666711807250977,
      "learning_rate": 2.813384016973126e-05,
      "loss": 8.0974,
      "step": 9900
    },
    {
      "epoch": 3.0655015080040213,
      "grad_norm": 1.0095900297164917,
      "learning_rate": 2.8111739745403116e-05,
      "loss": 8.1136,
      "step": 9910
    },
    {
      "epoch": 3.0685948495862654,
      "grad_norm": 1.0002144575119019,
      "learning_rate": 2.8089639321074967e-05,
      "loss": 8.0965,
      "step": 9920
    },
    {
      "epoch": 3.07168819116851,
      "grad_norm": 1.022759199142456,
      "learning_rate": 2.806753889674682e-05,
      "loss": 8.1026,
      "step": 9930
    },
    {
      "epoch": 3.074781532750754,
      "grad_norm": 0.903852105140686,
      "learning_rate": 2.8045438472418672e-05,
      "loss": 8.1113,
      "step": 9940
    },
    {
      "epoch": 3.0778748743329984,
      "grad_norm": 0.9230719208717346,
      "learning_rate": 2.8023338048090526e-05,
      "loss": 8.1115,
      "step": 9950
    },
    {
      "epoch": 3.0809682159152425,
      "grad_norm": 0.5162269473075867,
      "learning_rate": 2.8001237623762377e-05,
      "loss": 8.1062,
      "step": 9960
    },
    {
      "epoch": 3.0840615574974866,
      "grad_norm": 0.7275529503822327,
      "learning_rate": 2.797913719943423e-05,
      "loss": 8.1093,
      "step": 9970
    },
    {
      "epoch": 3.087154899079731,
      "grad_norm": 0.6984078884124756,
      "learning_rate": 2.7957036775106083e-05,
      "loss": 8.115,
      "step": 9980
    },
    {
      "epoch": 3.090248240661975,
      "grad_norm": 0.5768193602561951,
      "learning_rate": 2.7934936350777934e-05,
      "loss": 8.0994,
      "step": 9990
    },
    {
      "epoch": 3.093341582244219,
      "grad_norm": 0.6336302161216736,
      "learning_rate": 2.7912835926449788e-05,
      "loss": 8.1033,
      "step": 10000
    },
    {
      "epoch": 3.0964349238264637,
      "grad_norm": 0.8876256346702576,
      "learning_rate": 2.789073550212164e-05,
      "loss": 8.1141,
      "step": 10010
    },
    {
      "epoch": 3.0995282654087077,
      "grad_norm": 0.8742994666099548,
      "learning_rate": 2.7868635077793493e-05,
      "loss": 8.109,
      "step": 10020
    },
    {
      "epoch": 3.102621606990952,
      "grad_norm": 0.8469825387001038,
      "learning_rate": 2.784653465346535e-05,
      "loss": 8.1036,
      "step": 10030
    },
    {
      "epoch": 3.1057149485731963,
      "grad_norm": 0.5281784534454346,
      "learning_rate": 2.7824434229137202e-05,
      "loss": 8.1059,
      "step": 10040
    },
    {
      "epoch": 3.1088082901554404,
      "grad_norm": 0.7395058870315552,
      "learning_rate": 2.7802333804809056e-05,
      "loss": 8.0962,
      "step": 10050
    },
    {
      "epoch": 3.111901631737685,
      "grad_norm": 0.8850139379501343,
      "learning_rate": 2.7780233380480907e-05,
      "loss": 8.1022,
      "step": 10060
    },
    {
      "epoch": 3.114994973319929,
      "grad_norm": 0.5404447317123413,
      "learning_rate": 2.775813295615276e-05,
      "loss": 8.0916,
      "step": 10070
    },
    {
      "epoch": 3.118088314902173,
      "grad_norm": 0.7866524457931519,
      "learning_rate": 2.7736032531824612e-05,
      "loss": 8.1133,
      "step": 10080
    },
    {
      "epoch": 3.1211816564844175,
      "grad_norm": 0.883952260017395,
      "learning_rate": 2.7713932107496467e-05,
      "loss": 8.1186,
      "step": 10090
    },
    {
      "epoch": 3.1242749980666615,
      "grad_norm": 0.8230529427528381,
      "learning_rate": 2.7691831683168318e-05,
      "loss": 8.0949,
      "step": 10100
    },
    {
      "epoch": 3.1273683396489056,
      "grad_norm": 0.7071940898895264,
      "learning_rate": 2.766973125884017e-05,
      "loss": 8.1082,
      "step": 10110
    },
    {
      "epoch": 3.13046168123115,
      "grad_norm": 0.7847083210945129,
      "learning_rate": 2.7647630834512023e-05,
      "loss": 8.1168,
      "step": 10120
    },
    {
      "epoch": 3.133555022813394,
      "grad_norm": 0.8094547390937805,
      "learning_rate": 2.7625530410183874e-05,
      "loss": 8.0932,
      "step": 10130
    },
    {
      "epoch": 3.136648364395638,
      "grad_norm": 1.084108591079712,
      "learning_rate": 2.7603429985855728e-05,
      "loss": 8.1063,
      "step": 10140
    },
    {
      "epoch": 3.1397417059778827,
      "grad_norm": 1.3549492359161377,
      "learning_rate": 2.758132956152758e-05,
      "loss": 8.0973,
      "step": 10150
    },
    {
      "epoch": 3.1428350475601268,
      "grad_norm": 1.0578364133834839,
      "learning_rate": 2.7559229137199437e-05,
      "loss": 8.0985,
      "step": 10160
    },
    {
      "epoch": 3.1459283891423713,
      "grad_norm": 0.8843377828598022,
      "learning_rate": 2.753712871287129e-05,
      "loss": 8.102,
      "step": 10170
    },
    {
      "epoch": 3.1490217307246153,
      "grad_norm": 0.7174914479255676,
      "learning_rate": 2.7515028288543142e-05,
      "loss": 8.0851,
      "step": 10180
    },
    {
      "epoch": 3.1521150723068594,
      "grad_norm": 1.5227183103561401,
      "learning_rate": 2.7492927864214997e-05,
      "loss": 8.1142,
      "step": 10190
    },
    {
      "epoch": 3.155208413889104,
      "grad_norm": 1.1208131313323975,
      "learning_rate": 2.7470827439886847e-05,
      "loss": 8.1087,
      "step": 10200
    },
    {
      "epoch": 3.158301755471348,
      "grad_norm": 0.5456786751747131,
      "learning_rate": 2.7448727015558702e-05,
      "loss": 8.1058,
      "step": 10210
    },
    {
      "epoch": 3.161395097053592,
      "grad_norm": 0.8475409150123596,
      "learning_rate": 2.7426626591230553e-05,
      "loss": 8.1081,
      "step": 10220
    },
    {
      "epoch": 3.1644884386358365,
      "grad_norm": 0.8401370644569397,
      "learning_rate": 2.7404526166902407e-05,
      "loss": 8.0968,
      "step": 10230
    },
    {
      "epoch": 3.1675817802180806,
      "grad_norm": 1.2489978075027466,
      "learning_rate": 2.7382425742574258e-05,
      "loss": 8.0884,
      "step": 10240
    },
    {
      "epoch": 3.1706751218003246,
      "grad_norm": 0.7131091356277466,
      "learning_rate": 2.736032531824611e-05,
      "loss": 8.107,
      "step": 10250
    },
    {
      "epoch": 3.173768463382569,
      "grad_norm": 1.099993348121643,
      "learning_rate": 2.7338224893917963e-05,
      "loss": 8.0937,
      "step": 10260
    },
    {
      "epoch": 3.176861804964813,
      "grad_norm": 0.8480978608131409,
      "learning_rate": 2.7316124469589814e-05,
      "loss": 8.0946,
      "step": 10270
    },
    {
      "epoch": 3.1799551465470577,
      "grad_norm": 0.9558651447296143,
      "learning_rate": 2.729402404526167e-05,
      "loss": 8.1071,
      "step": 10280
    },
    {
      "epoch": 3.1830484881293017,
      "grad_norm": 0.6734650731086731,
      "learning_rate": 2.727192362093352e-05,
      "loss": 8.1006,
      "step": 10290
    },
    {
      "epoch": 3.186141829711546,
      "grad_norm": 0.9376435875892639,
      "learning_rate": 2.7249823196605377e-05,
      "loss": 8.0917,
      "step": 10300
    },
    {
      "epoch": 3.1892351712937903,
      "grad_norm": 0.8801725506782532,
      "learning_rate": 2.722772277227723e-05,
      "loss": 8.1017,
      "step": 10310
    },
    {
      "epoch": 3.1923285128760344,
      "grad_norm": 0.8488035202026367,
      "learning_rate": 2.7205622347949083e-05,
      "loss": 8.1161,
      "step": 10320
    },
    {
      "epoch": 3.1954218544582784,
      "grad_norm": 0.6034402847290039,
      "learning_rate": 2.7183521923620937e-05,
      "loss": 8.1087,
      "step": 10330
    },
    {
      "epoch": 3.198515196040523,
      "grad_norm": 0.6377551555633545,
      "learning_rate": 2.7161421499292788e-05,
      "loss": 8.106,
      "step": 10340
    },
    {
      "epoch": 3.201608537622767,
      "grad_norm": 0.8840941786766052,
      "learning_rate": 2.7139321074964642e-05,
      "loss": 8.1055,
      "step": 10350
    },
    {
      "epoch": 3.204701879205011,
      "grad_norm": 0.8724914193153381,
      "learning_rate": 2.7117220650636493e-05,
      "loss": 8.1064,
      "step": 10360
    },
    {
      "epoch": 3.2077952207872555,
      "grad_norm": 0.7110289931297302,
      "learning_rate": 2.7095120226308347e-05,
      "loss": 8.1003,
      "step": 10370
    },
    {
      "epoch": 3.2108885623694996,
      "grad_norm": 0.4816248118877411,
      "learning_rate": 2.70730198019802e-05,
      "loss": 8.1008,
      "step": 10380
    },
    {
      "epoch": 3.213981903951744,
      "grad_norm": 0.5340366959571838,
      "learning_rate": 2.705091937765205e-05,
      "loss": 8.1041,
      "step": 10390
    },
    {
      "epoch": 3.217075245533988,
      "grad_norm": 1.170303463935852,
      "learning_rate": 2.7028818953323904e-05,
      "loss": 8.1148,
      "step": 10400
    },
    {
      "epoch": 3.220168587116232,
      "grad_norm": 1.3181408643722534,
      "learning_rate": 2.7006718528995755e-05,
      "loss": 8.1034,
      "step": 10410
    },
    {
      "epoch": 3.2232619286984767,
      "grad_norm": 0.5492451190948486,
      "learning_rate": 2.698461810466761e-05,
      "loss": 8.1052,
      "step": 10420
    },
    {
      "epoch": 3.2263552702807208,
      "grad_norm": 0.4860824942588806,
      "learning_rate": 2.6962517680339467e-05,
      "loss": 8.0985,
      "step": 10430
    },
    {
      "epoch": 3.229448611862965,
      "grad_norm": 1.2488993406295776,
      "learning_rate": 2.6940417256011318e-05,
      "loss": 8.0925,
      "step": 10440
    },
    {
      "epoch": 3.2325419534452093,
      "grad_norm": 0.5049091577529907,
      "learning_rate": 2.6918316831683172e-05,
      "loss": 8.1049,
      "step": 10450
    },
    {
      "epoch": 3.2356352950274534,
      "grad_norm": 0.8086821436882019,
      "learning_rate": 2.6896216407355023e-05,
      "loss": 8.1021,
      "step": 10460
    },
    {
      "epoch": 3.2387286366096975,
      "grad_norm": 0.767903208732605,
      "learning_rate": 2.6874115983026877e-05,
      "loss": 8.0928,
      "step": 10470
    },
    {
      "epoch": 3.241821978191942,
      "grad_norm": 0.9076136946678162,
      "learning_rate": 2.6852015558698728e-05,
      "loss": 8.1073,
      "step": 10480
    },
    {
      "epoch": 3.244915319774186,
      "grad_norm": 0.7727258801460266,
      "learning_rate": 2.6829915134370582e-05,
      "loss": 8.1061,
      "step": 10490
    },
    {
      "epoch": 3.2480086613564305,
      "grad_norm": 0.9299774169921875,
      "learning_rate": 2.6807814710042433e-05,
      "loss": 8.1005,
      "step": 10500
    },
    {
      "epoch": 3.2511020029386746,
      "grad_norm": 0.8597429990768433,
      "learning_rate": 2.6785714285714288e-05,
      "loss": 8.101,
      "step": 10510
    },
    {
      "epoch": 3.2541953445209186,
      "grad_norm": 0.8112443685531616,
      "learning_rate": 2.676361386138614e-05,
      "loss": 8.1108,
      "step": 10520
    },
    {
      "epoch": 3.257288686103163,
      "grad_norm": 0.8010808825492859,
      "learning_rate": 2.674151343705799e-05,
      "loss": 8.1069,
      "step": 10530
    },
    {
      "epoch": 3.260382027685407,
      "grad_norm": 0.9142884612083435,
      "learning_rate": 2.6719413012729844e-05,
      "loss": 8.1093,
      "step": 10540
    },
    {
      "epoch": 3.2634753692676512,
      "grad_norm": 0.6666103005409241,
      "learning_rate": 2.6697312588401695e-05,
      "loss": 8.1138,
      "step": 10550
    },
    {
      "epoch": 3.2665687108498958,
      "grad_norm": 0.6035496592521667,
      "learning_rate": 2.6675212164073553e-05,
      "loss": 8.1176,
      "step": 10560
    },
    {
      "epoch": 3.26966205243214,
      "grad_norm": 1.1168321371078491,
      "learning_rate": 2.6653111739745407e-05,
      "loss": 8.111,
      "step": 10570
    },
    {
      "epoch": 3.272755394014384,
      "grad_norm": 0.9762133359909058,
      "learning_rate": 2.6631011315417258e-05,
      "loss": 8.1169,
      "step": 10580
    },
    {
      "epoch": 3.2758487355966284,
      "grad_norm": 0.6811995506286621,
      "learning_rate": 2.6608910891089112e-05,
      "loss": 8.1221,
      "step": 10590
    },
    {
      "epoch": 3.2789420771788724,
      "grad_norm": 0.6053406596183777,
      "learning_rate": 2.6586810466760963e-05,
      "loss": 8.1143,
      "step": 10600
    },
    {
      "epoch": 3.282035418761117,
      "grad_norm": 0.599897027015686,
      "learning_rate": 2.6564710042432817e-05,
      "loss": 8.1007,
      "step": 10610
    },
    {
      "epoch": 3.285128760343361,
      "grad_norm": 0.8028992414474487,
      "learning_rate": 2.654260961810467e-05,
      "loss": 8.1068,
      "step": 10620
    },
    {
      "epoch": 3.288222101925605,
      "grad_norm": 0.7862556576728821,
      "learning_rate": 2.6520509193776523e-05,
      "loss": 8.113,
      "step": 10630
    },
    {
      "epoch": 3.2913154435078495,
      "grad_norm": 0.7700480818748474,
      "learning_rate": 2.6498408769448374e-05,
      "loss": 8.103,
      "step": 10640
    },
    {
      "epoch": 3.2944087850900936,
      "grad_norm": 0.8608717918395996,
      "learning_rate": 2.6476308345120228e-05,
      "loss": 8.1089,
      "step": 10650
    },
    {
      "epoch": 3.2975021266723377,
      "grad_norm": 0.6745821237564087,
      "learning_rate": 2.645420792079208e-05,
      "loss": 8.0897,
      "step": 10660
    },
    {
      "epoch": 3.300595468254582,
      "grad_norm": 1.000858187675476,
      "learning_rate": 2.643210749646393e-05,
      "loss": 8.1162,
      "step": 10670
    },
    {
      "epoch": 3.3036888098368262,
      "grad_norm": 0.9222114682197571,
      "learning_rate": 2.6410007072135784e-05,
      "loss": 8.0992,
      "step": 10680
    },
    {
      "epoch": 3.3067821514190703,
      "grad_norm": 0.8219615817070007,
      "learning_rate": 2.6387906647807635e-05,
      "loss": 8.114,
      "step": 10690
    },
    {
      "epoch": 3.309875493001315,
      "grad_norm": 0.8828024864196777,
      "learning_rate": 2.6365806223479493e-05,
      "loss": 8.0951,
      "step": 10700
    },
    {
      "epoch": 3.312968834583559,
      "grad_norm": 0.9151893854141235,
      "learning_rate": 2.6343705799151347e-05,
      "loss": 8.1073,
      "step": 10710
    },
    {
      "epoch": 3.3160621761658033,
      "grad_norm": 0.8627604842185974,
      "learning_rate": 2.6321605374823198e-05,
      "loss": 8.1088,
      "step": 10720
    },
    {
      "epoch": 3.3191555177480474,
      "grad_norm": 0.5789752006530762,
      "learning_rate": 2.6299504950495053e-05,
      "loss": 8.1024,
      "step": 10730
    },
    {
      "epoch": 3.3222488593302915,
      "grad_norm": 1.0330649614334106,
      "learning_rate": 2.6277404526166903e-05,
      "loss": 8.1149,
      "step": 10740
    },
    {
      "epoch": 3.3253422009125355,
      "grad_norm": 0.996377170085907,
      "learning_rate": 2.6255304101838758e-05,
      "loss": 8.0982,
      "step": 10750
    },
    {
      "epoch": 3.32843554249478,
      "grad_norm": 0.801764726638794,
      "learning_rate": 2.623320367751061e-05,
      "loss": 8.0958,
      "step": 10760
    },
    {
      "epoch": 3.331528884077024,
      "grad_norm": 0.7370763421058655,
      "learning_rate": 2.6211103253182463e-05,
      "loss": 8.1014,
      "step": 10770
    },
    {
      "epoch": 3.3346222256592686,
      "grad_norm": 0.5048869848251343,
      "learning_rate": 2.6189002828854314e-05,
      "loss": 8.1054,
      "step": 10780
    },
    {
      "epoch": 3.3377155672415126,
      "grad_norm": 0.9257993698120117,
      "learning_rate": 2.616690240452617e-05,
      "loss": 8.0954,
      "step": 10790
    },
    {
      "epoch": 3.3408089088237567,
      "grad_norm": 0.7626732587814331,
      "learning_rate": 2.614480198019802e-05,
      "loss": 8.0915,
      "step": 10800
    },
    {
      "epoch": 3.343902250406001,
      "grad_norm": 0.6530042290687561,
      "learning_rate": 2.612270155586987e-05,
      "loss": 8.1037,
      "step": 10810
    },
    {
      "epoch": 3.3469955919882453,
      "grad_norm": 0.5945021510124207,
      "learning_rate": 2.6100601131541725e-05,
      "loss": 8.09,
      "step": 10820
    },
    {
      "epoch": 3.3500889335704898,
      "grad_norm": 0.6831685900688171,
      "learning_rate": 2.6078500707213582e-05,
      "loss": 8.0981,
      "step": 10830
    },
    {
      "epoch": 3.353182275152734,
      "grad_norm": 0.9660640954971313,
      "learning_rate": 2.6056400282885433e-05,
      "loss": 8.1146,
      "step": 10840
    },
    {
      "epoch": 3.356275616734978,
      "grad_norm": 0.9918832778930664,
      "learning_rate": 2.6034299858557288e-05,
      "loss": 8.1199,
      "step": 10850
    },
    {
      "epoch": 3.359368958317222,
      "grad_norm": 0.7425703406333923,
      "learning_rate": 2.601219943422914e-05,
      "loss": 8.0921,
      "step": 10860
    },
    {
      "epoch": 3.3624622998994664,
      "grad_norm": 0.8416071534156799,
      "learning_rate": 2.5990099009900993e-05,
      "loss": 8.0981,
      "step": 10870
    },
    {
      "epoch": 3.3655556414817105,
      "grad_norm": 1.227294683456421,
      "learning_rate": 2.5967998585572844e-05,
      "loss": 8.1318,
      "step": 10880
    },
    {
      "epoch": 3.368648983063955,
      "grad_norm": 1.0093448162078857,
      "learning_rate": 2.5945898161244698e-05,
      "loss": 8.1006,
      "step": 10890
    },
    {
      "epoch": 3.371742324646199,
      "grad_norm": 1.0424818992614746,
      "learning_rate": 2.592379773691655e-05,
      "loss": 8.0962,
      "step": 10900
    },
    {
      "epoch": 3.374835666228443,
      "grad_norm": 0.9508131742477417,
      "learning_rate": 2.5901697312588403e-05,
      "loss": 8.0845,
      "step": 10910
    },
    {
      "epoch": 3.3779290078106876,
      "grad_norm": 1.014140009880066,
      "learning_rate": 2.5879596888260254e-05,
      "loss": 8.0861,
      "step": 10920
    },
    {
      "epoch": 3.3810223493929317,
      "grad_norm": 0.6049110293388367,
      "learning_rate": 2.585749646393211e-05,
      "loss": 8.1038,
      "step": 10930
    },
    {
      "epoch": 3.384115690975176,
      "grad_norm": 0.7292504906654358,
      "learning_rate": 2.583539603960396e-05,
      "loss": 8.0941,
      "step": 10940
    },
    {
      "epoch": 3.3872090325574202,
      "grad_norm": 0.7692973613739014,
      "learning_rate": 2.581329561527581e-05,
      "loss": 8.0976,
      "step": 10950
    },
    {
      "epoch": 3.3903023741396643,
      "grad_norm": 0.6634624004364014,
      "learning_rate": 2.579119519094767e-05,
      "loss": 8.1048,
      "step": 10960
    },
    {
      "epoch": 3.3933957157219083,
      "grad_norm": 0.832862377166748,
      "learning_rate": 2.5769094766619523e-05,
      "loss": 8.1058,
      "step": 10970
    },
    {
      "epoch": 3.396489057304153,
      "grad_norm": 0.792634904384613,
      "learning_rate": 2.5746994342291374e-05,
      "loss": 8.1259,
      "step": 10980
    },
    {
      "epoch": 3.399582398886397,
      "grad_norm": 0.9821207523345947,
      "learning_rate": 2.5724893917963228e-05,
      "loss": 8.1043,
      "step": 10990
    },
    {
      "epoch": 3.4026757404686414,
      "grad_norm": 1.0299824476242065,
      "learning_rate": 2.570279349363508e-05,
      "loss": 8.1012,
      "step": 11000
    },
    {
      "epoch": 3.4057690820508855,
      "grad_norm": 1.0243135690689087,
      "learning_rate": 2.5680693069306933e-05,
      "loss": 8.0962,
      "step": 11010
    },
    {
      "epoch": 3.4088624236331295,
      "grad_norm": 0.6784717440605164,
      "learning_rate": 2.5658592644978784e-05,
      "loss": 8.1037,
      "step": 11020
    },
    {
      "epoch": 3.411955765215374,
      "grad_norm": 0.5318592190742493,
      "learning_rate": 2.563649222065064e-05,
      "loss": 8.1074,
      "step": 11030
    },
    {
      "epoch": 3.415049106797618,
      "grad_norm": 0.7155341506004333,
      "learning_rate": 2.561439179632249e-05,
      "loss": 8.121,
      "step": 11040
    },
    {
      "epoch": 3.4181424483798626,
      "grad_norm": 1.128713607788086,
      "learning_rate": 2.5592291371994344e-05,
      "loss": 8.0937,
      "step": 11050
    },
    {
      "epoch": 3.4212357899621066,
      "grad_norm": 0.853648841381073,
      "learning_rate": 2.5570190947666195e-05,
      "loss": 8.1003,
      "step": 11060
    },
    {
      "epoch": 3.4243291315443507,
      "grad_norm": 0.9434267282485962,
      "learning_rate": 2.554809052333805e-05,
      "loss": 8.1092,
      "step": 11070
    },
    {
      "epoch": 3.4274224731265948,
      "grad_norm": 0.789575457572937,
      "learning_rate": 2.55259900990099e-05,
      "loss": 8.1039,
      "step": 11080
    },
    {
      "epoch": 3.4305158147088393,
      "grad_norm": 0.6753472685813904,
      "learning_rate": 2.550388967468175e-05,
      "loss": 8.1001,
      "step": 11090
    },
    {
      "epoch": 3.4336091562910833,
      "grad_norm": 0.4465691149234772,
      "learning_rate": 2.5481789250353612e-05,
      "loss": 8.1063,
      "step": 11100
    },
    {
      "epoch": 3.436702497873328,
      "grad_norm": 0.8238527178764343,
      "learning_rate": 2.5459688826025463e-05,
      "loss": 8.0911,
      "step": 11110
    },
    {
      "epoch": 3.439795839455572,
      "grad_norm": 0.7504106760025024,
      "learning_rate": 2.5437588401697314e-05,
      "loss": 8.0931,
      "step": 11120
    },
    {
      "epoch": 3.442889181037816,
      "grad_norm": 0.9832870364189148,
      "learning_rate": 2.5415487977369168e-05,
      "loss": 8.1082,
      "step": 11130
    },
    {
      "epoch": 3.4459825226200604,
      "grad_norm": 1.1161816120147705,
      "learning_rate": 2.539338755304102e-05,
      "loss": 8.0966,
      "step": 11140
    },
    {
      "epoch": 3.4490758642023045,
      "grad_norm": 0.7530856728553772,
      "learning_rate": 2.5371287128712873e-05,
      "loss": 8.1109,
      "step": 11150
    },
    {
      "epoch": 3.452169205784549,
      "grad_norm": 0.6870070695877075,
      "learning_rate": 2.5349186704384724e-05,
      "loss": 8.1096,
      "step": 11160
    },
    {
      "epoch": 3.455262547366793,
      "grad_norm": 0.7706400752067566,
      "learning_rate": 2.532708628005658e-05,
      "loss": 8.0892,
      "step": 11170
    },
    {
      "epoch": 3.458355888949037,
      "grad_norm": 0.7305397391319275,
      "learning_rate": 2.530498585572843e-05,
      "loss": 8.1077,
      "step": 11180
    },
    {
      "epoch": 3.461449230531281,
      "grad_norm": 0.8493666648864746,
      "learning_rate": 2.5282885431400284e-05,
      "loss": 8.0998,
      "step": 11190
    },
    {
      "epoch": 3.4645425721135257,
      "grad_norm": 0.7701996564865112,
      "learning_rate": 2.5260785007072135e-05,
      "loss": 8.1022,
      "step": 11200
    },
    {
      "epoch": 3.4676359136957697,
      "grad_norm": 0.5574349164962769,
      "learning_rate": 2.523868458274399e-05,
      "loss": 8.1004,
      "step": 11210
    },
    {
      "epoch": 3.4707292552780142,
      "grad_norm": 0.6364221572875977,
      "learning_rate": 2.521658415841584e-05,
      "loss": 8.0852,
      "step": 11220
    },
    {
      "epoch": 3.4738225968602583,
      "grad_norm": 0.5897513628005981,
      "learning_rate": 2.5194483734087698e-05,
      "loss": 8.1052,
      "step": 11230
    },
    {
      "epoch": 3.4769159384425024,
      "grad_norm": 0.7260732054710388,
      "learning_rate": 2.5172383309759552e-05,
      "loss": 8.0914,
      "step": 11240
    },
    {
      "epoch": 3.480009280024747,
      "grad_norm": 0.7340152859687805,
      "learning_rate": 2.5150282885431403e-05,
      "loss": 8.1152,
      "step": 11250
    },
    {
      "epoch": 3.483102621606991,
      "grad_norm": 0.7422891855239868,
      "learning_rate": 2.5128182461103254e-05,
      "loss": 8.1078,
      "step": 11260
    },
    {
      "epoch": 3.4861959631892354,
      "grad_norm": 0.7885480523109436,
      "learning_rate": 2.510608203677511e-05,
      "loss": 8.1149,
      "step": 11270
    },
    {
      "epoch": 3.4892893047714795,
      "grad_norm": 0.6894049048423767,
      "learning_rate": 2.508398161244696e-05,
      "loss": 8.1159,
      "step": 11280
    },
    {
      "epoch": 3.4923826463537235,
      "grad_norm": 0.7820294499397278,
      "learning_rate": 2.5061881188118814e-05,
      "loss": 8.101,
      "step": 11290
    },
    {
      "epoch": 3.4954759879359676,
      "grad_norm": 0.6289247274398804,
      "learning_rate": 2.5039780763790665e-05,
      "loss": 8.111,
      "step": 11300
    },
    {
      "epoch": 3.498569329518212,
      "grad_norm": 0.8931630849838257,
      "learning_rate": 2.501768033946252e-05,
      "loss": 8.0927,
      "step": 11310
    },
    {
      "epoch": 3.501662671100456,
      "grad_norm": 0.6462466716766357,
      "learning_rate": 2.499557991513437e-05,
      "loss": 8.1134,
      "step": 11320
    },
    {
      "epoch": 3.5047560126827006,
      "grad_norm": 0.9078208208084106,
      "learning_rate": 2.4973479490806224e-05,
      "loss": 8.1185,
      "step": 11330
    },
    {
      "epoch": 3.5078493542649447,
      "grad_norm": 0.5990978479385376,
      "learning_rate": 2.495137906647808e-05,
      "loss": 8.0992,
      "step": 11340
    },
    {
      "epoch": 3.5109426958471888,
      "grad_norm": 0.7442519068717957,
      "learning_rate": 2.492927864214993e-05,
      "loss": 8.1061,
      "step": 11350
    },
    {
      "epoch": 3.5140360374294333,
      "grad_norm": 0.5777914524078369,
      "learning_rate": 2.4907178217821784e-05,
      "loss": 8.104,
      "step": 11360
    },
    {
      "epoch": 3.5171293790116773,
      "grad_norm": 0.6656046509742737,
      "learning_rate": 2.4885077793493635e-05,
      "loss": 8.1165,
      "step": 11370
    },
    {
      "epoch": 3.520222720593922,
      "grad_norm": 0.8046016097068787,
      "learning_rate": 2.486297736916549e-05,
      "loss": 8.0854,
      "step": 11380
    },
    {
      "epoch": 3.523316062176166,
      "grad_norm": 0.9777056574821472,
      "learning_rate": 2.484087694483734e-05,
      "loss": 8.1062,
      "step": 11390
    },
    {
      "epoch": 3.52640940375841,
      "grad_norm": 0.805345892906189,
      "learning_rate": 2.4818776520509195e-05,
      "loss": 8.1052,
      "step": 11400
    },
    {
      "epoch": 3.529502745340654,
      "grad_norm": 0.9235185384750366,
      "learning_rate": 2.479667609618105e-05,
      "loss": 8.1143,
      "step": 11410
    },
    {
      "epoch": 3.5325960869228985,
      "grad_norm": 0.5825061202049255,
      "learning_rate": 2.47745756718529e-05,
      "loss": 8.1107,
      "step": 11420
    },
    {
      "epoch": 3.5356894285051426,
      "grad_norm": 1.2757313251495361,
      "learning_rate": 2.4752475247524754e-05,
      "loss": 8.1102,
      "step": 11430
    },
    {
      "epoch": 3.538782770087387,
      "grad_norm": 0.6582181453704834,
      "learning_rate": 2.4730374823196605e-05,
      "loss": 8.1118,
      "step": 11440
    },
    {
      "epoch": 3.541876111669631,
      "grad_norm": 0.552203357219696,
      "learning_rate": 2.470827439886846e-05,
      "loss": 8.0976,
      "step": 11450
    },
    {
      "epoch": 3.544969453251875,
      "grad_norm": 0.5782486796379089,
      "learning_rate": 2.4686173974540314e-05,
      "loss": 8.0789,
      "step": 11460
    },
    {
      "epoch": 3.5480627948341197,
      "grad_norm": 0.8015296459197998,
      "learning_rate": 2.4664073550212165e-05,
      "loss": 8.0959,
      "step": 11470
    },
    {
      "epoch": 3.5511561364163637,
      "grad_norm": 0.8656583428382874,
      "learning_rate": 2.464197312588402e-05,
      "loss": 8.1116,
      "step": 11480
    },
    {
      "epoch": 3.5542494779986082,
      "grad_norm": 0.6272944808006287,
      "learning_rate": 2.461987270155587e-05,
      "loss": 8.1099,
      "step": 11490
    },
    {
      "epoch": 3.5573428195808523,
      "grad_norm": 0.8610741496086121,
      "learning_rate": 2.4597772277227724e-05,
      "loss": 8.1071,
      "step": 11500
    },
    {
      "epoch": 3.5604361611630964,
      "grad_norm": 0.6074814200401306,
      "learning_rate": 2.4575671852899575e-05,
      "loss": 8.0986,
      "step": 11510
    },
    {
      "epoch": 3.5635295027453404,
      "grad_norm": 1.0561699867248535,
      "learning_rate": 2.455357142857143e-05,
      "loss": 8.1117,
      "step": 11520
    },
    {
      "epoch": 3.566622844327585,
      "grad_norm": 0.65511155128479,
      "learning_rate": 2.4531471004243284e-05,
      "loss": 8.1183,
      "step": 11530
    },
    {
      "epoch": 3.569716185909829,
      "grad_norm": 0.7973815202713013,
      "learning_rate": 2.4509370579915135e-05,
      "loss": 8.1179,
      "step": 11540
    },
    {
      "epoch": 3.5728095274920735,
      "grad_norm": 0.8005743622779846,
      "learning_rate": 2.448727015558699e-05,
      "loss": 8.1032,
      "step": 11550
    },
    {
      "epoch": 3.5759028690743175,
      "grad_norm": 0.834827721118927,
      "learning_rate": 2.446516973125884e-05,
      "loss": 8.1008,
      "step": 11560
    },
    {
      "epoch": 3.5789962106565616,
      "grad_norm": 0.7012044787406921,
      "learning_rate": 2.4443069306930694e-05,
      "loss": 8.1085,
      "step": 11570
    },
    {
      "epoch": 3.582089552238806,
      "grad_norm": 0.8557447791099548,
      "learning_rate": 2.4420968882602545e-05,
      "loss": 8.1132,
      "step": 11580
    },
    {
      "epoch": 3.58518289382105,
      "grad_norm": 0.7214196920394897,
      "learning_rate": 2.43988684582744e-05,
      "loss": 8.0994,
      "step": 11590
    },
    {
      "epoch": 3.5882762354032947,
      "grad_norm": 1.0588473081588745,
      "learning_rate": 2.4376768033946254e-05,
      "loss": 8.1149,
      "step": 11600
    },
    {
      "epoch": 3.5913695769855387,
      "grad_norm": 0.7677791714668274,
      "learning_rate": 2.4354667609618105e-05,
      "loss": 8.0965,
      "step": 11610
    },
    {
      "epoch": 3.5944629185677828,
      "grad_norm": 1.0300345420837402,
      "learning_rate": 2.433256718528996e-05,
      "loss": 8.0952,
      "step": 11620
    },
    {
      "epoch": 3.597556260150027,
      "grad_norm": 1.1303727626800537,
      "learning_rate": 2.431046676096181e-05,
      "loss": 8.1065,
      "step": 11630
    },
    {
      "epoch": 3.6006496017322713,
      "grad_norm": 0.6124145984649658,
      "learning_rate": 2.4288366336633665e-05,
      "loss": 8.0975,
      "step": 11640
    },
    {
      "epoch": 3.6037429433145154,
      "grad_norm": 0.6149651408195496,
      "learning_rate": 2.4266265912305516e-05,
      "loss": 8.1108,
      "step": 11650
    },
    {
      "epoch": 3.60683628489676,
      "grad_norm": 0.7164033055305481,
      "learning_rate": 2.4244165487977373e-05,
      "loss": 8.1043,
      "step": 11660
    },
    {
      "epoch": 3.609929626479004,
      "grad_norm": 0.7561760544776917,
      "learning_rate": 2.4222065063649224e-05,
      "loss": 8.1125,
      "step": 11670
    },
    {
      "epoch": 3.613022968061248,
      "grad_norm": 0.9271063208580017,
      "learning_rate": 2.4199964639321075e-05,
      "loss": 8.1092,
      "step": 11680
    },
    {
      "epoch": 3.6161163096434925,
      "grad_norm": 0.9286230206489563,
      "learning_rate": 2.417786421499293e-05,
      "loss": 8.0978,
      "step": 11690
    },
    {
      "epoch": 3.6192096512257366,
      "grad_norm": 0.6517167091369629,
      "learning_rate": 2.415576379066478e-05,
      "loss": 8.099,
      "step": 11700
    },
    {
      "epoch": 3.622302992807981,
      "grad_norm": 0.6297550201416016,
      "learning_rate": 2.4133663366336635e-05,
      "loss": 8.0938,
      "step": 11710
    },
    {
      "epoch": 3.625396334390225,
      "grad_norm": 0.6664562225341797,
      "learning_rate": 2.4111562942008486e-05,
      "loss": 8.117,
      "step": 11720
    },
    {
      "epoch": 3.628489675972469,
      "grad_norm": 0.6474733948707581,
      "learning_rate": 2.4089462517680343e-05,
      "loss": 8.0969,
      "step": 11730
    },
    {
      "epoch": 3.6315830175547132,
      "grad_norm": 0.7206547260284424,
      "learning_rate": 2.4067362093352194e-05,
      "loss": 8.1016,
      "step": 11740
    },
    {
      "epoch": 3.6346763591369577,
      "grad_norm": 0.5733475685119629,
      "learning_rate": 2.4045261669024045e-05,
      "loss": 8.1002,
      "step": 11750
    },
    {
      "epoch": 3.637769700719202,
      "grad_norm": 0.7435771822929382,
      "learning_rate": 2.40231612446959e-05,
      "loss": 8.093,
      "step": 11760
    },
    {
      "epoch": 3.6408630423014463,
      "grad_norm": 0.7430015802383423,
      "learning_rate": 2.400106082036775e-05,
      "loss": 8.0996,
      "step": 11770
    },
    {
      "epoch": 3.6439563838836904,
      "grad_norm": 0.6231874227523804,
      "learning_rate": 2.3978960396039605e-05,
      "loss": 8.0972,
      "step": 11780
    },
    {
      "epoch": 3.6470497254659344,
      "grad_norm": 0.8493922352790833,
      "learning_rate": 2.3956859971711456e-05,
      "loss": 8.0946,
      "step": 11790
    },
    {
      "epoch": 3.650143067048179,
      "grad_norm": 0.765673041343689,
      "learning_rate": 2.3934759547383314e-05,
      "loss": 8.1169,
      "step": 11800
    },
    {
      "epoch": 3.653236408630423,
      "grad_norm": 0.7977613210678101,
      "learning_rate": 2.3912659123055165e-05,
      "loss": 8.0974,
      "step": 11810
    },
    {
      "epoch": 3.6563297502126675,
      "grad_norm": 1.1814916133880615,
      "learning_rate": 2.3890558698727015e-05,
      "loss": 8.1026,
      "step": 11820
    },
    {
      "epoch": 3.6594230917949115,
      "grad_norm": 0.7055432200431824,
      "learning_rate": 2.386845827439887e-05,
      "loss": 8.1038,
      "step": 11830
    },
    {
      "epoch": 3.6625164333771556,
      "grad_norm": 0.8180091977119446,
      "learning_rate": 2.384635785007072e-05,
      "loss": 8.1143,
      "step": 11840
    },
    {
      "epoch": 3.6656097749593997,
      "grad_norm": 0.8891481161117554,
      "learning_rate": 2.3824257425742575e-05,
      "loss": 8.1103,
      "step": 11850
    },
    {
      "epoch": 3.668703116541644,
      "grad_norm": 0.6856452822685242,
      "learning_rate": 2.380215700141443e-05,
      "loss": 8.1028,
      "step": 11860
    },
    {
      "epoch": 3.671796458123888,
      "grad_norm": 1.027034044265747,
      "learning_rate": 2.3780056577086284e-05,
      "loss": 8.1073,
      "step": 11870
    },
    {
      "epoch": 3.6748897997061327,
      "grad_norm": 0.5551108717918396,
      "learning_rate": 2.3757956152758135e-05,
      "loss": 8.0919,
      "step": 11880
    },
    {
      "epoch": 3.677983141288377,
      "grad_norm": 0.9678500890731812,
      "learning_rate": 2.3735855728429986e-05,
      "loss": 8.0962,
      "step": 11890
    },
    {
      "epoch": 3.681076482870621,
      "grad_norm": 0.7704302668571472,
      "learning_rate": 2.371375530410184e-05,
      "loss": 8.1009,
      "step": 11900
    },
    {
      "epoch": 3.6841698244528653,
      "grad_norm": 0.7322105169296265,
      "learning_rate": 2.369165487977369e-05,
      "loss": 8.099,
      "step": 11910
    },
    {
      "epoch": 3.6872631660351094,
      "grad_norm": 0.7687269449234009,
      "learning_rate": 2.3669554455445545e-05,
      "loss": 8.1092,
      "step": 11920
    },
    {
      "epoch": 3.690356507617354,
      "grad_norm": 0.6603623628616333,
      "learning_rate": 2.36474540311174e-05,
      "loss": 8.1068,
      "step": 11930
    },
    {
      "epoch": 3.693449849199598,
      "grad_norm": 0.8556596040725708,
      "learning_rate": 2.3625353606789254e-05,
      "loss": 8.0915,
      "step": 11940
    },
    {
      "epoch": 3.696543190781842,
      "grad_norm": 0.7415850758552551,
      "learning_rate": 2.3603253182461105e-05,
      "loss": 8.1204,
      "step": 11950
    },
    {
      "epoch": 3.699636532364086,
      "grad_norm": 0.7488853335380554,
      "learning_rate": 2.3581152758132956e-05,
      "loss": 8.0988,
      "step": 11960
    },
    {
      "epoch": 3.7027298739463306,
      "grad_norm": 0.7889118790626526,
      "learning_rate": 2.355905233380481e-05,
      "loss": 8.1141,
      "step": 11970
    },
    {
      "epoch": 3.7058232155285746,
      "grad_norm": 0.7262313961982727,
      "learning_rate": 2.353695190947666e-05,
      "loss": 8.1159,
      "step": 11980
    },
    {
      "epoch": 3.708916557110819,
      "grad_norm": 1.0776149034500122,
      "learning_rate": 2.3514851485148515e-05,
      "loss": 8.1086,
      "step": 11990
    },
    {
      "epoch": 3.712009898693063,
      "grad_norm": 1.2664437294006348,
      "learning_rate": 2.349275106082037e-05,
      "loss": 8.1028,
      "step": 12000
    },
    {
      "epoch": 3.7151032402753073,
      "grad_norm": 0.754374086856842,
      "learning_rate": 2.3470650636492224e-05,
      "loss": 8.1099,
      "step": 12010
    },
    {
      "epoch": 3.7181965818575518,
      "grad_norm": 0.5984283685684204,
      "learning_rate": 2.3448550212164075e-05,
      "loss": 8.1078,
      "step": 12020
    },
    {
      "epoch": 3.721289923439796,
      "grad_norm": 0.9657517671585083,
      "learning_rate": 2.3426449787835926e-05,
      "loss": 8.1038,
      "step": 12030
    },
    {
      "epoch": 3.7243832650220403,
      "grad_norm": 1.1449147462844849,
      "learning_rate": 2.340434936350778e-05,
      "loss": 8.101,
      "step": 12040
    },
    {
      "epoch": 3.7274766066042844,
      "grad_norm": 0.8093847632408142,
      "learning_rate": 2.338224893917963e-05,
      "loss": 8.0983,
      "step": 12050
    },
    {
      "epoch": 3.7305699481865284,
      "grad_norm": 0.6851373314857483,
      "learning_rate": 2.336014851485149e-05,
      "loss": 8.1021,
      "step": 12060
    },
    {
      "epoch": 3.7336632897687725,
      "grad_norm": 0.5898375511169434,
      "learning_rate": 2.333804809052334e-05,
      "loss": 8.1009,
      "step": 12070
    },
    {
      "epoch": 3.736756631351017,
      "grad_norm": 0.6686884164810181,
      "learning_rate": 2.3315947666195194e-05,
      "loss": 8.115,
      "step": 12080
    },
    {
      "epoch": 3.739849972933261,
      "grad_norm": 0.7025202512741089,
      "learning_rate": 2.3293847241867045e-05,
      "loss": 8.1148,
      "step": 12090
    },
    {
      "epoch": 3.7429433145155055,
      "grad_norm": 0.775547981262207,
      "learning_rate": 2.3271746817538896e-05,
      "loss": 8.1124,
      "step": 12100
    },
    {
      "epoch": 3.7460366560977496,
      "grad_norm": 0.788159966468811,
      "learning_rate": 2.324964639321075e-05,
      "loss": 8.0981,
      "step": 12110
    },
    {
      "epoch": 3.7491299976799937,
      "grad_norm": 0.8167747259140015,
      "learning_rate": 2.32275459688826e-05,
      "loss": 8.1112,
      "step": 12120
    },
    {
      "epoch": 3.752223339262238,
      "grad_norm": 1.0406444072723389,
      "learning_rate": 2.320544554455446e-05,
      "loss": 8.108,
      "step": 12130
    },
    {
      "epoch": 3.7553166808444822,
      "grad_norm": 0.7632429599761963,
      "learning_rate": 2.318334512022631e-05,
      "loss": 8.0723,
      "step": 12140
    },
    {
      "epoch": 3.7584100224267267,
      "grad_norm": 0.9803694486618042,
      "learning_rate": 2.3161244695898164e-05,
      "loss": 8.1042,
      "step": 12150
    },
    {
      "epoch": 3.761503364008971,
      "grad_norm": 0.6924657225608826,
      "learning_rate": 2.3139144271570015e-05,
      "loss": 8.1127,
      "step": 12160
    },
    {
      "epoch": 3.764596705591215,
      "grad_norm": 0.8107100129127502,
      "learning_rate": 2.3117043847241866e-05,
      "loss": 8.1137,
      "step": 12170
    },
    {
      "epoch": 3.767690047173459,
      "grad_norm": 0.712113618850708,
      "learning_rate": 2.309494342291372e-05,
      "loss": 8.1106,
      "step": 12180
    },
    {
      "epoch": 3.7707833887557034,
      "grad_norm": 0.8182639479637146,
      "learning_rate": 2.307284299858557e-05,
      "loss": 8.0995,
      "step": 12190
    },
    {
      "epoch": 3.7738767303379475,
      "grad_norm": 0.7950517535209656,
      "learning_rate": 2.305074257425743e-05,
      "loss": 8.1092,
      "step": 12200
    },
    {
      "epoch": 3.776970071920192,
      "grad_norm": 0.7742359638214111,
      "learning_rate": 2.302864214992928e-05,
      "loss": 8.1086,
      "step": 12210
    },
    {
      "epoch": 3.780063413502436,
      "grad_norm": 0.8650312423706055,
      "learning_rate": 2.300654172560113e-05,
      "loss": 8.1138,
      "step": 12220
    },
    {
      "epoch": 3.78315675508468,
      "grad_norm": 0.9999988079071045,
      "learning_rate": 2.2984441301272985e-05,
      "loss": 8.1145,
      "step": 12230
    },
    {
      "epoch": 3.7862500966669246,
      "grad_norm": 0.9465503096580505,
      "learning_rate": 2.2962340876944836e-05,
      "loss": 8.108,
      "step": 12240
    },
    {
      "epoch": 3.7893434382491686,
      "grad_norm": 0.8597802519798279,
      "learning_rate": 2.294024045261669e-05,
      "loss": 8.1096,
      "step": 12250
    },
    {
      "epoch": 3.792436779831413,
      "grad_norm": 0.5820393562316895,
      "learning_rate": 2.2918140028288545e-05,
      "loss": 8.0988,
      "step": 12260
    },
    {
      "epoch": 3.795530121413657,
      "grad_norm": 1.0791078805923462,
      "learning_rate": 2.28960396039604e-05,
      "loss": 8.1055,
      "step": 12270
    },
    {
      "epoch": 3.7986234629959013,
      "grad_norm": 1.310927391052246,
      "learning_rate": 2.287393917963225e-05,
      "loss": 8.1045,
      "step": 12280
    },
    {
      "epoch": 3.8017168045781453,
      "grad_norm": 0.9056339859962463,
      "learning_rate": 2.28518387553041e-05,
      "loss": 8.1033,
      "step": 12290
    },
    {
      "epoch": 3.80481014616039,
      "grad_norm": 1.0938446521759033,
      "learning_rate": 2.2829738330975956e-05,
      "loss": 8.101,
      "step": 12300
    },
    {
      "epoch": 3.807903487742634,
      "grad_norm": 0.9294905662536621,
      "learning_rate": 2.2807637906647807e-05,
      "loss": 8.1022,
      "step": 12310
    },
    {
      "epoch": 3.8109968293248784,
      "grad_norm": 0.610062837600708,
      "learning_rate": 2.278553748231966e-05,
      "loss": 8.0918,
      "step": 12320
    },
    {
      "epoch": 3.8140901709071224,
      "grad_norm": 0.5966021418571472,
      "learning_rate": 2.2763437057991515e-05,
      "loss": 8.099,
      "step": 12330
    },
    {
      "epoch": 3.8171835124893665,
      "grad_norm": 0.8067981600761414,
      "learning_rate": 2.274133663366337e-05,
      "loss": 8.1105,
      "step": 12340
    },
    {
      "epoch": 3.820276854071611,
      "grad_norm": 0.7344384789466858,
      "learning_rate": 2.271923620933522e-05,
      "loss": 8.1098,
      "step": 12350
    },
    {
      "epoch": 3.823370195653855,
      "grad_norm": 0.8760346174240112,
      "learning_rate": 2.269713578500707e-05,
      "loss": 8.0858,
      "step": 12360
    },
    {
      "epoch": 3.8264635372360996,
      "grad_norm": 0.7347402572631836,
      "learning_rate": 2.2675035360678926e-05,
      "loss": 8.1139,
      "step": 12370
    },
    {
      "epoch": 3.8295568788183436,
      "grad_norm": 0.7172052264213562,
      "learning_rate": 2.2652934936350777e-05,
      "loss": 8.1113,
      "step": 12380
    },
    {
      "epoch": 3.8326502204005877,
      "grad_norm": 1.0694178342819214,
      "learning_rate": 2.263083451202263e-05,
      "loss": 8.0983,
      "step": 12390
    },
    {
      "epoch": 3.8357435619828317,
      "grad_norm": 0.8179268836975098,
      "learning_rate": 2.2608734087694485e-05,
      "loss": 8.1077,
      "step": 12400
    },
    {
      "epoch": 3.8388369035650762,
      "grad_norm": 0.8312594294548035,
      "learning_rate": 2.258663366336634e-05,
      "loss": 8.0965,
      "step": 12410
    },
    {
      "epoch": 3.8419302451473203,
      "grad_norm": 0.7831716537475586,
      "learning_rate": 2.256453323903819e-05,
      "loss": 8.0982,
      "step": 12420
    },
    {
      "epoch": 3.845023586729565,
      "grad_norm": 0.7636587619781494,
      "learning_rate": 2.254243281471004e-05,
      "loss": 8.1061,
      "step": 12430
    },
    {
      "epoch": 3.848116928311809,
      "grad_norm": 1.0535497665405273,
      "learning_rate": 2.2520332390381896e-05,
      "loss": 8.0984,
      "step": 12440
    },
    {
      "epoch": 3.851210269894053,
      "grad_norm": 0.7738524079322815,
      "learning_rate": 2.2498231966053747e-05,
      "loss": 8.1057,
      "step": 12450
    },
    {
      "epoch": 3.8543036114762974,
      "grad_norm": 0.5989582538604736,
      "learning_rate": 2.2476131541725605e-05,
      "loss": 8.0974,
      "step": 12460
    },
    {
      "epoch": 3.8573969530585415,
      "grad_norm": 0.6576058864593506,
      "learning_rate": 2.2454031117397456e-05,
      "loss": 8.1131,
      "step": 12470
    },
    {
      "epoch": 3.860490294640786,
      "grad_norm": 0.4653606116771698,
      "learning_rate": 2.243193069306931e-05,
      "loss": 8.1033,
      "step": 12480
    },
    {
      "epoch": 3.86358363622303,
      "grad_norm": 0.6948969960212708,
      "learning_rate": 2.240983026874116e-05,
      "loss": 8.1121,
      "step": 12490
    },
    {
      "epoch": 3.866676977805274,
      "grad_norm": 0.6448131203651428,
      "learning_rate": 2.2387729844413012e-05,
      "loss": 8.0964,
      "step": 12500
    },
    {
      "epoch": 3.869770319387518,
      "grad_norm": 0.8628362417221069,
      "learning_rate": 2.2365629420084866e-05,
      "loss": 8.103,
      "step": 12510
    },
    {
      "epoch": 3.8728636609697626,
      "grad_norm": 0.5982326865196228,
      "learning_rate": 2.2343528995756717e-05,
      "loss": 8.0971,
      "step": 12520
    },
    {
      "epoch": 3.8759570025520067,
      "grad_norm": 0.782917857170105,
      "learning_rate": 2.2321428571428575e-05,
      "loss": 8.1147,
      "step": 12530
    },
    {
      "epoch": 3.879050344134251,
      "grad_norm": 0.48247870802879333,
      "learning_rate": 2.2299328147100426e-05,
      "loss": 8.1083,
      "step": 12540
    },
    {
      "epoch": 3.8821436857164953,
      "grad_norm": 0.8832669854164124,
      "learning_rate": 2.227722772277228e-05,
      "loss": 8.0962,
      "step": 12550
    },
    {
      "epoch": 3.8852370272987393,
      "grad_norm": 0.6849269866943359,
      "learning_rate": 2.225512729844413e-05,
      "loss": 8.0999,
      "step": 12560
    },
    {
      "epoch": 3.888330368880984,
      "grad_norm": 0.8913506865501404,
      "learning_rate": 2.2233026874115982e-05,
      "loss": 8.1014,
      "step": 12570
    },
    {
      "epoch": 3.891423710463228,
      "grad_norm": 0.8575028777122498,
      "learning_rate": 2.2210926449787836e-05,
      "loss": 8.1107,
      "step": 12580
    },
    {
      "epoch": 3.8945170520454724,
      "grad_norm": 0.5796117186546326,
      "learning_rate": 2.2188826025459687e-05,
      "loss": 8.0998,
      "step": 12590
    },
    {
      "epoch": 3.8976103936277164,
      "grad_norm": 0.5379560589790344,
      "learning_rate": 2.2166725601131545e-05,
      "loss": 8.1051,
      "step": 12600
    },
    {
      "epoch": 3.9007037352099605,
      "grad_norm": 0.5631392598152161,
      "learning_rate": 2.2144625176803396e-05,
      "loss": 8.0846,
      "step": 12610
    },
    {
      "epoch": 3.9037970767922046,
      "grad_norm": 0.7395532727241516,
      "learning_rate": 2.212252475247525e-05,
      "loss": 8.108,
      "step": 12620
    },
    {
      "epoch": 3.906890418374449,
      "grad_norm": 0.5533865690231323,
      "learning_rate": 2.21004243281471e-05,
      "loss": 8.1031,
      "step": 12630
    },
    {
      "epoch": 3.909983759956693,
      "grad_norm": 0.5846147537231445,
      "learning_rate": 2.2078323903818952e-05,
      "loss": 8.1095,
      "step": 12640
    },
    {
      "epoch": 3.9130771015389376,
      "grad_norm": 0.730043888092041,
      "learning_rate": 2.2056223479490806e-05,
      "loss": 8.1061,
      "step": 12650
    },
    {
      "epoch": 3.9161704431211817,
      "grad_norm": 0.8328079581260681,
      "learning_rate": 2.203412305516266e-05,
      "loss": 8.1053,
      "step": 12660
    },
    {
      "epoch": 3.9192637847034257,
      "grad_norm": 0.6462491154670715,
      "learning_rate": 2.2012022630834515e-05,
      "loss": 8.101,
      "step": 12670
    },
    {
      "epoch": 3.9223571262856702,
      "grad_norm": 0.9627012610435486,
      "learning_rate": 2.1989922206506366e-05,
      "loss": 8.1076,
      "step": 12680
    },
    {
      "epoch": 3.9254504678679143,
      "grad_norm": 1.363150715827942,
      "learning_rate": 2.196782178217822e-05,
      "loss": 8.1047,
      "step": 12690
    },
    {
      "epoch": 3.928543809450159,
      "grad_norm": 1.0151373147964478,
      "learning_rate": 2.194572135785007e-05,
      "loss": 8.103,
      "step": 12700
    },
    {
      "epoch": 3.931637151032403,
      "grad_norm": 0.5576894283294678,
      "learning_rate": 2.1923620933521922e-05,
      "loss": 8.1015,
      "step": 12710
    },
    {
      "epoch": 3.934730492614647,
      "grad_norm": 0.6014869213104248,
      "learning_rate": 2.1901520509193777e-05,
      "loss": 8.1058,
      "step": 12720
    },
    {
      "epoch": 3.937823834196891,
      "grad_norm": 0.710477888584137,
      "learning_rate": 2.187942008486563e-05,
      "loss": 8.1019,
      "step": 12730
    },
    {
      "epoch": 3.9409171757791355,
      "grad_norm": 0.6685858964920044,
      "learning_rate": 2.1857319660537485e-05,
      "loss": 8.1046,
      "step": 12740
    },
    {
      "epoch": 3.9440105173613795,
      "grad_norm": 0.9516357183456421,
      "learning_rate": 2.1835219236209336e-05,
      "loss": 8.1147,
      "step": 12750
    },
    {
      "epoch": 3.947103858943624,
      "grad_norm": 0.9290387630462646,
      "learning_rate": 2.181311881188119e-05,
      "loss": 8.0899,
      "step": 12760
    },
    {
      "epoch": 3.950197200525868,
      "grad_norm": 0.776019811630249,
      "learning_rate": 2.179101838755304e-05,
      "loss": 8.1029,
      "step": 12770
    },
    {
      "epoch": 3.953290542108112,
      "grad_norm": 0.6510490775108337,
      "learning_rate": 2.1768917963224892e-05,
      "loss": 8.0982,
      "step": 12780
    },
    {
      "epoch": 3.9563838836903567,
      "grad_norm": 1.0956438779830933,
      "learning_rate": 2.1746817538896747e-05,
      "loss": 8.0889,
      "step": 12790
    },
    {
      "epoch": 3.9594772252726007,
      "grad_norm": 0.848426342010498,
      "learning_rate": 2.17247171145686e-05,
      "loss": 8.1117,
      "step": 12800
    },
    {
      "epoch": 3.962570566854845,
      "grad_norm": 0.6948767304420471,
      "learning_rate": 2.1702616690240455e-05,
      "loss": 8.1012,
      "step": 12810
    },
    {
      "epoch": 3.9656639084370893,
      "grad_norm": 0.7312864661216736,
      "learning_rate": 2.1680516265912306e-05,
      "loss": 8.1157,
      "step": 12820
    },
    {
      "epoch": 3.9687572500193333,
      "grad_norm": 0.72239750623703,
      "learning_rate": 2.165841584158416e-05,
      "loss": 8.1124,
      "step": 12830
    },
    {
      "epoch": 3.9718505916015774,
      "grad_norm": 0.8603538870811462,
      "learning_rate": 2.163631541725601e-05,
      "loss": 8.103,
      "step": 12840
    },
    {
      "epoch": 3.974943933183822,
      "grad_norm": 0.572148859500885,
      "learning_rate": 2.1614214992927863e-05,
      "loss": 8.0941,
      "step": 12850
    },
    {
      "epoch": 3.978037274766066,
      "grad_norm": 0.8024046421051025,
      "learning_rate": 2.159211456859972e-05,
      "loss": 8.1006,
      "step": 12860
    },
    {
      "epoch": 3.9811306163483104,
      "grad_norm": 0.8555017113685608,
      "learning_rate": 2.157001414427157e-05,
      "loss": 8.107,
      "step": 12870
    },
    {
      "epoch": 3.9842239579305545,
      "grad_norm": 0.9062358736991882,
      "learning_rate": 2.1547913719943426e-05,
      "loss": 8.0931,
      "step": 12880
    },
    {
      "epoch": 3.9873172995127986,
      "grad_norm": 0.7969098091125488,
      "learning_rate": 2.1525813295615277e-05,
      "loss": 8.1041,
      "step": 12890
    },
    {
      "epoch": 3.990410641095043,
      "grad_norm": 0.6713497638702393,
      "learning_rate": 2.150371287128713e-05,
      "loss": 8.1218,
      "step": 12900
    },
    {
      "epoch": 3.993503982677287,
      "grad_norm": 0.9727796912193298,
      "learning_rate": 2.1481612446958982e-05,
      "loss": 8.1045,
      "step": 12910
    },
    {
      "epoch": 3.9965973242595316,
      "grad_norm": 0.8432744145393372,
      "learning_rate": 2.1459512022630833e-05,
      "loss": 8.1154,
      "step": 12920
    },
    {
      "epoch": 3.9996906658417757,
      "grad_norm": 0.6530187726020813,
      "learning_rate": 2.143741159830269e-05,
      "loss": 8.0942,
      "step": 12930
    },
    {
      "epoch": 4.00278400742402,
      "grad_norm": 0.6552506685256958,
      "learning_rate": 2.141531117397454e-05,
      "loss": 8.1108,
      "step": 12940
    },
    {
      "epoch": 4.005877349006264,
      "grad_norm": 1.1072865724563599,
      "learning_rate": 2.1393210749646396e-05,
      "loss": 8.112,
      "step": 12950
    },
    {
      "epoch": 4.008970690588508,
      "grad_norm": 0.6281117796897888,
      "learning_rate": 2.1371110325318247e-05,
      "loss": 8.1022,
      "step": 12960
    },
    {
      "epoch": 4.012064032170753,
      "grad_norm": 0.6871021389961243,
      "learning_rate": 2.13490099009901e-05,
      "loss": 8.0979,
      "step": 12970
    },
    {
      "epoch": 4.015157373752997,
      "grad_norm": 0.8364048600196838,
      "learning_rate": 2.1326909476661952e-05,
      "loss": 8.0817,
      "step": 12980
    },
    {
      "epoch": 4.018250715335241,
      "grad_norm": 0.8658652901649475,
      "learning_rate": 2.1304809052333803e-05,
      "loss": 8.1141,
      "step": 12990
    },
    {
      "epoch": 4.021344056917485,
      "grad_norm": 0.6728661060333252,
      "learning_rate": 2.128270862800566e-05,
      "loss": 8.1153,
      "step": 13000
    },
    {
      "epoch": 4.024437398499729,
      "grad_norm": 0.926964282989502,
      "learning_rate": 2.126060820367751e-05,
      "loss": 8.1018,
      "step": 13010
    },
    {
      "epoch": 4.027530740081974,
      "grad_norm": 0.622810959815979,
      "learning_rate": 2.1238507779349366e-05,
      "loss": 8.0973,
      "step": 13020
    },
    {
      "epoch": 4.030624081664218,
      "grad_norm": 0.768669068813324,
      "learning_rate": 2.1216407355021217e-05,
      "loss": 8.0953,
      "step": 13030
    },
    {
      "epoch": 4.033717423246462,
      "grad_norm": 0.7948973774909973,
      "learning_rate": 2.119430693069307e-05,
      "loss": 8.0711,
      "step": 13040
    },
    {
      "epoch": 4.036810764828706,
      "grad_norm": 0.7668564915657043,
      "learning_rate": 2.1172206506364922e-05,
      "loss": 8.1012,
      "step": 13050
    },
    {
      "epoch": 4.03990410641095,
      "grad_norm": 0.5751906037330627,
      "learning_rate": 2.1150106082036776e-05,
      "loss": 8.1064,
      "step": 13060
    },
    {
      "epoch": 4.042997447993194,
      "grad_norm": 0.6358420848846436,
      "learning_rate": 2.112800565770863e-05,
      "loss": 8.115,
      "step": 13070
    },
    {
      "epoch": 4.046090789575439,
      "grad_norm": 0.6196653246879578,
      "learning_rate": 2.1105905233380482e-05,
      "loss": 8.1161,
      "step": 13080
    },
    {
      "epoch": 4.049184131157683,
      "grad_norm": 0.8778455853462219,
      "learning_rate": 2.1083804809052336e-05,
      "loss": 8.0976,
      "step": 13090
    },
    {
      "epoch": 4.052277472739927,
      "grad_norm": 0.6614976525306702,
      "learning_rate": 2.1061704384724187e-05,
      "loss": 8.0907,
      "step": 13100
    },
    {
      "epoch": 4.055370814322171,
      "grad_norm": 0.8588985800743103,
      "learning_rate": 2.103960396039604e-05,
      "loss": 8.0952,
      "step": 13110
    },
    {
      "epoch": 4.0584641559044154,
      "grad_norm": 0.9557288885116577,
      "learning_rate": 2.1017503536067892e-05,
      "loss": 8.0975,
      "step": 13120
    },
    {
      "epoch": 4.06155749748666,
      "grad_norm": 0.7582816481590271,
      "learning_rate": 2.0995403111739747e-05,
      "loss": 8.105,
      "step": 13130
    },
    {
      "epoch": 4.0646508390689045,
      "grad_norm": 0.7118688225746155,
      "learning_rate": 2.09733026874116e-05,
      "loss": 8.1099,
      "step": 13140
    },
    {
      "epoch": 4.0677441806511485,
      "grad_norm": 0.796970009803772,
      "learning_rate": 2.0951202263083452e-05,
      "loss": 8.1011,
      "step": 13150
    },
    {
      "epoch": 4.070837522233393,
      "grad_norm": 1.0575810670852661,
      "learning_rate": 2.0929101838755306e-05,
      "loss": 8.1063,
      "step": 13160
    },
    {
      "epoch": 4.073930863815637,
      "grad_norm": 0.7603093981742859,
      "learning_rate": 2.0907001414427157e-05,
      "loss": 8.1051,
      "step": 13170
    },
    {
      "epoch": 4.077024205397881,
      "grad_norm": 0.8670388460159302,
      "learning_rate": 2.088490099009901e-05,
      "loss": 8.1051,
      "step": 13180
    },
    {
      "epoch": 4.080117546980126,
      "grad_norm": 0.8645461201667786,
      "learning_rate": 2.0862800565770862e-05,
      "loss": 8.1038,
      "step": 13190
    },
    {
      "epoch": 4.08321088856237,
      "grad_norm": 0.7118605971336365,
      "learning_rate": 2.0840700141442717e-05,
      "loss": 8.1121,
      "step": 13200
    },
    {
      "epoch": 4.086304230144614,
      "grad_norm": 0.6737720370292664,
      "learning_rate": 2.081859971711457e-05,
      "loss": 8.0962,
      "step": 13210
    },
    {
      "epoch": 4.089397571726858,
      "grad_norm": 0.9089017510414124,
      "learning_rate": 2.0796499292786422e-05,
      "loss": 8.1069,
      "step": 13220
    },
    {
      "epoch": 4.092490913309102,
      "grad_norm": 0.6076618432998657,
      "learning_rate": 2.0774398868458276e-05,
      "loss": 8.1106,
      "step": 13230
    },
    {
      "epoch": 4.095584254891347,
      "grad_norm": 0.6015578508377075,
      "learning_rate": 2.0752298444130127e-05,
      "loss": 8.0969,
      "step": 13240
    },
    {
      "epoch": 4.098677596473591,
      "grad_norm": 0.7112676501274109,
      "learning_rate": 2.073019801980198e-05,
      "loss": 8.0983,
      "step": 13250
    },
    {
      "epoch": 4.101770938055835,
      "grad_norm": 0.41620782017707825,
      "learning_rate": 2.0708097595473836e-05,
      "loss": 8.0996,
      "step": 13260
    },
    {
      "epoch": 4.104864279638079,
      "grad_norm": 0.7474857568740845,
      "learning_rate": 2.0685997171145687e-05,
      "loss": 8.1096,
      "step": 13270
    },
    {
      "epoch": 4.107957621220323,
      "grad_norm": 0.7982586622238159,
      "learning_rate": 2.066389674681754e-05,
      "loss": 8.1204,
      "step": 13280
    },
    {
      "epoch": 4.111050962802567,
      "grad_norm": 0.7809885144233704,
      "learning_rate": 2.0641796322489392e-05,
      "loss": 8.0808,
      "step": 13290
    },
    {
      "epoch": 4.114144304384812,
      "grad_norm": 0.5811080932617188,
      "learning_rate": 2.0619695898161247e-05,
      "loss": 8.0839,
      "step": 13300
    },
    {
      "epoch": 4.117237645967056,
      "grad_norm": 0.6013928651809692,
      "learning_rate": 2.0597595473833097e-05,
      "loss": 8.0959,
      "step": 13310
    },
    {
      "epoch": 4.1203309875493,
      "grad_norm": 1.1951632499694824,
      "learning_rate": 2.0575495049504952e-05,
      "loss": 8.1005,
      "step": 13320
    },
    {
      "epoch": 4.123424329131544,
      "grad_norm": 0.8136542439460754,
      "learning_rate": 2.0553394625176806e-05,
      "loss": 8.0951,
      "step": 13330
    },
    {
      "epoch": 4.126517670713788,
      "grad_norm": 0.7351477742195129,
      "learning_rate": 2.0531294200848657e-05,
      "loss": 8.1115,
      "step": 13340
    },
    {
      "epoch": 4.129611012296033,
      "grad_norm": 0.8208072185516357,
      "learning_rate": 2.050919377652051e-05,
      "loss": 8.1263,
      "step": 13350
    },
    {
      "epoch": 4.132704353878277,
      "grad_norm": 0.9531247019767761,
      "learning_rate": 2.0487093352192362e-05,
      "loss": 8.1147,
      "step": 13360
    },
    {
      "epoch": 4.135797695460521,
      "grad_norm": 0.7028853893280029,
      "learning_rate": 2.0464992927864217e-05,
      "loss": 8.1093,
      "step": 13370
    },
    {
      "epoch": 4.138891037042765,
      "grad_norm": 0.7915287017822266,
      "learning_rate": 2.0442892503536068e-05,
      "loss": 8.1,
      "step": 13380
    },
    {
      "epoch": 4.1419843786250095,
      "grad_norm": 0.6787287592887878,
      "learning_rate": 2.0420792079207922e-05,
      "loss": 8.091,
      "step": 13390
    },
    {
      "epoch": 4.1450777202072535,
      "grad_norm": 0.6043727993965149,
      "learning_rate": 2.0398691654879776e-05,
      "loss": 8.0871,
      "step": 13400
    },
    {
      "epoch": 4.1481710617894985,
      "grad_norm": 0.7483276128768921,
      "learning_rate": 2.0376591230551627e-05,
      "loss": 8.1024,
      "step": 13410
    },
    {
      "epoch": 4.1512644033717425,
      "grad_norm": 0.9061620235443115,
      "learning_rate": 2.035449080622348e-05,
      "loss": 8.0953,
      "step": 13420
    },
    {
      "epoch": 4.154357744953987,
      "grad_norm": 0.41004306077957153,
      "learning_rate": 2.0332390381895333e-05,
      "loss": 8.1003,
      "step": 13430
    },
    {
      "epoch": 4.157451086536231,
      "grad_norm": 0.6789014935493469,
      "learning_rate": 2.0310289957567187e-05,
      "loss": 8.106,
      "step": 13440
    },
    {
      "epoch": 4.160544428118475,
      "grad_norm": 0.6682287454605103,
      "learning_rate": 2.0288189533239038e-05,
      "loss": 8.0813,
      "step": 13450
    },
    {
      "epoch": 4.163637769700719,
      "grad_norm": 0.6092725396156311,
      "learning_rate": 2.0266089108910892e-05,
      "loss": 8.1161,
      "step": 13460
    },
    {
      "epoch": 4.166731111282964,
      "grad_norm": 0.8492941856384277,
      "learning_rate": 2.0243988684582746e-05,
      "loss": 8.1057,
      "step": 13470
    },
    {
      "epoch": 4.169824452865208,
      "grad_norm": 0.9979976415634155,
      "learning_rate": 2.0221888260254597e-05,
      "loss": 8.1076,
      "step": 13480
    },
    {
      "epoch": 4.172917794447452,
      "grad_norm": 0.8617967963218689,
      "learning_rate": 2.0199787835926452e-05,
      "loss": 8.1036,
      "step": 13490
    },
    {
      "epoch": 4.176011136029696,
      "grad_norm": 0.48488619923591614,
      "learning_rate": 2.0177687411598303e-05,
      "loss": 8.1077,
      "step": 13500
    },
    {
      "epoch": 4.17910447761194,
      "grad_norm": 0.7212694883346558,
      "learning_rate": 2.0155586987270157e-05,
      "loss": 8.1035,
      "step": 13510
    },
    {
      "epoch": 4.182197819194185,
      "grad_norm": 0.8792160749435425,
      "learning_rate": 2.0133486562942008e-05,
      "loss": 8.1064,
      "step": 13520
    },
    {
      "epoch": 4.185291160776429,
      "grad_norm": 0.6040487885475159,
      "learning_rate": 2.0111386138613862e-05,
      "loss": 8.0826,
      "step": 13530
    },
    {
      "epoch": 4.188384502358673,
      "grad_norm": 0.7954559922218323,
      "learning_rate": 2.0089285714285717e-05,
      "loss": 8.0818,
      "step": 13540
    },
    {
      "epoch": 4.191477843940917,
      "grad_norm": 0.7794458270072937,
      "learning_rate": 2.0067185289957568e-05,
      "loss": 8.1069,
      "step": 13550
    },
    {
      "epoch": 4.194571185523161,
      "grad_norm": 0.9344253540039062,
      "learning_rate": 2.0045084865629422e-05,
      "loss": 8.1082,
      "step": 13560
    },
    {
      "epoch": 4.197664527105406,
      "grad_norm": 0.9014893174171448,
      "learning_rate": 2.0022984441301273e-05,
      "loss": 8.101,
      "step": 13570
    },
    {
      "epoch": 4.20075786868765,
      "grad_norm": 1.0360552072525024,
      "learning_rate": 2.0000884016973127e-05,
      "loss": 8.0905,
      "step": 13580
    },
    {
      "epoch": 4.203851210269894,
      "grad_norm": 0.7112959623336792,
      "learning_rate": 1.9978783592644978e-05,
      "loss": 8.1059,
      "step": 13590
    },
    {
      "epoch": 4.206944551852138,
      "grad_norm": 1.0075664520263672,
      "learning_rate": 1.9956683168316832e-05,
      "loss": 8.0935,
      "step": 13600
    },
    {
      "epoch": 4.210037893434382,
      "grad_norm": 0.6670038104057312,
      "learning_rate": 1.9934582743988687e-05,
      "loss": 8.0875,
      "step": 13610
    },
    {
      "epoch": 4.213131235016626,
      "grad_norm": 0.6542025208473206,
      "learning_rate": 1.9912482319660538e-05,
      "loss": 8.0918,
      "step": 13620
    },
    {
      "epoch": 4.216224576598871,
      "grad_norm": 0.9150432348251343,
      "learning_rate": 1.9890381895332392e-05,
      "loss": 8.1118,
      "step": 13630
    },
    {
      "epoch": 4.219317918181115,
      "grad_norm": 0.5868713855743408,
      "learning_rate": 1.9868281471004243e-05,
      "loss": 8.0992,
      "step": 13640
    },
    {
      "epoch": 4.222411259763359,
      "grad_norm": 0.6024312973022461,
      "learning_rate": 1.9846181046676097e-05,
      "loss": 8.0766,
      "step": 13650
    },
    {
      "epoch": 4.2255046013456035,
      "grad_norm": 0.7607202529907227,
      "learning_rate": 1.982408062234795e-05,
      "loss": 8.091,
      "step": 13660
    },
    {
      "epoch": 4.2285979429278475,
      "grad_norm": 0.7876748442649841,
      "learning_rate": 1.9801980198019803e-05,
      "loss": 8.1043,
      "step": 13670
    },
    {
      "epoch": 4.231691284510092,
      "grad_norm": 0.6684665679931641,
      "learning_rate": 1.9779879773691657e-05,
      "loss": 8.0962,
      "step": 13680
    },
    {
      "epoch": 4.2347846260923365,
      "grad_norm": 0.7029377222061157,
      "learning_rate": 1.9757779349363508e-05,
      "loss": 8.0991,
      "step": 13690
    },
    {
      "epoch": 4.237877967674581,
      "grad_norm": 0.8959271311759949,
      "learning_rate": 1.9735678925035362e-05,
      "loss": 8.0992,
      "step": 13700
    },
    {
      "epoch": 4.240971309256825,
      "grad_norm": 0.7221759557723999,
      "learning_rate": 1.9713578500707213e-05,
      "loss": 8.0919,
      "step": 13710
    },
    {
      "epoch": 4.244064650839069,
      "grad_norm": 0.5133864879608154,
      "learning_rate": 1.9691478076379068e-05,
      "loss": 8.0899,
      "step": 13720
    },
    {
      "epoch": 4.247157992421313,
      "grad_norm": 0.75240558385849,
      "learning_rate": 1.9669377652050922e-05,
      "loss": 8.0914,
      "step": 13730
    },
    {
      "epoch": 4.250251334003558,
      "grad_norm": 0.6640309691429138,
      "learning_rate": 1.9647277227722773e-05,
      "loss": 8.1098,
      "step": 13740
    },
    {
      "epoch": 4.253344675585802,
      "grad_norm": 0.5197213888168335,
      "learning_rate": 1.9625176803394627e-05,
      "loss": 8.1029,
      "step": 13750
    },
    {
      "epoch": 4.256438017168046,
      "grad_norm": 0.5925953984260559,
      "learning_rate": 1.9603076379066478e-05,
      "loss": 8.1123,
      "step": 13760
    },
    {
      "epoch": 4.25953135875029,
      "grad_norm": 0.6526131629943848,
      "learning_rate": 1.9580975954738332e-05,
      "loss": 8.1003,
      "step": 13770
    },
    {
      "epoch": 4.262624700332534,
      "grad_norm": 0.7930484414100647,
      "learning_rate": 1.9558875530410183e-05,
      "loss": 8.1012,
      "step": 13780
    },
    {
      "epoch": 4.265718041914779,
      "grad_norm": 0.5415624976158142,
      "learning_rate": 1.9536775106082038e-05,
      "loss": 8.1035,
      "step": 13790
    },
    {
      "epoch": 4.268811383497023,
      "grad_norm": 0.5607356429100037,
      "learning_rate": 1.9514674681753892e-05,
      "loss": 8.1107,
      "step": 13800
    },
    {
      "epoch": 4.271904725079267,
      "grad_norm": 0.5800464153289795,
      "learning_rate": 1.9492574257425743e-05,
      "loss": 8.0957,
      "step": 13810
    },
    {
      "epoch": 4.274998066661511,
      "grad_norm": 0.8558887839317322,
      "learning_rate": 1.9470473833097597e-05,
      "loss": 8.1096,
      "step": 13820
    },
    {
      "epoch": 4.278091408243755,
      "grad_norm": 0.6938257217407227,
      "learning_rate": 1.9448373408769448e-05,
      "loss": 8.0944,
      "step": 13830
    },
    {
      "epoch": 4.281184749825999,
      "grad_norm": 0.6810865998268127,
      "learning_rate": 1.9426272984441303e-05,
      "loss": 8.1009,
      "step": 13840
    },
    {
      "epoch": 4.284278091408244,
      "grad_norm": 0.5137801170349121,
      "learning_rate": 1.9404172560113153e-05,
      "loss": 8.0916,
      "step": 13850
    },
    {
      "epoch": 4.287371432990488,
      "grad_norm": 0.6770243644714355,
      "learning_rate": 1.9382072135785008e-05,
      "loss": 8.0889,
      "step": 13860
    },
    {
      "epoch": 4.290464774572732,
      "grad_norm": 0.5638735294342041,
      "learning_rate": 1.9359971711456862e-05,
      "loss": 8.1122,
      "step": 13870
    },
    {
      "epoch": 4.293558116154976,
      "grad_norm": 1.2121888399124146,
      "learning_rate": 1.9337871287128713e-05,
      "loss": 8.1024,
      "step": 13880
    },
    {
      "epoch": 4.29665145773722,
      "grad_norm": 0.6478176116943359,
      "learning_rate": 1.9315770862800567e-05,
      "loss": 8.1132,
      "step": 13890
    },
    {
      "epoch": 4.299744799319464,
      "grad_norm": 0.5437384843826294,
      "learning_rate": 1.929367043847242e-05,
      "loss": 8.0935,
      "step": 13900
    },
    {
      "epoch": 4.302838140901709,
      "grad_norm": 0.6136879324913025,
      "learning_rate": 1.9271570014144273e-05,
      "loss": 8.0913,
      "step": 13910
    },
    {
      "epoch": 4.305931482483953,
      "grad_norm": 0.9032025933265686,
      "learning_rate": 1.9249469589816124e-05,
      "loss": 8.1208,
      "step": 13920
    },
    {
      "epoch": 4.3090248240661975,
      "grad_norm": 0.5888896584510803,
      "learning_rate": 1.9227369165487978e-05,
      "loss": 8.1106,
      "step": 13930
    },
    {
      "epoch": 4.3121181656484415,
      "grad_norm": 0.9585228562355042,
      "learning_rate": 1.9205268741159832e-05,
      "loss": 8.1008,
      "step": 13940
    },
    {
      "epoch": 4.315211507230686,
      "grad_norm": 0.598686158657074,
      "learning_rate": 1.9183168316831683e-05,
      "loss": 8.1014,
      "step": 13950
    },
    {
      "epoch": 4.3183048488129305,
      "grad_norm": 0.8854605555534363,
      "learning_rate": 1.9161067892503538e-05,
      "loss": 8.0911,
      "step": 13960
    },
    {
      "epoch": 4.321398190395175,
      "grad_norm": 0.732104480266571,
      "learning_rate": 1.913896746817539e-05,
      "loss": 8.0826,
      "step": 13970
    },
    {
      "epoch": 4.324491531977419,
      "grad_norm": 0.9150115847587585,
      "learning_rate": 1.9116867043847243e-05,
      "loss": 8.1121,
      "step": 13980
    },
    {
      "epoch": 4.327584873559663,
      "grad_norm": 0.9787803888320923,
      "learning_rate": 1.9094766619519094e-05,
      "loss": 8.1125,
      "step": 13990
    },
    {
      "epoch": 4.330678215141907,
      "grad_norm": 0.7636820077896118,
      "learning_rate": 1.9072666195190948e-05,
      "loss": 8.1148,
      "step": 14000
    },
    {
      "epoch": 4.333771556724152,
      "grad_norm": 0.6856809854507446,
      "learning_rate": 1.9050565770862802e-05,
      "loss": 8.0945,
      "step": 14010
    },
    {
      "epoch": 4.336864898306396,
      "grad_norm": 0.7859597206115723,
      "learning_rate": 1.9028465346534653e-05,
      "loss": 8.102,
      "step": 14020
    },
    {
      "epoch": 4.33995823988864,
      "grad_norm": 0.7528899312019348,
      "learning_rate": 1.9006364922206508e-05,
      "loss": 8.1054,
      "step": 14030
    },
    {
      "epoch": 4.343051581470884,
      "grad_norm": 0.8241452574729919,
      "learning_rate": 1.898426449787836e-05,
      "loss": 8.112,
      "step": 14040
    },
    {
      "epoch": 4.346144923053128,
      "grad_norm": 0.6564708352088928,
      "learning_rate": 1.8962164073550213e-05,
      "loss": 8.1149,
      "step": 14050
    },
    {
      "epoch": 4.349238264635372,
      "grad_norm": 0.783890426158905,
      "learning_rate": 1.8940063649222067e-05,
      "loss": 8.0811,
      "step": 14060
    },
    {
      "epoch": 4.352331606217617,
      "grad_norm": 0.7555451989173889,
      "learning_rate": 1.891796322489392e-05,
      "loss": 8.1188,
      "step": 14070
    },
    {
      "epoch": 4.355424947799861,
      "grad_norm": 0.7034308910369873,
      "learning_rate": 1.8895862800565773e-05,
      "loss": 8.1026,
      "step": 14080
    },
    {
      "epoch": 4.358518289382105,
      "grad_norm": 0.4048224687576294,
      "learning_rate": 1.8873762376237624e-05,
      "loss": 8.1084,
      "step": 14090
    },
    {
      "epoch": 4.361611630964349,
      "grad_norm": 0.6815595626831055,
      "learning_rate": 1.8851661951909478e-05,
      "loss": 8.109,
      "step": 14100
    },
    {
      "epoch": 4.364704972546593,
      "grad_norm": 0.6427213549613953,
      "learning_rate": 1.882956152758133e-05,
      "loss": 8.1011,
      "step": 14110
    },
    {
      "epoch": 4.367798314128837,
      "grad_norm": 0.6269468069076538,
      "learning_rate": 1.8807461103253183e-05,
      "loss": 8.1007,
      "step": 14120
    },
    {
      "epoch": 4.370891655711082,
      "grad_norm": 0.6853500008583069,
      "learning_rate": 1.8785360678925038e-05,
      "loss": 8.1005,
      "step": 14130
    },
    {
      "epoch": 4.373984997293326,
      "grad_norm": 0.818636417388916,
      "learning_rate": 1.876326025459689e-05,
      "loss": 8.1109,
      "step": 14140
    },
    {
      "epoch": 4.37707833887557,
      "grad_norm": 0.6839223504066467,
      "learning_rate": 1.8741159830268743e-05,
      "loss": 8.1077,
      "step": 14150
    },
    {
      "epoch": 4.380171680457814,
      "grad_norm": 0.8411465883255005,
      "learning_rate": 1.8719059405940594e-05,
      "loss": 8.0967,
      "step": 14160
    },
    {
      "epoch": 4.383265022040058,
      "grad_norm": 0.7295413613319397,
      "learning_rate": 1.8696958981612448e-05,
      "loss": 8.1066,
      "step": 14170
    },
    {
      "epoch": 4.386358363622303,
      "grad_norm": 1.0308502912521362,
      "learning_rate": 1.86748585572843e-05,
      "loss": 8.1032,
      "step": 14180
    },
    {
      "epoch": 4.389451705204547,
      "grad_norm": 0.7704592347145081,
      "learning_rate": 1.8652758132956153e-05,
      "loss": 8.0946,
      "step": 14190
    },
    {
      "epoch": 4.3925450467867915,
      "grad_norm": 0.5296130776405334,
      "learning_rate": 1.8630657708628008e-05,
      "loss": 8.0996,
      "step": 14200
    },
    {
      "epoch": 4.3956383883690355,
      "grad_norm": 0.7627878189086914,
      "learning_rate": 1.860855728429986e-05,
      "loss": 8.0933,
      "step": 14210
    },
    {
      "epoch": 4.39873172995128,
      "grad_norm": 0.8411035537719727,
      "learning_rate": 1.8586456859971713e-05,
      "loss": 8.0967,
      "step": 14220
    },
    {
      "epoch": 4.4018250715335245,
      "grad_norm": 0.8883367776870728,
      "learning_rate": 1.8564356435643564e-05,
      "loss": 8.1121,
      "step": 14230
    },
    {
      "epoch": 4.404918413115769,
      "grad_norm": 0.8624232411384583,
      "learning_rate": 1.8544466053748234e-05,
      "loss": 8.1039,
      "step": 14240
    },
    {
      "epoch": 4.408011754698013,
      "grad_norm": 0.9319435358047485,
      "learning_rate": 1.8522365629420085e-05,
      "loss": 8.1129,
      "step": 14250
    },
    {
      "epoch": 4.411105096280257,
      "grad_norm": 0.7143931984901428,
      "learning_rate": 1.850026520509194e-05,
      "loss": 8.1097,
      "step": 14260
    },
    {
      "epoch": 4.414198437862501,
      "grad_norm": 0.731287956237793,
      "learning_rate": 1.847816478076379e-05,
      "loss": 8.1115,
      "step": 14270
    },
    {
      "epoch": 4.417291779444745,
      "grad_norm": 0.7175251841545105,
      "learning_rate": 1.8456064356435644e-05,
      "loss": 8.0996,
      "step": 14280
    },
    {
      "epoch": 4.42038512102699,
      "grad_norm": 0.5983700752258301,
      "learning_rate": 1.84339639321075e-05,
      "loss": 8.0953,
      "step": 14290
    },
    {
      "epoch": 4.423478462609234,
      "grad_norm": 0.75003582239151,
      "learning_rate": 1.841186350777935e-05,
      "loss": 8.0981,
      "step": 14300
    },
    {
      "epoch": 4.426571804191478,
      "grad_norm": 0.7326407432556152,
      "learning_rate": 1.8389763083451204e-05,
      "loss": 8.0992,
      "step": 14310
    },
    {
      "epoch": 4.429665145773722,
      "grad_norm": 0.8120198845863342,
      "learning_rate": 1.8367662659123055e-05,
      "loss": 8.0872,
      "step": 14320
    },
    {
      "epoch": 4.432758487355966,
      "grad_norm": 0.8037434220314026,
      "learning_rate": 1.834556223479491e-05,
      "loss": 8.1073,
      "step": 14330
    },
    {
      "epoch": 4.43585182893821,
      "grad_norm": 0.7470185160636902,
      "learning_rate": 1.832346181046676e-05,
      "loss": 8.0926,
      "step": 14340
    },
    {
      "epoch": 4.438945170520455,
      "grad_norm": 0.7705841660499573,
      "learning_rate": 1.8301361386138615e-05,
      "loss": 8.1013,
      "step": 14350
    },
    {
      "epoch": 4.442038512102699,
      "grad_norm": 0.9622169733047485,
      "learning_rate": 1.827926096181047e-05,
      "loss": 8.0958,
      "step": 14360
    },
    {
      "epoch": 4.445131853684943,
      "grad_norm": 0.7929194569587708,
      "learning_rate": 1.825716053748232e-05,
      "loss": 8.1084,
      "step": 14370
    },
    {
      "epoch": 4.448225195267187,
      "grad_norm": 0.5823287963867188,
      "learning_rate": 1.8235060113154174e-05,
      "loss": 8.1132,
      "step": 14380
    },
    {
      "epoch": 4.451318536849431,
      "grad_norm": 0.8016645312309265,
      "learning_rate": 1.8212959688826025e-05,
      "loss": 8.1104,
      "step": 14390
    },
    {
      "epoch": 4.454411878431676,
      "grad_norm": 0.8509910106658936,
      "learning_rate": 1.819085926449788e-05,
      "loss": 8.1026,
      "step": 14400
    },
    {
      "epoch": 4.45750522001392,
      "grad_norm": 0.8567742705345154,
      "learning_rate": 1.8168758840169734e-05,
      "loss": 8.1043,
      "step": 14410
    },
    {
      "epoch": 4.460598561596164,
      "grad_norm": 1.0654091835021973,
      "learning_rate": 1.8146658415841585e-05,
      "loss": 8.1038,
      "step": 14420
    },
    {
      "epoch": 4.463691903178408,
      "grad_norm": 0.9960813522338867,
      "learning_rate": 1.812455799151344e-05,
      "loss": 8.0852,
      "step": 14430
    },
    {
      "epoch": 4.466785244760652,
      "grad_norm": 1.0156546831130981,
      "learning_rate": 1.810245756718529e-05,
      "loss": 8.1123,
      "step": 14440
    },
    {
      "epoch": 4.469878586342897,
      "grad_norm": 0.7577598690986633,
      "learning_rate": 1.8080357142857144e-05,
      "loss": 8.0943,
      "step": 14450
    },
    {
      "epoch": 4.472971927925141,
      "grad_norm": 0.6327225565910339,
      "learning_rate": 1.8058256718528995e-05,
      "loss": 8.1069,
      "step": 14460
    },
    {
      "epoch": 4.4760652695073855,
      "grad_norm": 0.6046829223632812,
      "learning_rate": 1.803615629420085e-05,
      "loss": 8.0979,
      "step": 14470
    },
    {
      "epoch": 4.4791586110896295,
      "grad_norm": 0.6674280762672424,
      "learning_rate": 1.8014055869872704e-05,
      "loss": 8.0995,
      "step": 14480
    },
    {
      "epoch": 4.482251952671874,
      "grad_norm": 0.8298107981681824,
      "learning_rate": 1.7991955445544555e-05,
      "loss": 8.1121,
      "step": 14490
    },
    {
      "epoch": 4.485345294254118,
      "grad_norm": 0.9423326849937439,
      "learning_rate": 1.796985502121641e-05,
      "loss": 8.0955,
      "step": 14500
    },
    {
      "epoch": 4.488438635836363,
      "grad_norm": 0.786767840385437,
      "learning_rate": 1.794775459688826e-05,
      "loss": 8.1124,
      "step": 14510
    },
    {
      "epoch": 4.491531977418607,
      "grad_norm": 0.7823501825332642,
      "learning_rate": 1.7925654172560114e-05,
      "loss": 8.0852,
      "step": 14520
    },
    {
      "epoch": 4.494625319000851,
      "grad_norm": 0.7172659039497375,
      "learning_rate": 1.7903553748231965e-05,
      "loss": 8.0922,
      "step": 14530
    },
    {
      "epoch": 4.497718660583095,
      "grad_norm": 0.7711994051933289,
      "learning_rate": 1.788145332390382e-05,
      "loss": 8.1093,
      "step": 14540
    },
    {
      "epoch": 4.500812002165339,
      "grad_norm": 0.960685670375824,
      "learning_rate": 1.7859352899575674e-05,
      "loss": 8.0915,
      "step": 14550
    },
    {
      "epoch": 4.503905343747583,
      "grad_norm": 1.0158456563949585,
      "learning_rate": 1.7837252475247525e-05,
      "loss": 8.1046,
      "step": 14560
    },
    {
      "epoch": 4.506998685329828,
      "grad_norm": 0.7915118932723999,
      "learning_rate": 1.781515205091938e-05,
      "loss": 8.0909,
      "step": 14570
    },
    {
      "epoch": 4.510092026912072,
      "grad_norm": 0.7325674891471863,
      "learning_rate": 1.779305162659123e-05,
      "loss": 8.1038,
      "step": 14580
    },
    {
      "epoch": 4.513185368494316,
      "grad_norm": 0.8725468516349792,
      "learning_rate": 1.7770951202263085e-05,
      "loss": 8.1077,
      "step": 14590
    },
    {
      "epoch": 4.51627871007656,
      "grad_norm": 0.7155962586402893,
      "learning_rate": 1.7748850777934936e-05,
      "loss": 8.099,
      "step": 14600
    },
    {
      "epoch": 4.519372051658804,
      "grad_norm": 0.8151288032531738,
      "learning_rate": 1.772675035360679e-05,
      "loss": 8.0922,
      "step": 14610
    },
    {
      "epoch": 4.522465393241049,
      "grad_norm": 0.7081550359725952,
      "learning_rate": 1.7704649929278644e-05,
      "loss": 8.0987,
      "step": 14620
    },
    {
      "epoch": 4.525558734823293,
      "grad_norm": 0.7026395201683044,
      "learning_rate": 1.7682549504950495e-05,
      "loss": 8.105,
      "step": 14630
    },
    {
      "epoch": 4.528652076405537,
      "grad_norm": 0.5331389307975769,
      "learning_rate": 1.766044908062235e-05,
      "loss": 8.0976,
      "step": 14640
    },
    {
      "epoch": 4.531745417987781,
      "grad_norm": 0.5798876285552979,
      "learning_rate": 1.76383486562942e-05,
      "loss": 8.1058,
      "step": 14650
    },
    {
      "epoch": 4.534838759570025,
      "grad_norm": 0.6383299231529236,
      "learning_rate": 1.7616248231966055e-05,
      "loss": 8.1022,
      "step": 14660
    },
    {
      "epoch": 4.53793210115227,
      "grad_norm": 0.5078263878822327,
      "learning_rate": 1.7594147807637906e-05,
      "loss": 8.1049,
      "step": 14670
    },
    {
      "epoch": 4.541025442734514,
      "grad_norm": 0.758458137512207,
      "learning_rate": 1.757204738330976e-05,
      "loss": 8.1087,
      "step": 14680
    },
    {
      "epoch": 4.544118784316758,
      "grad_norm": 0.7571190595626831,
      "learning_rate": 1.7549946958981614e-05,
      "loss": 8.1126,
      "step": 14690
    },
    {
      "epoch": 4.547212125899002,
      "grad_norm": 0.7151135802268982,
      "learning_rate": 1.7527846534653465e-05,
      "loss": 8.096,
      "step": 14700
    },
    {
      "epoch": 4.550305467481246,
      "grad_norm": 0.811011016368866,
      "learning_rate": 1.750574611032532e-05,
      "loss": 8.1058,
      "step": 14710
    },
    {
      "epoch": 4.5533988090634905,
      "grad_norm": 0.7643479704856873,
      "learning_rate": 1.748364568599717e-05,
      "loss": 8.1026,
      "step": 14720
    },
    {
      "epoch": 4.556492150645735,
      "grad_norm": 0.5543148517608643,
      "learning_rate": 1.7461545261669025e-05,
      "loss": 8.1005,
      "step": 14730
    },
    {
      "epoch": 4.5595854922279795,
      "grad_norm": 0.7140877842903137,
      "learning_rate": 1.7439444837340876e-05,
      "loss": 8.1026,
      "step": 14740
    },
    {
      "epoch": 4.5626788338102235,
      "grad_norm": 0.8558884859085083,
      "learning_rate": 1.741734441301273e-05,
      "loss": 8.0912,
      "step": 14750
    },
    {
      "epoch": 4.565772175392468,
      "grad_norm": 0.6510237455368042,
      "learning_rate": 1.7395243988684585e-05,
      "loss": 8.0874,
      "step": 14760
    },
    {
      "epoch": 4.568865516974712,
      "grad_norm": 0.72161465883255,
      "learning_rate": 1.7373143564356435e-05,
      "loss": 8.1049,
      "step": 14770
    },
    {
      "epoch": 4.571958858556956,
      "grad_norm": 0.974007785320282,
      "learning_rate": 1.735104314002829e-05,
      "loss": 8.1077,
      "step": 14780
    },
    {
      "epoch": 4.575052200139201,
      "grad_norm": 0.6772589087486267,
      "learning_rate": 1.732894271570014e-05,
      "loss": 8.1033,
      "step": 14790
    },
    {
      "epoch": 4.578145541721445,
      "grad_norm": 0.6076662540435791,
      "learning_rate": 1.7306842291371995e-05,
      "loss": 8.102,
      "step": 14800
    },
    {
      "epoch": 4.581238883303689,
      "grad_norm": 0.6735473871231079,
      "learning_rate": 1.728474186704385e-05,
      "loss": 8.1014,
      "step": 14810
    },
    {
      "epoch": 4.584332224885933,
      "grad_norm": 0.8401991128921509,
      "learning_rate": 1.72626414427157e-05,
      "loss": 8.112,
      "step": 14820
    },
    {
      "epoch": 4.587425566468177,
      "grad_norm": 0.9567863345146179,
      "learning_rate": 1.7240541018387555e-05,
      "loss": 8.1084,
      "step": 14830
    },
    {
      "epoch": 4.590518908050422,
      "grad_norm": 1.0058038234710693,
      "learning_rate": 1.7218440594059406e-05,
      "loss": 8.1051,
      "step": 14840
    },
    {
      "epoch": 4.593612249632666,
      "grad_norm": 0.6566473841667175,
      "learning_rate": 1.719634016973126e-05,
      "loss": 8.0994,
      "step": 14850
    },
    {
      "epoch": 4.59670559121491,
      "grad_norm": 0.9701124429702759,
      "learning_rate": 1.717423974540311e-05,
      "loss": 8.1043,
      "step": 14860
    },
    {
      "epoch": 4.599798932797154,
      "grad_norm": 0.960032045841217,
      "learning_rate": 1.7152139321074965e-05,
      "loss": 8.1028,
      "step": 14870
    },
    {
      "epoch": 4.602892274379398,
      "grad_norm": 0.894821047782898,
      "learning_rate": 1.713003889674682e-05,
      "loss": 8.1107,
      "step": 14880
    },
    {
      "epoch": 4.605985615961643,
      "grad_norm": 0.8980772495269775,
      "learning_rate": 1.710793847241867e-05,
      "loss": 8.0996,
      "step": 14890
    },
    {
      "epoch": 4.609078957543887,
      "grad_norm": 0.7843453884124756,
      "learning_rate": 1.7085838048090525e-05,
      "loss": 8.1051,
      "step": 14900
    },
    {
      "epoch": 4.612172299126131,
      "grad_norm": 0.8941006064414978,
      "learning_rate": 1.7063737623762376e-05,
      "loss": 8.0943,
      "step": 14910
    },
    {
      "epoch": 4.615265640708375,
      "grad_norm": 0.9125520586967468,
      "learning_rate": 1.704163719943423e-05,
      "loss": 8.1075,
      "step": 14920
    },
    {
      "epoch": 4.618358982290619,
      "grad_norm": 0.7657214999198914,
      "learning_rate": 1.701953677510608e-05,
      "loss": 8.1026,
      "step": 14930
    },
    {
      "epoch": 4.621452323872863,
      "grad_norm": 0.6281751394271851,
      "learning_rate": 1.6997436350777935e-05,
      "loss": 8.1088,
      "step": 14940
    },
    {
      "epoch": 4.624545665455108,
      "grad_norm": 0.7713119387626648,
      "learning_rate": 1.697533592644979e-05,
      "loss": 8.0961,
      "step": 14950
    },
    {
      "epoch": 4.627639007037352,
      "grad_norm": 0.8221123814582825,
      "learning_rate": 1.6955445544554456e-05,
      "loss": 8.0927,
      "step": 14960
    },
    {
      "epoch": 4.630732348619596,
      "grad_norm": 0.6875186562538147,
      "learning_rate": 1.693334512022631e-05,
      "loss": 8.0987,
      "step": 14970
    },
    {
      "epoch": 4.63382569020184,
      "grad_norm": 0.749578058719635,
      "learning_rate": 1.691124469589816e-05,
      "loss": 8.1047,
      "step": 14980
    },
    {
      "epoch": 4.6369190317840845,
      "grad_norm": 0.9658677577972412,
      "learning_rate": 1.6889144271570016e-05,
      "loss": 8.1024,
      "step": 14990
    },
    {
      "epoch": 4.6400123733663285,
      "grad_norm": 0.7067029476165771,
      "learning_rate": 1.6867043847241867e-05,
      "loss": 8.0942,
      "step": 15000
    },
    {
      "epoch": 4.6431057149485735,
      "grad_norm": 0.7610042095184326,
      "learning_rate": 1.684494342291372e-05,
      "loss": 8.0943,
      "step": 15010
    },
    {
      "epoch": 4.6461990565308176,
      "grad_norm": 0.8921234607696533,
      "learning_rate": 1.6822842998585572e-05,
      "loss": 8.0966,
      "step": 15020
    },
    {
      "epoch": 4.649292398113062,
      "grad_norm": 0.755446195602417,
      "learning_rate": 1.6800742574257426e-05,
      "loss": 8.1121,
      "step": 15030
    },
    {
      "epoch": 4.652385739695306,
      "grad_norm": 0.6665080189704895,
      "learning_rate": 1.677864214992928e-05,
      "loss": 8.1037,
      "step": 15040
    },
    {
      "epoch": 4.65547908127755,
      "grad_norm": 0.7006452083587646,
      "learning_rate": 1.6756541725601132e-05,
      "loss": 8.1043,
      "step": 15050
    },
    {
      "epoch": 4.658572422859795,
      "grad_norm": 0.7753766775131226,
      "learning_rate": 1.6734441301272986e-05,
      "loss": 8.1058,
      "step": 15060
    },
    {
      "epoch": 4.661665764442039,
      "grad_norm": 0.42284300923347473,
      "learning_rate": 1.6712340876944837e-05,
      "loss": 8.0964,
      "step": 15070
    },
    {
      "epoch": 4.664759106024283,
      "grad_norm": 0.6997082233428955,
      "learning_rate": 1.669024045261669e-05,
      "loss": 8.1072,
      "step": 15080
    },
    {
      "epoch": 4.667852447606527,
      "grad_norm": 0.9119889736175537,
      "learning_rate": 1.6668140028288546e-05,
      "loss": 8.0979,
      "step": 15090
    },
    {
      "epoch": 4.670945789188771,
      "grad_norm": 0.8211225271224976,
      "learning_rate": 1.6646039603960397e-05,
      "loss": 8.1139,
      "step": 15100
    },
    {
      "epoch": 4.674039130771016,
      "grad_norm": 0.7711447477340698,
      "learning_rate": 1.662393917963225e-05,
      "loss": 8.1032,
      "step": 15110
    },
    {
      "epoch": 4.67713247235326,
      "grad_norm": 0.8697813749313354,
      "learning_rate": 1.6601838755304102e-05,
      "loss": 8.1003,
      "step": 15120
    },
    {
      "epoch": 4.680225813935504,
      "grad_norm": 0.8067618608474731,
      "learning_rate": 1.6579738330975956e-05,
      "loss": 8.1076,
      "step": 15130
    },
    {
      "epoch": 4.683319155517748,
      "grad_norm": 0.7758587598800659,
      "learning_rate": 1.6557637906647807e-05,
      "loss": 8.1036,
      "step": 15140
    },
    {
      "epoch": 4.686412497099992,
      "grad_norm": 0.6310567259788513,
      "learning_rate": 1.653553748231966e-05,
      "loss": 8.1017,
      "step": 15150
    },
    {
      "epoch": 4.689505838682236,
      "grad_norm": 0.5721341371536255,
      "learning_rate": 1.6513437057991516e-05,
      "loss": 8.1118,
      "step": 15160
    },
    {
      "epoch": 4.692599180264481,
      "grad_norm": 0.9308609366416931,
      "learning_rate": 1.6491336633663367e-05,
      "loss": 8.1008,
      "step": 15170
    },
    {
      "epoch": 4.695692521846725,
      "grad_norm": 0.7628928422927856,
      "learning_rate": 1.646923620933522e-05,
      "loss": 8.1012,
      "step": 15180
    },
    {
      "epoch": 4.698785863428969,
      "grad_norm": 0.860072135925293,
      "learning_rate": 1.6447135785007072e-05,
      "loss": 8.0955,
      "step": 15190
    },
    {
      "epoch": 4.701879205011213,
      "grad_norm": 0.6699973940849304,
      "learning_rate": 1.6425035360678926e-05,
      "loss": 8.0982,
      "step": 15200
    },
    {
      "epoch": 4.704972546593457,
      "grad_norm": 0.8399010300636292,
      "learning_rate": 1.6402934936350777e-05,
      "loss": 8.1115,
      "step": 15210
    },
    {
      "epoch": 4.708065888175701,
      "grad_norm": 0.7448433041572571,
      "learning_rate": 1.638083451202263e-05,
      "loss": 8.0977,
      "step": 15220
    },
    {
      "epoch": 4.711159229757946,
      "grad_norm": 0.7215545773506165,
      "learning_rate": 1.6358734087694486e-05,
      "loss": 8.0965,
      "step": 15230
    },
    {
      "epoch": 4.71425257134019,
      "grad_norm": 0.7813366055488586,
      "learning_rate": 1.6336633663366337e-05,
      "loss": 8.0922,
      "step": 15240
    },
    {
      "epoch": 4.717345912922434,
      "grad_norm": 0.616788387298584,
      "learning_rate": 1.631453323903819e-05,
      "loss": 8.0994,
      "step": 15250
    },
    {
      "epoch": 4.7204392545046785,
      "grad_norm": 0.8325091600418091,
      "learning_rate": 1.6292432814710042e-05,
      "loss": 8.0963,
      "step": 15260
    },
    {
      "epoch": 4.7235325960869226,
      "grad_norm": 0.8220918774604797,
      "learning_rate": 1.6270332390381897e-05,
      "loss": 8.1029,
      "step": 15270
    },
    {
      "epoch": 4.7266259376691675,
      "grad_norm": 0.42495113611221313,
      "learning_rate": 1.6248231966053747e-05,
      "loss": 8.1031,
      "step": 15280
    },
    {
      "epoch": 4.729719279251412,
      "grad_norm": 0.6687672734260559,
      "learning_rate": 1.6226131541725602e-05,
      "loss": 8.0935,
      "step": 15290
    },
    {
      "epoch": 4.732812620833656,
      "grad_norm": 0.7816333770751953,
      "learning_rate": 1.6204031117397456e-05,
      "loss": 8.1044,
      "step": 15300
    },
    {
      "epoch": 4.7359059624159,
      "grad_norm": 0.5847042798995972,
      "learning_rate": 1.6181930693069307e-05,
      "loss": 8.1138,
      "step": 15310
    },
    {
      "epoch": 4.738999303998144,
      "grad_norm": 0.6203551292419434,
      "learning_rate": 1.615983026874116e-05,
      "loss": 8.1036,
      "step": 15320
    },
    {
      "epoch": 4.742092645580389,
      "grad_norm": 0.5725954174995422,
      "learning_rate": 1.6137729844413012e-05,
      "loss": 8.0897,
      "step": 15330
    },
    {
      "epoch": 4.745185987162633,
      "grad_norm": 1.2010843753814697,
      "learning_rate": 1.6115629420084867e-05,
      "loss": 8.1032,
      "step": 15340
    },
    {
      "epoch": 4.748279328744877,
      "grad_norm": 0.5495712161064148,
      "learning_rate": 1.6093528995756718e-05,
      "loss": 8.0944,
      "step": 15350
    },
    {
      "epoch": 4.751372670327121,
      "grad_norm": 0.8754666447639465,
      "learning_rate": 1.6071428571428572e-05,
      "loss": 8.1154,
      "step": 15360
    },
    {
      "epoch": 4.754466011909365,
      "grad_norm": 0.6197102069854736,
      "learning_rate": 1.6049328147100426e-05,
      "loss": 8.1064,
      "step": 15370
    },
    {
      "epoch": 4.757559353491609,
      "grad_norm": 0.7076538801193237,
      "learning_rate": 1.6027227722772277e-05,
      "loss": 8.0973,
      "step": 15380
    },
    {
      "epoch": 4.760652695073854,
      "grad_norm": 0.6063266396522522,
      "learning_rate": 1.600512729844413e-05,
      "loss": 8.1025,
      "step": 15390
    },
    {
      "epoch": 4.763746036656098,
      "grad_norm": 0.9720186591148376,
      "learning_rate": 1.5983026874115983e-05,
      "loss": 8.0973,
      "step": 15400
    },
    {
      "epoch": 4.766839378238342,
      "grad_norm": 0.7452898025512695,
      "learning_rate": 1.5960926449787837e-05,
      "loss": 8.0974,
      "step": 15410
    },
    {
      "epoch": 4.769932719820586,
      "grad_norm": 0.6861242055892944,
      "learning_rate": 1.5938826025459688e-05,
      "loss": 8.113,
      "step": 15420
    },
    {
      "epoch": 4.77302606140283,
      "grad_norm": 0.8652819991111755,
      "learning_rate": 1.5916725601131542e-05,
      "loss": 8.1085,
      "step": 15430
    },
    {
      "epoch": 4.776119402985074,
      "grad_norm": 0.8230326175689697,
      "learning_rate": 1.5894625176803396e-05,
      "loss": 8.0861,
      "step": 15440
    },
    {
      "epoch": 4.779212744567319,
      "grad_norm": 0.5419344902038574,
      "learning_rate": 1.5872524752475247e-05,
      "loss": 8.08,
      "step": 15450
    },
    {
      "epoch": 4.782306086149563,
      "grad_norm": 0.6409058570861816,
      "learning_rate": 1.5850424328147102e-05,
      "loss": 8.1134,
      "step": 15460
    },
    {
      "epoch": 4.785399427731807,
      "grad_norm": 0.7014784216880798,
      "learning_rate": 1.5828323903818953e-05,
      "loss": 8.0952,
      "step": 15470
    },
    {
      "epoch": 4.788492769314051,
      "grad_norm": 0.4405183792114258,
      "learning_rate": 1.5806223479490807e-05,
      "loss": 8.093,
      "step": 15480
    },
    {
      "epoch": 4.791586110896295,
      "grad_norm": 0.7840326428413391,
      "learning_rate": 1.578412305516266e-05,
      "loss": 8.1121,
      "step": 15490
    },
    {
      "epoch": 4.79467945247854,
      "grad_norm": 0.77994704246521,
      "learning_rate": 1.5762022630834512e-05,
      "loss": 8.1001,
      "step": 15500
    },
    {
      "epoch": 4.797772794060784,
      "grad_norm": 0.7218440771102905,
      "learning_rate": 1.5739922206506367e-05,
      "loss": 8.0922,
      "step": 15510
    },
    {
      "epoch": 4.800866135643028,
      "grad_norm": 0.6374032497406006,
      "learning_rate": 1.5717821782178218e-05,
      "loss": 8.0962,
      "step": 15520
    },
    {
      "epoch": 4.8039594772252725,
      "grad_norm": 0.7549035549163818,
      "learning_rate": 1.5695721357850072e-05,
      "loss": 8.1218,
      "step": 15530
    },
    {
      "epoch": 4.807052818807517,
      "grad_norm": 1.0098271369934082,
      "learning_rate": 1.5673620933521923e-05,
      "loss": 8.1154,
      "step": 15540
    },
    {
      "epoch": 4.8101461603897615,
      "grad_norm": 0.5600395202636719,
      "learning_rate": 1.5651520509193777e-05,
      "loss": 8.1108,
      "step": 15550
    },
    {
      "epoch": 4.813239501972006,
      "grad_norm": 0.5968921184539795,
      "learning_rate": 1.562942008486563e-05,
      "loss": 8.1147,
      "step": 15560
    },
    {
      "epoch": 4.81633284355425,
      "grad_norm": 0.9096850156784058,
      "learning_rate": 1.5607319660537482e-05,
      "loss": 8.1045,
      "step": 15570
    },
    {
      "epoch": 4.819426185136494,
      "grad_norm": 0.7260937690734863,
      "learning_rate": 1.5585219236209337e-05,
      "loss": 8.103,
      "step": 15580
    },
    {
      "epoch": 4.822519526718738,
      "grad_norm": 0.6925172805786133,
      "learning_rate": 1.5563118811881188e-05,
      "loss": 8.1069,
      "step": 15590
    },
    {
      "epoch": 4.825612868300982,
      "grad_norm": 0.5937725901603699,
      "learning_rate": 1.5541018387553042e-05,
      "loss": 8.1097,
      "step": 15600
    },
    {
      "epoch": 4.828706209883227,
      "grad_norm": 0.9341009855270386,
      "learning_rate": 1.5518917963224893e-05,
      "loss": 8.0857,
      "step": 15610
    },
    {
      "epoch": 4.831799551465471,
      "grad_norm": 0.5635971426963806,
      "learning_rate": 1.5496817538896747e-05,
      "loss": 8.1053,
      "step": 15620
    },
    {
      "epoch": 4.834892893047715,
      "grad_norm": 0.6970121264457703,
      "learning_rate": 1.54747171145686e-05,
      "loss": 8.1061,
      "step": 15630
    },
    {
      "epoch": 4.837986234629959,
      "grad_norm": 0.5477977991104126,
      "learning_rate": 1.5452616690240453e-05,
      "loss": 8.1105,
      "step": 15640
    },
    {
      "epoch": 4.841079576212203,
      "grad_norm": 0.6072338223457336,
      "learning_rate": 1.5430516265912307e-05,
      "loss": 8.1062,
      "step": 15650
    },
    {
      "epoch": 4.844172917794447,
      "grad_norm": 0.5431926250457764,
      "learning_rate": 1.5408415841584158e-05,
      "loss": 8.0931,
      "step": 15660
    },
    {
      "epoch": 4.847266259376692,
      "grad_norm": 0.6472682952880859,
      "learning_rate": 1.5386315417256012e-05,
      "loss": 8.1042,
      "step": 15670
    },
    {
      "epoch": 4.850359600958936,
      "grad_norm": 0.7168446779251099,
      "learning_rate": 1.5364214992927863e-05,
      "loss": 8.0992,
      "step": 15680
    },
    {
      "epoch": 4.85345294254118,
      "grad_norm": 0.7456526160240173,
      "learning_rate": 1.534211456859972e-05,
      "loss": 8.0902,
      "step": 15690
    },
    {
      "epoch": 4.856546284123424,
      "grad_norm": 0.5990282893180847,
      "learning_rate": 1.5320014144271572e-05,
      "loss": 8.0911,
      "step": 15700
    },
    {
      "epoch": 4.859639625705668,
      "grad_norm": 0.6650099754333496,
      "learning_rate": 1.5297913719943423e-05,
      "loss": 8.1076,
      "step": 15710
    },
    {
      "epoch": 4.862732967287913,
      "grad_norm": 0.7805476188659668,
      "learning_rate": 1.5275813295615277e-05,
      "loss": 8.1013,
      "step": 15720
    },
    {
      "epoch": 4.865826308870157,
      "grad_norm": 0.871866762638092,
      "learning_rate": 1.525371287128713e-05,
      "loss": 8.0961,
      "step": 15730
    },
    {
      "epoch": 4.868919650452401,
      "grad_norm": 0.7252680659294128,
      "learning_rate": 1.5231612446958982e-05,
      "loss": 8.0988,
      "step": 15740
    },
    {
      "epoch": 4.872012992034645,
      "grad_norm": 1.1288518905639648,
      "learning_rate": 1.5209512022630833e-05,
      "loss": 8.0984,
      "step": 15750
    },
    {
      "epoch": 4.875106333616889,
      "grad_norm": 1.0212650299072266,
      "learning_rate": 1.518741159830269e-05,
      "loss": 8.1045,
      "step": 15760
    },
    {
      "epoch": 4.878199675199134,
      "grad_norm": 0.7243284583091736,
      "learning_rate": 1.5165311173974542e-05,
      "loss": 8.0984,
      "step": 15770
    },
    {
      "epoch": 4.881293016781378,
      "grad_norm": 0.642611563205719,
      "learning_rate": 1.5143210749646395e-05,
      "loss": 8.1129,
      "step": 15780
    },
    {
      "epoch": 4.8843863583636224,
      "grad_norm": 0.5774455070495605,
      "learning_rate": 1.5121110325318247e-05,
      "loss": 8.0725,
      "step": 15790
    },
    {
      "epoch": 4.8874796999458665,
      "grad_norm": 0.6688645482063293,
      "learning_rate": 1.50990099009901e-05,
      "loss": 8.0922,
      "step": 15800
    },
    {
      "epoch": 4.890573041528111,
      "grad_norm": 0.9629164338111877,
      "learning_rate": 1.5076909476661953e-05,
      "loss": 8.115,
      "step": 15810
    },
    {
      "epoch": 4.893666383110355,
      "grad_norm": 0.6644682288169861,
      "learning_rate": 1.5054809052333803e-05,
      "loss": 8.0975,
      "step": 15820
    },
    {
      "epoch": 4.896759724692599,
      "grad_norm": 1.2804943323135376,
      "learning_rate": 1.503270862800566e-05,
      "loss": 8.1047,
      "step": 15830
    },
    {
      "epoch": 4.899853066274844,
      "grad_norm": 0.9158992171287537,
      "learning_rate": 1.5010608203677512e-05,
      "loss": 8.0912,
      "step": 15840
    },
    {
      "epoch": 4.902946407857088,
      "grad_norm": 0.9104302525520325,
      "learning_rate": 1.4988507779349365e-05,
      "loss": 8.1216,
      "step": 15850
    },
    {
      "epoch": 4.906039749439332,
      "grad_norm": 1.0281938314437866,
      "learning_rate": 1.4966407355021217e-05,
      "loss": 8.104,
      "step": 15860
    },
    {
      "epoch": 4.909133091021576,
      "grad_norm": 0.5753182768821716,
      "learning_rate": 1.494430693069307e-05,
      "loss": 8.1037,
      "step": 15870
    },
    {
      "epoch": 4.91222643260382,
      "grad_norm": 0.8281016945838928,
      "learning_rate": 1.4922206506364923e-05,
      "loss": 8.1088,
      "step": 15880
    },
    {
      "epoch": 4.915319774186065,
      "grad_norm": 0.8883852958679199,
      "learning_rate": 1.4900106082036777e-05,
      "loss": 8.1095,
      "step": 15890
    },
    {
      "epoch": 4.918413115768309,
      "grad_norm": 0.7834994792938232,
      "learning_rate": 1.487800565770863e-05,
      "loss": 8.0875,
      "step": 15900
    },
    {
      "epoch": 4.921506457350553,
      "grad_norm": 1.165504813194275,
      "learning_rate": 1.4855905233380482e-05,
      "loss": 8.1091,
      "step": 15910
    },
    {
      "epoch": 4.924599798932797,
      "grad_norm": 0.7532289624214172,
      "learning_rate": 1.4833804809052335e-05,
      "loss": 8.1091,
      "step": 15920
    },
    {
      "epoch": 4.927693140515041,
      "grad_norm": 0.9881443381309509,
      "learning_rate": 1.4811704384724188e-05,
      "loss": 8.0657,
      "step": 15930
    },
    {
      "epoch": 4.930786482097286,
      "grad_norm": 0.7366990447044373,
      "learning_rate": 1.478960396039604e-05,
      "loss": 8.106,
      "step": 15940
    },
    {
      "epoch": 4.93387982367953,
      "grad_norm": 0.5991972088813782,
      "learning_rate": 1.4767503536067893e-05,
      "loss": 8.0978,
      "step": 15950
    },
    {
      "epoch": 4.936973165261774,
      "grad_norm": 0.5837808847427368,
      "learning_rate": 1.4745403111739747e-05,
      "loss": 8.1013,
      "step": 15960
    },
    {
      "epoch": 4.940066506844018,
      "grad_norm": 0.7657656669616699,
      "learning_rate": 1.47233026874116e-05,
      "loss": 8.0996,
      "step": 15970
    },
    {
      "epoch": 4.943159848426262,
      "grad_norm": 0.6333307027816772,
      "learning_rate": 1.4701202263083452e-05,
      "loss": 8.0928,
      "step": 15980
    },
    {
      "epoch": 4.946253190008507,
      "grad_norm": 0.6625046730041504,
      "learning_rate": 1.4679101838755305e-05,
      "loss": 8.1016,
      "step": 15990
    },
    {
      "epoch": 4.949346531590751,
      "grad_norm": 0.6150280833244324,
      "learning_rate": 1.4657001414427158e-05,
      "loss": 8.1007,
      "step": 16000
    },
    {
      "epoch": 4.952439873172995,
      "grad_norm": 0.8312826156616211,
      "learning_rate": 1.463490099009901e-05,
      "loss": 8.1105,
      "step": 16010
    },
    {
      "epoch": 4.955533214755239,
      "grad_norm": 0.583595335483551,
      "learning_rate": 1.4615010608203677e-05,
      "loss": 8.1077,
      "step": 16020
    },
    {
      "epoch": 4.958626556337483,
      "grad_norm": 0.6639449000358582,
      "learning_rate": 1.459291018387553e-05,
      "loss": 8.0861,
      "step": 16030
    },
    {
      "epoch": 4.9617198979197275,
      "grad_norm": 0.6179271936416626,
      "learning_rate": 1.4570809759547386e-05,
      "loss": 8.1033,
      "step": 16040
    },
    {
      "epoch": 4.9648132395019715,
      "grad_norm": 0.8606950044631958,
      "learning_rate": 1.4548709335219238e-05,
      "loss": 8.0998,
      "step": 16050
    },
    {
      "epoch": 4.9679065810842165,
      "grad_norm": 0.749901294708252,
      "learning_rate": 1.452660891089109e-05,
      "loss": 8.1065,
      "step": 16060
    },
    {
      "epoch": 4.9709999226664605,
      "grad_norm": 0.5269764065742493,
      "learning_rate": 1.4504508486562943e-05,
      "loss": 8.096,
      "step": 16070
    },
    {
      "epoch": 4.974093264248705,
      "grad_norm": 0.5714900493621826,
      "learning_rate": 1.4482408062234794e-05,
      "loss": 8.1058,
      "step": 16080
    },
    {
      "epoch": 4.977186605830949,
      "grad_norm": 0.5442277193069458,
      "learning_rate": 1.4460307637906647e-05,
      "loss": 8.0989,
      "step": 16090
    },
    {
      "epoch": 4.980279947413193,
      "grad_norm": 0.7707448601722717,
      "learning_rate": 1.44382072135785e-05,
      "loss": 8.0979,
      "step": 16100
    },
    {
      "epoch": 4.983373288995438,
      "grad_norm": 0.8023332953453064,
      "learning_rate": 1.4416106789250356e-05,
      "loss": 8.1287,
      "step": 16110
    },
    {
      "epoch": 4.986466630577682,
      "grad_norm": 0.5887309908866882,
      "learning_rate": 1.4394006364922208e-05,
      "loss": 8.0959,
      "step": 16120
    },
    {
      "epoch": 4.989559972159926,
      "grad_norm": 0.65346759557724,
      "learning_rate": 1.4371905940594061e-05,
      "loss": 8.1064,
      "step": 16130
    },
    {
      "epoch": 4.99265331374217,
      "grad_norm": 0.7976763844490051,
      "learning_rate": 1.4349805516265914e-05,
      "loss": 8.1175,
      "step": 16140
    },
    {
      "epoch": 4.995746655324414,
      "grad_norm": 0.6008458733558655,
      "learning_rate": 1.4327705091937765e-05,
      "loss": 8.1017,
      "step": 16150
    },
    {
      "epoch": 4.998839996906659,
      "grad_norm": 0.5690858364105225,
      "learning_rate": 1.4305604667609617e-05,
      "loss": 8.0908,
      "step": 16160
    },
    {
      "epoch": 5.001933338488903,
      "grad_norm": 0.8186638951301575,
      "learning_rate": 1.4283504243281473e-05,
      "loss": 8.1074,
      "step": 16170
    },
    {
      "epoch": 5.005026680071147,
      "grad_norm": 0.7810150384902954,
      "learning_rate": 1.4261403818953326e-05,
      "loss": 8.1022,
      "step": 16180
    },
    {
      "epoch": 5.008120021653391,
      "grad_norm": 0.6328790783882141,
      "learning_rate": 1.4239303394625179e-05,
      "loss": 8.1005,
      "step": 16190
    },
    {
      "epoch": 5.011213363235635,
      "grad_norm": 0.7922280430793762,
      "learning_rate": 1.4217202970297031e-05,
      "loss": 8.0944,
      "step": 16200
    },
    {
      "epoch": 5.014306704817879,
      "grad_norm": 1.03416109085083,
      "learning_rate": 1.4195102545968884e-05,
      "loss": 8.1087,
      "step": 16210
    },
    {
      "epoch": 5.017400046400124,
      "grad_norm": 0.7601829171180725,
      "learning_rate": 1.4173002121640735e-05,
      "loss": 8.0956,
      "step": 16220
    },
    {
      "epoch": 5.020493387982368,
      "grad_norm": 0.6503462195396423,
      "learning_rate": 1.4150901697312587e-05,
      "loss": 8.1045,
      "step": 16230
    },
    {
      "epoch": 5.023586729564612,
      "grad_norm": 0.3979361355304718,
      "learning_rate": 1.4128801272984443e-05,
      "loss": 8.0969,
      "step": 16240
    },
    {
      "epoch": 5.026680071146856,
      "grad_norm": 0.5212022662162781,
      "learning_rate": 1.4106700848656296e-05,
      "loss": 8.0925,
      "step": 16250
    },
    {
      "epoch": 5.0297734127291,
      "grad_norm": 0.6563586592674255,
      "learning_rate": 1.4084600424328149e-05,
      "loss": 8.0931,
      "step": 16260
    },
    {
      "epoch": 5.032866754311345,
      "grad_norm": 0.7478793263435364,
      "learning_rate": 1.4062500000000001e-05,
      "loss": 8.09,
      "step": 16270
    },
    {
      "epoch": 5.035960095893589,
      "grad_norm": 0.9159552454948425,
      "learning_rate": 1.4040399575671854e-05,
      "loss": 8.1028,
      "step": 16280
    },
    {
      "epoch": 5.039053437475833,
      "grad_norm": 0.4855981469154358,
      "learning_rate": 1.4018299151343705e-05,
      "loss": 8.1084,
      "step": 16290
    },
    {
      "epoch": 5.042146779058077,
      "grad_norm": 1.3265995979309082,
      "learning_rate": 1.3996198727015558e-05,
      "loss": 8.0937,
      "step": 16300
    },
    {
      "epoch": 5.0452401206403215,
      "grad_norm": 0.7795005440711975,
      "learning_rate": 1.3974098302687414e-05,
      "loss": 8.0879,
      "step": 16310
    },
    {
      "epoch": 5.0483334622225655,
      "grad_norm": 0.8034344911575317,
      "learning_rate": 1.3951997878359266e-05,
      "loss": 8.0904,
      "step": 16320
    },
    {
      "epoch": 5.0514268038048105,
      "grad_norm": 0.6631861329078674,
      "learning_rate": 1.3929897454031119e-05,
      "loss": 8.101,
      "step": 16330
    },
    {
      "epoch": 5.0545201453870545,
      "grad_norm": 0.49253860116004944,
      "learning_rate": 1.3907797029702971e-05,
      "loss": 8.1156,
      "step": 16340
    },
    {
      "epoch": 5.057613486969299,
      "grad_norm": 0.9592735767364502,
      "learning_rate": 1.3885696605374824e-05,
      "loss": 8.1048,
      "step": 16350
    },
    {
      "epoch": 5.060706828551543,
      "grad_norm": 0.5956780910491943,
      "learning_rate": 1.3863596181046675e-05,
      "loss": 8.1073,
      "step": 16360
    },
    {
      "epoch": 5.063800170133787,
      "grad_norm": 0.5605176091194153,
      "learning_rate": 1.3841495756718528e-05,
      "loss": 8.1091,
      "step": 16370
    },
    {
      "epoch": 5.066893511716032,
      "grad_norm": 0.5416126847267151,
      "learning_rate": 1.3819395332390384e-05,
      "loss": 8.0867,
      "step": 16380
    },
    {
      "epoch": 5.069986853298276,
      "grad_norm": 0.6583813428878784,
      "learning_rate": 1.3797294908062236e-05,
      "loss": 8.0873,
      "step": 16390
    },
    {
      "epoch": 5.07308019488052,
      "grad_norm": 0.7041524052619934,
      "learning_rate": 1.3775194483734089e-05,
      "loss": 8.1059,
      "step": 16400
    },
    {
      "epoch": 5.076173536462764,
      "grad_norm": 0.7576868534088135,
      "learning_rate": 1.3753094059405942e-05,
      "loss": 8.1046,
      "step": 16410
    },
    {
      "epoch": 5.079266878045008,
      "grad_norm": 0.4391545355319977,
      "learning_rate": 1.3730993635077794e-05,
      "loss": 8.0875,
      "step": 16420
    },
    {
      "epoch": 5.082360219627252,
      "grad_norm": 0.8290860056877136,
      "learning_rate": 1.3708893210749645e-05,
      "loss": 8.1045,
      "step": 16430
    },
    {
      "epoch": 5.085453561209497,
      "grad_norm": 0.7873147130012512,
      "learning_rate": 1.3686792786421501e-05,
      "loss": 8.0899,
      "step": 16440
    },
    {
      "epoch": 5.088546902791741,
      "grad_norm": 0.6736086010932922,
      "learning_rate": 1.3664692362093354e-05,
      "loss": 8.1087,
      "step": 16450
    },
    {
      "epoch": 5.091640244373985,
      "grad_norm": 0.870066225528717,
      "learning_rate": 1.3642591937765207e-05,
      "loss": 8.0994,
      "step": 16460
    },
    {
      "epoch": 5.094733585956229,
      "grad_norm": 0.8107249736785889,
      "learning_rate": 1.3620491513437059e-05,
      "loss": 8.1219,
      "step": 16470
    },
    {
      "epoch": 5.097826927538473,
      "grad_norm": 0.6077970266342163,
      "learning_rate": 1.3598391089108912e-05,
      "loss": 8.0996,
      "step": 16480
    },
    {
      "epoch": 5.100920269120718,
      "grad_norm": 0.7980647683143616,
      "learning_rate": 1.3576290664780764e-05,
      "loss": 8.0927,
      "step": 16490
    },
    {
      "epoch": 5.104013610702962,
      "grad_norm": 0.6925227642059326,
      "learning_rate": 1.3554190240452615e-05,
      "loss": 8.1004,
      "step": 16500
    },
    {
      "epoch": 5.107106952285206,
      "grad_norm": 0.6923226714134216,
      "learning_rate": 1.3532089816124471e-05,
      "loss": 8.105,
      "step": 16510
    },
    {
      "epoch": 5.11020029386745,
      "grad_norm": 0.5805438160896301,
      "learning_rate": 1.3509989391796324e-05,
      "loss": 8.0884,
      "step": 16520
    },
    {
      "epoch": 5.113293635449694,
      "grad_norm": 0.5506868958473206,
      "learning_rate": 1.3487888967468177e-05,
      "loss": 8.0936,
      "step": 16530
    },
    {
      "epoch": 5.116386977031938,
      "grad_norm": 0.767069399356842,
      "learning_rate": 1.346578854314003e-05,
      "loss": 8.1128,
      "step": 16540
    },
    {
      "epoch": 5.119480318614183,
      "grad_norm": 1.0922785997390747,
      "learning_rate": 1.3443688118811882e-05,
      "loss": 8.1131,
      "step": 16550
    },
    {
      "epoch": 5.122573660196427,
      "grad_norm": 0.7211973667144775,
      "learning_rate": 1.3421587694483735e-05,
      "loss": 8.1034,
      "step": 16560
    },
    {
      "epoch": 5.125667001778671,
      "grad_norm": 0.5526270270347595,
      "learning_rate": 1.3399487270155586e-05,
      "loss": 8.0996,
      "step": 16570
    },
    {
      "epoch": 5.1287603433609155,
      "grad_norm": 0.7881585955619812,
      "learning_rate": 1.3377386845827442e-05,
      "loss": 8.0926,
      "step": 16580
    },
    {
      "epoch": 5.1318536849431595,
      "grad_norm": 0.8277202844619751,
      "learning_rate": 1.3355286421499294e-05,
      "loss": 8.1007,
      "step": 16590
    },
    {
      "epoch": 5.1349470265254045,
      "grad_norm": 0.5994413495063782,
      "learning_rate": 1.3333185997171147e-05,
      "loss": 8.1139,
      "step": 16600
    },
    {
      "epoch": 5.1380403681076485,
      "grad_norm": 0.6443746089935303,
      "learning_rate": 1.3311085572843e-05,
      "loss": 8.1003,
      "step": 16610
    },
    {
      "epoch": 5.141133709689893,
      "grad_norm": 0.6713635325431824,
      "learning_rate": 1.3288985148514852e-05,
      "loss": 8.1046,
      "step": 16620
    },
    {
      "epoch": 5.144227051272137,
      "grad_norm": 0.7269635200500488,
      "learning_rate": 1.3266884724186705e-05,
      "loss": 8.1004,
      "step": 16630
    },
    {
      "epoch": 5.147320392854381,
      "grad_norm": 0.9459056258201599,
      "learning_rate": 1.3244784299858559e-05,
      "loss": 8.1052,
      "step": 16640
    },
    {
      "epoch": 5.150413734436625,
      "grad_norm": 0.8317445516586304,
      "learning_rate": 1.3222683875530412e-05,
      "loss": 8.1043,
      "step": 16650
    },
    {
      "epoch": 5.15350707601887,
      "grad_norm": 0.9128361344337463,
      "learning_rate": 1.3200583451202264e-05,
      "loss": 8.1102,
      "step": 16660
    },
    {
      "epoch": 5.156600417601114,
      "grad_norm": 0.571669340133667,
      "learning_rate": 1.3178483026874117e-05,
      "loss": 8.0935,
      "step": 16670
    },
    {
      "epoch": 5.159693759183358,
      "grad_norm": 0.967597246170044,
      "learning_rate": 1.315638260254597e-05,
      "loss": 8.1009,
      "step": 16680
    },
    {
      "epoch": 5.162787100765602,
      "grad_norm": 0.9057184457778931,
      "learning_rate": 1.3134282178217822e-05,
      "loss": 8.1006,
      "step": 16690
    },
    {
      "epoch": 5.165880442347846,
      "grad_norm": 0.6549640893936157,
      "learning_rate": 1.3112181753889675e-05,
      "loss": 8.1102,
      "step": 16700
    },
    {
      "epoch": 5.168973783930091,
      "grad_norm": 0.609119713306427,
      "learning_rate": 1.309008132956153e-05,
      "loss": 8.0765,
      "step": 16710
    },
    {
      "epoch": 5.172067125512335,
      "grad_norm": 0.452239990234375,
      "learning_rate": 1.3067980905233382e-05,
      "loss": 8.0872,
      "step": 16720
    },
    {
      "epoch": 5.175160467094579,
      "grad_norm": 0.6626444458961487,
      "learning_rate": 1.3045880480905235e-05,
      "loss": 8.1021,
      "step": 16730
    },
    {
      "epoch": 5.178253808676823,
      "grad_norm": 0.4475065767765045,
      "learning_rate": 1.3023780056577087e-05,
      "loss": 8.1014,
      "step": 16740
    },
    {
      "epoch": 5.181347150259067,
      "grad_norm": 0.6928220391273499,
      "learning_rate": 1.300167963224894e-05,
      "loss": 8.1016,
      "step": 16750
    },
    {
      "epoch": 5.184440491841311,
      "grad_norm": 1.017356514930725,
      "learning_rate": 1.2979579207920792e-05,
      "loss": 8.0842,
      "step": 16760
    },
    {
      "epoch": 5.187533833423556,
      "grad_norm": 0.7288728952407837,
      "learning_rate": 1.2957478783592645e-05,
      "loss": 8.0997,
      "step": 16770
    },
    {
      "epoch": 5.1906271750058,
      "grad_norm": 0.8468811511993408,
      "learning_rate": 1.29353783592645e-05,
      "loss": 8.07,
      "step": 16780
    },
    {
      "epoch": 5.193720516588044,
      "grad_norm": 0.6877371668815613,
      "learning_rate": 1.2913277934936352e-05,
      "loss": 8.1046,
      "step": 16790
    },
    {
      "epoch": 5.196813858170288,
      "grad_norm": 0.7139245271682739,
      "learning_rate": 1.2891177510608205e-05,
      "loss": 8.0891,
      "step": 16800
    },
    {
      "epoch": 5.199907199752532,
      "grad_norm": 0.8454135656356812,
      "learning_rate": 1.2869077086280057e-05,
      "loss": 8.1082,
      "step": 16810
    },
    {
      "epoch": 5.203000541334777,
      "grad_norm": 0.5900562405586243,
      "learning_rate": 1.284697666195191e-05,
      "loss": 8.1116,
      "step": 16820
    },
    {
      "epoch": 5.206093882917021,
      "grad_norm": 0.8090713620185852,
      "learning_rate": 1.2824876237623763e-05,
      "loss": 8.101,
      "step": 16830
    },
    {
      "epoch": 5.209187224499265,
      "grad_norm": 0.9497917890548706,
      "learning_rate": 1.2802775813295617e-05,
      "loss": 8.1041,
      "step": 16840
    },
    {
      "epoch": 5.2122805660815095,
      "grad_norm": 0.7137184143066406,
      "learning_rate": 1.278067538896747e-05,
      "loss": 8.1005,
      "step": 16850
    },
    {
      "epoch": 5.2153739076637535,
      "grad_norm": 0.4893914461135864,
      "learning_rate": 1.2758574964639322e-05,
      "loss": 8.0984,
      "step": 16860
    },
    {
      "epoch": 5.218467249245998,
      "grad_norm": 0.6862518191337585,
      "learning_rate": 1.2736474540311175e-05,
      "loss": 8.1066,
      "step": 16870
    },
    {
      "epoch": 5.2215605908282425,
      "grad_norm": 0.8012530207633972,
      "learning_rate": 1.2714374115983027e-05,
      "loss": 8.1085,
      "step": 16880
    },
    {
      "epoch": 5.224653932410487,
      "grad_norm": 0.8102368116378784,
      "learning_rate": 1.269227369165488e-05,
      "loss": 8.0968,
      "step": 16890
    },
    {
      "epoch": 5.227747273992731,
      "grad_norm": 0.799444854259491,
      "learning_rate": 1.2670173267326733e-05,
      "loss": 8.1081,
      "step": 16900
    },
    {
      "epoch": 5.230840615574975,
      "grad_norm": 0.9932907819747925,
      "learning_rate": 1.2648072842998587e-05,
      "loss": 8.1029,
      "step": 16910
    },
    {
      "epoch": 5.233933957157219,
      "grad_norm": 0.7241348624229431,
      "learning_rate": 1.262597241867044e-05,
      "loss": 8.1053,
      "step": 16920
    },
    {
      "epoch": 5.237027298739464,
      "grad_norm": 0.7964839339256287,
      "learning_rate": 1.2603871994342292e-05,
      "loss": 8.1026,
      "step": 16930
    },
    {
      "epoch": 5.240120640321708,
      "grad_norm": 0.6314296722412109,
      "learning_rate": 1.2581771570014145e-05,
      "loss": 8.0961,
      "step": 16940
    },
    {
      "epoch": 5.243213981903952,
      "grad_norm": 0.5719936490058899,
      "learning_rate": 1.2559671145685998e-05,
      "loss": 8.1013,
      "step": 16950
    },
    {
      "epoch": 5.246307323486196,
      "grad_norm": 0.6487951278686523,
      "learning_rate": 1.253757072135785e-05,
      "loss": 8.0956,
      "step": 16960
    },
    {
      "epoch": 5.24940066506844,
      "grad_norm": 0.7664041519165039,
      "learning_rate": 1.2515470297029703e-05,
      "loss": 8.1015,
      "step": 16970
    },
    {
      "epoch": 5.252494006650684,
      "grad_norm": 0.7480698823928833,
      "learning_rate": 1.2493369872701556e-05,
      "loss": 8.1009,
      "step": 16980
    },
    {
      "epoch": 5.255587348232929,
      "grad_norm": 0.8506630659103394,
      "learning_rate": 1.247126944837341e-05,
      "loss": 8.1003,
      "step": 16990
    },
    {
      "epoch": 5.258680689815173,
      "grad_norm": 0.7684091925621033,
      "learning_rate": 1.2449169024045263e-05,
      "loss": 8.0825,
      "step": 17000
    },
    {
      "epoch": 5.261774031397417,
      "grad_norm": 0.9785832762718201,
      "learning_rate": 1.2427068599717115e-05,
      "loss": 8.1031,
      "step": 17010
    },
    {
      "epoch": 5.264867372979661,
      "grad_norm": 1.0705697536468506,
      "learning_rate": 1.2404968175388968e-05,
      "loss": 8.0904,
      "step": 17020
    },
    {
      "epoch": 5.267960714561905,
      "grad_norm": 0.5770659446716309,
      "learning_rate": 1.238286775106082e-05,
      "loss": 8.1036,
      "step": 17030
    },
    {
      "epoch": 5.27105405614415,
      "grad_norm": 0.6370108723640442,
      "learning_rate": 1.2360767326732673e-05,
      "loss": 8.0933,
      "step": 17040
    },
    {
      "epoch": 5.274147397726394,
      "grad_norm": 0.736381471157074,
      "learning_rate": 1.2338666902404526e-05,
      "loss": 8.1102,
      "step": 17050
    },
    {
      "epoch": 5.277240739308638,
      "grad_norm": 0.5457741618156433,
      "learning_rate": 1.231656647807638e-05,
      "loss": 8.0957,
      "step": 17060
    },
    {
      "epoch": 5.280334080890882,
      "grad_norm": 0.9190177321434021,
      "learning_rate": 1.2294466053748233e-05,
      "loss": 8.098,
      "step": 17070
    },
    {
      "epoch": 5.283427422473126,
      "grad_norm": 0.5595306754112244,
      "learning_rate": 1.2272365629420085e-05,
      "loss": 8.0892,
      "step": 17080
    },
    {
      "epoch": 5.28652076405537,
      "grad_norm": 0.5720602869987488,
      "learning_rate": 1.225026520509194e-05,
      "loss": 8.1127,
      "step": 17090
    },
    {
      "epoch": 5.289614105637615,
      "grad_norm": 0.7402272820472717,
      "learning_rate": 1.222816478076379e-05,
      "loss": 8.0775,
      "step": 17100
    },
    {
      "epoch": 5.292707447219859,
      "grad_norm": 0.7833654284477234,
      "learning_rate": 1.2206064356435643e-05,
      "loss": 8.0969,
      "step": 17110
    },
    {
      "epoch": 5.2958007888021035,
      "grad_norm": 0.9430662393569946,
      "learning_rate": 1.2183963932107496e-05,
      "loss": 8.1017,
      "step": 17120
    },
    {
      "epoch": 5.2988941303843475,
      "grad_norm": 0.5098643898963928,
      "learning_rate": 1.216186350777935e-05,
      "loss": 8.0932,
      "step": 17130
    },
    {
      "epoch": 5.301987471966592,
      "grad_norm": 0.7392948269844055,
      "learning_rate": 1.2139763083451203e-05,
      "loss": 8.096,
      "step": 17140
    },
    {
      "epoch": 5.3050808135488365,
      "grad_norm": 0.6591774225234985,
      "learning_rate": 1.2117662659123055e-05,
      "loss": 8.0935,
      "step": 17150
    },
    {
      "epoch": 5.308174155131081,
      "grad_norm": 0.8362192511558533,
      "learning_rate": 1.209556223479491e-05,
      "loss": 8.1031,
      "step": 17160
    },
    {
      "epoch": 5.311267496713325,
      "grad_norm": 0.7024705410003662,
      "learning_rate": 1.207346181046676e-05,
      "loss": 8.1078,
      "step": 17170
    },
    {
      "epoch": 5.314360838295569,
      "grad_norm": 0.5966013669967651,
      "learning_rate": 1.2051361386138613e-05,
      "loss": 8.1048,
      "step": 17180
    },
    {
      "epoch": 5.317454179877813,
      "grad_norm": 0.8532769083976746,
      "learning_rate": 1.2029260961810468e-05,
      "loss": 8.1012,
      "step": 17190
    },
    {
      "epoch": 5.320547521460057,
      "grad_norm": 0.7380558252334595,
      "learning_rate": 1.200716053748232e-05,
      "loss": 8.0995,
      "step": 17200
    },
    {
      "epoch": 5.323640863042302,
      "grad_norm": 0.8822140097618103,
      "learning_rate": 1.1985060113154173e-05,
      "loss": 8.1006,
      "step": 17210
    },
    {
      "epoch": 5.326734204624546,
      "grad_norm": 0.41259196400642395,
      "learning_rate": 1.1962959688826026e-05,
      "loss": 8.0913,
      "step": 17220
    },
    {
      "epoch": 5.32982754620679,
      "grad_norm": 0.616011381149292,
      "learning_rate": 1.194085926449788e-05,
      "loss": 8.0927,
      "step": 17230
    },
    {
      "epoch": 5.332920887789034,
      "grad_norm": 0.7059027552604675,
      "learning_rate": 1.1918758840169731e-05,
      "loss": 8.1104,
      "step": 17240
    },
    {
      "epoch": 5.336014229371278,
      "grad_norm": 0.8947246670722961,
      "learning_rate": 1.1896658415841584e-05,
      "loss": 8.1092,
      "step": 17250
    },
    {
      "epoch": 5.339107570953523,
      "grad_norm": 0.6884945034980774,
      "learning_rate": 1.1874557991513438e-05,
      "loss": 8.091,
      "step": 17260
    },
    {
      "epoch": 5.342200912535767,
      "grad_norm": 0.6984691619873047,
      "learning_rate": 1.185245756718529e-05,
      "loss": 8.0846,
      "step": 17270
    },
    {
      "epoch": 5.345294254118011,
      "grad_norm": 0.4718811511993408,
      "learning_rate": 1.1830357142857143e-05,
      "loss": 8.1074,
      "step": 17280
    },
    {
      "epoch": 5.348387595700255,
      "grad_norm": 0.778989851474762,
      "learning_rate": 1.1808256718528997e-05,
      "loss": 8.0924,
      "step": 17290
    },
    {
      "epoch": 5.351480937282499,
      "grad_norm": 0.6647940874099731,
      "learning_rate": 1.178615629420085e-05,
      "loss": 8.101,
      "step": 17300
    },
    {
      "epoch": 5.354574278864743,
      "grad_norm": 0.8247287273406982,
      "learning_rate": 1.1764055869872701e-05,
      "loss": 8.094,
      "step": 17310
    },
    {
      "epoch": 5.357667620446988,
      "grad_norm": 0.7737724184989929,
      "learning_rate": 1.1741955445544554e-05,
      "loss": 8.1024,
      "step": 17320
    },
    {
      "epoch": 5.360760962029232,
      "grad_norm": 0.7554641366004944,
      "learning_rate": 1.1719855021216408e-05,
      "loss": 8.1057,
      "step": 17330
    },
    {
      "epoch": 5.363854303611476,
      "grad_norm": 0.9327564835548401,
      "learning_rate": 1.169775459688826e-05,
      "loss": 8.1102,
      "step": 17340
    },
    {
      "epoch": 5.36694764519372,
      "grad_norm": 0.6420229077339172,
      "learning_rate": 1.1675654172560113e-05,
      "loss": 8.102,
      "step": 17350
    },
    {
      "epoch": 5.370040986775964,
      "grad_norm": 0.8326901197433472,
      "learning_rate": 1.1653553748231968e-05,
      "loss": 8.1112,
      "step": 17360
    },
    {
      "epoch": 5.373134328358209,
      "grad_norm": 0.8969852924346924,
      "learning_rate": 1.163145332390382e-05,
      "loss": 8.1051,
      "step": 17370
    },
    {
      "epoch": 5.376227669940453,
      "grad_norm": 0.6861928701400757,
      "learning_rate": 1.1609352899575671e-05,
      "loss": 8.1026,
      "step": 17380
    },
    {
      "epoch": 5.3793210115226975,
      "grad_norm": 0.7043851613998413,
      "learning_rate": 1.1587252475247526e-05,
      "loss": 8.1018,
      "step": 17390
    },
    {
      "epoch": 5.3824143531049415,
      "grad_norm": 0.8680208921432495,
      "learning_rate": 1.1565152050919378e-05,
      "loss": 8.1176,
      "step": 17400
    },
    {
      "epoch": 5.385507694687186,
      "grad_norm": 0.935437798500061,
      "learning_rate": 1.154305162659123e-05,
      "loss": 8.1093,
      "step": 17410
    },
    {
      "epoch": 5.38860103626943,
      "grad_norm": 0.5092025399208069,
      "learning_rate": 1.1520951202263083e-05,
      "loss": 8.1118,
      "step": 17420
    },
    {
      "epoch": 5.391694377851675,
      "grad_norm": 0.797340452671051,
      "learning_rate": 1.1498850777934938e-05,
      "loss": 8.1061,
      "step": 17430
    },
    {
      "epoch": 5.394787719433919,
      "grad_norm": 0.5744377970695496,
      "learning_rate": 1.147675035360679e-05,
      "loss": 8.0983,
      "step": 17440
    },
    {
      "epoch": 5.397881061016163,
      "grad_norm": 0.696394681930542,
      "learning_rate": 1.1454649929278641e-05,
      "loss": 8.1035,
      "step": 17450
    },
    {
      "epoch": 5.400974402598407,
      "grad_norm": 0.8272891044616699,
      "learning_rate": 1.1432549504950496e-05,
      "loss": 8.1006,
      "step": 17460
    },
    {
      "epoch": 5.404067744180651,
      "grad_norm": 0.6487737894058228,
      "learning_rate": 1.1410449080622348e-05,
      "loss": 8.0989,
      "step": 17470
    },
    {
      "epoch": 5.407161085762896,
      "grad_norm": 0.7546008229255676,
      "learning_rate": 1.1388348656294201e-05,
      "loss": 8.1039,
      "step": 17480
    },
    {
      "epoch": 5.41025442734514,
      "grad_norm": 0.9264069199562073,
      "learning_rate": 1.1366248231966055e-05,
      "loss": 8.0875,
      "step": 17490
    },
    {
      "epoch": 5.413347768927384,
      "grad_norm": 0.7738224864006042,
      "learning_rate": 1.1344147807637908e-05,
      "loss": 8.0876,
      "step": 17500
    },
    {
      "epoch": 5.416441110509628,
      "grad_norm": 0.8117641806602478,
      "learning_rate": 1.132204738330976e-05,
      "loss": 8.087,
      "step": 17510
    },
    {
      "epoch": 5.419534452091872,
      "grad_norm": 0.6690555810928345,
      "learning_rate": 1.1299946958981612e-05,
      "loss": 8.0983,
      "step": 17520
    },
    {
      "epoch": 5.422627793674116,
      "grad_norm": 0.428462952375412,
      "learning_rate": 1.1277846534653466e-05,
      "loss": 8.0975,
      "step": 17530
    },
    {
      "epoch": 5.425721135256361,
      "grad_norm": 0.3702941834926605,
      "learning_rate": 1.1255746110325319e-05,
      "loss": 8.0966,
      "step": 17540
    },
    {
      "epoch": 5.428814476838605,
      "grad_norm": 0.6794368624687195,
      "learning_rate": 1.1233645685997171e-05,
      "loss": 8.0947,
      "step": 17550
    },
    {
      "epoch": 5.431907818420849,
      "grad_norm": 0.7506880164146423,
      "learning_rate": 1.1211545261669025e-05,
      "loss": 8.1156,
      "step": 17560
    },
    {
      "epoch": 5.435001160003093,
      "grad_norm": 0.9564061164855957,
      "learning_rate": 1.1189444837340878e-05,
      "loss": 8.1124,
      "step": 17570
    },
    {
      "epoch": 5.438094501585337,
      "grad_norm": 0.8979299068450928,
      "learning_rate": 1.116734441301273e-05,
      "loss": 8.1124,
      "step": 17580
    },
    {
      "epoch": 5.441187843167582,
      "grad_norm": 0.8475054502487183,
      "learning_rate": 1.1145243988684583e-05,
      "loss": 8.1072,
      "step": 17590
    },
    {
      "epoch": 5.444281184749826,
      "grad_norm": 0.7598840594291687,
      "learning_rate": 1.1123143564356436e-05,
      "loss": 8.0906,
      "step": 17600
    },
    {
      "epoch": 5.44737452633207,
      "grad_norm": 0.8860732913017273,
      "learning_rate": 1.1101043140028289e-05,
      "loss": 8.0985,
      "step": 17610
    },
    {
      "epoch": 5.450467867914314,
      "grad_norm": 0.6329754590988159,
      "learning_rate": 1.1078942715700141e-05,
      "loss": 8.0956,
      "step": 17620
    },
    {
      "epoch": 5.453561209496558,
      "grad_norm": 0.6848506927490234,
      "learning_rate": 1.1056842291371996e-05,
      "loss": 8.1123,
      "step": 17630
    },
    {
      "epoch": 5.4566545510788025,
      "grad_norm": 0.7074747085571289,
      "learning_rate": 1.1034741867043848e-05,
      "loss": 8.1032,
      "step": 17640
    },
    {
      "epoch": 5.459747892661047,
      "grad_norm": 0.7331995368003845,
      "learning_rate": 1.1012641442715701e-05,
      "loss": 8.1012,
      "step": 17650
    },
    {
      "epoch": 5.4628412342432915,
      "grad_norm": 0.9864194989204407,
      "learning_rate": 1.0990541018387554e-05,
      "loss": 8.1064,
      "step": 17660
    },
    {
      "epoch": 5.4659345758255355,
      "grad_norm": 0.4296112656593323,
      "learning_rate": 1.0968440594059406e-05,
      "loss": 8.0973,
      "step": 17670
    },
    {
      "epoch": 5.46902791740778,
      "grad_norm": 0.7155394554138184,
      "learning_rate": 1.0946340169731259e-05,
      "loss": 8.1041,
      "step": 17680
    },
    {
      "epoch": 5.472121258990024,
      "grad_norm": 0.6018670797348022,
      "learning_rate": 1.0924239745403113e-05,
      "loss": 8.0974,
      "step": 17690
    },
    {
      "epoch": 5.475214600572269,
      "grad_norm": 0.7443027496337891,
      "learning_rate": 1.0902139321074966e-05,
      "loss": 8.0989,
      "step": 17700
    },
    {
      "epoch": 5.478307942154513,
      "grad_norm": 0.7402263283729553,
      "learning_rate": 1.0880038896746818e-05,
      "loss": 8.0932,
      "step": 17710
    },
    {
      "epoch": 5.481401283736757,
      "grad_norm": 0.6602736115455627,
      "learning_rate": 1.0857938472418671e-05,
      "loss": 8.1002,
      "step": 17720
    },
    {
      "epoch": 5.484494625319001,
      "grad_norm": 0.6829214692115784,
      "learning_rate": 1.0835838048090524e-05,
      "loss": 8.0955,
      "step": 17730
    },
    {
      "epoch": 5.487587966901245,
      "grad_norm": 0.720913290977478,
      "learning_rate": 1.0813737623762376e-05,
      "loss": 8.0978,
      "step": 17740
    },
    {
      "epoch": 5.490681308483489,
      "grad_norm": 1.0788335800170898,
      "learning_rate": 1.0791637199434229e-05,
      "loss": 8.1102,
      "step": 17750
    },
    {
      "epoch": 5.493774650065734,
      "grad_norm": 0.7382562160491943,
      "learning_rate": 1.0769536775106083e-05,
      "loss": 8.1189,
      "step": 17760
    },
    {
      "epoch": 5.496867991647978,
      "grad_norm": 0.9205969572067261,
      "learning_rate": 1.0747436350777936e-05,
      "loss": 8.106,
      "step": 17770
    },
    {
      "epoch": 5.499961333230222,
      "grad_norm": 0.424570769071579,
      "learning_rate": 1.0725335926449789e-05,
      "loss": 8.0919,
      "step": 17780
    },
    {
      "epoch": 5.503054674812466,
      "grad_norm": 0.6454920768737793,
      "learning_rate": 1.0703235502121641e-05,
      "loss": 8.0972,
      "step": 17790
    },
    {
      "epoch": 5.50614801639471,
      "grad_norm": 0.884508490562439,
      "learning_rate": 1.0681135077793494e-05,
      "loss": 8.0907,
      "step": 17800
    },
    {
      "epoch": 5.509241357976954,
      "grad_norm": 0.5198333859443665,
      "learning_rate": 1.0659034653465347e-05,
      "loss": 8.1046,
      "step": 17810
    },
    {
      "epoch": 5.512334699559199,
      "grad_norm": 0.7012289762496948,
      "learning_rate": 1.0636934229137199e-05,
      "loss": 8.0828,
      "step": 17820
    },
    {
      "epoch": 5.515428041141443,
      "grad_norm": 0.4790947735309601,
      "learning_rate": 1.0614833804809053e-05,
      "loss": 8.0917,
      "step": 17830
    },
    {
      "epoch": 5.518521382723687,
      "grad_norm": 0.6448053121566772,
      "learning_rate": 1.0592733380480906e-05,
      "loss": 8.1103,
      "step": 17840
    },
    {
      "epoch": 5.521614724305931,
      "grad_norm": 0.668482780456543,
      "learning_rate": 1.0570632956152759e-05,
      "loss": 8.1107,
      "step": 17850
    },
    {
      "epoch": 5.524708065888175,
      "grad_norm": 0.5587652325630188,
      "learning_rate": 1.0548532531824611e-05,
      "loss": 8.0976,
      "step": 17860
    },
    {
      "epoch": 5.52780140747042,
      "grad_norm": 0.7359919548034668,
      "learning_rate": 1.0526432107496464e-05,
      "loss": 8.0901,
      "step": 17870
    },
    {
      "epoch": 5.530894749052664,
      "grad_norm": 0.6524255275726318,
      "learning_rate": 1.0504331683168317e-05,
      "loss": 8.102,
      "step": 17880
    },
    {
      "epoch": 5.533988090634908,
      "grad_norm": 0.7672533392906189,
      "learning_rate": 1.0482231258840171e-05,
      "loss": 8.0988,
      "step": 17890
    },
    {
      "epoch": 5.537081432217152,
      "grad_norm": 0.8043408989906311,
      "learning_rate": 1.0460130834512024e-05,
      "loss": 8.0815,
      "step": 17900
    },
    {
      "epoch": 5.5401747737993965,
      "grad_norm": 0.6729921102523804,
      "learning_rate": 1.0438030410183876e-05,
      "loss": 8.105,
      "step": 17910
    },
    {
      "epoch": 5.543268115381641,
      "grad_norm": 0.8838595747947693,
      "learning_rate": 1.0415929985855729e-05,
      "loss": 8.1052,
      "step": 17920
    },
    {
      "epoch": 5.5463614569638855,
      "grad_norm": 0.5975838303565979,
      "learning_rate": 1.0393829561527582e-05,
      "loss": 8.1026,
      "step": 17930
    },
    {
      "epoch": 5.5494547985461296,
      "grad_norm": 0.8637212514877319,
      "learning_rate": 1.0371729137199434e-05,
      "loss": 8.1014,
      "step": 17940
    },
    {
      "epoch": 5.552548140128374,
      "grad_norm": 0.7515595555305481,
      "learning_rate": 1.0349628712871287e-05,
      "loss": 8.105,
      "step": 17950
    },
    {
      "epoch": 5.555641481710618,
      "grad_norm": 0.6111090183258057,
      "learning_rate": 1.0327528288543141e-05,
      "loss": 8.0997,
      "step": 17960
    },
    {
      "epoch": 5.558734823292862,
      "grad_norm": 0.7627365589141846,
      "learning_rate": 1.0305427864214994e-05,
      "loss": 8.0974,
      "step": 17970
    },
    {
      "epoch": 5.561828164875107,
      "grad_norm": 0.8257339000701904,
      "learning_rate": 1.0283327439886846e-05,
      "loss": 8.1115,
      "step": 17980
    },
    {
      "epoch": 5.564921506457351,
      "grad_norm": 0.8570767641067505,
      "learning_rate": 1.0261227015558699e-05,
      "loss": 8.1152,
      "step": 17990
    },
    {
      "epoch": 5.568014848039595,
      "grad_norm": 0.577887237071991,
      "learning_rate": 1.0239126591230552e-05,
      "loss": 8.1011,
      "step": 18000
    },
    {
      "epoch": 5.571108189621839,
      "grad_norm": 0.5842135548591614,
      "learning_rate": 1.0217026166902404e-05,
      "loss": 8.0986,
      "step": 18010
    },
    {
      "epoch": 5.574201531204083,
      "grad_norm": 0.7137885093688965,
      "learning_rate": 1.0194925742574257e-05,
      "loss": 8.1065,
      "step": 18020
    },
    {
      "epoch": 5.577294872786327,
      "grad_norm": 0.6788021326065063,
      "learning_rate": 1.0172825318246111e-05,
      "loss": 8.1068,
      "step": 18030
    },
    {
      "epoch": 5.580388214368572,
      "grad_norm": 0.6302919387817383,
      "learning_rate": 1.0150724893917964e-05,
      "loss": 8.0892,
      "step": 18040
    },
    {
      "epoch": 5.583481555950816,
      "grad_norm": 0.9019985198974609,
      "learning_rate": 1.0128624469589817e-05,
      "loss": 8.098,
      "step": 18050
    },
    {
      "epoch": 5.58657489753306,
      "grad_norm": 0.7397143244743347,
      "learning_rate": 1.010652404526167e-05,
      "loss": 8.0993,
      "step": 18060
    },
    {
      "epoch": 5.589668239115304,
      "grad_norm": 0.8792948126792908,
      "learning_rate": 1.0084423620933522e-05,
      "loss": 8.0958,
      "step": 18070
    },
    {
      "epoch": 5.592761580697548,
      "grad_norm": 0.7423568367958069,
      "learning_rate": 1.0062323196605375e-05,
      "loss": 8.1043,
      "step": 18080
    },
    {
      "epoch": 5.595854922279793,
      "grad_norm": 0.5572481155395508,
      "learning_rate": 1.0040222772277229e-05,
      "loss": 8.1223,
      "step": 18090
    },
    {
      "epoch": 5.598948263862037,
      "grad_norm": 0.7938687205314636,
      "learning_rate": 1.0018122347949081e-05,
      "loss": 8.1083,
      "step": 18100
    },
    {
      "epoch": 5.602041605444281,
      "grad_norm": 0.9278644323348999,
      "learning_rate": 9.996021923620934e-06,
      "loss": 8.0969,
      "step": 18110
    },
    {
      "epoch": 5.605134947026525,
      "grad_norm": 0.8025288581848145,
      "learning_rate": 9.973921499292787e-06,
      "loss": 8.0984,
      "step": 18120
    },
    {
      "epoch": 5.608228288608769,
      "grad_norm": 0.521286129951477,
      "learning_rate": 9.95182107496464e-06,
      "loss": 8.1053,
      "step": 18130
    },
    {
      "epoch": 5.611321630191014,
      "grad_norm": 0.6725338697433472,
      "learning_rate": 9.929720650636492e-06,
      "loss": 8.0836,
      "step": 18140
    },
    {
      "epoch": 5.614414971773258,
      "grad_norm": 0.8196897506713867,
      "learning_rate": 9.907620226308345e-06,
      "loss": 8.0979,
      "step": 18150
    },
    {
      "epoch": 5.617508313355502,
      "grad_norm": 0.7054080367088318,
      "learning_rate": 9.885519801980199e-06,
      "loss": 8.1029,
      "step": 18160
    },
    {
      "epoch": 5.620601654937746,
      "grad_norm": 0.5798433423042297,
      "learning_rate": 9.863419377652052e-06,
      "loss": 8.0897,
      "step": 18170
    },
    {
      "epoch": 5.6236949965199905,
      "grad_norm": 0.7606979608535767,
      "learning_rate": 9.841318953323904e-06,
      "loss": 8.1048,
      "step": 18180
    },
    {
      "epoch": 5.6267883381022346,
      "grad_norm": 0.5427142977714539,
      "learning_rate": 9.819218528995759e-06,
      "loss": 8.0874,
      "step": 18190
    },
    {
      "epoch": 5.6298816796844795,
      "grad_norm": 0.413700670003891,
      "learning_rate": 9.79711810466761e-06,
      "loss": 8.1076,
      "step": 18200
    },
    {
      "epoch": 5.632975021266724,
      "grad_norm": 0.7689654231071472,
      "learning_rate": 9.775017680339462e-06,
      "loss": 8.0899,
      "step": 18210
    },
    {
      "epoch": 5.636068362848968,
      "grad_norm": 0.6892092227935791,
      "learning_rate": 9.752917256011315e-06,
      "loss": 8.1066,
      "step": 18220
    },
    {
      "epoch": 5.639161704431212,
      "grad_norm": 0.4881833493709564,
      "learning_rate": 9.73081683168317e-06,
      "loss": 8.1162,
      "step": 18230
    },
    {
      "epoch": 5.642255046013456,
      "grad_norm": 0.7593145370483398,
      "learning_rate": 9.708716407355022e-06,
      "loss": 8.1028,
      "step": 18240
    },
    {
      "epoch": 5.6453483875957,
      "grad_norm": 0.8456023335456848,
      "learning_rate": 9.686615983026874e-06,
      "loss": 8.0928,
      "step": 18250
    },
    {
      "epoch": 5.648441729177945,
      "grad_norm": 0.998232901096344,
      "learning_rate": 9.664515558698729e-06,
      "loss": 8.1047,
      "step": 18260
    },
    {
      "epoch": 5.651535070760189,
      "grad_norm": 0.8542212247848511,
      "learning_rate": 9.64241513437058e-06,
      "loss": 8.1048,
      "step": 18270
    },
    {
      "epoch": 5.654628412342433,
      "grad_norm": 0.6913552284240723,
      "learning_rate": 9.620314710042432e-06,
      "loss": 8.1154,
      "step": 18280
    },
    {
      "epoch": 5.657721753924677,
      "grad_norm": 0.5154241323471069,
      "learning_rate": 9.598214285714287e-06,
      "loss": 8.0868,
      "step": 18290
    },
    {
      "epoch": 5.660815095506921,
      "grad_norm": 0.8247050046920776,
      "learning_rate": 9.57611386138614e-06,
      "loss": 8.101,
      "step": 18300
    },
    {
      "epoch": 5.663908437089166,
      "grad_norm": 0.5278342962265015,
      "learning_rate": 9.554013437057992e-06,
      "loss": 8.0998,
      "step": 18310
    },
    {
      "epoch": 5.66700177867141,
      "grad_norm": 0.7571467757225037,
      "learning_rate": 9.531913012729845e-06,
      "loss": 8.1085,
      "step": 18320
    },
    {
      "epoch": 5.670095120253654,
      "grad_norm": 0.8068660497665405,
      "learning_rate": 9.509812588401697e-06,
      "loss": 8.0931,
      "step": 18330
    },
    {
      "epoch": 5.673188461835898,
      "grad_norm": 0.5510546565055847,
      "learning_rate": 9.48771216407355e-06,
      "loss": 8.0954,
      "step": 18340
    },
    {
      "epoch": 5.676281803418142,
      "grad_norm": 0.43498072028160095,
      "learning_rate": 9.465611739745403e-06,
      "loss": 8.102,
      "step": 18350
    },
    {
      "epoch": 5.679375145000387,
      "grad_norm": 0.8435181379318237,
      "learning_rate": 9.443511315417257e-06,
      "loss": 8.1108,
      "step": 18360
    },
    {
      "epoch": 5.682468486582631,
      "grad_norm": 0.7391342520713806,
      "learning_rate": 9.42141089108911e-06,
      "loss": 8.0858,
      "step": 18370
    },
    {
      "epoch": 5.685561828164875,
      "grad_norm": 0.641563355922699,
      "learning_rate": 9.399310466760962e-06,
      "loss": 8.1133,
      "step": 18380
    },
    {
      "epoch": 5.688655169747119,
      "grad_norm": 0.7332121133804321,
      "learning_rate": 9.377210042432816e-06,
      "loss": 8.1006,
      "step": 18390
    },
    {
      "epoch": 5.691748511329363,
      "grad_norm": 0.7361878156661987,
      "learning_rate": 9.355109618104667e-06,
      "loss": 8.106,
      "step": 18400
    },
    {
      "epoch": 5.694841852911607,
      "grad_norm": 0.6359607577323914,
      "learning_rate": 9.33300919377652e-06,
      "loss": 8.0938,
      "step": 18410
    },
    {
      "epoch": 5.697935194493852,
      "grad_norm": 0.9688820838928223,
      "learning_rate": 9.310908769448373e-06,
      "loss": 8.1006,
      "step": 18420
    },
    {
      "epoch": 5.701028536076096,
      "grad_norm": 0.41234931349754333,
      "learning_rate": 9.288808345120227e-06,
      "loss": 8.1089,
      "step": 18430
    },
    {
      "epoch": 5.7041218776583404,
      "grad_norm": 0.9978736042976379,
      "learning_rate": 9.26670792079208e-06,
      "loss": 8.1032,
      "step": 18440
    },
    {
      "epoch": 5.7072152192405845,
      "grad_norm": 0.8606383204460144,
      "learning_rate": 9.244607496463932e-06,
      "loss": 8.108,
      "step": 18450
    },
    {
      "epoch": 5.710308560822829,
      "grad_norm": 0.7504776120185852,
      "learning_rate": 9.222507072135787e-06,
      "loss": 8.0839,
      "step": 18460
    },
    {
      "epoch": 5.713401902405073,
      "grad_norm": 0.614591658115387,
      "learning_rate": 9.200406647807638e-06,
      "loss": 8.0956,
      "step": 18470
    },
    {
      "epoch": 5.716495243987318,
      "grad_norm": 0.6779970526695251,
      "learning_rate": 9.17830622347949e-06,
      "loss": 8.1007,
      "step": 18480
    },
    {
      "epoch": 5.719588585569562,
      "grad_norm": 0.528508722782135,
      "learning_rate": 9.156205799151345e-06,
      "loss": 8.0972,
      "step": 18490
    },
    {
      "epoch": 5.722681927151806,
      "grad_norm": 0.8984450101852417,
      "learning_rate": 9.134105374823197e-06,
      "loss": 8.0988,
      "step": 18500
    },
    {
      "epoch": 5.72577526873405,
      "grad_norm": 0.5055950880050659,
      "learning_rate": 9.11200495049505e-06,
      "loss": 8.107,
      "step": 18510
    },
    {
      "epoch": 5.728868610316294,
      "grad_norm": 0.7662785649299622,
      "learning_rate": 9.089904526166902e-06,
      "loss": 8.1012,
      "step": 18520
    },
    {
      "epoch": 5.731961951898539,
      "grad_norm": 0.7478196024894714,
      "learning_rate": 9.067804101838757e-06,
      "loss": 8.0982,
      "step": 18530
    },
    {
      "epoch": 5.735055293480783,
      "grad_norm": 0.6777620315551758,
      "learning_rate": 9.045703677510608e-06,
      "loss": 8.0882,
      "step": 18540
    },
    {
      "epoch": 5.738148635063027,
      "grad_norm": 0.609757661819458,
      "learning_rate": 9.02360325318246e-06,
      "loss": 8.0972,
      "step": 18550
    },
    {
      "epoch": 5.741241976645271,
      "grad_norm": 0.785984218120575,
      "learning_rate": 9.001502828854315e-06,
      "loss": 8.0895,
      "step": 18560
    },
    {
      "epoch": 5.744335318227515,
      "grad_norm": 0.7749826908111572,
      "learning_rate": 8.979402404526167e-06,
      "loss": 8.1053,
      "step": 18570
    },
    {
      "epoch": 5.74742865980976,
      "grad_norm": 0.7598055601119995,
      "learning_rate": 8.95730198019802e-06,
      "loss": 8.1003,
      "step": 18580
    },
    {
      "epoch": 5.750522001392004,
      "grad_norm": 0.6498898267745972,
      "learning_rate": 8.935201555869874e-06,
      "loss": 8.1078,
      "step": 18590
    },
    {
      "epoch": 5.753615342974248,
      "grad_norm": 0.5289391279220581,
      "learning_rate": 8.91531117397454e-06,
      "loss": 8.1009,
      "step": 18600
    },
    {
      "epoch": 5.756708684556492,
      "grad_norm": 0.4941353499889374,
      "learning_rate": 8.893210749646393e-06,
      "loss": 8.1158,
      "step": 18610
    },
    {
      "epoch": 5.759802026138736,
      "grad_norm": 0.4731060862541199,
      "learning_rate": 8.871110325318246e-06,
      "loss": 8.1104,
      "step": 18620
    },
    {
      "epoch": 5.76289536772098,
      "grad_norm": 0.69637531042099,
      "learning_rate": 8.849009900990099e-06,
      "loss": 8.0882,
      "step": 18630
    },
    {
      "epoch": 5.765988709303225,
      "grad_norm": 0.7704150676727295,
      "learning_rate": 8.826909476661953e-06,
      "loss": 8.1122,
      "step": 18640
    },
    {
      "epoch": 5.769082050885469,
      "grad_norm": 0.8681312799453735,
      "learning_rate": 8.804809052333806e-06,
      "loss": 8.1045,
      "step": 18650
    },
    {
      "epoch": 5.772175392467713,
      "grad_norm": 0.7187643051147461,
      "learning_rate": 8.782708628005658e-06,
      "loss": 8.0941,
      "step": 18660
    },
    {
      "epoch": 5.775268734049957,
      "grad_norm": 0.80228191614151,
      "learning_rate": 8.760608203677511e-06,
      "loss": 8.1105,
      "step": 18670
    },
    {
      "epoch": 5.778362075632201,
      "grad_norm": 0.6967880725860596,
      "learning_rate": 8.738507779349364e-06,
      "loss": 8.0894,
      "step": 18680
    },
    {
      "epoch": 5.7814554172144454,
      "grad_norm": 0.7685202360153198,
      "learning_rate": 8.716407355021216e-06,
      "loss": 8.0865,
      "step": 18690
    },
    {
      "epoch": 5.78454875879669,
      "grad_norm": 0.5034071207046509,
      "learning_rate": 8.694306930693069e-06,
      "loss": 8.1046,
      "step": 18700
    },
    {
      "epoch": 5.7876421003789345,
      "grad_norm": 0.6208709478378296,
      "learning_rate": 8.672206506364923e-06,
      "loss": 8.0996,
      "step": 18710
    },
    {
      "epoch": 5.7907354419611785,
      "grad_norm": 0.5704313516616821,
      "learning_rate": 8.650106082036776e-06,
      "loss": 8.1001,
      "step": 18720
    },
    {
      "epoch": 5.793828783543423,
      "grad_norm": 0.7332965731620789,
      "learning_rate": 8.628005657708629e-06,
      "loss": 8.1,
      "step": 18730
    },
    {
      "epoch": 5.796922125125667,
      "grad_norm": 0.623408317565918,
      "learning_rate": 8.605905233380481e-06,
      "loss": 8.086,
      "step": 18740
    },
    {
      "epoch": 5.800015466707912,
      "grad_norm": 0.4816455841064453,
      "learning_rate": 8.583804809052334e-06,
      "loss": 8.111,
      "step": 18750
    },
    {
      "epoch": 5.803108808290156,
      "grad_norm": 0.6057630777359009,
      "learning_rate": 8.561704384724186e-06,
      "loss": 8.1063,
      "step": 18760
    },
    {
      "epoch": 5.8062021498724,
      "grad_norm": 0.4548504054546356,
      "learning_rate": 8.53960396039604e-06,
      "loss": 8.1007,
      "step": 18770
    },
    {
      "epoch": 5.809295491454644,
      "grad_norm": 0.568464994430542,
      "learning_rate": 8.517503536067893e-06,
      "loss": 8.104,
      "step": 18780
    },
    {
      "epoch": 5.812388833036888,
      "grad_norm": 0.6556369662284851,
      "learning_rate": 8.495403111739746e-06,
      "loss": 8.0954,
      "step": 18790
    },
    {
      "epoch": 5.815482174619133,
      "grad_norm": 0.4834486246109009,
      "learning_rate": 8.473302687411599e-06,
      "loss": 8.108,
      "step": 18800
    },
    {
      "epoch": 5.818575516201377,
      "grad_norm": 0.7326622009277344,
      "learning_rate": 8.451202263083451e-06,
      "loss": 8.1049,
      "step": 18810
    },
    {
      "epoch": 5.821668857783621,
      "grad_norm": 0.6198880672454834,
      "learning_rate": 8.429101838755304e-06,
      "loss": 8.1107,
      "step": 18820
    },
    {
      "epoch": 5.824762199365865,
      "grad_norm": 0.6267486214637756,
      "learning_rate": 8.407001414427157e-06,
      "loss": 8.089,
      "step": 18830
    },
    {
      "epoch": 5.827855540948109,
      "grad_norm": 0.5493327975273132,
      "learning_rate": 8.384900990099011e-06,
      "loss": 8.0966,
      "step": 18840
    },
    {
      "epoch": 5.830948882530353,
      "grad_norm": 0.7062795162200928,
      "learning_rate": 8.362800565770864e-06,
      "loss": 8.1076,
      "step": 18850
    },
    {
      "epoch": 5.834042224112598,
      "grad_norm": 0.6356467604637146,
      "learning_rate": 8.340700141442716e-06,
      "loss": 8.0999,
      "step": 18860
    },
    {
      "epoch": 5.837135565694842,
      "grad_norm": 0.8182083964347839,
      "learning_rate": 8.318599717114569e-06,
      "loss": 8.0955,
      "step": 18870
    },
    {
      "epoch": 5.840228907277086,
      "grad_norm": 0.6523952484130859,
      "learning_rate": 8.296499292786421e-06,
      "loss": 8.1121,
      "step": 18880
    },
    {
      "epoch": 5.84332224885933,
      "grad_norm": 0.7767564058303833,
      "learning_rate": 8.274398868458274e-06,
      "loss": 8.1108,
      "step": 18890
    },
    {
      "epoch": 5.846415590441574,
      "grad_norm": 0.8805996179580688,
      "learning_rate": 8.252298444130127e-06,
      "loss": 8.0966,
      "step": 18900
    },
    {
      "epoch": 5.849508932023818,
      "grad_norm": 1.01856529712677,
      "learning_rate": 8.230198019801981e-06,
      "loss": 8.1005,
      "step": 18910
    },
    {
      "epoch": 5.852602273606063,
      "grad_norm": 0.6576774716377258,
      "learning_rate": 8.208097595473834e-06,
      "loss": 8.0964,
      "step": 18920
    },
    {
      "epoch": 5.855695615188307,
      "grad_norm": 0.6888189911842346,
      "learning_rate": 8.185997171145686e-06,
      "loss": 8.091,
      "step": 18930
    },
    {
      "epoch": 5.858788956770551,
      "grad_norm": 0.7007412314414978,
      "learning_rate": 8.163896746817539e-06,
      "loss": 8.108,
      "step": 18940
    },
    {
      "epoch": 5.861882298352795,
      "grad_norm": 0.6585376262664795,
      "learning_rate": 8.141796322489392e-06,
      "loss": 8.1093,
      "step": 18950
    },
    {
      "epoch": 5.8649756399350395,
      "grad_norm": 0.8626119494438171,
      "learning_rate": 8.119695898161244e-06,
      "loss": 8.096,
      "step": 18960
    },
    {
      "epoch": 5.868068981517284,
      "grad_norm": 0.8378884792327881,
      "learning_rate": 8.097595473833099e-06,
      "loss": 8.1073,
      "step": 18970
    },
    {
      "epoch": 5.8711623230995285,
      "grad_norm": 0.7587245106697083,
      "learning_rate": 8.075495049504951e-06,
      "loss": 8.1007,
      "step": 18980
    },
    {
      "epoch": 5.8742556646817725,
      "grad_norm": 0.8435784578323364,
      "learning_rate": 8.053394625176804e-06,
      "loss": 8.0967,
      "step": 18990
    },
    {
      "epoch": 5.877349006264017,
      "grad_norm": 1.1405563354492188,
      "learning_rate": 8.031294200848657e-06,
      "loss": 8.0864,
      "step": 19000
    },
    {
      "epoch": 5.880442347846261,
      "grad_norm": 0.8348748087882996,
      "learning_rate": 8.009193776520509e-06,
      "loss": 8.0983,
      "step": 19010
    },
    {
      "epoch": 5.883535689428506,
      "grad_norm": 0.7650048136711121,
      "learning_rate": 7.987093352192362e-06,
      "loss": 8.1028,
      "step": 19020
    },
    {
      "epoch": 5.88662903101075,
      "grad_norm": 0.7917642593383789,
      "learning_rate": 7.964992927864214e-06,
      "loss": 8.0943,
      "step": 19030
    },
    {
      "epoch": 5.889722372592994,
      "grad_norm": 0.6385145783424377,
      "learning_rate": 7.942892503536069e-06,
      "loss": 8.0836,
      "step": 19040
    },
    {
      "epoch": 5.892815714175238,
      "grad_norm": 0.6056700944900513,
      "learning_rate": 7.920792079207921e-06,
      "loss": 8.1043,
      "step": 19050
    },
    {
      "epoch": 5.895909055757482,
      "grad_norm": 0.752684473991394,
      "learning_rate": 7.898691654879774e-06,
      "loss": 8.0807,
      "step": 19060
    },
    {
      "epoch": 5.899002397339726,
      "grad_norm": 0.9300031661987305,
      "learning_rate": 7.876591230551628e-06,
      "loss": 8.0882,
      "step": 19070
    },
    {
      "epoch": 5.902095738921971,
      "grad_norm": 0.9388210773468018,
      "learning_rate": 7.85449080622348e-06,
      "loss": 8.1001,
      "step": 19080
    },
    {
      "epoch": 5.905189080504215,
      "grad_norm": 0.56050705909729,
      "learning_rate": 7.832390381895332e-06,
      "loss": 8.1082,
      "step": 19090
    },
    {
      "epoch": 5.908282422086459,
      "grad_norm": 0.782230794429779,
      "learning_rate": 7.810289957567185e-06,
      "loss": 8.0895,
      "step": 19100
    },
    {
      "epoch": 5.911375763668703,
      "grad_norm": 0.7185804843902588,
      "learning_rate": 7.788189533239039e-06,
      "loss": 8.1007,
      "step": 19110
    },
    {
      "epoch": 5.914469105250947,
      "grad_norm": 0.6558647751808167,
      "learning_rate": 7.766089108910892e-06,
      "loss": 8.0852,
      "step": 19120
    },
    {
      "epoch": 5.917562446833191,
      "grad_norm": 0.9369121193885803,
      "learning_rate": 7.743988684582744e-06,
      "loss": 8.1021,
      "step": 19130
    },
    {
      "epoch": 5.920655788415436,
      "grad_norm": 0.6783807277679443,
      "learning_rate": 7.721888260254599e-06,
      "loss": 8.1025,
      "step": 19140
    },
    {
      "epoch": 5.92374912999768,
      "grad_norm": 0.6189871430397034,
      "learning_rate": 7.69978783592645e-06,
      "loss": 8.0857,
      "step": 19150
    },
    {
      "epoch": 5.926842471579924,
      "grad_norm": 0.7621331214904785,
      "learning_rate": 7.677687411598302e-06,
      "loss": 8.1004,
      "step": 19160
    },
    {
      "epoch": 5.929935813162168,
      "grad_norm": 0.7914484143257141,
      "learning_rate": 7.655586987270156e-06,
      "loss": 8.0904,
      "step": 19170
    },
    {
      "epoch": 5.933029154744412,
      "grad_norm": 0.7029533982276917,
      "learning_rate": 7.633486562942009e-06,
      "loss": 8.1121,
      "step": 19180
    },
    {
      "epoch": 5.936122496326657,
      "grad_norm": 0.9589506983757019,
      "learning_rate": 7.611386138613862e-06,
      "loss": 8.1184,
      "step": 19190
    },
    {
      "epoch": 5.939215837908901,
      "grad_norm": 0.5053191184997559,
      "learning_rate": 7.589285714285714e-06,
      "loss": 8.1058,
      "step": 19200
    },
    {
      "epoch": 5.942309179491145,
      "grad_norm": 0.6750139594078064,
      "learning_rate": 7.567185289957568e-06,
      "loss": 8.0911,
      "step": 19210
    },
    {
      "epoch": 5.945402521073389,
      "grad_norm": 0.5934101343154907,
      "learning_rate": 7.5450848656294205e-06,
      "loss": 8.0825,
      "step": 19220
    },
    {
      "epoch": 5.9484958626556335,
      "grad_norm": 0.8027035593986511,
      "learning_rate": 7.522984441301273e-06,
      "loss": 8.0895,
      "step": 19230
    },
    {
      "epoch": 5.951589204237878,
      "grad_norm": 0.7709491848945618,
      "learning_rate": 7.500884016973127e-06,
      "loss": 8.0842,
      "step": 19240
    },
    {
      "epoch": 5.9546825458201225,
      "grad_norm": 0.6756032109260559,
      "learning_rate": 7.478783592644979e-06,
      "loss": 8.1017,
      "step": 19250
    },
    {
      "epoch": 5.9577758874023665,
      "grad_norm": 0.7944580912590027,
      "learning_rate": 7.456683168316832e-06,
      "loss": 8.082,
      "step": 19260
    },
    {
      "epoch": 5.960869228984611,
      "grad_norm": 0.6070273518562317,
      "learning_rate": 7.434582743988685e-06,
      "loss": 8.0932,
      "step": 19270
    },
    {
      "epoch": 5.963962570566855,
      "grad_norm": 0.8665937185287476,
      "learning_rate": 7.412482319660538e-06,
      "loss": 8.1027,
      "step": 19280
    },
    {
      "epoch": 5.967055912149099,
      "grad_norm": 0.7136266231536865,
      "learning_rate": 7.390381895332391e-06,
      "loss": 8.0833,
      "step": 19290
    },
    {
      "epoch": 5.970149253731344,
      "grad_norm": 0.64417564868927,
      "learning_rate": 7.368281471004243e-06,
      "loss": 8.0889,
      "step": 19300
    },
    {
      "epoch": 5.973242595313588,
      "grad_norm": 0.6690847873687744,
      "learning_rate": 7.346181046676097e-06,
      "loss": 8.0887,
      "step": 19310
    },
    {
      "epoch": 5.976335936895832,
      "grad_norm": 0.6755616664886475,
      "learning_rate": 7.324080622347949e-06,
      "loss": 8.1011,
      "step": 19320
    },
    {
      "epoch": 5.979429278478076,
      "grad_norm": 0.7905982136726379,
      "learning_rate": 7.301980198019802e-06,
      "loss": 8.0814,
      "step": 19330
    },
    {
      "epoch": 5.98252262006032,
      "grad_norm": 0.805712103843689,
      "learning_rate": 7.2798797736916555e-06,
      "loss": 8.0909,
      "step": 19340
    },
    {
      "epoch": 5.985615961642564,
      "grad_norm": 0.751884400844574,
      "learning_rate": 7.257779349363508e-06,
      "loss": 8.102,
      "step": 19350
    },
    {
      "epoch": 5.988709303224809,
      "grad_norm": 0.6069028377532959,
      "learning_rate": 7.235678925035361e-06,
      "loss": 8.1064,
      "step": 19360
    },
    {
      "epoch": 5.991802644807053,
      "grad_norm": 0.829917848110199,
      "learning_rate": 7.213578500707214e-06,
      "loss": 8.1062,
      "step": 19370
    },
    {
      "epoch": 5.994895986389297,
      "grad_norm": 0.6678981781005859,
      "learning_rate": 7.191478076379067e-06,
      "loss": 8.113,
      "step": 19380
    },
    {
      "epoch": 5.997989327971541,
      "grad_norm": 0.6955202221870422,
      "learning_rate": 7.1693776520509195e-06,
      "loss": 8.0934,
      "step": 19390
    }
  ],
  "logging_steps": 10,
  "max_steps": 22624,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 7,
  "save_steps": 500,
  "total_flos": 8.049802323706675e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
