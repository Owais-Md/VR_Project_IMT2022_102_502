{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9997679993813317,
  "eval_steps": 500,
  "global_step": 3232,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 2.2978525161743164,
      "learning_rate": 4.9982319660537485e-05,
      "loss": 10.3112,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.697009563446045,
      "learning_rate": 4.9962429278642154e-05,
      "loss": 10.1044,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 1.9493228197097778,
      "learning_rate": 4.9940328854314005e-05,
      "loss": 9.9147,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 1.2537775039672852,
      "learning_rate": 4.9918228429985856e-05,
      "loss": 9.7383,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 0.9784691333770752,
      "learning_rate": 4.9896128005657714e-05,
      "loss": 9.5668,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 1.3054183721542358,
      "learning_rate": 4.9874027581329565e-05,
      "loss": 9.4386,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 1.3254427909851074,
      "learning_rate": 4.9851927157001416e-05,
      "loss": 9.3105,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.8787651062011719,
      "learning_rate": 4.982982673267327e-05,
      "loss": 9.1853,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.7771647572517395,
      "learning_rate": 4.980772630834512e-05,
      "loss": 9.0639,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.7927806377410889,
      "learning_rate": 4.9785625884016976e-05,
      "loss": 8.9485,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 19.9823055267334,
      "learning_rate": 4.976352545968883e-05,
      "loss": 8.8797,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.7020925879478455,
      "learning_rate": 4.9741425035360684e-05,
      "loss": 8.7873,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.6891294717788696,
      "learning_rate": 4.9719324611032535e-05,
      "loss": 8.7412,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.6724461317062378,
      "learning_rate": 4.9697224186704386e-05,
      "loss": 8.6824,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.6063492298126221,
      "learning_rate": 4.9675123762376244e-05,
      "loss": 8.6219,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.649882435798645,
      "learning_rate": 4.9653023338048095e-05,
      "loss": 8.576,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.8346493244171143,
      "learning_rate": 4.9630922913719946e-05,
      "loss": 8.5714,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.7668482661247253,
      "learning_rate": 4.96088224893918e-05,
      "loss": 8.4979,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.81424480676651,
      "learning_rate": 4.9586722065063654e-05,
      "loss": 8.4936,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 1.412393569946289,
      "learning_rate": 4.9564621640735505e-05,
      "loss": 8.4625,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 0.9642157554626465,
      "learning_rate": 4.9542521216407356e-05,
      "loss": 8.439,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.5858935713768005,
      "learning_rate": 4.952042079207921e-05,
      "loss": 8.4209,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 1.2992138862609863,
      "learning_rate": 4.949832036775106e-05,
      "loss": 8.3929,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 1.6960211992263794,
      "learning_rate": 4.9476219943422916e-05,
      "loss": 8.4018,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 1.0526018142700195,
      "learning_rate": 4.9454119519094774e-05,
      "loss": 8.374,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 0.9247509241104126,
      "learning_rate": 4.9432019094766625e-05,
      "loss": 8.3619,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.650263249874115,
      "learning_rate": 4.9409918670438475e-05,
      "loss": 8.3572,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 0.8367137908935547,
      "learning_rate": 4.9387818246110326e-05,
      "loss": 8.3374,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 0.5976635813713074,
      "learning_rate": 4.9365717821782184e-05,
      "loss": 8.3021,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 1.133556604385376,
      "learning_rate": 4.9343617397454035e-05,
      "loss": 8.3094,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 2.212977409362793,
      "learning_rate": 4.9321516973125886e-05,
      "loss": 8.2983,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 0.8591291308403015,
      "learning_rate": 4.929941654879774e-05,
      "loss": 8.254,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.7119815349578857,
      "learning_rate": 4.9277316124469595e-05,
      "loss": 8.2402,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.860245406627655,
      "learning_rate": 4.9255215700141446e-05,
      "loss": 8.245,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 1.2802064418792725,
      "learning_rate": 4.9233115275813297e-05,
      "loss": 8.2269,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 0.6481074690818787,
      "learning_rate": 4.921101485148515e-05,
      "loss": 8.2231,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 1.1971083879470825,
      "learning_rate": 4.9188914427157e-05,
      "loss": 8.2189,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 1.7580838203430176,
      "learning_rate": 4.9166814002828856e-05,
      "loss": 8.2223,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 1.2845189571380615,
      "learning_rate": 4.9144713578500714e-05,
      "loss": 8.2078,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 2.1474101543426514,
      "learning_rate": 4.9122613154172565e-05,
      "loss": 8.2149,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 0.6936149001121521,
      "learning_rate": 4.9100512729844416e-05,
      "loss": 8.1938,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 2.1909961700439453,
      "learning_rate": 4.907841230551627e-05,
      "loss": 8.2125,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 0.7333984971046448,
      "learning_rate": 4.9056311881188124e-05,
      "loss": 8.1978,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 1.0266189575195312,
      "learning_rate": 4.9034211456859975e-05,
      "loss": 8.2031,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 0.7094117403030396,
      "learning_rate": 4.9012111032531826e-05,
      "loss": 8.1962,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 1.4269354343414307,
      "learning_rate": 4.899001060820368e-05,
      "loss": 8.2066,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.6357672810554504,
      "learning_rate": 4.8967910183875535e-05,
      "loss": 8.1813,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 1.524726390838623,
      "learning_rate": 4.8945809759547386e-05,
      "loss": 8.1788,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 1.4942302703857422,
      "learning_rate": 4.892370933521924e-05,
      "loss": 8.1809,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.7273604273796082,
      "learning_rate": 4.890160891089109e-05,
      "loss": 8.1683,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 1.1210496425628662,
      "learning_rate": 4.8879508486562946e-05,
      "loss": 8.1691,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 1.029540777206421,
      "learning_rate": 4.8857408062234796e-05,
      "loss": 8.1916,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 1.556471347808838,
      "learning_rate": 4.8835307637906654e-05,
      "loss": 8.1873,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.7839841246604919,
      "learning_rate": 4.8813207213578505e-05,
      "loss": 8.1909,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 2.0269980430603027,
      "learning_rate": 4.8791106789250356e-05,
      "loss": 8.1842,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 1.3354445695877075,
      "learning_rate": 4.876900636492221e-05,
      "loss": 8.1838,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.6603084206581116,
      "learning_rate": 4.8746905940594065e-05,
      "loss": 8.1838,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 1.0205258131027222,
      "learning_rate": 4.8724805516265916e-05,
      "loss": 8.1684,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 0.6809381246566772,
      "learning_rate": 4.870270509193777e-05,
      "loss": 8.1736,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 0.7379045486450195,
      "learning_rate": 4.868060466760962e-05,
      "loss": 8.1663,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 1.0289238691329956,
      "learning_rate": 4.8658504243281475e-05,
      "loss": 8.1652,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 1.4722477197647095,
      "learning_rate": 4.8636403818953326e-05,
      "loss": 8.1719,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.985119640827179,
      "learning_rate": 4.861430339462518e-05,
      "loss": 8.1764,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 2.193415880203247,
      "learning_rate": 4.859220297029703e-05,
      "loss": 8.1621,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.7924662828445435,
      "learning_rate": 4.8570102545968886e-05,
      "loss": 8.1579,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.7754663228988647,
      "learning_rate": 4.854800212164074e-05,
      "loss": 8.1731,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 1.1253646612167358,
      "learning_rate": 4.8525901697312595e-05,
      "loss": 8.1679,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 1.7341458797454834,
      "learning_rate": 4.8503801272984445e-05,
      "loss": 8.1671,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 1.9493780136108398,
      "learning_rate": 4.8481700848656296e-05,
      "loss": 8.1735,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 3.367149829864502,
      "learning_rate": 4.845960042432815e-05,
      "loss": 8.1516,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 1.8500995635986328,
      "learning_rate": 4.8437500000000005e-05,
      "loss": 8.1614,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 0.8552808165550232,
      "learning_rate": 4.8415399575671856e-05,
      "loss": 8.1589,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 1.2967503070831299,
      "learning_rate": 4.839329915134371e-05,
      "loss": 8.1755,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 1.5053879022598267,
      "learning_rate": 4.837119872701556e-05,
      "loss": 8.1735,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 0.7907457947731018,
      "learning_rate": 4.8349098302687416e-05,
      "loss": 8.1565,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 0.8512012958526611,
      "learning_rate": 4.8326997878359267e-05,
      "loss": 8.1454,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 0.8684555292129517,
      "learning_rate": 4.830489745403112e-05,
      "loss": 8.1468,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 2.083754301071167,
      "learning_rate": 4.8282797029702975e-05,
      "loss": 8.1667,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 0.9785476922988892,
      "learning_rate": 4.8260696605374826e-05,
      "loss": 8.1745,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 0.8621866703033447,
      "learning_rate": 4.823859618104668e-05,
      "loss": 8.1734,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 1.065608024597168,
      "learning_rate": 4.8216495756718535e-05,
      "loss": 8.1508,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.7921701669692993,
      "learning_rate": 4.8194395332390386e-05,
      "loss": 8.1613,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.9583138823509216,
      "learning_rate": 4.817229490806224e-05,
      "loss": 8.1486,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 1.781218409538269,
      "learning_rate": 4.815019448373409e-05,
      "loss": 8.1601,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 1.411521315574646,
      "learning_rate": 4.8128094059405945e-05,
      "loss": 8.166,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.6862319707870483,
      "learning_rate": 4.8105993635077796e-05,
      "loss": 8.1452,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.910425066947937,
      "learning_rate": 4.808389321074965e-05,
      "loss": 8.162,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 1.6667001247406006,
      "learning_rate": 4.80617927864215e-05,
      "loss": 8.1382,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 1.0555663108825684,
      "learning_rate": 4.803969236209335e-05,
      "loss": 8.1397,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 1.4642082452774048,
      "learning_rate": 4.801759193776521e-05,
      "loss": 8.159,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.7943904399871826,
      "learning_rate": 4.799549151343706e-05,
      "loss": 8.1628,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 1.0083155632019043,
      "learning_rate": 4.7973391089108916e-05,
      "loss": 8.1653,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 0.8918737173080444,
      "learning_rate": 4.7951290664780766e-05,
      "loss": 8.1491,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 1.3468626737594604,
      "learning_rate": 4.792919024045262e-05,
      "loss": 8.1569,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 1.7926127910614014,
      "learning_rate": 4.7907089816124475e-05,
      "loss": 8.145,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 2.6235406398773193,
      "learning_rate": 4.7884989391796326e-05,
      "loss": 8.1438,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 1.613265037536621,
      "learning_rate": 4.786288896746818e-05,
      "loss": 8.1618,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 1.0502805709838867,
      "learning_rate": 4.784078854314003e-05,
      "loss": 8.1507,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.9201352596282959,
      "learning_rate": 4.7818688118811886e-05,
      "loss": 8.1586,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 0.9506177306175232,
      "learning_rate": 4.779658769448374e-05,
      "loss": 8.1499,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 0.8562129139900208,
      "learning_rate": 4.777448727015559e-05,
      "loss": 8.1545,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 1.137046456336975,
      "learning_rate": 4.775238684582744e-05,
      "loss": 8.1428,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 4.624128341674805,
      "learning_rate": 4.773028642149929e-05,
      "loss": 8.159,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 0.9169142246246338,
      "learning_rate": 4.770818599717115e-05,
      "loss": 8.1528,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 4.3146209716796875,
      "learning_rate": 4.7686085572843005e-05,
      "loss": 8.1421,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 1.1886427402496338,
      "learning_rate": 4.7663985148514856e-05,
      "loss": 8.1575,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 1.0226562023162842,
      "learning_rate": 4.764188472418671e-05,
      "loss": 8.1538,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 0.9351243376731873,
      "learning_rate": 4.761978429985856e-05,
      "loss": 8.1418,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.6706920862197876,
      "learning_rate": 4.7597683875530415e-05,
      "loss": 8.1471,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.1914315223693848,
      "learning_rate": 4.7575583451202266e-05,
      "loss": 8.1389,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 0.7992492914199829,
      "learning_rate": 4.755348302687412e-05,
      "loss": 8.1572,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 1.4357860088348389,
      "learning_rate": 4.753138260254597e-05,
      "loss": 8.147,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 0.9896734952926636,
      "learning_rate": 4.7509282178217826e-05,
      "loss": 8.1361,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 0.7115346193313599,
      "learning_rate": 4.748718175388968e-05,
      "loss": 8.1448,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 1.3110510110855103,
      "learning_rate": 4.746508132956153e-05,
      "loss": 8.1237,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 1.0065312385559082,
      "learning_rate": 4.744298090523338e-05,
      "loss": 8.135,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 1.3407121896743774,
      "learning_rate": 4.742088048090523e-05,
      "loss": 8.1458,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 1.2935097217559814,
      "learning_rate": 4.7398780056577094e-05,
      "loss": 8.1379,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 0.9102542996406555,
      "learning_rate": 4.7376679632248945e-05,
      "loss": 8.1432,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 2.4544589519500732,
      "learning_rate": 4.7354579207920796e-05,
      "loss": 8.1546,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 0.9030328989028931,
      "learning_rate": 4.733247878359265e-05,
      "loss": 8.1316,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 0.8985339403152466,
      "learning_rate": 4.73103783592645e-05,
      "loss": 8.1446,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 1.6634750366210938,
      "learning_rate": 4.7288277934936356e-05,
      "loss": 8.1418,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 3.069864511489868,
      "learning_rate": 4.726617751060821e-05,
      "loss": 8.1398,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 0.7385414838790894,
      "learning_rate": 4.724407708628006e-05,
      "loss": 8.1339,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 0.7566729187965393,
      "learning_rate": 4.722197666195191e-05,
      "loss": 8.1308,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 2.357363224029541,
      "learning_rate": 4.7199876237623766e-05,
      "loss": 8.1348,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 0.7281991839408875,
      "learning_rate": 4.717777581329562e-05,
      "loss": 8.1437,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 0.8826242685317993,
      "learning_rate": 4.715567538896747e-05,
      "loss": 8.1458,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 0.9293636083602905,
      "learning_rate": 4.713357496463932e-05,
      "loss": 8.1498,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 2.375598907470703,
      "learning_rate": 4.711147454031117e-05,
      "loss": 8.1371,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 1.2174696922302246,
      "learning_rate": 4.7089374115983035e-05,
      "loss": 8.1515,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 1.1991961002349854,
      "learning_rate": 4.7067273691654886e-05,
      "loss": 8.1401,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 0.7864306569099426,
      "learning_rate": 4.7045173267326737e-05,
      "loss": 8.1199,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 0.7330999374389648,
      "learning_rate": 4.702307284299859e-05,
      "loss": 8.1305,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 0.8905574083328247,
      "learning_rate": 4.700097241867044e-05,
      "loss": 8.1283,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 1.4793035984039307,
      "learning_rate": 4.6978871994342296e-05,
      "loss": 8.1219,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.665165901184082,
      "learning_rate": 4.695677157001415e-05,
      "loss": 8.131,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.6748032569885254,
      "learning_rate": 4.6934671145686e-05,
      "loss": 8.1446,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 0.8435137271881104,
      "learning_rate": 4.691257072135785e-05,
      "loss": 8.1182,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 0.8907887935638428,
      "learning_rate": 4.689047029702971e-05,
      "loss": 8.1445,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 0.9058837294578552,
      "learning_rate": 4.686836987270156e-05,
      "loss": 8.1434,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 2.731689214706421,
      "learning_rate": 4.684626944837341e-05,
      "loss": 8.143,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 1.3596062660217285,
      "learning_rate": 4.682416902404526e-05,
      "loss": 8.1403,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 1.5356475114822388,
      "learning_rate": 4.680206859971712e-05,
      "loss": 8.14,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 2.6440606117248535,
      "learning_rate": 4.6779968175388975e-05,
      "loss": 8.1299,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 1.2428972721099854,
      "learning_rate": 4.6757867751060826e-05,
      "loss": 8.134,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 2.447598695755005,
      "learning_rate": 4.673576732673268e-05,
      "loss": 8.1356,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 2.3701610565185547,
      "learning_rate": 4.671366690240453e-05,
      "loss": 8.1562,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 0.5955577492713928,
      "learning_rate": 4.669156647807638e-05,
      "loss": 8.1211,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 1.9649672508239746,
      "learning_rate": 4.6669466053748236e-05,
      "loss": 8.1387,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 1.5700713396072388,
      "learning_rate": 4.664736562942009e-05,
      "loss": 8.144,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 1.2700107097625732,
      "learning_rate": 4.662526520509194e-05,
      "loss": 8.1341,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 1.2104922533035278,
      "learning_rate": 4.660316478076379e-05,
      "loss": 8.1475,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 1.0661829710006714,
      "learning_rate": 4.658106435643565e-05,
      "loss": 8.1214,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.8409711122512817,
      "learning_rate": 4.65589639321075e-05,
      "loss": 8.1108,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 3.221015214920044,
      "learning_rate": 4.653686350777935e-05,
      "loss": 8.1492,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 1.6262084245681763,
      "learning_rate": 4.6514763083451207e-05,
      "loss": 8.1294,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 2.6076228618621826,
      "learning_rate": 4.649266265912306e-05,
      "loss": 8.1244,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 0.7324378490447998,
      "learning_rate": 4.6470562234794915e-05,
      "loss": 8.1377,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 0.8694437742233276,
      "learning_rate": 4.6448461810466766e-05,
      "loss": 8.1318,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 1.190567135810852,
      "learning_rate": 4.642636138613862e-05,
      "loss": 8.1313,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 1.044485092163086,
      "learning_rate": 4.640426096181047e-05,
      "loss": 8.1099,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 1.5426740646362305,
      "learning_rate": 4.638216053748232e-05,
      "loss": 8.1393,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 1.1623259782791138,
      "learning_rate": 4.636006011315418e-05,
      "loss": 8.1356,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 1.1647775173187256,
      "learning_rate": 4.633795968882603e-05,
      "loss": 8.1274,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 1.2641135454177856,
      "learning_rate": 4.631585926449788e-05,
      "loss": 8.1376,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 1.3211780786514282,
      "learning_rate": 4.629375884016973e-05,
      "loss": 8.116,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 1.3174850940704346,
      "learning_rate": 4.627165841584159e-05,
      "loss": 8.1222,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 1.4946551322937012,
      "learning_rate": 4.624955799151344e-05,
      "loss": 8.1274,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 1.0001411437988281,
      "learning_rate": 4.622745756718529e-05,
      "loss": 8.1387,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 1.0746123790740967,
      "learning_rate": 4.620535714285715e-05,
      "loss": 8.1302,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 1.3485393524169922,
      "learning_rate": 4.6183256718529e-05,
      "loss": 8.1223,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 1.9648051261901855,
      "learning_rate": 4.6161156294200856e-05,
      "loss": 8.1313,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 1.2787649631500244,
      "learning_rate": 4.6139055869872707e-05,
      "loss": 8.1354,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 0.7873222231864929,
      "learning_rate": 4.611695544554456e-05,
      "loss": 8.1405,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 2.2081856727600098,
      "learning_rate": 4.609485502121641e-05,
      "loss": 8.1167,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 0.6028438806533813,
      "learning_rate": 4.607275459688826e-05,
      "loss": 8.1369,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 0.9986103773117065,
      "learning_rate": 4.605065417256012e-05,
      "loss": 8.1335,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 0.6742298603057861,
      "learning_rate": 4.602855374823197e-05,
      "loss": 8.1261,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 1.2286006212234497,
      "learning_rate": 4.600645332390382e-05,
      "loss": 8.1172,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 0.8139893412590027,
      "learning_rate": 4.598435289957567e-05,
      "loss": 8.1406,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 1.3433475494384766,
      "learning_rate": 4.596225247524753e-05,
      "loss": 8.1309,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 1.5777674913406372,
      "learning_rate": 4.594015205091938e-05,
      "loss": 8.1259,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 0.9981272220611572,
      "learning_rate": 4.5918051626591236e-05,
      "loss": 8.1216,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 0.880254864692688,
      "learning_rate": 4.589595120226309e-05,
      "loss": 8.1399,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 1.2760629653930664,
      "learning_rate": 4.587385077793494e-05,
      "loss": 8.15,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 1.6959831714630127,
      "learning_rate": 4.5851750353606796e-05,
      "loss": 8.1277,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.7466061115264893,
      "learning_rate": 4.582964992927865e-05,
      "loss": 8.1268,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 1.1226886510849,
      "learning_rate": 4.58075495049505e-05,
      "loss": 8.1372,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 1.5224499702453613,
      "learning_rate": 4.578544908062235e-05,
      "loss": 8.13,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 1.4019792079925537,
      "learning_rate": 4.57633486562942e-05,
      "loss": 8.1344,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.8918095827102661,
      "learning_rate": 4.574124823196606e-05,
      "loss": 8.1311,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 0.9084849953651428,
      "learning_rate": 4.571914780763791e-05,
      "loss": 8.1366,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 0.7314757704734802,
      "learning_rate": 4.569704738330976e-05,
      "loss": 8.1226,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 0.8405730128288269,
      "learning_rate": 4.567494695898161e-05,
      "loss": 8.1186,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 0.7289599776268005,
      "learning_rate": 4.565284653465347e-05,
      "loss": 8.1128,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 2.0056517124176025,
      "learning_rate": 4.5630746110325326e-05,
      "loss": 8.1228,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 1.4175626039505005,
      "learning_rate": 4.5608645685997177e-05,
      "loss": 8.1313,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 1.7403383255004883,
      "learning_rate": 4.558654526166903e-05,
      "loss": 8.1341,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.9728478789329529,
      "learning_rate": 4.556444483734088e-05,
      "loss": 8.1278,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 0.9657310843467712,
      "learning_rate": 4.5542344413012736e-05,
      "loss": 8.1455,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 0.7016928791999817,
      "learning_rate": 4.552024398868459e-05,
      "loss": 8.1257,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 1.750995397567749,
      "learning_rate": 4.549814356435644e-05,
      "loss": 8.1391,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 3.0268523693084717,
      "learning_rate": 4.547604314002829e-05,
      "loss": 8.1331,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 2.257249116897583,
      "learning_rate": 4.545394271570014e-05,
      "loss": 8.1268,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 0.7347825169563293,
      "learning_rate": 4.5431842291372e-05,
      "loss": 8.1315,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 2.116239070892334,
      "learning_rate": 4.540974186704385e-05,
      "loss": 8.1446,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 2.6621463298797607,
      "learning_rate": 4.53876414427157e-05,
      "loss": 8.1376,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 0.8156794309616089,
      "learning_rate": 4.536554101838755e-05,
      "loss": 8.1435,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 1.779827356338501,
      "learning_rate": 4.534344059405941e-05,
      "loss": 8.1286,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 3.6931838989257812,
      "learning_rate": 4.5321340169731266e-05,
      "loss": 8.1238,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 1.4945954084396362,
      "learning_rate": 4.529923974540312e-05,
      "loss": 8.1367,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 1.838985800743103,
      "learning_rate": 4.527713932107497e-05,
      "loss": 8.1363,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 1.0356295108795166,
      "learning_rate": 4.525503889674682e-05,
      "loss": 8.1453,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 2.1280863285064697,
      "learning_rate": 4.5232938472418677e-05,
      "loss": 8.1204,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 2.179682731628418,
      "learning_rate": 4.521083804809053e-05,
      "loss": 8.1256,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 1.5666074752807617,
      "learning_rate": 4.518873762376238e-05,
      "loss": 8.1255,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 1.3898165225982666,
      "learning_rate": 4.516663719943423e-05,
      "loss": 8.1194,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 1.6152321100234985,
      "learning_rate": 4.514453677510608e-05,
      "loss": 8.1377,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 0.9872780442237854,
      "learning_rate": 4.512243635077794e-05,
      "loss": 8.1251,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 1.3864965438842773,
      "learning_rate": 4.510033592644979e-05,
      "loss": 8.1347,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 1.6805392503738403,
      "learning_rate": 4.507823550212164e-05,
      "loss": 8.1309,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 0.8944875597953796,
      "learning_rate": 4.505613507779349e-05,
      "loss": 8.1284,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.7924079895019531,
      "learning_rate": 4.503403465346535e-05,
      "loss": 8.1483,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 2.8641250133514404,
      "learning_rate": 4.5011934229137206e-05,
      "loss": 8.1206,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 2.6894214153289795,
      "learning_rate": 4.498983380480906e-05,
      "loss": 8.1099,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 1.2986316680908203,
      "learning_rate": 4.496773338048091e-05,
      "loss": 8.1174,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 0.7564579248428345,
      "learning_rate": 4.494563295615276e-05,
      "loss": 8.1263,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 2.1276535987854004,
      "learning_rate": 4.492353253182462e-05,
      "loss": 8.1272,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 1.3789678812026978,
      "learning_rate": 4.490143210749647e-05,
      "loss": 8.1159,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.9833824634552002,
      "learning_rate": 4.487933168316832e-05,
      "loss": 8.1261,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 1.6973011493682861,
      "learning_rate": 4.485723125884017e-05,
      "loss": 8.1218,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 1.2579853534698486,
      "learning_rate": 4.483513083451202e-05,
      "loss": 8.1356,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 1.0678609609603882,
      "learning_rate": 4.481303041018388e-05,
      "loss": 8.1115,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 1.0080513954162598,
      "learning_rate": 4.479092998585573e-05,
      "loss": 8.1416,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 1.728085994720459,
      "learning_rate": 4.476882956152758e-05,
      "loss": 8.1256,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 1.0013184547424316,
      "learning_rate": 4.474672913719944e-05,
      "loss": 8.1265,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 1.2561790943145752,
      "learning_rate": 4.472462871287129e-05,
      "loss": 8.1165,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 2.669976234436035,
      "learning_rate": 4.470252828854315e-05,
      "loss": 8.117,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 1.0632567405700684,
      "learning_rate": 4.4680427864215e-05,
      "loss": 8.1222,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 2.326310396194458,
      "learning_rate": 4.465832743988685e-05,
      "loss": 8.1339,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 1.9265451431274414,
      "learning_rate": 4.46362270155587e-05,
      "loss": 8.1283,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 1.9307504892349243,
      "learning_rate": 4.461412659123056e-05,
      "loss": 8.1387,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 1.3953375816345215,
      "learning_rate": 4.459202616690241e-05,
      "loss": 8.1301,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.7817894220352173,
      "learning_rate": 4.456992574257426e-05,
      "loss": 8.1312,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.9156811237335205,
      "learning_rate": 4.454782531824611e-05,
      "loss": 8.1052,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 1.400242567062378,
      "learning_rate": 4.452572489391796e-05,
      "loss": 8.1224,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 1.0398952960968018,
      "learning_rate": 4.450362446958982e-05,
      "loss": 8.1375,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 0.8242036700248718,
      "learning_rate": 4.448152404526167e-05,
      "loss": 8.1193,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.8784144520759583,
      "learning_rate": 4.445942362093352e-05,
      "loss": 8.1386,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 0.7130635976791382,
      "learning_rate": 4.443732319660538e-05,
      "loss": 8.122,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 0.9169226288795471,
      "learning_rate": 4.441522277227723e-05,
      "loss": 8.0999,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 0.8026123642921448,
      "learning_rate": 4.439312234794909e-05,
      "loss": 8.1268,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 3.0320956707000732,
      "learning_rate": 4.437102192362094e-05,
      "loss": 8.1179,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 1.7157331705093384,
      "learning_rate": 4.434892149929279e-05,
      "loss": 8.1289,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 1.0207972526550293,
      "learning_rate": 4.432682107496464e-05,
      "loss": 8.1293,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 1.2355436086654663,
      "learning_rate": 4.43047206506365e-05,
      "loss": 8.1244,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 0.8576319813728333,
      "learning_rate": 4.428262022630835e-05,
      "loss": 8.1309,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 1.5930135250091553,
      "learning_rate": 4.42605198019802e-05,
      "loss": 8.1319,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 1.2188942432403564,
      "learning_rate": 4.423841937765205e-05,
      "loss": 8.1204,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 1.5238261222839355,
      "learning_rate": 4.42163189533239e-05,
      "loss": 8.1038,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 1.3246231079101562,
      "learning_rate": 4.419421852899576e-05,
      "loss": 8.1085,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 2.379167318344116,
      "learning_rate": 4.417211810466761e-05,
      "loss": 8.1276,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 1.0612999200820923,
      "learning_rate": 4.415001768033947e-05,
      "loss": 8.1354,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 1.0679903030395508,
      "learning_rate": 4.412791725601132e-05,
      "loss": 8.1209,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 1.2253437042236328,
      "learning_rate": 4.410581683168317e-05,
      "loss": 8.1256,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 1.4744584560394287,
      "learning_rate": 4.408371640735503e-05,
      "loss": 8.141,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 0.7224366664886475,
      "learning_rate": 4.406161598302688e-05,
      "loss": 8.1234,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 1.8724217414855957,
      "learning_rate": 4.403951555869873e-05,
      "loss": 8.1192,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 0.7011707425117493,
      "learning_rate": 4.401741513437058e-05,
      "loss": 8.1254,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 0.8190743923187256,
      "learning_rate": 4.399531471004244e-05,
      "loss": 8.1438,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 1.3879348039627075,
      "learning_rate": 4.397321428571429e-05,
      "loss": 8.1342,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 1.4845677614212036,
      "learning_rate": 4.395111386138614e-05,
      "loss": 8.1168,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.7718964219093323,
      "learning_rate": 4.392901343705799e-05,
      "loss": 8.132,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 0.7378916144371033,
      "learning_rate": 4.390691301272984e-05,
      "loss": 8.1368,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 0.7976837754249573,
      "learning_rate": 4.38848125884017e-05,
      "loss": 8.1308,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 1.2536346912384033,
      "learning_rate": 4.386271216407356e-05,
      "loss": 8.1242,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.9537434577941895,
      "learning_rate": 4.384061173974541e-05,
      "loss": 8.1134,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.9349040389060974,
      "learning_rate": 4.381851131541726e-05,
      "loss": 8.1374,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.7789999842643738,
      "learning_rate": 4.379641089108911e-05,
      "loss": 8.1113,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 1.2549035549163818,
      "learning_rate": 4.377431046676097e-05,
      "loss": 8.1103,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 1.6388391256332397,
      "learning_rate": 4.375221004243282e-05,
      "loss": 8.1508,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.7326608300209045,
      "learning_rate": 4.373010961810467e-05,
      "loss": 8.1253,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.9195551872253418,
      "learning_rate": 4.370800919377652e-05,
      "loss": 8.1298,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 1.2265956401824951,
      "learning_rate": 4.368590876944838e-05,
      "loss": 8.1217,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.8913390040397644,
      "learning_rate": 4.366380834512023e-05,
      "loss": 8.1237,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 1.0213226079940796,
      "learning_rate": 4.364170792079208e-05,
      "loss": 8.1215,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 1.399215817451477,
      "learning_rate": 4.361960749646393e-05,
      "loss": 8.12,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 2.366387128829956,
      "learning_rate": 4.359750707213578e-05,
      "loss": 8.132,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 1.5653551816940308,
      "learning_rate": 4.357540664780764e-05,
      "loss": 8.1367,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 2.0236313343048096,
      "learning_rate": 4.35533062234795e-05,
      "loss": 8.1309,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 2.4536983966827393,
      "learning_rate": 4.353120579915135e-05,
      "loss": 8.1306,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 0.7204288840293884,
      "learning_rate": 4.35091053748232e-05,
      "loss": 8.1167,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 0.8028860688209534,
      "learning_rate": 4.348700495049505e-05,
      "loss": 8.1145,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 1.3860328197479248,
      "learning_rate": 4.346490452616691e-05,
      "loss": 8.1142,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 0.9409781694412231,
      "learning_rate": 4.344280410183876e-05,
      "loss": 8.1198,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 2.06516695022583,
      "learning_rate": 4.342070367751061e-05,
      "loss": 8.1193,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 1.0741915702819824,
      "learning_rate": 4.339860325318246e-05,
      "loss": 8.1338,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 1.0689867734909058,
      "learning_rate": 4.337650282885432e-05,
      "loss": 8.13,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.6213433146476746,
      "learning_rate": 4.335440240452617e-05,
      "loss": 8.1026,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.8502789735794067,
      "learning_rate": 4.333230198019802e-05,
      "loss": 8.1249,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.8461669087409973,
      "learning_rate": 4.331020155586987e-05,
      "loss": 8.1218,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 1.2638654708862305,
      "learning_rate": 4.328810113154172e-05,
      "loss": 8.1162,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 1.817523717880249,
      "learning_rate": 4.326600070721358e-05,
      "loss": 8.1324,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 1.7943769693374634,
      "learning_rate": 4.324390028288544e-05,
      "loss": 8.1317,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 1.5107828378677368,
      "learning_rate": 4.322179985855729e-05,
      "loss": 8.1342,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 1.071122407913208,
      "learning_rate": 4.319969943422914e-05,
      "loss": 8.1342,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 1.332771897315979,
      "learning_rate": 4.317759900990099e-05,
      "loss": 8.1103,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 1.1416230201721191,
      "learning_rate": 4.315549858557285e-05,
      "loss": 8.1236,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 0.8420162200927734,
      "learning_rate": 4.31333981612447e-05,
      "loss": 8.1285,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 2.129894256591797,
      "learning_rate": 4.311129773691655e-05,
      "loss": 8.113,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 0.8761067986488342,
      "learning_rate": 4.30891973125884e-05,
      "loss": 8.1245,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 1.02657949924469,
      "learning_rate": 4.306709688826026e-05,
      "loss": 8.1165,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 0.9194222092628479,
      "learning_rate": 4.304499646393211e-05,
      "loss": 8.1331,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.9798198938369751,
      "learning_rate": 4.302289603960396e-05,
      "loss": 8.127,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 0.6078124046325684,
      "learning_rate": 4.300079561527581e-05,
      "loss": 8.1163,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 0.9838354587554932,
      "learning_rate": 4.297869519094767e-05,
      "loss": 8.1217,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.9231829047203064,
      "learning_rate": 4.295659476661952e-05,
      "loss": 8.116,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 1.1258717775344849,
      "learning_rate": 4.293449434229138e-05,
      "loss": 8.1171,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 1.7347180843353271,
      "learning_rate": 4.291239391796323e-05,
      "loss": 8.1217,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.7764060497283936,
      "learning_rate": 4.289029349363508e-05,
      "loss": 8.1162,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 1.1703591346740723,
      "learning_rate": 4.286819306930693e-05,
      "loss": 8.1115,
      "step": 3230
    }
  ],
  "logging_steps": 10,
  "max_steps": 22624,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 7,
  "save_steps": 500,
  "total_flos": 1.3416337206177792e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
