{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.998839996906659,
  "eval_steps": 500,
  "global_step": 16160,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 1.2247768640518188,
      "learning_rate": 4.997524752475248e-05,
      "loss": 10.3209,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.124623417854309,
      "learning_rate": 4.9944306930693075e-05,
      "loss": 10.1005,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 1.021990418434143,
      "learning_rate": 4.9913366336633664e-05,
      "loss": 9.9043,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 0.7722101211547852,
      "learning_rate": 4.988242574257426e-05,
      "loss": 9.7366,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 0.6074748635292053,
      "learning_rate": 4.9851485148514855e-05,
      "loss": 9.5712,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 0.7219138145446777,
      "learning_rate": 4.982054455445545e-05,
      "loss": 9.4653,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 0.8722349405288696,
      "learning_rate": 4.978960396039604e-05,
      "loss": 9.3728,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.5499579310417175,
      "learning_rate": 4.975866336633664e-05,
      "loss": 9.2528,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.47239771485328674,
      "learning_rate": 4.972772277227723e-05,
      "loss": 9.1253,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.41896697878837585,
      "learning_rate": 4.9696782178217825e-05,
      "loss": 9.0156,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 0.4210807681083679,
      "learning_rate": 4.966584158415842e-05,
      "loss": 8.9628,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.4038824439048767,
      "learning_rate": 4.9634900990099016e-05,
      "loss": 8.8833,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.38877707719802856,
      "learning_rate": 4.9603960396039604e-05,
      "loss": 8.82,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.4252713918685913,
      "learning_rate": 4.95730198019802e-05,
      "loss": 8.7565,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.34656330943107605,
      "learning_rate": 4.9542079207920795e-05,
      "loss": 8.6942,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.4074937701225281,
      "learning_rate": 4.951113861386139e-05,
      "loss": 8.6437,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.37689608335494995,
      "learning_rate": 4.948019801980198e-05,
      "loss": 8.6368,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.47150588035583496,
      "learning_rate": 4.944925742574258e-05,
      "loss": 8.56,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.3021436631679535,
      "learning_rate": 4.941831683168317e-05,
      "loss": 8.5539,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 0.634679913520813,
      "learning_rate": 4.9387376237623765e-05,
      "loss": 8.521,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 0.43083757162094116,
      "learning_rate": 4.935643564356436e-05,
      "loss": 8.4952,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.3269064128398895,
      "learning_rate": 4.9325495049504956e-05,
      "loss": 8.4702,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 0.3242396414279938,
      "learning_rate": 4.9294554455445545e-05,
      "loss": 8.4377,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 0.42160049080848694,
      "learning_rate": 4.926361386138614e-05,
      "loss": 8.4423,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 0.7685427665710449,
      "learning_rate": 4.9232673267326735e-05,
      "loss": 8.4116,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 0.7840954065322876,
      "learning_rate": 4.920173267326733e-05,
      "loss": 8.3975,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.536022961139679,
      "learning_rate": 4.917079207920792e-05,
      "loss": 8.3935,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 0.43331119418144226,
      "learning_rate": 4.913985148514852e-05,
      "loss": 8.376,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 0.3564979135990143,
      "learning_rate": 4.910891089108911e-05,
      "loss": 8.3399,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 0.5969781279563904,
      "learning_rate": 4.9077970297029706e-05,
      "loss": 8.3449,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 1.029236912727356,
      "learning_rate": 4.90470297029703e-05,
      "loss": 8.3278,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 0.8592106699943542,
      "learning_rate": 4.9016089108910896e-05,
      "loss": 8.3073,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.37601804733276367,
      "learning_rate": 4.8985148514851485e-05,
      "loss": 8.3079,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.495095431804657,
      "learning_rate": 4.895420792079208e-05,
      "loss": 8.3113,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 0.5858795642852783,
      "learning_rate": 4.8923267326732676e-05,
      "loss": 8.291,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 0.7599595189094543,
      "learning_rate": 4.889232673267327e-05,
      "loss": 8.2882,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 0.5483265519142151,
      "learning_rate": 4.886138613861386e-05,
      "loss": 8.283,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 0.34301531314849854,
      "learning_rate": 4.883044554455446e-05,
      "loss": 8.2843,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 0.36520102620124817,
      "learning_rate": 4.879950495049505e-05,
      "loss": 8.2674,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 0.7266101241111755,
      "learning_rate": 4.8768564356435646e-05,
      "loss": 8.2721,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 0.8032565712928772,
      "learning_rate": 4.873762376237624e-05,
      "loss": 8.2506,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 0.9643657207489014,
      "learning_rate": 4.870668316831684e-05,
      "loss": 8.2656,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 1.3137482404708862,
      "learning_rate": 4.8675742574257425e-05,
      "loss": 8.2541,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 0.6304842233657837,
      "learning_rate": 4.864480198019802e-05,
      "loss": 8.2586,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 0.8513209223747253,
      "learning_rate": 4.8613861386138616e-05,
      "loss": 8.2515,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 0.35217440128326416,
      "learning_rate": 4.858292079207921e-05,
      "loss": 8.2625,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.4499775767326355,
      "learning_rate": 4.85519801980198e-05,
      "loss": 8.2373,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 0.37050744891166687,
      "learning_rate": 4.85210396039604e-05,
      "loss": 8.2332,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 0.5508533716201782,
      "learning_rate": 4.849009900990099e-05,
      "loss": 8.2356,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.7395459413528442,
      "learning_rate": 4.8459158415841586e-05,
      "loss": 8.2237,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 0.3201032280921936,
      "learning_rate": 4.842821782178218e-05,
      "loss": 8.2246,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 0.5138698816299438,
      "learning_rate": 4.839727722772278e-05,
      "loss": 8.2469,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 0.7068663835525513,
      "learning_rate": 4.8366336633663366e-05,
      "loss": 8.2411,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.3541112542152405,
      "learning_rate": 4.833539603960396e-05,
      "loss": 8.2452,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 0.28638482093811035,
      "learning_rate": 4.8304455445544556e-05,
      "loss": 8.239,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 0.49469882249832153,
      "learning_rate": 4.827351485148515e-05,
      "loss": 8.2383,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.36606070399284363,
      "learning_rate": 4.824257425742574e-05,
      "loss": 8.2376,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 0.3310873806476593,
      "learning_rate": 4.821163366336634e-05,
      "loss": 8.222,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 0.2834576964378357,
      "learning_rate": 4.818069306930693e-05,
      "loss": 8.2282,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 0.40760523080825806,
      "learning_rate": 4.8149752475247527e-05,
      "loss": 8.2198,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 0.4820593595504761,
      "learning_rate": 4.811881188118812e-05,
      "loss": 8.2187,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 0.32940781116485596,
      "learning_rate": 4.808787128712872e-05,
      "loss": 8.2253,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.7309168577194214,
      "learning_rate": 4.8056930693069306e-05,
      "loss": 8.2312,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 1.516921877861023,
      "learning_rate": 4.80259900990099e-05,
      "loss": 8.2177,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.6382942795753479,
      "learning_rate": 4.79950495049505e-05,
      "loss": 8.2123,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.31368473172187805,
      "learning_rate": 4.796410891089109e-05,
      "loss": 8.2271,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 0.5464399456977844,
      "learning_rate": 4.793316831683168e-05,
      "loss": 8.223,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 0.9966047406196594,
      "learning_rate": 4.790222772277228e-05,
      "loss": 8.2221,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 0.7671545743942261,
      "learning_rate": 4.787128712871287e-05,
      "loss": 8.2289,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 0.41397276520729065,
      "learning_rate": 4.784034653465347e-05,
      "loss": 8.2058,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 0.4239896237850189,
      "learning_rate": 4.780940594059406e-05,
      "loss": 8.2152,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 0.5347060561180115,
      "learning_rate": 4.777846534653466e-05,
      "loss": 8.2136,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 0.8777751326560974,
      "learning_rate": 4.7747524752475246e-05,
      "loss": 8.2308,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 0.7064963579177856,
      "learning_rate": 4.771658415841584e-05,
      "loss": 8.2296,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 0.6401854157447815,
      "learning_rate": 4.768564356435644e-05,
      "loss": 8.2099,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 0.5358422994613647,
      "learning_rate": 4.765470297029703e-05,
      "loss": 8.1989,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 1.2079377174377441,
      "learning_rate": 4.762376237623762e-05,
      "loss": 8.2005,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 0.5266783833503723,
      "learning_rate": 4.759282178217822e-05,
      "loss": 8.2206,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 1.395314335823059,
      "learning_rate": 4.756188118811881e-05,
      "loss": 8.2298,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 0.30735427141189575,
      "learning_rate": 4.753094059405941e-05,
      "loss": 8.2272,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 0.37388095259666443,
      "learning_rate": 4.75e-05,
      "loss": 8.2045,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.8053635954856873,
      "learning_rate": 4.74690594059406e-05,
      "loss": 8.216,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.5549759268760681,
      "learning_rate": 4.743811881188119e-05,
      "loss": 8.2045,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 0.41379955410957336,
      "learning_rate": 4.740717821782178e-05,
      "loss": 8.2138,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 0.49880656599998474,
      "learning_rate": 4.737623762376238e-05,
      "loss": 8.2189,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.8625946044921875,
      "learning_rate": 4.734529702970297e-05,
      "loss": 8.2007,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.6511316299438477,
      "learning_rate": 4.731435643564356e-05,
      "loss": 8.216,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 0.6263079643249512,
      "learning_rate": 4.7283415841584164e-05,
      "loss": 8.1913,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 0.4104655981063843,
      "learning_rate": 4.725247524752475e-05,
      "loss": 8.1926,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 0.6455371975898743,
      "learning_rate": 4.722153465346535e-05,
      "loss": 8.2126,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.42285627126693726,
      "learning_rate": 4.719059405940594e-05,
      "loss": 8.2182,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 0.3862406611442566,
      "learning_rate": 4.715965346534654e-05,
      "loss": 8.2198,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 0.7337411642074585,
      "learning_rate": 4.712871287128713e-05,
      "loss": 8.2039,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 0.5963287353515625,
      "learning_rate": 4.709777227722772e-05,
      "loss": 8.2107,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 1.2074090242385864,
      "learning_rate": 4.706683168316832e-05,
      "loss": 8.199,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 0.7971219420433044,
      "learning_rate": 4.703589108910891e-05,
      "loss": 8.198,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 1.079160451889038,
      "learning_rate": 4.70049504950495e-05,
      "loss": 8.216,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 0.9745631217956543,
      "learning_rate": 4.6974009900990104e-05,
      "loss": 8.205,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.5134568810462952,
      "learning_rate": 4.694306930693069e-05,
      "loss": 8.2127,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 0.5138888359069824,
      "learning_rate": 4.691212871287129e-05,
      "loss": 8.2052,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 0.40537601709365845,
      "learning_rate": 4.688118811881188e-05,
      "loss": 8.2065,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 0.48841580748558044,
      "learning_rate": 4.685024752475248e-05,
      "loss": 8.197,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 0.7114644646644592,
      "learning_rate": 4.6819306930693074e-05,
      "loss": 8.2106,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 0.5906206965446472,
      "learning_rate": 4.678836633663366e-05,
      "loss": 8.2063,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 0.5407156944274902,
      "learning_rate": 4.6757425742574265e-05,
      "loss": 8.1951,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 0.6469225287437439,
      "learning_rate": 4.6726485148514853e-05,
      "loss": 8.2108,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 0.7905026078224182,
      "learning_rate": 4.669554455445545e-05,
      "loss": 8.2066,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 1.1467782258987427,
      "learning_rate": 4.6664603960396044e-05,
      "loss": 8.1957,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.5464600920677185,
      "learning_rate": 4.663366336633664e-05,
      "loss": 8.2019,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.2692526578903198,
      "learning_rate": 4.660272277227723e-05,
      "loss": 8.1918,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 1.0630919933319092,
      "learning_rate": 4.6571782178217824e-05,
      "loss": 8.2114,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 0.7366971969604492,
      "learning_rate": 4.654084158415842e-05,
      "loss": 8.2015,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 0.9527750015258789,
      "learning_rate": 4.6509900990099014e-05,
      "loss": 8.1882,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 0.5087741613388062,
      "learning_rate": 4.64789603960396e-05,
      "loss": 8.1986,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 0.36688852310180664,
      "learning_rate": 4.6448019801980205e-05,
      "loss": 8.1762,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 0.4138261079788208,
      "learning_rate": 4.6417079207920794e-05,
      "loss": 8.189,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 0.9040499925613403,
      "learning_rate": 4.638613861386139e-05,
      "loss": 8.1979,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 0.46051475405693054,
      "learning_rate": 4.6355198019801985e-05,
      "loss": 8.1926,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 0.7377279996871948,
      "learning_rate": 4.632425742574258e-05,
      "loss": 8.1965,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 0.35922303795814514,
      "learning_rate": 4.629331683168317e-05,
      "loss": 8.2081,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 0.7749016880989075,
      "learning_rate": 4.6262376237623764e-05,
      "loss": 8.1848,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 0.5312947034835815,
      "learning_rate": 4.623143564356436e-05,
      "loss": 8.1993,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 0.4701521396636963,
      "learning_rate": 4.6200495049504955e-05,
      "loss": 8.1961,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 0.5874415636062622,
      "learning_rate": 4.616955445544554e-05,
      "loss": 8.1919,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 0.5899882316589355,
      "learning_rate": 4.6138613861386145e-05,
      "loss": 8.1866,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 0.6891916394233704,
      "learning_rate": 4.6107673267326734e-05,
      "loss": 8.1861,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 0.45299622416496277,
      "learning_rate": 4.607673267326733e-05,
      "loss": 8.1874,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 1.070456862449646,
      "learning_rate": 4.6045792079207925e-05,
      "loss": 8.1963,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 0.5316533446311951,
      "learning_rate": 4.601485148514852e-05,
      "loss": 8.1986,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 0.5455809831619263,
      "learning_rate": 4.598391089108911e-05,
      "loss": 8.2036,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 0.6389105319976807,
      "learning_rate": 4.5952970297029704e-05,
      "loss": 8.1907,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 0.4612026512622833,
      "learning_rate": 4.59220297029703e-05,
      "loss": 8.2057,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 0.5475612282752991,
      "learning_rate": 4.5891089108910895e-05,
      "loss": 8.1943,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 0.7979524731636047,
      "learning_rate": 4.5860148514851484e-05,
      "loss": 8.1736,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 0.8579632043838501,
      "learning_rate": 4.5829207920792086e-05,
      "loss": 8.1832,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 0.39259180426597595,
      "learning_rate": 4.5798267326732674e-05,
      "loss": 8.1819,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 0.6323119401931763,
      "learning_rate": 4.576732673267327e-05,
      "loss": 8.173,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.39811235666275024,
      "learning_rate": 4.5736386138613865e-05,
      "loss": 8.1844,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.238008737564087,
      "learning_rate": 4.570544554455446e-05,
      "loss": 8.1994,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 0.7153070569038391,
      "learning_rate": 4.567450495049505e-05,
      "loss": 8.1717,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 0.7053283452987671,
      "learning_rate": 4.5643564356435645e-05,
      "loss": 8.1978,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 1.0989347696304321,
      "learning_rate": 4.561262376237624e-05,
      "loss": 8.1986,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 0.5493602156639099,
      "learning_rate": 4.5581683168316835e-05,
      "loss": 8.1952,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 0.639211893081665,
      "learning_rate": 4.5550742574257424e-05,
      "loss": 8.195,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 0.6570772528648376,
      "learning_rate": 4.5519801980198026e-05,
      "loss": 8.194,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 0.7170979380607605,
      "learning_rate": 4.5488861386138615e-05,
      "loss": 8.1849,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 0.3791988790035248,
      "learning_rate": 4.545792079207921e-05,
      "loss": 8.1876,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 0.5699512362480164,
      "learning_rate": 4.5426980198019805e-05,
      "loss": 8.1877,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 0.5981238484382629,
      "learning_rate": 4.53960396039604e-05,
      "loss": 8.2093,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 0.32610607147216797,
      "learning_rate": 4.536509900990099e-05,
      "loss": 8.1761,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 1.279944896697998,
      "learning_rate": 4.5334158415841585e-05,
      "loss": 8.1933,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 0.6262332201004028,
      "learning_rate": 4.530321782178218e-05,
      "loss": 8.1985,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 0.40352341532707214,
      "learning_rate": 4.5272277227722776e-05,
      "loss": 8.1891,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 0.7490780353546143,
      "learning_rate": 4.5241336633663364e-05,
      "loss": 8.2028,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 0.5756144523620605,
      "learning_rate": 4.5210396039603966e-05,
      "loss": 8.1738,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.49029240012168884,
      "learning_rate": 4.5179455445544555e-05,
      "loss": 8.1631,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 0.5849631428718567,
      "learning_rate": 4.514851485148515e-05,
      "loss": 8.2034,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 1.2421512603759766,
      "learning_rate": 4.5117574257425746e-05,
      "loss": 8.1833,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 0.7801458835601807,
      "learning_rate": 4.508663366336634e-05,
      "loss": 8.1783,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 0.5695696473121643,
      "learning_rate": 4.505569306930693e-05,
      "loss": 8.1896,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 0.6546632051467896,
      "learning_rate": 4.5024752475247525e-05,
      "loss": 8.1863,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 1.2603425979614258,
      "learning_rate": 4.499381188118812e-05,
      "loss": 8.1848,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 1.300681233406067,
      "learning_rate": 4.4962871287128716e-05,
      "loss": 8.164,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 0.5545843839645386,
      "learning_rate": 4.4931930693069305e-05,
      "loss": 8.1922,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 0.6221284866333008,
      "learning_rate": 4.490099009900991e-05,
      "loss": 8.1893,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 0.3282677233219147,
      "learning_rate": 4.4870049504950495e-05,
      "loss": 8.1813,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 0.9480931162834167,
      "learning_rate": 4.483910891089109e-05,
      "loss": 8.1902,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 0.5979871153831482,
      "learning_rate": 4.4808168316831686e-05,
      "loss": 8.1678,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 0.49511781334877014,
      "learning_rate": 4.477722772277228e-05,
      "loss": 8.1751,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 0.34118643403053284,
      "learning_rate": 4.474628712871287e-05,
      "loss": 8.182,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 1.4929418563842773,
      "learning_rate": 4.4715346534653465e-05,
      "loss": 8.1935,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 0.4099706709384918,
      "learning_rate": 4.468440594059406e-05,
      "loss": 8.1847,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 0.5727508068084717,
      "learning_rate": 4.4653465346534656e-05,
      "loss": 8.1765,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 1.2658605575561523,
      "learning_rate": 4.4622524752475245e-05,
      "loss": 8.185,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 0.8700957894325256,
      "learning_rate": 4.459158415841585e-05,
      "loss": 8.1904,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 0.5159096717834473,
      "learning_rate": 4.4560643564356436e-05,
      "loss": 8.196,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 1.5243291854858398,
      "learning_rate": 4.452970297029703e-05,
      "loss": 8.1689,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 1.0142402648925781,
      "learning_rate": 4.4498762376237626e-05,
      "loss": 8.1918,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 1.3960062265396118,
      "learning_rate": 4.446782178217822e-05,
      "loss": 8.1873,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 1.2078458070755005,
      "learning_rate": 4.443688118811881e-05,
      "loss": 8.1798,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 1.5516475439071655,
      "learning_rate": 4.4405940594059406e-05,
      "loss": 8.1711,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 0.869965672492981,
      "learning_rate": 4.4375e-05,
      "loss": 8.1954,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 1.2989387512207031,
      "learning_rate": 4.4344059405940597e-05,
      "loss": 8.1859,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 0.4227568209171295,
      "learning_rate": 4.4313118811881185e-05,
      "loss": 8.1799,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 0.47840455174446106,
      "learning_rate": 4.428217821782179e-05,
      "loss": 8.1762,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 0.6492992043495178,
      "learning_rate": 4.4251237623762376e-05,
      "loss": 8.1945,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 0.4004427194595337,
      "learning_rate": 4.422029702970297e-05,
      "loss": 8.2049,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 0.8578277230262756,
      "learning_rate": 4.418935643564357e-05,
      "loss": 8.182,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.9048387408256531,
      "learning_rate": 4.415841584158416e-05,
      "loss": 8.1799,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 0.46773746609687805,
      "learning_rate": 4.412747524752475e-05,
      "loss": 8.1909,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 0.33044618368148804,
      "learning_rate": 4.4096534653465346e-05,
      "loss": 8.1837,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 1.1874784231185913,
      "learning_rate": 4.406559405940594e-05,
      "loss": 8.1886,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.4573058784008026,
      "learning_rate": 4.403465346534654e-05,
      "loss": 8.1852,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 1.0154377222061157,
      "learning_rate": 4.4003712871287126e-05,
      "loss": 8.1911,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 0.5081018805503845,
      "learning_rate": 4.397277227722773e-05,
      "loss": 8.1769,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 0.48365461826324463,
      "learning_rate": 4.3941831683168316e-05,
      "loss": 8.1715,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 0.49079352617263794,
      "learning_rate": 4.391089108910891e-05,
      "loss": 8.1659,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 0.9408277273178101,
      "learning_rate": 4.387995049504951e-05,
      "loss": 8.176,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 0.7148870229721069,
      "learning_rate": 4.38490099009901e-05,
      "loss": 8.1861,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 0.42110419273376465,
      "learning_rate": 4.381806930693069e-05,
      "loss": 8.1883,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.34316784143447876,
      "learning_rate": 4.3787128712871286e-05,
      "loss": 8.181,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 0.4234250783920288,
      "learning_rate": 4.375618811881188e-05,
      "loss": 8.2,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 0.3691387176513672,
      "learning_rate": 4.372524752475248e-05,
      "loss": 8.1787,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 0.44123542308807373,
      "learning_rate": 4.369430693069307e-05,
      "loss": 8.1924,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 0.45960983633995056,
      "learning_rate": 4.366336633663367e-05,
      "loss": 8.1876,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 0.43252694606781006,
      "learning_rate": 4.363242574257426e-05,
      "loss": 8.1805,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 0.3911884129047394,
      "learning_rate": 4.360148514851485e-05,
      "loss": 8.1835,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 0.3393804430961609,
      "learning_rate": 4.357054455445545e-05,
      "loss": 8.1993,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 0.5701378583908081,
      "learning_rate": 4.353960396039604e-05,
      "loss": 8.1902,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 0.4500882625579834,
      "learning_rate": 4.350866336633664e-05,
      "loss": 8.1975,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 0.36670684814453125,
      "learning_rate": 4.347772277227723e-05,
      "loss": 8.1825,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 0.4888262450695038,
      "learning_rate": 4.344678217821783e-05,
      "loss": 8.1775,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 0.575025200843811,
      "learning_rate": 4.341584158415842e-05,
      "loss": 8.1913,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 1.005854845046997,
      "learning_rate": 4.338490099009901e-05,
      "loss": 8.1912,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 1.063337802886963,
      "learning_rate": 4.335396039603961e-05,
      "loss": 8.2002,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 0.5566020607948303,
      "learning_rate": 4.3323019801980204e-05,
      "loss": 8.1743,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 0.6357192993164062,
      "learning_rate": 4.329207920792079e-05,
      "loss": 8.1795,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 0.46208086609840393,
      "learning_rate": 4.326113861386139e-05,
      "loss": 8.1783,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 0.5193877220153809,
      "learning_rate": 4.323019801980198e-05,
      "loss": 8.1738,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 0.5517786145210266,
      "learning_rate": 4.319925742574258e-05,
      "loss": 8.191,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 0.4851343631744385,
      "learning_rate": 4.316831683168317e-05,
      "loss": 8.18,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 0.4999909996986389,
      "learning_rate": 4.313737623762377e-05,
      "loss": 8.1893,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 1.0605361461639404,
      "learning_rate": 4.310643564356436e-05,
      "loss": 8.1847,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 1.3921605348587036,
      "learning_rate": 4.307549504950495e-05,
      "loss": 8.181,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.6739130616188049,
      "learning_rate": 4.304455445544555e-05,
      "loss": 8.202,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 0.7772232294082642,
      "learning_rate": 4.3013613861386144e-05,
      "loss": 8.1742,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 0.41060587763786316,
      "learning_rate": 4.298267326732673e-05,
      "loss": 8.1627,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 0.37843480706214905,
      "learning_rate": 4.295173267326733e-05,
      "loss": 8.1715,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 1.0329080820083618,
      "learning_rate": 4.2920792079207923e-05,
      "loss": 8.1796,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 0.31238245964050293,
      "learning_rate": 4.289294554455446e-05,
      "loss": 8.1815,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 0.4785993695259094,
      "learning_rate": 4.286200495049505e-05,
      "loss": 8.1691,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.8877915143966675,
      "learning_rate": 4.2831064356435644e-05,
      "loss": 8.1794,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 0.8918452858924866,
      "learning_rate": 4.280012376237624e-05,
      "loss": 8.176,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 0.9933529496192932,
      "learning_rate": 4.2769183168316835e-05,
      "loss": 8.1888,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 0.4408724904060364,
      "learning_rate": 4.273824257425742e-05,
      "loss": 8.1638,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 0.3677043616771698,
      "learning_rate": 4.2707301980198025e-05,
      "loss": 8.1954,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 0.8111699223518372,
      "learning_rate": 4.2676361386138614e-05,
      "loss": 8.1806,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 0.732003927230835,
      "learning_rate": 4.264542079207921e-05,
      "loss": 8.1785,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 0.7386962175369263,
      "learning_rate": 4.2614480198019805e-05,
      "loss": 8.1704,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 0.35727474093437195,
      "learning_rate": 4.25835396039604e-05,
      "loss": 8.1704,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 0.5202633738517761,
      "learning_rate": 4.255259900990099e-05,
      "loss": 8.1765,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 0.7533573508262634,
      "learning_rate": 4.2521658415841584e-05,
      "loss": 8.1878,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 0.5169065594673157,
      "learning_rate": 4.249071782178218e-05,
      "loss": 8.1817,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 0.3747626543045044,
      "learning_rate": 4.2459777227722775e-05,
      "loss": 8.1904,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 0.8353764414787292,
      "learning_rate": 4.2428836633663364e-05,
      "loss": 8.1847,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.47149935364723206,
      "learning_rate": 4.2397896039603966e-05,
      "loss": 8.1857,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.5039201378822327,
      "learning_rate": 4.2366955445544554e-05,
      "loss": 8.1584,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 0.39962735772132874,
      "learning_rate": 4.233601485148515e-05,
      "loss": 8.1755,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 0.4213845729827881,
      "learning_rate": 4.2305074257425745e-05,
      "loss": 8.1911,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 0.7187732458114624,
      "learning_rate": 4.227413366336634e-05,
      "loss": 8.1729,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.3765415549278259,
      "learning_rate": 4.224319306930693e-05,
      "loss": 8.1917,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 0.4099287986755371,
      "learning_rate": 4.2212252475247525e-05,
      "loss": 8.1763,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 0.5655828714370728,
      "learning_rate": 4.218131188118813e-05,
      "loss": 8.1521,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 0.3301551640033722,
      "learning_rate": 4.2150371287128715e-05,
      "loss": 8.1814,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 1.0062203407287598,
      "learning_rate": 4.211943069306931e-05,
      "loss": 8.17,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 0.3695231080055237,
      "learning_rate": 4.2088490099009906e-05,
      "loss": 8.1822,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 0.5440958738327026,
      "learning_rate": 4.20575495049505e-05,
      "loss": 8.1831,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 0.5230558514595032,
      "learning_rate": 4.202660891089109e-05,
      "loss": 8.1775,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 0.7811059951782227,
      "learning_rate": 4.1995668316831686e-05,
      "loss": 8.1838,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 0.7115465402603149,
      "learning_rate": 4.196472772277228e-05,
      "loss": 8.184,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 0.5242425799369812,
      "learning_rate": 4.1933787128712876e-05,
      "loss": 8.172,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 0.6848767399787903,
      "learning_rate": 4.1902846534653465e-05,
      "loss": 8.1575,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 0.9786973595619202,
      "learning_rate": 4.187190594059407e-05,
      "loss": 8.1607,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 0.40516024827957153,
      "learning_rate": 4.1840965346534656e-05,
      "loss": 8.1797,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 0.4008958339691162,
      "learning_rate": 4.181002475247525e-05,
      "loss": 8.1882,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 0.9412573575973511,
      "learning_rate": 4.1779084158415846e-05,
      "loss": 8.1745,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 0.5629944205284119,
      "learning_rate": 4.174814356435644e-05,
      "loss": 8.1801,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 0.6402981877326965,
      "learning_rate": 4.171720297029703e-05,
      "loss": 8.1934,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 0.9342613816261292,
      "learning_rate": 4.1686262376237626e-05,
      "loss": 8.177,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 1.2738996744155884,
      "learning_rate": 4.165532178217822e-05,
      "loss": 8.1711,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 0.5073431134223938,
      "learning_rate": 4.1624381188118817e-05,
      "loss": 8.1783,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 0.41454824805259705,
      "learning_rate": 4.1593440594059405e-05,
      "loss": 8.1973,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 1.1496150493621826,
      "learning_rate": 4.156250000000001e-05,
      "loss": 8.1858,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 0.4803610146045685,
      "learning_rate": 4.1531559405940596e-05,
      "loss": 8.1708,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.34687677025794983,
      "learning_rate": 4.150061881188119e-05,
      "loss": 8.1863,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 0.5241560339927673,
      "learning_rate": 4.146967821782179e-05,
      "loss": 8.1912,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 0.7502440810203552,
      "learning_rate": 4.143873762376238e-05,
      "loss": 8.1845,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 0.9989693760871887,
      "learning_rate": 4.140779702970297e-05,
      "loss": 8.1773,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.6086286902427673,
      "learning_rate": 4.1376856435643566e-05,
      "loss": 8.1675,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.40269002318382263,
      "learning_rate": 4.134591584158416e-05,
      "loss": 8.1915,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.3288612365722656,
      "learning_rate": 4.131497524752476e-05,
      "loss": 8.1653,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 0.5025574564933777,
      "learning_rate": 4.1284034653465346e-05,
      "loss": 8.1629,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 0.3438236117362976,
      "learning_rate": 4.125309405940595e-05,
      "loss": 8.2046,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.5780764818191528,
      "learning_rate": 4.1222153465346536e-05,
      "loss": 8.1778,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.39652180671691895,
      "learning_rate": 4.119121287128713e-05,
      "loss": 8.1823,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 0.6715706586837769,
      "learning_rate": 4.116027227722773e-05,
      "loss": 8.1738,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.3920033276081085,
      "learning_rate": 4.112933168316832e-05,
      "loss": 8.1781,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 0.39238977432250977,
      "learning_rate": 4.109839108910891e-05,
      "loss": 8.1748,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 0.3157040774822235,
      "learning_rate": 4.1067450495049506e-05,
      "loss": 8.1726,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 0.7599718570709229,
      "learning_rate": 4.10365099009901e-05,
      "loss": 8.1836,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 1.2285321950912476,
      "learning_rate": 4.10055693069307e-05,
      "loss": 8.1901,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 0.9450066089630127,
      "learning_rate": 4.0974628712871286e-05,
      "loss": 8.1854,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 1.1108378171920776,
      "learning_rate": 4.094368811881189e-05,
      "loss": 8.1845,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 0.4957656264305115,
      "learning_rate": 4.091274752475248e-05,
      "loss": 8.1684,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 0.7190476059913635,
      "learning_rate": 4.088180693069307e-05,
      "loss": 8.1668,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 0.3910123109817505,
      "learning_rate": 4.085086633663367e-05,
      "loss": 8.1671,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 0.6209633946418762,
      "learning_rate": 4.081992574257426e-05,
      "loss": 8.1741,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 0.6628877520561218,
      "learning_rate": 4.078898514851485e-05,
      "loss": 8.1724,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 1.0487492084503174,
      "learning_rate": 4.075804455445545e-05,
      "loss": 8.1865,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 0.7827565670013428,
      "learning_rate": 4.072710396039604e-05,
      "loss": 8.1843,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.3538581430912018,
      "learning_rate": 4.069616336633664e-05,
      "loss": 8.1539,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.47143933176994324,
      "learning_rate": 4.0665222772277226e-05,
      "loss": 8.1788,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.5566879510879517,
      "learning_rate": 4.063428217821783e-05,
      "loss": 8.175,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 0.7525437474250793,
      "learning_rate": 4.060334158415842e-05,
      "loss": 8.1687,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 1.0151294469833374,
      "learning_rate": 4.057240099009901e-05,
      "loss": 8.1862,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 0.4681243896484375,
      "learning_rate": 4.054146039603961e-05,
      "loss": 8.1858,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 0.8434861898422241,
      "learning_rate": 4.05105198019802e-05,
      "loss": 8.1885,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 0.3857128918170929,
      "learning_rate": 4.047957920792079e-05,
      "loss": 8.1875,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 0.7138345241546631,
      "learning_rate": 4.044863861386139e-05,
      "loss": 8.1638,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 0.500750720500946,
      "learning_rate": 4.041769801980198e-05,
      "loss": 8.1765,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 0.5411683917045593,
      "learning_rate": 4.038675742574258e-05,
      "loss": 8.1828,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 0.559773325920105,
      "learning_rate": 4.0355816831683166e-05,
      "loss": 8.1659,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 0.5506566166877747,
      "learning_rate": 4.032487623762377e-05,
      "loss": 8.1777,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 1.041272521018982,
      "learning_rate": 4.029393564356436e-05,
      "loss": 8.1708,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 0.5980176329612732,
      "learning_rate": 4.0266089108910894e-05,
      "loss": 8.1868,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.670759916305542,
      "learning_rate": 4.023514851485149e-05,
      "loss": 8.1805,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 0.4230586588382721,
      "learning_rate": 4.020420792079208e-05,
      "loss": 8.1705,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 0.525131106376648,
      "learning_rate": 4.017326732673268e-05,
      "loss": 8.1747,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.4320549964904785,
      "learning_rate": 4.014232673267327e-05,
      "loss": 8.1686,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 0.784473717212677,
      "learning_rate": 4.0111386138613864e-05,
      "loss": 8.1712,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 0.6629128456115723,
      "learning_rate": 4.008044554455446e-05,
      "loss": 8.175,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.6317083835601807,
      "learning_rate": 4.0049504950495055e-05,
      "loss": 8.17,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 0.9099751114845276,
      "learning_rate": 4.001856435643564e-05,
      "loss": 8.1644,
      "step": 3230
    },
    {
      "epoch": 1.002242672647127,
      "grad_norm": 0.6348801255226135,
      "learning_rate": 3.998762376237624e-05,
      "loss": 8.1658,
      "step": 3240
    },
    {
      "epoch": 1.0053360142293712,
      "grad_norm": 0.6351884603500366,
      "learning_rate": 3.9956683168316834e-05,
      "loss": 8.1827,
      "step": 3250
    },
    {
      "epoch": 1.0084293558116155,
      "grad_norm": 0.49575138092041016,
      "learning_rate": 3.992574257425743e-05,
      "loss": 8.1627,
      "step": 3260
    },
    {
      "epoch": 1.0115226973938598,
      "grad_norm": 0.5132550001144409,
      "learning_rate": 3.989480198019802e-05,
      "loss": 8.1571,
      "step": 3270
    },
    {
      "epoch": 1.0146160389761039,
      "grad_norm": 0.5768378973007202,
      "learning_rate": 3.986386138613862e-05,
      "loss": 8.1652,
      "step": 3280
    },
    {
      "epoch": 1.0177093805583481,
      "grad_norm": 0.7471145391464233,
      "learning_rate": 3.983292079207921e-05,
      "loss": 8.1656,
      "step": 3290
    },
    {
      "epoch": 1.0208027221405924,
      "grad_norm": 1.101629376411438,
      "learning_rate": 3.9801980198019804e-05,
      "loss": 8.1832,
      "step": 3300
    },
    {
      "epoch": 1.0238960637228367,
      "grad_norm": 1.0176151990890503,
      "learning_rate": 3.97710396039604e-05,
      "loss": 8.1837,
      "step": 3310
    },
    {
      "epoch": 1.0269894053050808,
      "grad_norm": 0.5809954404830933,
      "learning_rate": 3.9740099009900995e-05,
      "loss": 8.1816,
      "step": 3320
    },
    {
      "epoch": 1.030082746887325,
      "grad_norm": 0.6899362802505493,
      "learning_rate": 3.9709158415841584e-05,
      "loss": 8.1649,
      "step": 3330
    },
    {
      "epoch": 1.0331760884695693,
      "grad_norm": 0.38334810733795166,
      "learning_rate": 3.967821782178218e-05,
      "loss": 8.1794,
      "step": 3340
    },
    {
      "epoch": 1.0362694300518134,
      "grad_norm": 0.3732758164405823,
      "learning_rate": 3.9647277227722774e-05,
      "loss": 8.1618,
      "step": 3350
    },
    {
      "epoch": 1.0393627716340577,
      "grad_norm": 0.5150274634361267,
      "learning_rate": 3.961633663366337e-05,
      "loss": 8.1762,
      "step": 3360
    },
    {
      "epoch": 1.042456113216302,
      "grad_norm": 0.37129005789756775,
      "learning_rate": 3.958539603960396e-05,
      "loss": 8.1802,
      "step": 3370
    },
    {
      "epoch": 1.0455494547985462,
      "grad_norm": 0.3833334445953369,
      "learning_rate": 3.955445544554456e-05,
      "loss": 8.1613,
      "step": 3380
    },
    {
      "epoch": 1.0486427963807903,
      "grad_norm": 0.35206758975982666,
      "learning_rate": 3.952351485148515e-05,
      "loss": 8.1632,
      "step": 3390
    },
    {
      "epoch": 1.0517361379630346,
      "grad_norm": 0.32711541652679443,
      "learning_rate": 3.9492574257425745e-05,
      "loss": 8.1759,
      "step": 3400
    },
    {
      "epoch": 1.0548294795452788,
      "grad_norm": 0.547103226184845,
      "learning_rate": 3.946163366336634e-05,
      "loss": 8.1713,
      "step": 3410
    },
    {
      "epoch": 1.057922821127523,
      "grad_norm": 0.6114354133605957,
      "learning_rate": 3.9430693069306935e-05,
      "loss": 8.1596,
      "step": 3420
    },
    {
      "epoch": 1.0610161627097672,
      "grad_norm": 0.4550657570362091,
      "learning_rate": 3.9399752475247524e-05,
      "loss": 8.1756,
      "step": 3430
    },
    {
      "epoch": 1.0641095042920115,
      "grad_norm": 0.6956331133842468,
      "learning_rate": 3.936881188118812e-05,
      "loss": 8.1698,
      "step": 3440
    },
    {
      "epoch": 1.0672028458742557,
      "grad_norm": 0.41102033853530884,
      "learning_rate": 3.9337871287128715e-05,
      "loss": 8.1826,
      "step": 3450
    },
    {
      "epoch": 1.0702961874564998,
      "grad_norm": 0.6142637729644775,
      "learning_rate": 3.930693069306931e-05,
      "loss": 8.1853,
      "step": 3460
    },
    {
      "epoch": 1.073389529038744,
      "grad_norm": 0.5655266642570496,
      "learning_rate": 3.92759900990099e-05,
      "loss": 8.167,
      "step": 3470
    },
    {
      "epoch": 1.0764828706209884,
      "grad_norm": 0.5725334882736206,
      "learning_rate": 3.92450495049505e-05,
      "loss": 8.1823,
      "step": 3480
    },
    {
      "epoch": 1.0795762122032326,
      "grad_norm": 0.7935580015182495,
      "learning_rate": 3.921410891089109e-05,
      "loss": 8.1934,
      "step": 3490
    },
    {
      "epoch": 1.0826695537854767,
      "grad_norm": 0.3340928852558136,
      "learning_rate": 3.9183168316831685e-05,
      "loss": 8.1648,
      "step": 3500
    },
    {
      "epoch": 1.085762895367721,
      "grad_norm": 0.35854464769363403,
      "learning_rate": 3.915222772277228e-05,
      "loss": 8.1636,
      "step": 3510
    },
    {
      "epoch": 1.0888562369499653,
      "grad_norm": 0.9309941530227661,
      "learning_rate": 3.9121287128712876e-05,
      "loss": 8.1722,
      "step": 3520
    },
    {
      "epoch": 1.0919495785322093,
      "grad_norm": 0.47656118869781494,
      "learning_rate": 3.9090346534653464e-05,
      "loss": 8.1864,
      "step": 3530
    },
    {
      "epoch": 1.0950429201144536,
      "grad_norm": 0.555409848690033,
      "learning_rate": 3.905940594059406e-05,
      "loss": 8.1765,
      "step": 3540
    },
    {
      "epoch": 1.0981362616966979,
      "grad_norm": 0.44393184781074524,
      "learning_rate": 3.9028465346534655e-05,
      "loss": 8.1811,
      "step": 3550
    },
    {
      "epoch": 1.1012296032789421,
      "grad_norm": 0.5170543193817139,
      "learning_rate": 3.899752475247525e-05,
      "loss": 8.1641,
      "step": 3560
    },
    {
      "epoch": 1.1043229448611862,
      "grad_norm": 0.7430124282836914,
      "learning_rate": 3.896658415841584e-05,
      "loss": 8.1791,
      "step": 3570
    },
    {
      "epoch": 1.1074162864434305,
      "grad_norm": 0.7570461630821228,
      "learning_rate": 3.893564356435644e-05,
      "loss": 8.1683,
      "step": 3580
    },
    {
      "epoch": 1.1105096280256748,
      "grad_norm": 0.49783843755722046,
      "learning_rate": 3.890470297029703e-05,
      "loss": 8.1652,
      "step": 3590
    },
    {
      "epoch": 1.113602969607919,
      "grad_norm": 1.001810073852539,
      "learning_rate": 3.8873762376237625e-05,
      "loss": 8.1802,
      "step": 3600
    },
    {
      "epoch": 1.116696311190163,
      "grad_norm": 0.8178627490997314,
      "learning_rate": 3.884282178217822e-05,
      "loss": 8.1762,
      "step": 3610
    },
    {
      "epoch": 1.1197896527724074,
      "grad_norm": 0.42753517627716064,
      "learning_rate": 3.8811881188118816e-05,
      "loss": 8.1685,
      "step": 3620
    },
    {
      "epoch": 1.1228829943546517,
      "grad_norm": 1.1133396625518799,
      "learning_rate": 3.8780940594059405e-05,
      "loss": 8.1936,
      "step": 3630
    },
    {
      "epoch": 1.1259763359368957,
      "grad_norm": 0.9697826504707336,
      "learning_rate": 3.875e-05,
      "loss": 8.1711,
      "step": 3640
    },
    {
      "epoch": 1.12906967751914,
      "grad_norm": 0.44727304577827454,
      "learning_rate": 3.8719059405940595e-05,
      "loss": 8.1778,
      "step": 3650
    },
    {
      "epoch": 1.1321630191013843,
      "grad_norm": 0.43446990847587585,
      "learning_rate": 3.868811881188119e-05,
      "loss": 8.1816,
      "step": 3660
    },
    {
      "epoch": 1.1352563606836286,
      "grad_norm": 0.6489114761352539,
      "learning_rate": 3.865717821782178e-05,
      "loss": 8.1639,
      "step": 3670
    },
    {
      "epoch": 1.1383497022658726,
      "grad_norm": 0.330847829580307,
      "learning_rate": 3.862623762376238e-05,
      "loss": 8.1666,
      "step": 3680
    },
    {
      "epoch": 1.141443043848117,
      "grad_norm": 0.5782990455627441,
      "learning_rate": 3.859529702970297e-05,
      "loss": 8.1611,
      "step": 3690
    },
    {
      "epoch": 1.1445363854303612,
      "grad_norm": 0.5211999416351318,
      "learning_rate": 3.8564356435643566e-05,
      "loss": 8.1719,
      "step": 3700
    },
    {
      "epoch": 1.1476297270126055,
      "grad_norm": 0.8030667901039124,
      "learning_rate": 3.853341584158416e-05,
      "loss": 8.1579,
      "step": 3710
    },
    {
      "epoch": 1.1507230685948495,
      "grad_norm": 0.39496004581451416,
      "learning_rate": 3.8502475247524756e-05,
      "loss": 8.1703,
      "step": 3720
    },
    {
      "epoch": 1.1538164101770938,
      "grad_norm": 0.3681910037994385,
      "learning_rate": 3.8471534653465345e-05,
      "loss": 8.1657,
      "step": 3730
    },
    {
      "epoch": 1.156909751759338,
      "grad_norm": 0.6077080965042114,
      "learning_rate": 3.844059405940594e-05,
      "loss": 8.1798,
      "step": 3740
    },
    {
      "epoch": 1.1600030933415821,
      "grad_norm": 0.5253683924674988,
      "learning_rate": 3.8409653465346536e-05,
      "loss": 8.1634,
      "step": 3750
    },
    {
      "epoch": 1.1630964349238264,
      "grad_norm": 0.4561350345611572,
      "learning_rate": 3.837871287128713e-05,
      "loss": 8.1762,
      "step": 3760
    },
    {
      "epoch": 1.1661897765060707,
      "grad_norm": 0.32342031598091125,
      "learning_rate": 3.834777227722772e-05,
      "loss": 8.1678,
      "step": 3770
    },
    {
      "epoch": 1.169283118088315,
      "grad_norm": 0.5644944906234741,
      "learning_rate": 3.831683168316832e-05,
      "loss": 8.1674,
      "step": 3780
    },
    {
      "epoch": 1.172376459670559,
      "grad_norm": 0.30638423562049866,
      "learning_rate": 3.828589108910891e-05,
      "loss": 8.1786,
      "step": 3790
    },
    {
      "epoch": 1.1754698012528033,
      "grad_norm": 0.6631038188934326,
      "learning_rate": 3.8254950495049506e-05,
      "loss": 8.1827,
      "step": 3800
    },
    {
      "epoch": 1.1785631428350476,
      "grad_norm": 0.483622282743454,
      "learning_rate": 3.82240099009901e-05,
      "loss": 8.1757,
      "step": 3810
    },
    {
      "epoch": 1.1816564844172919,
      "grad_norm": 0.5524351000785828,
      "learning_rate": 3.81930693069307e-05,
      "loss": 8.177,
      "step": 3820
    },
    {
      "epoch": 1.184749825999536,
      "grad_norm": 0.402566522359848,
      "learning_rate": 3.8162128712871285e-05,
      "loss": 8.1556,
      "step": 3830
    },
    {
      "epoch": 1.1878431675817802,
      "grad_norm": 0.3322209417819977,
      "learning_rate": 3.813118811881188e-05,
      "loss": 8.177,
      "step": 3840
    },
    {
      "epoch": 1.1909365091640245,
      "grad_norm": 0.5382733941078186,
      "learning_rate": 3.8100247524752476e-05,
      "loss": 8.1699,
      "step": 3850
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 0.45760926604270935,
      "learning_rate": 3.806930693069307e-05,
      "loss": 8.1755,
      "step": 3860
    },
    {
      "epoch": 1.1971231923285128,
      "grad_norm": 0.5602229833602905,
      "learning_rate": 3.803836633663366e-05,
      "loss": 8.164,
      "step": 3870
    },
    {
      "epoch": 1.200216533910757,
      "grad_norm": 0.4149854779243469,
      "learning_rate": 3.800742574257426e-05,
      "loss": 8.1819,
      "step": 3880
    },
    {
      "epoch": 1.2033098754930014,
      "grad_norm": 0.48022422194480896,
      "learning_rate": 3.797648514851485e-05,
      "loss": 8.1655,
      "step": 3890
    },
    {
      "epoch": 1.2064032170752454,
      "grad_norm": 0.4676516354084015,
      "learning_rate": 3.7945544554455446e-05,
      "loss": 8.1685,
      "step": 3900
    },
    {
      "epoch": 1.2094965586574897,
      "grad_norm": 0.4167201817035675,
      "learning_rate": 3.791460396039604e-05,
      "loss": 8.1707,
      "step": 3910
    },
    {
      "epoch": 1.212589900239734,
      "grad_norm": 0.5347010493278503,
      "learning_rate": 3.788366336633664e-05,
      "loss": 8.1682,
      "step": 3920
    },
    {
      "epoch": 1.2156832418219783,
      "grad_norm": 0.4695780575275421,
      "learning_rate": 3.7852722772277226e-05,
      "loss": 8.156,
      "step": 3930
    },
    {
      "epoch": 1.2187765834042223,
      "grad_norm": 0.42510026693344116,
      "learning_rate": 3.782178217821782e-05,
      "loss": 8.1825,
      "step": 3940
    },
    {
      "epoch": 1.2218699249864666,
      "grad_norm": 0.43321600556373596,
      "learning_rate": 3.7790841584158416e-05,
      "loss": 8.1656,
      "step": 3950
    },
    {
      "epoch": 1.224963266568711,
      "grad_norm": 1.0992358922958374,
      "learning_rate": 3.775990099009901e-05,
      "loss": 8.1703,
      "step": 3960
    },
    {
      "epoch": 1.228056608150955,
      "grad_norm": 0.9599977731704712,
      "learning_rate": 3.77289603960396e-05,
      "loss": 8.173,
      "step": 3970
    },
    {
      "epoch": 1.2311499497331992,
      "grad_norm": 1.2803956270217896,
      "learning_rate": 3.76980198019802e-05,
      "loss": 8.1604,
      "step": 3980
    },
    {
      "epoch": 1.2342432913154435,
      "grad_norm": 0.8005143404006958,
      "learning_rate": 3.766707920792079e-05,
      "loss": 8.1719,
      "step": 3990
    },
    {
      "epoch": 1.2373366328976878,
      "grad_norm": 0.36335453391075134,
      "learning_rate": 3.7636138613861386e-05,
      "loss": 8.175,
      "step": 4000
    },
    {
      "epoch": 1.2404299744799319,
      "grad_norm": 0.4224262237548828,
      "learning_rate": 3.760519801980198e-05,
      "loss": 8.1753,
      "step": 4010
    },
    {
      "epoch": 1.2435233160621761,
      "grad_norm": 0.38806942105293274,
      "learning_rate": 3.757425742574258e-05,
      "loss": 8.1707,
      "step": 4020
    },
    {
      "epoch": 1.2466166576444204,
      "grad_norm": 0.5307180881500244,
      "learning_rate": 3.7543316831683166e-05,
      "loss": 8.1719,
      "step": 4030
    },
    {
      "epoch": 1.2497099992266647,
      "grad_norm": 0.4562041759490967,
      "learning_rate": 3.751237623762376e-05,
      "loss": 8.1728,
      "step": 4040
    },
    {
      "epoch": 1.2528033408089088,
      "grad_norm": 0.4697134494781494,
      "learning_rate": 3.7481435643564363e-05,
      "loss": 8.1585,
      "step": 4050
    },
    {
      "epoch": 1.255896682391153,
      "grad_norm": 0.8708680272102356,
      "learning_rate": 3.745049504950495e-05,
      "loss": 8.1632,
      "step": 4060
    },
    {
      "epoch": 1.2589900239733973,
      "grad_norm": 0.4955705404281616,
      "learning_rate": 3.741955445544555e-05,
      "loss": 8.1591,
      "step": 4070
    },
    {
      "epoch": 1.2620833655556414,
      "grad_norm": 0.5836232900619507,
      "learning_rate": 3.738861386138614e-05,
      "loss": 8.193,
      "step": 4080
    },
    {
      "epoch": 1.2651767071378857,
      "grad_norm": 0.5244494676589966,
      "learning_rate": 3.735767326732674e-05,
      "loss": 8.1871,
      "step": 4090
    },
    {
      "epoch": 1.26827004872013,
      "grad_norm": 0.6525848507881165,
      "learning_rate": 3.732673267326733e-05,
      "loss": 8.161,
      "step": 4100
    },
    {
      "epoch": 1.2713633903023742,
      "grad_norm": 0.5583269000053406,
      "learning_rate": 3.729579207920792e-05,
      "loss": 8.1575,
      "step": 4110
    },
    {
      "epoch": 1.2744567318846183,
      "grad_norm": 0.46303462982177734,
      "learning_rate": 3.726485148514852e-05,
      "loss": 8.1534,
      "step": 4120
    },
    {
      "epoch": 1.2775500734668626,
      "grad_norm": 0.557054877281189,
      "learning_rate": 3.723391089108911e-05,
      "loss": 8.1714,
      "step": 4130
    },
    {
      "epoch": 1.2806434150491068,
      "grad_norm": 0.7607975602149963,
      "learning_rate": 3.72029702970297e-05,
      "loss": 8.1728,
      "step": 4140
    },
    {
      "epoch": 1.2837367566313511,
      "grad_norm": 0.9218924641609192,
      "learning_rate": 3.7172029702970304e-05,
      "loss": 8.173,
      "step": 4150
    },
    {
      "epoch": 1.2868300982135952,
      "grad_norm": 0.668659508228302,
      "learning_rate": 3.714108910891089e-05,
      "loss": 8.1776,
      "step": 4160
    },
    {
      "epoch": 1.2899234397958395,
      "grad_norm": 0.41032758355140686,
      "learning_rate": 3.711014851485149e-05,
      "loss": 8.1802,
      "step": 4170
    },
    {
      "epoch": 1.2930167813780837,
      "grad_norm": 0.8800378441810608,
      "learning_rate": 3.707920792079208e-05,
      "loss": 8.1745,
      "step": 4180
    },
    {
      "epoch": 1.2961101229603278,
      "grad_norm": 0.7247767448425293,
      "learning_rate": 3.704826732673268e-05,
      "loss": 8.1615,
      "step": 4190
    },
    {
      "epoch": 1.299203464542572,
      "grad_norm": 0.5068981647491455,
      "learning_rate": 3.701732673267327e-05,
      "loss": 8.1646,
      "step": 4200
    },
    {
      "epoch": 1.3022968061248164,
      "grad_norm": 0.6992303729057312,
      "learning_rate": 3.698638613861386e-05,
      "loss": 8.1743,
      "step": 4210
    },
    {
      "epoch": 1.3053901477070606,
      "grad_norm": 0.426561564207077,
      "learning_rate": 3.695544554455446e-05,
      "loss": 8.1777,
      "step": 4220
    },
    {
      "epoch": 1.3084834892893047,
      "grad_norm": 0.9839187264442444,
      "learning_rate": 3.692450495049505e-05,
      "loss": 8.1657,
      "step": 4230
    },
    {
      "epoch": 1.311576830871549,
      "grad_norm": 0.41461479663848877,
      "learning_rate": 3.689356435643564e-05,
      "loss": 8.1715,
      "step": 4240
    },
    {
      "epoch": 1.3146701724537933,
      "grad_norm": 0.3226483464241028,
      "learning_rate": 3.6862623762376244e-05,
      "loss": 8.1669,
      "step": 4250
    },
    {
      "epoch": 1.3177635140360375,
      "grad_norm": 0.5627478957176208,
      "learning_rate": 3.683168316831683e-05,
      "loss": 8.157,
      "step": 4260
    },
    {
      "epoch": 1.3208568556182816,
      "grad_norm": 0.5755347013473511,
      "learning_rate": 3.680074257425743e-05,
      "loss": 8.1722,
      "step": 4270
    },
    {
      "epoch": 1.3239501972005259,
      "grad_norm": 0.37570324540138245,
      "learning_rate": 3.6769801980198023e-05,
      "loss": 8.1653,
      "step": 4280
    },
    {
      "epoch": 1.3270435387827701,
      "grad_norm": 0.8652839660644531,
      "learning_rate": 3.673886138613862e-05,
      "loss": 8.1751,
      "step": 4290
    },
    {
      "epoch": 1.3301368803650142,
      "grad_norm": 0.3733169734477997,
      "learning_rate": 3.670792079207921e-05,
      "loss": 8.1631,
      "step": 4300
    },
    {
      "epoch": 1.3332302219472585,
      "grad_norm": 0.9434254169464111,
      "learning_rate": 3.66769801980198e-05,
      "loss": 8.1522,
      "step": 4310
    },
    {
      "epoch": 1.3363235635295028,
      "grad_norm": 0.6055405735969543,
      "learning_rate": 3.66460396039604e-05,
      "loss": 8.1708,
      "step": 4320
    },
    {
      "epoch": 1.339416905111747,
      "grad_norm": 0.5891613364219666,
      "learning_rate": 3.6615099009900994e-05,
      "loss": 8.1746,
      "step": 4330
    },
    {
      "epoch": 1.342510246693991,
      "grad_norm": 0.3987656533718109,
      "learning_rate": 3.658415841584158e-05,
      "loss": 8.1634,
      "step": 4340
    },
    {
      "epoch": 1.3456035882762354,
      "grad_norm": 0.6014886498451233,
      "learning_rate": 3.6553217821782184e-05,
      "loss": 8.1682,
      "step": 4350
    },
    {
      "epoch": 1.3486969298584797,
      "grad_norm": 0.5136784315109253,
      "learning_rate": 3.652227722772277e-05,
      "loss": 8.1773,
      "step": 4360
    },
    {
      "epoch": 1.351790271440724,
      "grad_norm": 0.38470447063446045,
      "learning_rate": 3.649133663366337e-05,
      "loss": 8.1756,
      "step": 4370
    },
    {
      "epoch": 1.354883613022968,
      "grad_norm": 0.8244905471801758,
      "learning_rate": 3.6460396039603964e-05,
      "loss": 8.1826,
      "step": 4380
    },
    {
      "epoch": 1.3579769546052123,
      "grad_norm": 0.3686842620372772,
      "learning_rate": 3.642945544554456e-05,
      "loss": 8.1695,
      "step": 4390
    },
    {
      "epoch": 1.3610702961874566,
      "grad_norm": 0.43954411149024963,
      "learning_rate": 3.639851485148515e-05,
      "loss": 8.1605,
      "step": 4400
    },
    {
      "epoch": 1.3641636377697006,
      "grad_norm": 0.4477417469024658,
      "learning_rate": 3.636757425742574e-05,
      "loss": 8.1666,
      "step": 4410
    },
    {
      "epoch": 1.367256979351945,
      "grad_norm": 0.5241524577140808,
      "learning_rate": 3.633663366336634e-05,
      "loss": 8.1785,
      "step": 4420
    },
    {
      "epoch": 1.3703503209341892,
      "grad_norm": 0.2962719798088074,
      "learning_rate": 3.6305693069306934e-05,
      "loss": 8.187,
      "step": 4430
    },
    {
      "epoch": 1.3734436625164332,
      "grad_norm": 0.8285160660743713,
      "learning_rate": 3.627475247524752e-05,
      "loss": 8.1703,
      "step": 4440
    },
    {
      "epoch": 1.3765370040986775,
      "grad_norm": 0.3879597783088684,
      "learning_rate": 3.6243811881188125e-05,
      "loss": 8.163,
      "step": 4450
    },
    {
      "epoch": 1.3796303456809218,
      "grad_norm": 0.4938552677631378,
      "learning_rate": 3.621287128712871e-05,
      "loss": 8.1732,
      "step": 4460
    },
    {
      "epoch": 1.382723687263166,
      "grad_norm": 0.4401392936706543,
      "learning_rate": 3.618193069306931e-05,
      "loss": 8.17,
      "step": 4470
    },
    {
      "epoch": 1.3858170288454104,
      "grad_norm": 0.37958964705467224,
      "learning_rate": 3.6150990099009904e-05,
      "loss": 8.1634,
      "step": 4480
    },
    {
      "epoch": 1.3889103704276544,
      "grad_norm": 0.7732598781585693,
      "learning_rate": 3.61200495049505e-05,
      "loss": 8.1703,
      "step": 4490
    },
    {
      "epoch": 1.3920037120098987,
      "grad_norm": 0.40246984362602234,
      "learning_rate": 3.608910891089109e-05,
      "loss": 8.1577,
      "step": 4500
    },
    {
      "epoch": 1.395097053592143,
      "grad_norm": 0.3205011785030365,
      "learning_rate": 3.6058168316831683e-05,
      "loss": 8.1631,
      "step": 4510
    },
    {
      "epoch": 1.398190395174387,
      "grad_norm": 0.33983364701271057,
      "learning_rate": 3.602722772277228e-05,
      "loss": 8.1603,
      "step": 4520
    },
    {
      "epoch": 1.4012837367566313,
      "grad_norm": 0.46958866715431213,
      "learning_rate": 3.5996287128712874e-05,
      "loss": 8.1823,
      "step": 4530
    },
    {
      "epoch": 1.4043770783388756,
      "grad_norm": 0.4307252764701843,
      "learning_rate": 3.596534653465346e-05,
      "loss": 8.1825,
      "step": 4540
    },
    {
      "epoch": 1.4074704199211197,
      "grad_norm": 0.6592034101486206,
      "learning_rate": 3.5934405940594065e-05,
      "loss": 8.1597,
      "step": 4550
    },
    {
      "epoch": 1.410563761503364,
      "grad_norm": 0.4989628493785858,
      "learning_rate": 3.5903465346534654e-05,
      "loss": 8.1793,
      "step": 4560
    },
    {
      "epoch": 1.4136571030856082,
      "grad_norm": 0.5643389225006104,
      "learning_rate": 3.587252475247525e-05,
      "loss": 8.1824,
      "step": 4570
    },
    {
      "epoch": 1.4167504446678525,
      "grad_norm": 0.49968382716178894,
      "learning_rate": 3.5841584158415844e-05,
      "loss": 8.166,
      "step": 4580
    },
    {
      "epoch": 1.4198437862500968,
      "grad_norm": 0.5922833681106567,
      "learning_rate": 3.581064356435644e-05,
      "loss": 8.1561,
      "step": 4590
    },
    {
      "epoch": 1.4229371278323408,
      "grad_norm": 0.6850570440292358,
      "learning_rate": 3.577970297029703e-05,
      "loss": 8.1506,
      "step": 4600
    },
    {
      "epoch": 1.4260304694145851,
      "grad_norm": 0.5602772831916809,
      "learning_rate": 3.5748762376237624e-05,
      "loss": 8.16,
      "step": 4610
    },
    {
      "epoch": 1.4291238109968294,
      "grad_norm": 0.4395243227481842,
      "learning_rate": 3.571782178217822e-05,
      "loss": 8.1673,
      "step": 4620
    },
    {
      "epoch": 1.4322171525790734,
      "grad_norm": 0.6080784797668457,
      "learning_rate": 3.5686881188118815e-05,
      "loss": 8.1748,
      "step": 4630
    },
    {
      "epoch": 1.4353104941613177,
      "grad_norm": 0.3845883905887604,
      "learning_rate": 3.56559405940594e-05,
      "loss": 8.171,
      "step": 4640
    },
    {
      "epoch": 1.438403835743562,
      "grad_norm": 0.4283733665943146,
      "learning_rate": 3.5625000000000005e-05,
      "loss": 8.1745,
      "step": 4650
    },
    {
      "epoch": 1.441497177325806,
      "grad_norm": 0.6565195918083191,
      "learning_rate": 3.5594059405940594e-05,
      "loss": 8.1655,
      "step": 4660
    },
    {
      "epoch": 1.4445905189080503,
      "grad_norm": 0.3326095938682556,
      "learning_rate": 3.556311881188119e-05,
      "loss": 8.1761,
      "step": 4670
    },
    {
      "epoch": 1.4476838604902946,
      "grad_norm": 0.4399820864200592,
      "learning_rate": 3.5532178217821785e-05,
      "loss": 8.1724,
      "step": 4680
    },
    {
      "epoch": 1.450777202072539,
      "grad_norm": 0.3996255397796631,
      "learning_rate": 3.550123762376238e-05,
      "loss": 8.1613,
      "step": 4690
    },
    {
      "epoch": 1.4538705436547832,
      "grad_norm": 0.5074949860572815,
      "learning_rate": 3.547029702970297e-05,
      "loss": 8.1715,
      "step": 4700
    },
    {
      "epoch": 1.4569638852370272,
      "grad_norm": 0.3887541592121124,
      "learning_rate": 3.5439356435643564e-05,
      "loss": 8.1706,
      "step": 4710
    },
    {
      "epoch": 1.4600572268192715,
      "grad_norm": 0.5005621314048767,
      "learning_rate": 3.540841584158416e-05,
      "loss": 8.1748,
      "step": 4720
    },
    {
      "epoch": 1.4631505684015158,
      "grad_norm": 0.39048025012016296,
      "learning_rate": 3.5377475247524755e-05,
      "loss": 8.1709,
      "step": 4730
    },
    {
      "epoch": 1.4662439099837599,
      "grad_norm": 0.7762143015861511,
      "learning_rate": 3.5346534653465344e-05,
      "loss": 8.1678,
      "step": 4740
    },
    {
      "epoch": 1.4693372515660041,
      "grad_norm": 0.3947969377040863,
      "learning_rate": 3.5315594059405946e-05,
      "loss": 8.1658,
      "step": 4750
    },
    {
      "epoch": 1.4724305931482484,
      "grad_norm": 0.44880813360214233,
      "learning_rate": 3.5284653465346534e-05,
      "loss": 8.1729,
      "step": 4760
    },
    {
      "epoch": 1.4755239347304925,
      "grad_norm": 0.7534545063972473,
      "learning_rate": 3.525371287128713e-05,
      "loss": 8.1698,
      "step": 4770
    },
    {
      "epoch": 1.4786172763127368,
      "grad_norm": 0.4010668992996216,
      "learning_rate": 3.5222772277227725e-05,
      "loss": 8.178,
      "step": 4780
    },
    {
      "epoch": 1.481710617894981,
      "grad_norm": 0.8206009268760681,
      "learning_rate": 3.519183168316832e-05,
      "loss": 8.171,
      "step": 4790
    },
    {
      "epoch": 1.4848039594772253,
      "grad_norm": 0.4439012408256531,
      "learning_rate": 3.516089108910891e-05,
      "loss": 8.1549,
      "step": 4800
    },
    {
      "epoch": 1.4878973010594696,
      "grad_norm": 0.5290629863739014,
      "learning_rate": 3.5129950495049504e-05,
      "loss": 8.1697,
      "step": 4810
    },
    {
      "epoch": 1.4909906426417137,
      "grad_norm": 0.3769945204257965,
      "learning_rate": 3.50990099009901e-05,
      "loss": 8.1671,
      "step": 4820
    },
    {
      "epoch": 1.494083984223958,
      "grad_norm": 0.5498239398002625,
      "learning_rate": 3.5068069306930695e-05,
      "loss": 8.1802,
      "step": 4830
    },
    {
      "epoch": 1.4971773258062022,
      "grad_norm": 0.6855065226554871,
      "learning_rate": 3.5037128712871284e-05,
      "loss": 8.1601,
      "step": 4840
    },
    {
      "epoch": 1.5002706673884463,
      "grad_norm": 0.3817634880542755,
      "learning_rate": 3.5006188118811886e-05,
      "loss": 8.1554,
      "step": 4850
    },
    {
      "epoch": 1.5033640089706906,
      "grad_norm": 0.37547963857650757,
      "learning_rate": 3.4975247524752475e-05,
      "loss": 8.1553,
      "step": 4860
    },
    {
      "epoch": 1.5064573505529348,
      "grad_norm": 0.6712613701820374,
      "learning_rate": 3.494430693069307e-05,
      "loss": 8.1666,
      "step": 4870
    },
    {
      "epoch": 1.509550692135179,
      "grad_norm": 0.3988344073295593,
      "learning_rate": 3.4913366336633665e-05,
      "loss": 8.173,
      "step": 4880
    },
    {
      "epoch": 1.5126440337174234,
      "grad_norm": 0.3839277923107147,
      "learning_rate": 3.488242574257426e-05,
      "loss": 8.1692,
      "step": 4890
    },
    {
      "epoch": 1.5157373752996675,
      "grad_norm": 0.3647114634513855,
      "learning_rate": 3.485148514851485e-05,
      "loss": 8.1721,
      "step": 4900
    },
    {
      "epoch": 1.5188307168819117,
      "grad_norm": 0.552370011806488,
      "learning_rate": 3.4820544554455445e-05,
      "loss": 8.1632,
      "step": 4910
    },
    {
      "epoch": 1.521924058464156,
      "grad_norm": 0.3812751770019531,
      "learning_rate": 3.478960396039604e-05,
      "loss": 8.174,
      "step": 4920
    },
    {
      "epoch": 1.5250174000464,
      "grad_norm": 0.4775852859020233,
      "learning_rate": 3.4758663366336636e-05,
      "loss": 8.1518,
      "step": 4930
    },
    {
      "epoch": 1.5281107416286444,
      "grad_norm": 0.584564745426178,
      "learning_rate": 3.4727722772277224e-05,
      "loss": 8.1758,
      "step": 4940
    },
    {
      "epoch": 1.5312040832108886,
      "grad_norm": 0.46649807691574097,
      "learning_rate": 3.4696782178217826e-05,
      "loss": 8.1678,
      "step": 4950
    },
    {
      "epoch": 1.5342974247931327,
      "grad_norm": 0.3264271318912506,
      "learning_rate": 3.4665841584158415e-05,
      "loss": 8.1818,
      "step": 4960
    },
    {
      "epoch": 1.537390766375377,
      "grad_norm": 0.4605574607849121,
      "learning_rate": 3.463490099009901e-05,
      "loss": 8.1705,
      "step": 4970
    },
    {
      "epoch": 1.5404841079576213,
      "grad_norm": 0.9003764986991882,
      "learning_rate": 3.4603960396039606e-05,
      "loss": 8.1768,
      "step": 4980
    },
    {
      "epoch": 1.5435774495398653,
      "grad_norm": 0.5597843527793884,
      "learning_rate": 3.45730198019802e-05,
      "loss": 8.1704,
      "step": 4990
    },
    {
      "epoch": 1.5466707911221098,
      "grad_norm": 0.543602705001831,
      "learning_rate": 3.454207920792079e-05,
      "loss": 8.1579,
      "step": 5000
    },
    {
      "epoch": 1.5497641327043539,
      "grad_norm": 0.49471497535705566,
      "learning_rate": 3.4511138613861385e-05,
      "loss": 8.1809,
      "step": 5010
    },
    {
      "epoch": 1.5528574742865981,
      "grad_norm": 0.5354956984519958,
      "learning_rate": 3.448019801980198e-05,
      "loss": 8.1488,
      "step": 5020
    },
    {
      "epoch": 1.5559508158688424,
      "grad_norm": 0.5647304058074951,
      "learning_rate": 3.4449257425742576e-05,
      "loss": 8.1686,
      "step": 5030
    },
    {
      "epoch": 1.5590441574510865,
      "grad_norm": 0.422491192817688,
      "learning_rate": 3.4418316831683164e-05,
      "loss": 8.1609,
      "step": 5040
    },
    {
      "epoch": 1.5621374990333308,
      "grad_norm": 0.38561728596687317,
      "learning_rate": 3.438737623762377e-05,
      "loss": 8.156,
      "step": 5050
    },
    {
      "epoch": 1.565230840615575,
      "grad_norm": 0.38403090834617615,
      "learning_rate": 3.435643564356436e-05,
      "loss": 8.1821,
      "step": 5060
    },
    {
      "epoch": 1.568324182197819,
      "grad_norm": 0.6117347478866577,
      "learning_rate": 3.432549504950495e-05,
      "loss": 8.1476,
      "step": 5070
    },
    {
      "epoch": 1.5714175237800634,
      "grad_norm": 0.38010790944099426,
      "learning_rate": 3.4294554455445546e-05,
      "loss": 8.1662,
      "step": 5080
    },
    {
      "epoch": 1.5745108653623077,
      "grad_norm": 0.3715837299823761,
      "learning_rate": 3.426361386138614e-05,
      "loss": 8.1764,
      "step": 5090
    },
    {
      "epoch": 1.5776042069445517,
      "grad_norm": 0.42724743485450745,
      "learning_rate": 3.423267326732674e-05,
      "loss": 8.1749,
      "step": 5100
    },
    {
      "epoch": 1.5806975485267962,
      "grad_norm": 0.6574492454528809,
      "learning_rate": 3.4201732673267325e-05,
      "loss": 8.1776,
      "step": 5110
    },
    {
      "epoch": 1.5837908901090403,
      "grad_norm": 0.48728203773498535,
      "learning_rate": 3.417079207920793e-05,
      "loss": 8.1605,
      "step": 5120
    },
    {
      "epoch": 1.5868842316912846,
      "grad_norm": 0.44939401745796204,
      "learning_rate": 3.4139851485148516e-05,
      "loss": 8.1595,
      "step": 5130
    },
    {
      "epoch": 1.5899775732735288,
      "grad_norm": 0.3535826504230499,
      "learning_rate": 3.410891089108911e-05,
      "loss": 8.1674,
      "step": 5140
    },
    {
      "epoch": 1.593070914855773,
      "grad_norm": 0.5897693634033203,
      "learning_rate": 3.407797029702971e-05,
      "loss": 8.182,
      "step": 5150
    },
    {
      "epoch": 1.5961642564380172,
      "grad_norm": 0.5427448749542236,
      "learning_rate": 3.40470297029703e-05,
      "loss": 8.1654,
      "step": 5160
    },
    {
      "epoch": 1.5992575980202615,
      "grad_norm": 0.24038855731487274,
      "learning_rate": 3.401608910891089e-05,
      "loss": 8.1541,
      "step": 5170
    },
    {
      "epoch": 1.6023509396025055,
      "grad_norm": 0.3429105281829834,
      "learning_rate": 3.3985148514851486e-05,
      "loss": 8.1753,
      "step": 5180
    },
    {
      "epoch": 1.6054442811847498,
      "grad_norm": 0.452815979719162,
      "learning_rate": 3.395420792079208e-05,
      "loss": 8.1682,
      "step": 5190
    },
    {
      "epoch": 1.608537622766994,
      "grad_norm": 0.35909295082092285,
      "learning_rate": 3.392326732673268e-05,
      "loss": 8.1641,
      "step": 5200
    },
    {
      "epoch": 1.6116309643492381,
      "grad_norm": 0.3611293435096741,
      "learning_rate": 3.3892326732673266e-05,
      "loss": 8.1748,
      "step": 5210
    },
    {
      "epoch": 1.6147243059314826,
      "grad_norm": 0.5077478289604187,
      "learning_rate": 3.386138613861387e-05,
      "loss": 8.1515,
      "step": 5220
    },
    {
      "epoch": 1.6178176475137267,
      "grad_norm": 0.4081264138221741,
      "learning_rate": 3.3830445544554456e-05,
      "loss": 8.1743,
      "step": 5230
    },
    {
      "epoch": 1.620910989095971,
      "grad_norm": 0.5259987115859985,
      "learning_rate": 3.379950495049505e-05,
      "loss": 8.1829,
      "step": 5240
    },
    {
      "epoch": 1.6240043306782153,
      "grad_norm": 0.5444220900535583,
      "learning_rate": 3.376856435643565e-05,
      "loss": 8.1728,
      "step": 5250
    },
    {
      "epoch": 1.6270976722604593,
      "grad_norm": 0.6380795836448669,
      "learning_rate": 3.373762376237624e-05,
      "loss": 8.1742,
      "step": 5260
    },
    {
      "epoch": 1.6301910138427036,
      "grad_norm": 0.3863075375556946,
      "learning_rate": 3.370668316831683e-05,
      "loss": 8.1711,
      "step": 5270
    },
    {
      "epoch": 1.6332843554249479,
      "grad_norm": 0.5768603086471558,
      "learning_rate": 3.367574257425743e-05,
      "loss": 8.1878,
      "step": 5280
    },
    {
      "epoch": 1.636377697007192,
      "grad_norm": 0.3852751553058624,
      "learning_rate": 3.364480198019802e-05,
      "loss": 8.1601,
      "step": 5290
    },
    {
      "epoch": 1.6394710385894362,
      "grad_norm": 0.3354731500148773,
      "learning_rate": 3.361386138613862e-05,
      "loss": 8.1672,
      "step": 5300
    },
    {
      "epoch": 1.6425643801716805,
      "grad_norm": 0.3479693531990051,
      "learning_rate": 3.3582920792079206e-05,
      "loss": 8.1654,
      "step": 5310
    },
    {
      "epoch": 1.6456577217539246,
      "grad_norm": 0.3127713203430176,
      "learning_rate": 3.355198019801981e-05,
      "loss": 8.1622,
      "step": 5320
    },
    {
      "epoch": 1.648751063336169,
      "grad_norm": 0.5513017773628235,
      "learning_rate": 3.35210396039604e-05,
      "loss": 8.154,
      "step": 5330
    },
    {
      "epoch": 1.6518444049184131,
      "grad_norm": 0.41051268577575684,
      "learning_rate": 3.349009900990099e-05,
      "loss": 8.1582,
      "step": 5340
    },
    {
      "epoch": 1.6549377465006572,
      "grad_norm": 0.43726861476898193,
      "learning_rate": 3.345915841584159e-05,
      "loss": 8.1662,
      "step": 5350
    },
    {
      "epoch": 1.6580310880829017,
      "grad_norm": 0.5665802955627441,
      "learning_rate": 3.342821782178218e-05,
      "loss": 8.1607,
      "step": 5360
    },
    {
      "epoch": 1.6611244296651457,
      "grad_norm": 0.4607682526111603,
      "learning_rate": 3.339727722772277e-05,
      "loss": 8.173,
      "step": 5370
    },
    {
      "epoch": 1.66421777124739,
      "grad_norm": 0.6222150921821594,
      "learning_rate": 3.336633663366337e-05,
      "loss": 8.1755,
      "step": 5380
    },
    {
      "epoch": 1.6673111128296343,
      "grad_norm": 0.41316312551498413,
      "learning_rate": 3.333539603960396e-05,
      "loss": 8.1651,
      "step": 5390
    },
    {
      "epoch": 1.6704044544118783,
      "grad_norm": 0.5107308626174927,
      "learning_rate": 3.330445544554456e-05,
      "loss": 8.1745,
      "step": 5400
    },
    {
      "epoch": 1.6734977959941226,
      "grad_norm": 0.37601420283317566,
      "learning_rate": 3.3273514851485146e-05,
      "loss": 8.153,
      "step": 5410
    },
    {
      "epoch": 1.676591137576367,
      "grad_norm": 0.48310065269470215,
      "learning_rate": 3.324257425742575e-05,
      "loss": 8.1413,
      "step": 5420
    },
    {
      "epoch": 1.679684479158611,
      "grad_norm": 0.398745596408844,
      "learning_rate": 3.321163366336634e-05,
      "loss": 8.1496,
      "step": 5430
    },
    {
      "epoch": 1.6827778207408555,
      "grad_norm": 0.49276360869407654,
      "learning_rate": 3.318069306930693e-05,
      "loss": 8.1767,
      "step": 5440
    },
    {
      "epoch": 1.6858711623230995,
      "grad_norm": 0.6312670707702637,
      "learning_rate": 3.314975247524753e-05,
      "loss": 8.1767,
      "step": 5450
    },
    {
      "epoch": 1.6889645039053436,
      "grad_norm": 0.5154419541358948,
      "learning_rate": 3.311881188118812e-05,
      "loss": 8.1722,
      "step": 5460
    },
    {
      "epoch": 1.692057845487588,
      "grad_norm": 0.5624434351921082,
      "learning_rate": 3.308787128712871e-05,
      "loss": 8.1775,
      "step": 5470
    },
    {
      "epoch": 1.6951511870698321,
      "grad_norm": 0.4224127531051636,
      "learning_rate": 3.305693069306931e-05,
      "loss": 8.1758,
      "step": 5480
    },
    {
      "epoch": 1.6982445286520764,
      "grad_norm": 0.3973025679588318,
      "learning_rate": 3.30259900990099e-05,
      "loss": 8.1749,
      "step": 5490
    },
    {
      "epoch": 1.7013378702343207,
      "grad_norm": 0.38488975167274475,
      "learning_rate": 3.29950495049505e-05,
      "loss": 8.1586,
      "step": 5500
    },
    {
      "epoch": 1.7044312118165648,
      "grad_norm": 0.4509301781654358,
      "learning_rate": 3.296410891089109e-05,
      "loss": 8.1612,
      "step": 5510
    },
    {
      "epoch": 1.707524553398809,
      "grad_norm": 0.6962047815322876,
      "learning_rate": 3.293316831683169e-05,
      "loss": 8.1817,
      "step": 5520
    },
    {
      "epoch": 1.7106178949810533,
      "grad_norm": 0.5579096078872681,
      "learning_rate": 3.290222772277228e-05,
      "loss": 8.1584,
      "step": 5530
    },
    {
      "epoch": 1.7137112365632974,
      "grad_norm": 0.3957138955593109,
      "learning_rate": 3.287128712871287e-05,
      "loss": 8.1646,
      "step": 5540
    },
    {
      "epoch": 1.7168045781455419,
      "grad_norm": 0.5171396136283875,
      "learning_rate": 3.284034653465347e-05,
      "loss": 8.1717,
      "step": 5550
    },
    {
      "epoch": 1.719897919727786,
      "grad_norm": 0.9275133609771729,
      "learning_rate": 3.2809405940594064e-05,
      "loss": 8.1604,
      "step": 5560
    },
    {
      "epoch": 1.72299126131003,
      "grad_norm": 0.664205014705658,
      "learning_rate": 3.277846534653465e-05,
      "loss": 8.1716,
      "step": 5570
    },
    {
      "epoch": 1.7260846028922745,
      "grad_norm": 0.29770612716674805,
      "learning_rate": 3.274752475247525e-05,
      "loss": 8.1692,
      "step": 5580
    },
    {
      "epoch": 1.7291779444745186,
      "grad_norm": 0.5155131220817566,
      "learning_rate": 3.271658415841584e-05,
      "loss": 8.1601,
      "step": 5590
    },
    {
      "epoch": 1.7322712860567628,
      "grad_norm": 0.7578244209289551,
      "learning_rate": 3.268564356435644e-05,
      "loss": 8.1851,
      "step": 5600
    },
    {
      "epoch": 1.7353646276390071,
      "grad_norm": 0.34336432814598083,
      "learning_rate": 3.265470297029703e-05,
      "loss": 8.1685,
      "step": 5610
    },
    {
      "epoch": 1.7384579692212512,
      "grad_norm": 0.47416606545448303,
      "learning_rate": 3.262376237623763e-05,
      "loss": 8.1736,
      "step": 5620
    },
    {
      "epoch": 1.7415513108034955,
      "grad_norm": 0.5468149185180664,
      "learning_rate": 3.259282178217822e-05,
      "loss": 8.1736,
      "step": 5630
    },
    {
      "epoch": 1.7446446523857397,
      "grad_norm": 0.6188908219337463,
      "learning_rate": 3.256188118811881e-05,
      "loss": 8.1779,
      "step": 5640
    },
    {
      "epoch": 1.7477379939679838,
      "grad_norm": 0.4727085530757904,
      "learning_rate": 3.253094059405941e-05,
      "loss": 8.1658,
      "step": 5650
    },
    {
      "epoch": 1.7508313355502283,
      "grad_norm": 0.44943541288375854,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 8.1708,
      "step": 5660
    },
    {
      "epoch": 1.7539246771324724,
      "grad_norm": 0.47309285402297974,
      "learning_rate": 3.246905940594059e-05,
      "loss": 8.1791,
      "step": 5670
    },
    {
      "epoch": 1.7570180187147164,
      "grad_norm": 0.3204560875892639,
      "learning_rate": 3.243811881188119e-05,
      "loss": 8.1805,
      "step": 5680
    },
    {
      "epoch": 1.760111360296961,
      "grad_norm": 0.38255470991134644,
      "learning_rate": 3.240717821782178e-05,
      "loss": 8.1718,
      "step": 5690
    },
    {
      "epoch": 1.763204701879205,
      "grad_norm": 0.53676837682724,
      "learning_rate": 3.237623762376238e-05,
      "loss": 8.1646,
      "step": 5700
    },
    {
      "epoch": 1.7662980434614493,
      "grad_norm": 0.40324854850769043,
      "learning_rate": 3.234529702970297e-05,
      "loss": 8.1775,
      "step": 5710
    },
    {
      "epoch": 1.7693913850436935,
      "grad_norm": 0.463257372379303,
      "learning_rate": 3.231435643564357e-05,
      "loss": 8.1595,
      "step": 5720
    },
    {
      "epoch": 1.7724847266259376,
      "grad_norm": 0.3828296363353729,
      "learning_rate": 3.228341584158416e-05,
      "loss": 8.1613,
      "step": 5730
    },
    {
      "epoch": 1.7755780682081819,
      "grad_norm": 0.4375844895839691,
      "learning_rate": 3.2252475247524753e-05,
      "loss": 8.1722,
      "step": 5740
    },
    {
      "epoch": 1.7786714097904262,
      "grad_norm": 0.39044901728630066,
      "learning_rate": 3.222153465346535e-05,
      "loss": 8.1539,
      "step": 5750
    },
    {
      "epoch": 1.7817647513726702,
      "grad_norm": 0.5345316529273987,
      "learning_rate": 3.2190594059405944e-05,
      "loss": 8.1605,
      "step": 5760
    },
    {
      "epoch": 1.7848580929549147,
      "grad_norm": 0.3460783064365387,
      "learning_rate": 3.215965346534653e-05,
      "loss": 8.17,
      "step": 5770
    },
    {
      "epoch": 1.7879514345371588,
      "grad_norm": 0.3588245213031769,
      "learning_rate": 3.212871287128713e-05,
      "loss": 8.1633,
      "step": 5780
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 0.3924850523471832,
      "learning_rate": 3.2097772277227724e-05,
      "loss": 8.1677,
      "step": 5790
    },
    {
      "epoch": 1.7941381177016473,
      "grad_norm": 0.5430410504341125,
      "learning_rate": 3.206683168316832e-05,
      "loss": 8.1645,
      "step": 5800
    },
    {
      "epoch": 1.7972314592838914,
      "grad_norm": 0.3443145751953125,
      "learning_rate": 3.203589108910891e-05,
      "loss": 8.1671,
      "step": 5810
    },
    {
      "epoch": 1.8003248008661357,
      "grad_norm": 0.37150895595550537,
      "learning_rate": 3.200495049504951e-05,
      "loss": 8.1668,
      "step": 5820
    },
    {
      "epoch": 1.80341814244838,
      "grad_norm": 0.30918431282043457,
      "learning_rate": 3.19740099009901e-05,
      "loss": 8.1578,
      "step": 5830
    },
    {
      "epoch": 1.806511484030624,
      "grad_norm": 0.3542013168334961,
      "learning_rate": 3.1943069306930694e-05,
      "loss": 8.1553,
      "step": 5840
    },
    {
      "epoch": 1.8096048256128683,
      "grad_norm": 0.3483993709087372,
      "learning_rate": 3.191212871287129e-05,
      "loss": 8.1637,
      "step": 5850
    },
    {
      "epoch": 1.8126981671951126,
      "grad_norm": 0.4810127019882202,
      "learning_rate": 3.1881188118811885e-05,
      "loss": 8.1771,
      "step": 5860
    },
    {
      "epoch": 1.8157915087773566,
      "grad_norm": 0.5056300759315491,
      "learning_rate": 3.185024752475247e-05,
      "loss": 8.1522,
      "step": 5870
    },
    {
      "epoch": 1.8188848503596011,
      "grad_norm": 0.6009425520896912,
      "learning_rate": 3.181930693069307e-05,
      "loss": 8.1671,
      "step": 5880
    },
    {
      "epoch": 1.8219781919418452,
      "grad_norm": 0.6519317030906677,
      "learning_rate": 3.1788366336633664e-05,
      "loss": 8.174,
      "step": 5890
    },
    {
      "epoch": 1.8250715335240892,
      "grad_norm": 0.703974187374115,
      "learning_rate": 3.175742574257426e-05,
      "loss": 8.1559,
      "step": 5900
    },
    {
      "epoch": 1.8281648751063337,
      "grad_norm": 0.5562006235122681,
      "learning_rate": 3.172648514851485e-05,
      "loss": 8.1526,
      "step": 5910
    },
    {
      "epoch": 1.8312582166885778,
      "grad_norm": 0.5313023924827576,
      "learning_rate": 3.169554455445545e-05,
      "loss": 8.1676,
      "step": 5920
    },
    {
      "epoch": 1.834351558270822,
      "grad_norm": 0.41276589035987854,
      "learning_rate": 3.166460396039604e-05,
      "loss": 8.1671,
      "step": 5930
    },
    {
      "epoch": 1.8374448998530664,
      "grad_norm": 0.4327923059463501,
      "learning_rate": 3.1633663366336634e-05,
      "loss": 8.1748,
      "step": 5940
    },
    {
      "epoch": 1.8405382414353104,
      "grad_norm": 0.3966115117073059,
      "learning_rate": 3.160272277227723e-05,
      "loss": 8.1777,
      "step": 5950
    },
    {
      "epoch": 1.8436315830175547,
      "grad_norm": 0.3539911210536957,
      "learning_rate": 3.1571782178217825e-05,
      "loss": 8.1697,
      "step": 5960
    },
    {
      "epoch": 1.846724924599799,
      "grad_norm": 0.4217887818813324,
      "learning_rate": 3.1540841584158413e-05,
      "loss": 8.1684,
      "step": 5970
    },
    {
      "epoch": 1.849818266182043,
      "grad_norm": 0.4553893208503723,
      "learning_rate": 3.150990099009901e-05,
      "loss": 8.1554,
      "step": 5980
    },
    {
      "epoch": 1.8529116077642873,
      "grad_norm": 0.5374435782432556,
      "learning_rate": 3.1478960396039604e-05,
      "loss": 8.1627,
      "step": 5990
    },
    {
      "epoch": 1.8560049493465316,
      "grad_norm": 0.509392261505127,
      "learning_rate": 3.14480198019802e-05,
      "loss": 8.1506,
      "step": 6000
    },
    {
      "epoch": 1.8590982909287757,
      "grad_norm": 0.34193235635757446,
      "learning_rate": 3.141707920792079e-05,
      "loss": 8.1786,
      "step": 6010
    },
    {
      "epoch": 1.8621916325110202,
      "grad_norm": 0.38229405879974365,
      "learning_rate": 3.138613861386139e-05,
      "loss": 8.1639,
      "step": 6020
    },
    {
      "epoch": 1.8652849740932642,
      "grad_norm": 0.3702782094478607,
      "learning_rate": 3.135519801980198e-05,
      "loss": 8.1692,
      "step": 6030
    },
    {
      "epoch": 1.8683783156755085,
      "grad_norm": 0.800041913986206,
      "learning_rate": 3.1324257425742574e-05,
      "loss": 8.1778,
      "step": 6040
    },
    {
      "epoch": 1.8714716572577528,
      "grad_norm": 0.4144209325313568,
      "learning_rate": 3.129331683168317e-05,
      "loss": 8.156,
      "step": 6050
    },
    {
      "epoch": 1.8745649988399968,
      "grad_norm": 0.6745917201042175,
      "learning_rate": 3.1262376237623765e-05,
      "loss": 8.1526,
      "step": 6060
    },
    {
      "epoch": 1.8776583404222411,
      "grad_norm": 1.0255248546600342,
      "learning_rate": 3.123143564356436e-05,
      "loss": 8.1224,
      "step": 6070
    },
    {
      "epoch": 1.8807516820044854,
      "grad_norm": 0.4061252474784851,
      "learning_rate": 3.120049504950495e-05,
      "loss": 8.121,
      "step": 6080
    },
    {
      "epoch": 1.8838450235867295,
      "grad_norm": 0.4317517578601837,
      "learning_rate": 3.116955445544555e-05,
      "loss": 8.1069,
      "step": 6090
    },
    {
      "epoch": 1.8869383651689737,
      "grad_norm": 0.41372567415237427,
      "learning_rate": 3.113861386138614e-05,
      "loss": 8.0946,
      "step": 6100
    },
    {
      "epoch": 1.890031706751218,
      "grad_norm": 0.42606809735298157,
      "learning_rate": 3.1107673267326735e-05,
      "loss": 8.1071,
      "step": 6110
    },
    {
      "epoch": 1.893125048333462,
      "grad_norm": 0.33545902371406555,
      "learning_rate": 3.107673267326733e-05,
      "loss": 8.1192,
      "step": 6120
    },
    {
      "epoch": 1.8962183899157066,
      "grad_norm": 0.3417273461818695,
      "learning_rate": 3.1045792079207926e-05,
      "loss": 8.0993,
      "step": 6130
    },
    {
      "epoch": 1.8993117314979506,
      "grad_norm": 0.412545382976532,
      "learning_rate": 3.1014851485148515e-05,
      "loss": 8.1159,
      "step": 6140
    },
    {
      "epoch": 1.902405073080195,
      "grad_norm": 0.5296400785446167,
      "learning_rate": 3.098391089108911e-05,
      "loss": 8.1178,
      "step": 6150
    },
    {
      "epoch": 1.9054984146624392,
      "grad_norm": 0.42873015999794006,
      "learning_rate": 3.0952970297029706e-05,
      "loss": 8.0996,
      "step": 6160
    },
    {
      "epoch": 1.9085917562446832,
      "grad_norm": 0.42323389649391174,
      "learning_rate": 3.09220297029703e-05,
      "loss": 8.105,
      "step": 6170
    },
    {
      "epoch": 1.9116850978269275,
      "grad_norm": 0.45119592547416687,
      "learning_rate": 3.089108910891089e-05,
      "loss": 8.1182,
      "step": 6180
    },
    {
      "epoch": 1.9147784394091718,
      "grad_norm": 0.5001340508460999,
      "learning_rate": 3.086014851485149e-05,
      "loss": 8.116,
      "step": 6190
    },
    {
      "epoch": 1.9178717809914159,
      "grad_norm": 0.39245572686195374,
      "learning_rate": 3.082920792079208e-05,
      "loss": 8.1243,
      "step": 6200
    },
    {
      "epoch": 1.9209651225736601,
      "grad_norm": 0.5061200261116028,
      "learning_rate": 3.0798267326732676e-05,
      "loss": 8.1109,
      "step": 6210
    },
    {
      "epoch": 1.9240584641559044,
      "grad_norm": 0.3941931128501892,
      "learning_rate": 3.076732673267327e-05,
      "loss": 8.1014,
      "step": 6220
    },
    {
      "epoch": 1.9271518057381485,
      "grad_norm": 0.5197978019714355,
      "learning_rate": 3.0736386138613866e-05,
      "loss": 8.1153,
      "step": 6230
    },
    {
      "epoch": 1.930245147320393,
      "grad_norm": 0.7460987567901611,
      "learning_rate": 3.0705445544554455e-05,
      "loss": 8.1252,
      "step": 6240
    },
    {
      "epoch": 1.933338488902637,
      "grad_norm": 0.4997379183769226,
      "learning_rate": 3.067450495049505e-05,
      "loss": 8.0901,
      "step": 6250
    },
    {
      "epoch": 1.9364318304848813,
      "grad_norm": 0.8712663054466248,
      "learning_rate": 3.0643564356435646e-05,
      "loss": 8.1145,
      "step": 6260
    },
    {
      "epoch": 1.9395251720671256,
      "grad_norm": 0.36371588706970215,
      "learning_rate": 3.061262376237624e-05,
      "loss": 8.1115,
      "step": 6270
    },
    {
      "epoch": 1.9426185136493697,
      "grad_norm": 0.484379380941391,
      "learning_rate": 3.058168316831683e-05,
      "loss": 8.126,
      "step": 6280
    },
    {
      "epoch": 1.945711855231614,
      "grad_norm": 0.3575598895549774,
      "learning_rate": 3.055074257425743e-05,
      "loss": 8.1077,
      "step": 6290
    },
    {
      "epoch": 1.9488051968138582,
      "grad_norm": 0.3711893558502197,
      "learning_rate": 3.051980198019802e-05,
      "loss": 8.1117,
      "step": 6300
    },
    {
      "epoch": 1.9518985383961023,
      "grad_norm": 0.3766615390777588,
      "learning_rate": 3.0488861386138616e-05,
      "loss": 8.1125,
      "step": 6310
    },
    {
      "epoch": 1.9549918799783466,
      "grad_norm": 0.45748838782310486,
      "learning_rate": 3.0457920792079208e-05,
      "loss": 8.0993,
      "step": 6320
    },
    {
      "epoch": 1.9580852215605908,
      "grad_norm": 0.589434027671814,
      "learning_rate": 3.0426980198019807e-05,
      "loss": 8.1005,
      "step": 6330
    },
    {
      "epoch": 1.961178563142835,
      "grad_norm": 0.6021839380264282,
      "learning_rate": 3.0396039603960395e-05,
      "loss": 8.1191,
      "step": 6340
    },
    {
      "epoch": 1.9642719047250794,
      "grad_norm": 0.5807575583457947,
      "learning_rate": 3.0365099009900994e-05,
      "loss": 8.11,
      "step": 6350
    },
    {
      "epoch": 1.9673652463073235,
      "grad_norm": 0.4501745104789734,
      "learning_rate": 3.0334158415841586e-05,
      "loss": 8.1128,
      "step": 6360
    },
    {
      "epoch": 1.9704585878895677,
      "grad_norm": 0.36723577976226807,
      "learning_rate": 3.030321782178218e-05,
      "loss": 8.1194,
      "step": 6370
    },
    {
      "epoch": 1.973551929471812,
      "grad_norm": 0.7119986414909363,
      "learning_rate": 3.0272277227722774e-05,
      "loss": 8.1207,
      "step": 6380
    },
    {
      "epoch": 1.976645271054056,
      "grad_norm": 0.4717801809310913,
      "learning_rate": 3.024133663366337e-05,
      "loss": 8.1098,
      "step": 6390
    },
    {
      "epoch": 1.9797386126363004,
      "grad_norm": 0.4402429163455963,
      "learning_rate": 3.021039603960396e-05,
      "loss": 8.1027,
      "step": 6400
    },
    {
      "epoch": 1.9828319542185446,
      "grad_norm": 0.42858076095581055,
      "learning_rate": 3.0179455445544556e-05,
      "loss": 8.1241,
      "step": 6410
    },
    {
      "epoch": 1.9859252958007887,
      "grad_norm": 0.608566403388977,
      "learning_rate": 3.014851485148515e-05,
      "loss": 8.1125,
      "step": 6420
    },
    {
      "epoch": 1.989018637383033,
      "grad_norm": 0.37344932556152344,
      "learning_rate": 3.0117574257425747e-05,
      "loss": 8.1169,
      "step": 6430
    },
    {
      "epoch": 1.9921119789652773,
      "grad_norm": 0.6136317253112793,
      "learning_rate": 3.0086633663366336e-05,
      "loss": 8.1131,
      "step": 6440
    },
    {
      "epoch": 1.9952053205475213,
      "grad_norm": 0.5757606029510498,
      "learning_rate": 3.0055693069306934e-05,
      "loss": 8.1191,
      "step": 6450
    },
    {
      "epoch": 1.9982986621297658,
      "grad_norm": 0.6195966601371765,
      "learning_rate": 3.0024752475247526e-05,
      "loss": 8.1067,
      "step": 6460
    },
    {
      "epoch": 2.00139200371201,
      "grad_norm": 0.44545766711235046,
      "learning_rate": 2.9993811881188122e-05,
      "loss": 8.1047,
      "step": 6470
    },
    {
      "epoch": 2.004485345294254,
      "grad_norm": 0.47453534603118896,
      "learning_rate": 2.9962871287128714e-05,
      "loss": 8.1164,
      "step": 6480
    },
    {
      "epoch": 2.0075786868764984,
      "grad_norm": 0.39511996507644653,
      "learning_rate": 2.993193069306931e-05,
      "loss": 8.1057,
      "step": 6490
    },
    {
      "epoch": 2.0106720284587425,
      "grad_norm": 0.3776174485683441,
      "learning_rate": 2.99009900990099e-05,
      "loss": 8.1126,
      "step": 6500
    },
    {
      "epoch": 2.013765370040987,
      "grad_norm": 0.39943927526474,
      "learning_rate": 2.9870049504950497e-05,
      "loss": 8.1005,
      "step": 6510
    },
    {
      "epoch": 2.016858711623231,
      "grad_norm": 0.5308811664581299,
      "learning_rate": 2.983910891089109e-05,
      "loss": 8.115,
      "step": 6520
    },
    {
      "epoch": 2.019952053205475,
      "grad_norm": 0.5195642113685608,
      "learning_rate": 2.9808168316831687e-05,
      "loss": 8.1042,
      "step": 6530
    },
    {
      "epoch": 2.0230453947877196,
      "grad_norm": 0.3700476586818695,
      "learning_rate": 2.9777227722772276e-05,
      "loss": 8.1061,
      "step": 6540
    },
    {
      "epoch": 2.0261387363699637,
      "grad_norm": 0.4367985129356384,
      "learning_rate": 2.9746287128712875e-05,
      "loss": 8.1126,
      "step": 6550
    },
    {
      "epoch": 2.0292320779522077,
      "grad_norm": 0.5004454851150513,
      "learning_rate": 2.9715346534653467e-05,
      "loss": 8.1075,
      "step": 6560
    },
    {
      "epoch": 2.0323254195344522,
      "grad_norm": 0.4433349370956421,
      "learning_rate": 2.9684405940594062e-05,
      "loss": 8.1064,
      "step": 6570
    },
    {
      "epoch": 2.0354187611166963,
      "grad_norm": 0.6525294780731201,
      "learning_rate": 2.9653465346534654e-05,
      "loss": 8.1117,
      "step": 6580
    },
    {
      "epoch": 2.0385121026989403,
      "grad_norm": 0.3422076106071472,
      "learning_rate": 2.962252475247525e-05,
      "loss": 8.1107,
      "step": 6590
    },
    {
      "epoch": 2.041605444281185,
      "grad_norm": 0.668213963508606,
      "learning_rate": 2.959158415841584e-05,
      "loss": 8.1212,
      "step": 6600
    },
    {
      "epoch": 2.044698785863429,
      "grad_norm": 0.7291648983955383,
      "learning_rate": 2.9560643564356437e-05,
      "loss": 8.1022,
      "step": 6610
    },
    {
      "epoch": 2.0477921274456734,
      "grad_norm": 0.38672634959220886,
      "learning_rate": 2.952970297029703e-05,
      "loss": 8.1037,
      "step": 6620
    },
    {
      "epoch": 2.0508854690279175,
      "grad_norm": 0.35856810212135315,
      "learning_rate": 2.9498762376237628e-05,
      "loss": 8.1106,
      "step": 6630
    },
    {
      "epoch": 2.0539788106101615,
      "grad_norm": 0.4729677438735962,
      "learning_rate": 2.9467821782178216e-05,
      "loss": 8.1006,
      "step": 6640
    },
    {
      "epoch": 2.057072152192406,
      "grad_norm": 0.39660975337028503,
      "learning_rate": 2.9436881188118815e-05,
      "loss": 8.1166,
      "step": 6650
    },
    {
      "epoch": 2.06016549377465,
      "grad_norm": 0.38243186473846436,
      "learning_rate": 2.9405940594059407e-05,
      "loss": 8.1216,
      "step": 6660
    },
    {
      "epoch": 2.063258835356894,
      "grad_norm": 0.5077926516532898,
      "learning_rate": 2.9375000000000003e-05,
      "loss": 8.0951,
      "step": 6670
    },
    {
      "epoch": 2.0663521769391386,
      "grad_norm": 0.5306602716445923,
      "learning_rate": 2.9344059405940595e-05,
      "loss": 8.0963,
      "step": 6680
    },
    {
      "epoch": 2.0694455185213827,
      "grad_norm": 0.4762689471244812,
      "learning_rate": 2.931311881188119e-05,
      "loss": 8.1112,
      "step": 6690
    },
    {
      "epoch": 2.0725388601036268,
      "grad_norm": 0.6522422432899475,
      "learning_rate": 2.9282178217821782e-05,
      "loss": 8.1083,
      "step": 6700
    },
    {
      "epoch": 2.0756322016858713,
      "grad_norm": 0.4138133227825165,
      "learning_rate": 2.9251237623762377e-05,
      "loss": 8.1081,
      "step": 6710
    },
    {
      "epoch": 2.0787255432681153,
      "grad_norm": 0.3456277549266815,
      "learning_rate": 2.922029702970297e-05,
      "loss": 8.0982,
      "step": 6720
    },
    {
      "epoch": 2.0818188848503594,
      "grad_norm": 0.45005276799201965,
      "learning_rate": 2.9189356435643568e-05,
      "loss": 8.097,
      "step": 6730
    },
    {
      "epoch": 2.084912226432604,
      "grad_norm": 0.3984406888484955,
      "learning_rate": 2.9158415841584157e-05,
      "loss": 8.1282,
      "step": 6740
    },
    {
      "epoch": 2.088005568014848,
      "grad_norm": 0.3794633448123932,
      "learning_rate": 2.9127475247524755e-05,
      "loss": 8.0942,
      "step": 6750
    },
    {
      "epoch": 2.0910989095970924,
      "grad_norm": 0.5428169369697571,
      "learning_rate": 2.9096534653465347e-05,
      "loss": 8.1158,
      "step": 6760
    },
    {
      "epoch": 2.0941922511793365,
      "grad_norm": 0.4719012975692749,
      "learning_rate": 2.9065594059405943e-05,
      "loss": 8.1166,
      "step": 6770
    },
    {
      "epoch": 2.0972855927615806,
      "grad_norm": 0.9148817658424377,
      "learning_rate": 2.9034653465346535e-05,
      "loss": 8.1097,
      "step": 6780
    },
    {
      "epoch": 2.100378934343825,
      "grad_norm": 0.813014030456543,
      "learning_rate": 2.900371287128713e-05,
      "loss": 8.1175,
      "step": 6790
    },
    {
      "epoch": 2.103472275926069,
      "grad_norm": 0.507468581199646,
      "learning_rate": 2.8972772277227722e-05,
      "loss": 8.1068,
      "step": 6800
    },
    {
      "epoch": 2.106565617508313,
      "grad_norm": 0.4508984386920929,
      "learning_rate": 2.8941831683168318e-05,
      "loss": 8.1114,
      "step": 6810
    },
    {
      "epoch": 2.1096589590905577,
      "grad_norm": 0.42152875661849976,
      "learning_rate": 2.891089108910891e-05,
      "loss": 8.1172,
      "step": 6820
    },
    {
      "epoch": 2.1127523006728017,
      "grad_norm": 0.5839388966560364,
      "learning_rate": 2.887995049504951e-05,
      "loss": 8.1096,
      "step": 6830
    },
    {
      "epoch": 2.115845642255046,
      "grad_norm": 0.42553675174713135,
      "learning_rate": 2.8849009900990097e-05,
      "loss": 8.1118,
      "step": 6840
    },
    {
      "epoch": 2.1189389838372903,
      "grad_norm": 0.5957943797111511,
      "learning_rate": 2.8818069306930696e-05,
      "loss": 8.1083,
      "step": 6850
    },
    {
      "epoch": 2.1220323254195343,
      "grad_norm": 0.3855045437812805,
      "learning_rate": 2.8787128712871288e-05,
      "loss": 8.1092,
      "step": 6860
    },
    {
      "epoch": 2.125125667001779,
      "grad_norm": 0.41911375522613525,
      "learning_rate": 2.8756188118811883e-05,
      "loss": 8.1081,
      "step": 6870
    },
    {
      "epoch": 2.128219008584023,
      "grad_norm": 0.41209110617637634,
      "learning_rate": 2.8725247524752475e-05,
      "loss": 8.113,
      "step": 6880
    },
    {
      "epoch": 2.131312350166267,
      "grad_norm": 0.4470077455043793,
      "learning_rate": 2.869430693069307e-05,
      "loss": 8.1041,
      "step": 6890
    },
    {
      "epoch": 2.1344056917485115,
      "grad_norm": 0.5966583490371704,
      "learning_rate": 2.8663366336633663e-05,
      "loss": 8.1167,
      "step": 6900
    },
    {
      "epoch": 2.1374990333307555,
      "grad_norm": 0.3641496002674103,
      "learning_rate": 2.8632425742574258e-05,
      "loss": 8.1162,
      "step": 6910
    },
    {
      "epoch": 2.1405923749129996,
      "grad_norm": 0.42155152559280396,
      "learning_rate": 2.860148514851485e-05,
      "loss": 8.1014,
      "step": 6920
    },
    {
      "epoch": 2.143685716495244,
      "grad_norm": 0.5870181322097778,
      "learning_rate": 2.857054455445545e-05,
      "loss": 8.1152,
      "step": 6930
    },
    {
      "epoch": 2.146779058077488,
      "grad_norm": 0.6089041233062744,
      "learning_rate": 2.8539603960396037e-05,
      "loss": 8.1056,
      "step": 6940
    },
    {
      "epoch": 2.149872399659732,
      "grad_norm": 0.6636606454849243,
      "learning_rate": 2.8508663366336636e-05,
      "loss": 8.1035,
      "step": 6950
    },
    {
      "epoch": 2.1529657412419767,
      "grad_norm": 0.3926096558570862,
      "learning_rate": 2.8477722772277228e-05,
      "loss": 8.1086,
      "step": 6960
    },
    {
      "epoch": 2.1560590828242208,
      "grad_norm": 0.4976501762866974,
      "learning_rate": 2.8446782178217823e-05,
      "loss": 8.0974,
      "step": 6970
    },
    {
      "epoch": 2.1591524244064653,
      "grad_norm": 0.4579189717769623,
      "learning_rate": 2.8415841584158415e-05,
      "loss": 8.1039,
      "step": 6980
    },
    {
      "epoch": 2.1622457659887093,
      "grad_norm": 0.34147486090660095,
      "learning_rate": 2.838490099009901e-05,
      "loss": 8.1142,
      "step": 6990
    },
    {
      "epoch": 2.1653391075709534,
      "grad_norm": 0.6019530892372131,
      "learning_rate": 2.8353960396039603e-05,
      "loss": 8.1117,
      "step": 7000
    },
    {
      "epoch": 2.168432449153198,
      "grad_norm": 0.3721618354320526,
      "learning_rate": 2.8323019801980198e-05,
      "loss": 8.1167,
      "step": 7010
    },
    {
      "epoch": 2.171525790735442,
      "grad_norm": 0.42363935708999634,
      "learning_rate": 2.829207920792079e-05,
      "loss": 8.1191,
      "step": 7020
    },
    {
      "epoch": 2.174619132317686,
      "grad_norm": 0.3539220690727234,
      "learning_rate": 2.826113861386139e-05,
      "loss": 8.0983,
      "step": 7030
    },
    {
      "epoch": 2.1777124738999305,
      "grad_norm": 0.5180476903915405,
      "learning_rate": 2.8230198019801978e-05,
      "loss": 8.1208,
      "step": 7040
    },
    {
      "epoch": 2.1808058154821746,
      "grad_norm": 0.41559600830078125,
      "learning_rate": 2.8199257425742576e-05,
      "loss": 8.1164,
      "step": 7050
    },
    {
      "epoch": 2.1838991570644186,
      "grad_norm": 0.39671704173088074,
      "learning_rate": 2.816831683168317e-05,
      "loss": 8.1127,
      "step": 7060
    },
    {
      "epoch": 2.186992498646663,
      "grad_norm": 0.505501925945282,
      "learning_rate": 2.8137376237623764e-05,
      "loss": 8.0988,
      "step": 7070
    },
    {
      "epoch": 2.190085840228907,
      "grad_norm": 0.37925300002098083,
      "learning_rate": 2.810643564356436e-05,
      "loss": 8.0943,
      "step": 7080
    },
    {
      "epoch": 2.1931791818111517,
      "grad_norm": 0.8718864917755127,
      "learning_rate": 2.807549504950495e-05,
      "loss": 8.1145,
      "step": 7090
    },
    {
      "epoch": 2.1962725233933957,
      "grad_norm": 0.3877681791782379,
      "learning_rate": 2.804455445544555e-05,
      "loss": 8.1003,
      "step": 7100
    },
    {
      "epoch": 2.19936586497564,
      "grad_norm": 0.3574279546737671,
      "learning_rate": 2.801361386138614e-05,
      "loss": 8.1109,
      "step": 7110
    },
    {
      "epoch": 2.2024592065578843,
      "grad_norm": 0.3894767165184021,
      "learning_rate": 2.7982673267326737e-05,
      "loss": 8.0955,
      "step": 7120
    },
    {
      "epoch": 2.2055525481401284,
      "grad_norm": 0.4102941155433655,
      "learning_rate": 2.795173267326733e-05,
      "loss": 8.1154,
      "step": 7130
    },
    {
      "epoch": 2.2086458897223724,
      "grad_norm": 0.4889371395111084,
      "learning_rate": 2.7920792079207925e-05,
      "loss": 8.0978,
      "step": 7140
    },
    {
      "epoch": 2.211739231304617,
      "grad_norm": 0.31770917773246765,
      "learning_rate": 2.7889851485148517e-05,
      "loss": 8.1096,
      "step": 7150
    },
    {
      "epoch": 2.214832572886861,
      "grad_norm": 0.5176501870155334,
      "learning_rate": 2.7858910891089112e-05,
      "loss": 8.0986,
      "step": 7160
    },
    {
      "epoch": 2.217925914469105,
      "grad_norm": 0.5542957782745361,
      "learning_rate": 2.7827970297029704e-05,
      "loss": 8.1103,
      "step": 7170
    },
    {
      "epoch": 2.2210192560513495,
      "grad_norm": 0.7326028943061829,
      "learning_rate": 2.77970297029703e-05,
      "loss": 8.1087,
      "step": 7180
    },
    {
      "epoch": 2.2241125976335936,
      "grad_norm": 0.3868766725063324,
      "learning_rate": 2.776608910891089e-05,
      "loss": 8.098,
      "step": 7190
    },
    {
      "epoch": 2.227205939215838,
      "grad_norm": 0.4015921354293823,
      "learning_rate": 2.773514851485149e-05,
      "loss": 8.1119,
      "step": 7200
    },
    {
      "epoch": 2.230299280798082,
      "grad_norm": 0.673748791217804,
      "learning_rate": 2.770420792079208e-05,
      "loss": 8.0978,
      "step": 7210
    },
    {
      "epoch": 2.233392622380326,
      "grad_norm": 0.5301743149757385,
      "learning_rate": 2.7673267326732678e-05,
      "loss": 8.1047,
      "step": 7220
    },
    {
      "epoch": 2.2364859639625707,
      "grad_norm": 0.502481997013092,
      "learning_rate": 2.764232673267327e-05,
      "loss": 8.1242,
      "step": 7230
    },
    {
      "epoch": 2.2395793055448148,
      "grad_norm": 0.42827290296554565,
      "learning_rate": 2.7611386138613865e-05,
      "loss": 8.1029,
      "step": 7240
    },
    {
      "epoch": 2.242672647127059,
      "grad_norm": 0.3687129318714142,
      "learning_rate": 2.7580445544554457e-05,
      "loss": 8.1003,
      "step": 7250
    },
    {
      "epoch": 2.2457659887093033,
      "grad_norm": 0.41148293018341064,
      "learning_rate": 2.7549504950495052e-05,
      "loss": 8.1079,
      "step": 7260
    },
    {
      "epoch": 2.2488593302915474,
      "grad_norm": 0.45747482776641846,
      "learning_rate": 2.7518564356435644e-05,
      "loss": 8.1163,
      "step": 7270
    },
    {
      "epoch": 2.2519526718737914,
      "grad_norm": 0.7945466041564941,
      "learning_rate": 2.748762376237624e-05,
      "loss": 8.1084,
      "step": 7280
    },
    {
      "epoch": 2.255046013456036,
      "grad_norm": 0.3970637023448944,
      "learning_rate": 2.7456683168316832e-05,
      "loss": 8.1098,
      "step": 7290
    },
    {
      "epoch": 2.25813935503828,
      "grad_norm": 0.7280101180076599,
      "learning_rate": 2.742574257425743e-05,
      "loss": 8.1239,
      "step": 7300
    },
    {
      "epoch": 2.2612326966205245,
      "grad_norm": 0.46626928448677063,
      "learning_rate": 2.739480198019802e-05,
      "loss": 8.098,
      "step": 7310
    },
    {
      "epoch": 2.2643260382027686,
      "grad_norm": 0.47784602642059326,
      "learning_rate": 2.7363861386138618e-05,
      "loss": 8.1064,
      "step": 7320
    },
    {
      "epoch": 2.2674193797850126,
      "grad_norm": 0.44570571184158325,
      "learning_rate": 2.733292079207921e-05,
      "loss": 8.126,
      "step": 7330
    },
    {
      "epoch": 2.270512721367257,
      "grad_norm": 0.521054744720459,
      "learning_rate": 2.7301980198019805e-05,
      "loss": 8.0991,
      "step": 7340
    },
    {
      "epoch": 2.273606062949501,
      "grad_norm": 0.5249581933021545,
      "learning_rate": 2.7271039603960397e-05,
      "loss": 8.1193,
      "step": 7350
    },
    {
      "epoch": 2.2766994045317452,
      "grad_norm": 0.6648766398429871,
      "learning_rate": 2.7240099009900993e-05,
      "loss": 8.1007,
      "step": 7360
    },
    {
      "epoch": 2.2797927461139897,
      "grad_norm": 0.38151946663856506,
      "learning_rate": 2.7209158415841585e-05,
      "loss": 8.1023,
      "step": 7370
    },
    {
      "epoch": 2.282886087696234,
      "grad_norm": 0.40292951464653015,
      "learning_rate": 2.717821782178218e-05,
      "loss": 8.1189,
      "step": 7380
    },
    {
      "epoch": 2.285979429278478,
      "grad_norm": 0.4993983805179596,
      "learning_rate": 2.7147277227722772e-05,
      "loss": 8.1099,
      "step": 7390
    },
    {
      "epoch": 2.2890727708607224,
      "grad_norm": 0.30684417486190796,
      "learning_rate": 2.711633663366337e-05,
      "loss": 8.112,
      "step": 7400
    },
    {
      "epoch": 2.2921661124429664,
      "grad_norm": 0.37284648418426514,
      "learning_rate": 2.708539603960396e-05,
      "loss": 8.1187,
      "step": 7410
    },
    {
      "epoch": 2.295259454025211,
      "grad_norm": 0.38848331570625305,
      "learning_rate": 2.7054455445544558e-05,
      "loss": 8.1079,
      "step": 7420
    },
    {
      "epoch": 2.298352795607455,
      "grad_norm": 0.3627287745475769,
      "learning_rate": 2.702351485148515e-05,
      "loss": 8.0898,
      "step": 7430
    },
    {
      "epoch": 2.301446137189699,
      "grad_norm": 0.4727779030799866,
      "learning_rate": 2.6992574257425746e-05,
      "loss": 8.1187,
      "step": 7440
    },
    {
      "epoch": 2.3045394787719435,
      "grad_norm": 0.470659464597702,
      "learning_rate": 2.6961633663366338e-05,
      "loss": 8.1124,
      "step": 7450
    },
    {
      "epoch": 2.3076328203541876,
      "grad_norm": 0.4522012174129486,
      "learning_rate": 2.6930693069306933e-05,
      "loss": 8.0984,
      "step": 7460
    },
    {
      "epoch": 2.3107261619364317,
      "grad_norm": 0.3745891749858856,
      "learning_rate": 2.6899752475247525e-05,
      "loss": 8.1088,
      "step": 7470
    },
    {
      "epoch": 2.313819503518676,
      "grad_norm": 0.3798051178455353,
      "learning_rate": 2.686881188118812e-05,
      "loss": 8.0946,
      "step": 7480
    },
    {
      "epoch": 2.31691284510092,
      "grad_norm": 0.4513620138168335,
      "learning_rate": 2.6837871287128712e-05,
      "loss": 8.1062,
      "step": 7490
    },
    {
      "epoch": 2.3200061866831643,
      "grad_norm": 0.47107288241386414,
      "learning_rate": 2.680693069306931e-05,
      "loss": 8.1187,
      "step": 7500
    },
    {
      "epoch": 2.3230995282654088,
      "grad_norm": 0.5348942875862122,
      "learning_rate": 2.67759900990099e-05,
      "loss": 8.1012,
      "step": 7510
    },
    {
      "epoch": 2.326192869847653,
      "grad_norm": 0.3327547311782837,
      "learning_rate": 2.67450495049505e-05,
      "loss": 8.1043,
      "step": 7520
    },
    {
      "epoch": 2.3292862114298973,
      "grad_norm": 0.5593411922454834,
      "learning_rate": 2.671410891089109e-05,
      "loss": 8.1122,
      "step": 7530
    },
    {
      "epoch": 2.3323795530121414,
      "grad_norm": 0.31392183899879456,
      "learning_rate": 2.6683168316831686e-05,
      "loss": 8.0971,
      "step": 7540
    },
    {
      "epoch": 2.3354728945943855,
      "grad_norm": 0.5643736124038696,
      "learning_rate": 2.6652227722772278e-05,
      "loss": 8.1124,
      "step": 7550
    },
    {
      "epoch": 2.33856623617663,
      "grad_norm": 0.5148528218269348,
      "learning_rate": 2.6621287128712873e-05,
      "loss": 8.1049,
      "step": 7560
    },
    {
      "epoch": 2.341659577758874,
      "grad_norm": 0.43620559573173523,
      "learning_rate": 2.6590346534653465e-05,
      "loss": 8.1102,
      "step": 7570
    },
    {
      "epoch": 2.344752919341118,
      "grad_norm": 0.5097543001174927,
      "learning_rate": 2.655940594059406e-05,
      "loss": 8.1267,
      "step": 7580
    },
    {
      "epoch": 2.3478462609233626,
      "grad_norm": 0.408526748418808,
      "learning_rate": 2.6528465346534653e-05,
      "loss": 8.0977,
      "step": 7590
    },
    {
      "epoch": 2.3509396025056066,
      "grad_norm": 0.3220125734806061,
      "learning_rate": 2.649752475247525e-05,
      "loss": 8.1111,
      "step": 7600
    },
    {
      "epoch": 2.3540329440878507,
      "grad_norm": 0.45548003911972046,
      "learning_rate": 2.646658415841584e-05,
      "loss": 8.1111,
      "step": 7610
    },
    {
      "epoch": 2.357126285670095,
      "grad_norm": 0.48648762702941895,
      "learning_rate": 2.643564356435644e-05,
      "loss": 8.0953,
      "step": 7620
    },
    {
      "epoch": 2.3602196272523392,
      "grad_norm": 0.6467950940132141,
      "learning_rate": 2.640470297029703e-05,
      "loss": 8.1187,
      "step": 7630
    },
    {
      "epoch": 2.3633129688345837,
      "grad_norm": 0.4756968319416046,
      "learning_rate": 2.6373762376237626e-05,
      "loss": 8.1114,
      "step": 7640
    },
    {
      "epoch": 2.366406310416828,
      "grad_norm": 0.41047510504722595,
      "learning_rate": 2.634282178217822e-05,
      "loss": 8.1074,
      "step": 7650
    },
    {
      "epoch": 2.369499651999072,
      "grad_norm": 0.31534433364868164,
      "learning_rate": 2.6311881188118814e-05,
      "loss": 8.104,
      "step": 7660
    },
    {
      "epoch": 2.3725929935813164,
      "grad_norm": 0.47555968165397644,
      "learning_rate": 2.6280940594059406e-05,
      "loss": 8.1203,
      "step": 7670
    },
    {
      "epoch": 2.3756863351635604,
      "grad_norm": 0.6126089096069336,
      "learning_rate": 2.625e-05,
      "loss": 8.1417,
      "step": 7680
    },
    {
      "epoch": 2.3787796767458045,
      "grad_norm": 0.41265594959259033,
      "learning_rate": 2.6219059405940593e-05,
      "loss": 8.1051,
      "step": 7690
    },
    {
      "epoch": 2.381873018328049,
      "grad_norm": 0.3348105251789093,
      "learning_rate": 2.6188118811881192e-05,
      "loss": 8.1008,
      "step": 7700
    },
    {
      "epoch": 2.384966359910293,
      "grad_norm": 0.5802624821662903,
      "learning_rate": 2.615717821782178e-05,
      "loss": 8.1122,
      "step": 7710
    },
    {
      "epoch": 2.388059701492537,
      "grad_norm": 0.4920975863933563,
      "learning_rate": 2.612623762376238e-05,
      "loss": 8.1192,
      "step": 7720
    },
    {
      "epoch": 2.3911530430747816,
      "grad_norm": 0.5800136923789978,
      "learning_rate": 2.609529702970297e-05,
      "loss": 8.1072,
      "step": 7730
    },
    {
      "epoch": 2.3942463846570257,
      "grad_norm": 0.3740622103214264,
      "learning_rate": 2.6064356435643567e-05,
      "loss": 8.1063,
      "step": 7740
    },
    {
      "epoch": 2.39733972623927,
      "grad_norm": 0.41874203085899353,
      "learning_rate": 2.603341584158416e-05,
      "loss": 8.1144,
      "step": 7750
    },
    {
      "epoch": 2.400433067821514,
      "grad_norm": 0.5509190559387207,
      "learning_rate": 2.6002475247524754e-05,
      "loss": 8.11,
      "step": 7760
    },
    {
      "epoch": 2.4035264094037583,
      "grad_norm": 0.44342416524887085,
      "learning_rate": 2.5971534653465346e-05,
      "loss": 8.1163,
      "step": 7770
    },
    {
      "epoch": 2.406619750986003,
      "grad_norm": 0.4791869521141052,
      "learning_rate": 2.594059405940594e-05,
      "loss": 8.1019,
      "step": 7780
    },
    {
      "epoch": 2.409713092568247,
      "grad_norm": 0.4454892873764038,
      "learning_rate": 2.5909653465346533e-05,
      "loss": 8.1131,
      "step": 7790
    },
    {
      "epoch": 2.412806434150491,
      "grad_norm": 0.3071057200431824,
      "learning_rate": 2.5878712871287132e-05,
      "loss": 8.1082,
      "step": 7800
    },
    {
      "epoch": 2.4158997757327354,
      "grad_norm": 0.4443412125110626,
      "learning_rate": 2.584777227722772e-05,
      "loss": 8.1148,
      "step": 7810
    },
    {
      "epoch": 2.4189931173149795,
      "grad_norm": 0.4621613025665283,
      "learning_rate": 2.581683168316832e-05,
      "loss": 8.1081,
      "step": 7820
    },
    {
      "epoch": 2.4220864588972235,
      "grad_norm": 0.3333241939544678,
      "learning_rate": 2.578589108910891e-05,
      "loss": 8.1002,
      "step": 7830
    },
    {
      "epoch": 2.425179800479468,
      "grad_norm": 0.42298874258995056,
      "learning_rate": 2.5754950495049507e-05,
      "loss": 8.1055,
      "step": 7840
    },
    {
      "epoch": 2.428273142061712,
      "grad_norm": 0.3564538061618805,
      "learning_rate": 2.57240099009901e-05,
      "loss": 8.1166,
      "step": 7850
    },
    {
      "epoch": 2.4313664836439566,
      "grad_norm": 0.4536409378051758,
      "learning_rate": 2.5693069306930694e-05,
      "loss": 8.1124,
      "step": 7860
    },
    {
      "epoch": 2.4344598252262006,
      "grad_norm": 0.5571862459182739,
      "learning_rate": 2.5662128712871286e-05,
      "loss": 8.1118,
      "step": 7870
    },
    {
      "epoch": 2.4375531668084447,
      "grad_norm": 0.49318647384643555,
      "learning_rate": 2.5631188118811882e-05,
      "loss": 8.0944,
      "step": 7880
    },
    {
      "epoch": 2.440646508390689,
      "grad_norm": 0.42048346996307373,
      "learning_rate": 2.5600247524752474e-05,
      "loss": 8.1123,
      "step": 7890
    },
    {
      "epoch": 2.4437398499729333,
      "grad_norm": 0.44303643703460693,
      "learning_rate": 2.5569306930693073e-05,
      "loss": 8.1102,
      "step": 7900
    },
    {
      "epoch": 2.4468331915551773,
      "grad_norm": 0.4222244918346405,
      "learning_rate": 2.553836633663366e-05,
      "loss": 8.1276,
      "step": 7910
    },
    {
      "epoch": 2.449926533137422,
      "grad_norm": 0.44194501638412476,
      "learning_rate": 2.550742574257426e-05,
      "loss": 8.1054,
      "step": 7920
    },
    {
      "epoch": 2.453019874719666,
      "grad_norm": 0.4718707203865051,
      "learning_rate": 2.5476485148514852e-05,
      "loss": 8.1201,
      "step": 7930
    },
    {
      "epoch": 2.45611321630191,
      "grad_norm": 0.3100089430809021,
      "learning_rate": 2.5445544554455447e-05,
      "loss": 8.1012,
      "step": 7940
    },
    {
      "epoch": 2.4592065578841544,
      "grad_norm": 0.353790819644928,
      "learning_rate": 2.541460396039604e-05,
      "loss": 8.1123,
      "step": 7950
    },
    {
      "epoch": 2.4622998994663985,
      "grad_norm": 0.5453819036483765,
      "learning_rate": 2.5383663366336635e-05,
      "loss": 8.1065,
      "step": 7960
    },
    {
      "epoch": 2.465393241048643,
      "grad_norm": 0.40025588870048523,
      "learning_rate": 2.5352722772277227e-05,
      "loss": 8.1292,
      "step": 7970
    },
    {
      "epoch": 2.468486582630887,
      "grad_norm": 0.38151437044143677,
      "learning_rate": 2.5321782178217822e-05,
      "loss": 8.1123,
      "step": 7980
    },
    {
      "epoch": 2.471579924213131,
      "grad_norm": 0.26372960209846497,
      "learning_rate": 2.5290841584158414e-05,
      "loss": 8.1047,
      "step": 7990
    },
    {
      "epoch": 2.4746732657953756,
      "grad_norm": 0.4096117913722992,
      "learning_rate": 2.5259900990099013e-05,
      "loss": 8.1034,
      "step": 8000
    },
    {
      "epoch": 2.4777666073776197,
      "grad_norm": 0.5157427787780762,
      "learning_rate": 2.52289603960396e-05,
      "loss": 8.1046,
      "step": 8010
    },
    {
      "epoch": 2.4808599489598637,
      "grad_norm": 0.5132339000701904,
      "learning_rate": 2.51980198019802e-05,
      "loss": 8.1081,
      "step": 8020
    },
    {
      "epoch": 2.4839532905421082,
      "grad_norm": 0.416756272315979,
      "learning_rate": 2.5167079207920792e-05,
      "loss": 8.1128,
      "step": 8030
    },
    {
      "epoch": 2.4870466321243523,
      "grad_norm": 0.36893022060394287,
      "learning_rate": 2.5136138613861388e-05,
      "loss": 8.1128,
      "step": 8040
    },
    {
      "epoch": 2.4901399737065963,
      "grad_norm": 0.6825616955757141,
      "learning_rate": 2.510519801980198e-05,
      "loss": 8.1193,
      "step": 8050
    },
    {
      "epoch": 2.493233315288841,
      "grad_norm": 0.5882079601287842,
      "learning_rate": 2.5074257425742575e-05,
      "loss": 8.1156,
      "step": 8060
    },
    {
      "epoch": 2.496326656871085,
      "grad_norm": 0.7520038485527039,
      "learning_rate": 2.5043316831683167e-05,
      "loss": 8.1078,
      "step": 8070
    },
    {
      "epoch": 2.4994199984533294,
      "grad_norm": 0.40223345160484314,
      "learning_rate": 2.5012376237623762e-05,
      "loss": 8.1139,
      "step": 8080
    },
    {
      "epoch": 2.5025133400355735,
      "grad_norm": 0.29529109597206116,
      "learning_rate": 2.4981435643564358e-05,
      "loss": 8.1061,
      "step": 8090
    },
    {
      "epoch": 2.5056066816178175,
      "grad_norm": 0.4694910943508148,
      "learning_rate": 2.4950495049504953e-05,
      "loss": 8.113,
      "step": 8100
    },
    {
      "epoch": 2.508700023200062,
      "grad_norm": 0.48692968487739563,
      "learning_rate": 2.4919554455445545e-05,
      "loss": 8.1173,
      "step": 8110
    },
    {
      "epoch": 2.511793364782306,
      "grad_norm": 0.42500075697898865,
      "learning_rate": 2.488861386138614e-05,
      "loss": 8.1216,
      "step": 8120
    },
    {
      "epoch": 2.51488670636455,
      "grad_norm": 0.31642571091651917,
      "learning_rate": 2.4857673267326733e-05,
      "loss": 8.0986,
      "step": 8130
    },
    {
      "epoch": 2.5179800479467946,
      "grad_norm": 0.2659047842025757,
      "learning_rate": 2.4826732673267328e-05,
      "loss": 8.1068,
      "step": 8140
    },
    {
      "epoch": 2.5210733895290387,
      "grad_norm": 0.3895564079284668,
      "learning_rate": 2.4795792079207923e-05,
      "loss": 8.0969,
      "step": 8150
    },
    {
      "epoch": 2.5241667311112828,
      "grad_norm": 0.36387428641319275,
      "learning_rate": 2.4764851485148515e-05,
      "loss": 8.1019,
      "step": 8160
    },
    {
      "epoch": 2.5272600726935273,
      "grad_norm": 0.48615995049476624,
      "learning_rate": 2.473391089108911e-05,
      "loss": 8.11,
      "step": 8170
    },
    {
      "epoch": 2.5303534142757713,
      "grad_norm": 0.9303953051567078,
      "learning_rate": 2.4702970297029703e-05,
      "loss": 8.1238,
      "step": 8180
    },
    {
      "epoch": 2.533446755858016,
      "grad_norm": 0.4390532374382019,
      "learning_rate": 2.4672029702970298e-05,
      "loss": 8.0966,
      "step": 8190
    },
    {
      "epoch": 2.53654009744026,
      "grad_norm": 0.4628516137599945,
      "learning_rate": 2.4641089108910893e-05,
      "loss": 8.0922,
      "step": 8200
    },
    {
      "epoch": 2.539633439022504,
      "grad_norm": 0.43858447670936584,
      "learning_rate": 2.4610148514851485e-05,
      "loss": 8.1075,
      "step": 8210
    },
    {
      "epoch": 2.5427267806047484,
      "grad_norm": 0.46960246562957764,
      "learning_rate": 2.457920792079208e-05,
      "loss": 8.1014,
      "step": 8220
    },
    {
      "epoch": 2.5458201221869925,
      "grad_norm": 0.47983983159065247,
      "learning_rate": 2.4548267326732673e-05,
      "loss": 8.1181,
      "step": 8230
    },
    {
      "epoch": 2.5489134637692366,
      "grad_norm": 0.3488848805427551,
      "learning_rate": 2.4517326732673268e-05,
      "loss": 8.0983,
      "step": 8240
    },
    {
      "epoch": 2.552006805351481,
      "grad_norm": 0.41462790966033936,
      "learning_rate": 2.4486386138613864e-05,
      "loss": 8.0966,
      "step": 8250
    },
    {
      "epoch": 2.555100146933725,
      "grad_norm": 0.3149162828922272,
      "learning_rate": 2.4455445544554456e-05,
      "loss": 8.091,
      "step": 8260
    },
    {
      "epoch": 2.558193488515969,
      "grad_norm": 0.3292813301086426,
      "learning_rate": 2.442450495049505e-05,
      "loss": 8.1013,
      "step": 8270
    },
    {
      "epoch": 2.5612868300982137,
      "grad_norm": 0.7644737958908081,
      "learning_rate": 2.4393564356435643e-05,
      "loss": 8.0907,
      "step": 8280
    },
    {
      "epoch": 2.5643801716804577,
      "grad_norm": 0.7846313118934631,
      "learning_rate": 2.436262376237624e-05,
      "loss": 8.1019,
      "step": 8290
    },
    {
      "epoch": 2.5674735132627022,
      "grad_norm": 0.6896189451217651,
      "learning_rate": 2.4331683168316834e-05,
      "loss": 8.1158,
      "step": 8300
    },
    {
      "epoch": 2.5705668548449463,
      "grad_norm": 0.7290159463882446,
      "learning_rate": 2.4300742574257426e-05,
      "loss": 8.1098,
      "step": 8310
    },
    {
      "epoch": 2.5736601964271903,
      "grad_norm": 0.402951717376709,
      "learning_rate": 2.426980198019802e-05,
      "loss": 8.0977,
      "step": 8320
    },
    {
      "epoch": 2.576753538009435,
      "grad_norm": 0.4045652449131012,
      "learning_rate": 2.4238861386138613e-05,
      "loss": 8.1057,
      "step": 8330
    },
    {
      "epoch": 2.579846879591679,
      "grad_norm": 0.44975143671035767,
      "learning_rate": 2.420792079207921e-05,
      "loss": 8.1044,
      "step": 8340
    },
    {
      "epoch": 2.582940221173923,
      "grad_norm": 0.4273267090320587,
      "learning_rate": 2.4176980198019804e-05,
      "loss": 8.1158,
      "step": 8350
    },
    {
      "epoch": 2.5860335627561675,
      "grad_norm": 0.32711219787597656,
      "learning_rate": 2.4146039603960396e-05,
      "loss": 8.1077,
      "step": 8360
    },
    {
      "epoch": 2.5891269043384115,
      "grad_norm": 0.2764568030834198,
      "learning_rate": 2.411509900990099e-05,
      "loss": 8.1139,
      "step": 8370
    },
    {
      "epoch": 2.5922202459206556,
      "grad_norm": 0.34994998574256897,
      "learning_rate": 2.4084158415841583e-05,
      "loss": 8.1149,
      "step": 8380
    },
    {
      "epoch": 2.5953135875029,
      "grad_norm": 0.4056667685508728,
      "learning_rate": 2.405321782178218e-05,
      "loss": 8.1077,
      "step": 8390
    },
    {
      "epoch": 2.598406929085144,
      "grad_norm": 0.4969797730445862,
      "learning_rate": 2.4022277227722774e-05,
      "loss": 8.1046,
      "step": 8400
    },
    {
      "epoch": 2.6015002706673886,
      "grad_norm": 0.30368414521217346,
      "learning_rate": 2.3991336633663366e-05,
      "loss": 8.0986,
      "step": 8410
    },
    {
      "epoch": 2.6045936122496327,
      "grad_norm": 0.39709845185279846,
      "learning_rate": 2.396039603960396e-05,
      "loss": 8.0858,
      "step": 8420
    },
    {
      "epoch": 2.6076869538318768,
      "grad_norm": 0.36873000860214233,
      "learning_rate": 2.3929455445544553e-05,
      "loss": 8.1193,
      "step": 8430
    },
    {
      "epoch": 2.6107802954141213,
      "grad_norm": 0.4145767092704773,
      "learning_rate": 2.389851485148515e-05,
      "loss": 8.1074,
      "step": 8440
    },
    {
      "epoch": 2.6138736369963653,
      "grad_norm": 0.4097910523414612,
      "learning_rate": 2.3867574257425744e-05,
      "loss": 8.1026,
      "step": 8450
    },
    {
      "epoch": 2.6169669785786094,
      "grad_norm": 0.4797244668006897,
      "learning_rate": 2.3836633663366336e-05,
      "loss": 8.1128,
      "step": 8460
    },
    {
      "epoch": 2.620060320160854,
      "grad_norm": 0.4686642289161682,
      "learning_rate": 2.380569306930693e-05,
      "loss": 8.121,
      "step": 8470
    },
    {
      "epoch": 2.623153661743098,
      "grad_norm": 0.30009472370147705,
      "learning_rate": 2.3774752475247524e-05,
      "loss": 8.0954,
      "step": 8480
    },
    {
      "epoch": 2.626247003325342,
      "grad_norm": 0.5142483711242676,
      "learning_rate": 2.374381188118812e-05,
      "loss": 8.1028,
      "step": 8490
    },
    {
      "epoch": 2.6293403449075865,
      "grad_norm": 0.37695255875587463,
      "learning_rate": 2.3712871287128714e-05,
      "loss": 8.1133,
      "step": 8500
    },
    {
      "epoch": 2.6324336864898306,
      "grad_norm": 0.40328967571258545,
      "learning_rate": 2.3681930693069306e-05,
      "loss": 8.0959,
      "step": 8510
    },
    {
      "epoch": 2.635527028072075,
      "grad_norm": 0.38678988814353943,
      "learning_rate": 2.3650990099009902e-05,
      "loss": 8.1159,
      "step": 8520
    },
    {
      "epoch": 2.638620369654319,
      "grad_norm": 0.49387454986572266,
      "learning_rate": 2.3620049504950494e-05,
      "loss": 8.1063,
      "step": 8530
    },
    {
      "epoch": 2.641713711236563,
      "grad_norm": 0.33106115460395813,
      "learning_rate": 2.358910891089109e-05,
      "loss": 8.0916,
      "step": 8540
    },
    {
      "epoch": 2.6448070528188077,
      "grad_norm": 0.5519963502883911,
      "learning_rate": 2.3558168316831685e-05,
      "loss": 8.1021,
      "step": 8550
    },
    {
      "epoch": 2.6479003944010517,
      "grad_norm": 0.4291223883628845,
      "learning_rate": 2.3527227722772277e-05,
      "loss": 8.1216,
      "step": 8560
    },
    {
      "epoch": 2.650993735983296,
      "grad_norm": 0.3903982639312744,
      "learning_rate": 2.3496287128712872e-05,
      "loss": 8.1093,
      "step": 8570
    },
    {
      "epoch": 2.6540870775655403,
      "grad_norm": 0.3249850869178772,
      "learning_rate": 2.3465346534653464e-05,
      "loss": 8.11,
      "step": 8580
    },
    {
      "epoch": 2.6571804191477844,
      "grad_norm": 0.44734060764312744,
      "learning_rate": 2.3434405940594063e-05,
      "loss": 8.1144,
      "step": 8590
    },
    {
      "epoch": 2.6602737607300284,
      "grad_norm": 0.4101562201976776,
      "learning_rate": 2.3403465346534655e-05,
      "loss": 8.0856,
      "step": 8600
    },
    {
      "epoch": 2.663367102312273,
      "grad_norm": 0.47082284092903137,
      "learning_rate": 2.337252475247525e-05,
      "loss": 8.1039,
      "step": 8610
    },
    {
      "epoch": 2.666460443894517,
      "grad_norm": 0.4993501901626587,
      "learning_rate": 2.3341584158415846e-05,
      "loss": 8.0986,
      "step": 8620
    },
    {
      "epoch": 2.6695537854767615,
      "grad_norm": 0.4394413232803345,
      "learning_rate": 2.3310643564356438e-05,
      "loss": 8.1,
      "step": 8630
    },
    {
      "epoch": 2.6726471270590055,
      "grad_norm": 0.29313698410987854,
      "learning_rate": 2.3279702970297033e-05,
      "loss": 8.0907,
      "step": 8640
    },
    {
      "epoch": 2.6757404686412496,
      "grad_norm": 0.3887993395328522,
      "learning_rate": 2.3248762376237625e-05,
      "loss": 8.1146,
      "step": 8650
    },
    {
      "epoch": 2.678833810223494,
      "grad_norm": 0.5040184855461121,
      "learning_rate": 2.321782178217822e-05,
      "loss": 8.1218,
      "step": 8660
    },
    {
      "epoch": 2.681927151805738,
      "grad_norm": 0.3571512997150421,
      "learning_rate": 2.3186881188118816e-05,
      "loss": 8.1132,
      "step": 8670
    },
    {
      "epoch": 2.685020493387982,
      "grad_norm": 0.4044680893421173,
      "learning_rate": 2.3155940594059408e-05,
      "loss": 8.0941,
      "step": 8680
    },
    {
      "epoch": 2.6881138349702267,
      "grad_norm": 0.40673696994781494,
      "learning_rate": 2.3125000000000003e-05,
      "loss": 8.1072,
      "step": 8690
    },
    {
      "epoch": 2.6912071765524708,
      "grad_norm": 0.45775845646858215,
      "learning_rate": 2.3094059405940595e-05,
      "loss": 8.0944,
      "step": 8700
    },
    {
      "epoch": 2.694300518134715,
      "grad_norm": 0.41260266304016113,
      "learning_rate": 2.306311881188119e-05,
      "loss": 8.1107,
      "step": 8710
    },
    {
      "epoch": 2.6973938597169593,
      "grad_norm": 0.5472255945205688,
      "learning_rate": 2.3032178217821786e-05,
      "loss": 8.1239,
      "step": 8720
    },
    {
      "epoch": 2.7004872012992034,
      "grad_norm": 0.5373114347457886,
      "learning_rate": 2.3001237623762378e-05,
      "loss": 8.1209,
      "step": 8730
    },
    {
      "epoch": 2.703580542881448,
      "grad_norm": 0.49815770983695984,
      "learning_rate": 2.2970297029702973e-05,
      "loss": 8.1004,
      "step": 8740
    },
    {
      "epoch": 2.706673884463692,
      "grad_norm": 0.5631648898124695,
      "learning_rate": 2.2939356435643565e-05,
      "loss": 8.0841,
      "step": 8750
    },
    {
      "epoch": 2.709767226045936,
      "grad_norm": 0.5709866285324097,
      "learning_rate": 2.290841584158416e-05,
      "loss": 8.1162,
      "step": 8760
    },
    {
      "epoch": 2.7128605676281805,
      "grad_norm": 0.497070848941803,
      "learning_rate": 2.2877475247524756e-05,
      "loss": 8.1114,
      "step": 8770
    },
    {
      "epoch": 2.7159539092104246,
      "grad_norm": 0.47337231040000916,
      "learning_rate": 2.2846534653465348e-05,
      "loss": 8.1159,
      "step": 8780
    },
    {
      "epoch": 2.7190472507926686,
      "grad_norm": 0.5615905523300171,
      "learning_rate": 2.2815594059405943e-05,
      "loss": 8.1264,
      "step": 8790
    },
    {
      "epoch": 2.722140592374913,
      "grad_norm": 0.548971951007843,
      "learning_rate": 2.2784653465346535e-05,
      "loss": 8.1164,
      "step": 8800
    },
    {
      "epoch": 2.725233933957157,
      "grad_norm": 0.6845041513442993,
      "learning_rate": 2.275371287128713e-05,
      "loss": 8.0917,
      "step": 8810
    },
    {
      "epoch": 2.7283272755394012,
      "grad_norm": 0.4545358121395111,
      "learning_rate": 2.2722772277227726e-05,
      "loss": 8.1041,
      "step": 8820
    },
    {
      "epoch": 2.7314206171216457,
      "grad_norm": 0.540631115436554,
      "learning_rate": 2.2691831683168318e-05,
      "loss": 8.1217,
      "step": 8830
    },
    {
      "epoch": 2.73451395870389,
      "grad_norm": 0.3414331376552582,
      "learning_rate": 2.2660891089108914e-05,
      "loss": 8.0955,
      "step": 8840
    },
    {
      "epoch": 2.7376073002861343,
      "grad_norm": 0.35995253920555115,
      "learning_rate": 2.2629950495049506e-05,
      "loss": 8.1176,
      "step": 8850
    },
    {
      "epoch": 2.7407006418683784,
      "grad_norm": 0.42509540915489197,
      "learning_rate": 2.25990099009901e-05,
      "loss": 8.0974,
      "step": 8860
    },
    {
      "epoch": 2.7437939834506224,
      "grad_norm": 0.4948987662792206,
      "learning_rate": 2.2568069306930696e-05,
      "loss": 8.1113,
      "step": 8870
    },
    {
      "epoch": 2.7468873250328665,
      "grad_norm": 0.5057565569877625,
      "learning_rate": 2.253712871287129e-05,
      "loss": 8.112,
      "step": 8880
    },
    {
      "epoch": 2.749980666615111,
      "grad_norm": 0.41152164340019226,
      "learning_rate": 2.2506188118811884e-05,
      "loss": 8.0931,
      "step": 8890
    },
    {
      "epoch": 2.753074008197355,
      "grad_norm": 0.3883711099624634,
      "learning_rate": 2.2475247524752476e-05,
      "loss": 8.0993,
      "step": 8900
    },
    {
      "epoch": 2.7561673497795995,
      "grad_norm": 0.3591054081916809,
      "learning_rate": 2.244430693069307e-05,
      "loss": 8.1054,
      "step": 8910
    },
    {
      "epoch": 2.7592606913618436,
      "grad_norm": 0.45989978313446045,
      "learning_rate": 2.2413366336633666e-05,
      "loss": 8.1122,
      "step": 8920
    },
    {
      "epoch": 2.7623540329440877,
      "grad_norm": 0.34185993671417236,
      "learning_rate": 2.238242574257426e-05,
      "loss": 8.1099,
      "step": 8930
    },
    {
      "epoch": 2.765447374526332,
      "grad_norm": 0.454632431268692,
      "learning_rate": 2.2351485148514854e-05,
      "loss": 8.1042,
      "step": 8940
    },
    {
      "epoch": 2.768540716108576,
      "grad_norm": 0.441683828830719,
      "learning_rate": 2.2320544554455446e-05,
      "loss": 8.0991,
      "step": 8950
    },
    {
      "epoch": 2.7716340576908207,
      "grad_norm": 0.28231513500213623,
      "learning_rate": 2.228960396039604e-05,
      "loss": 8.0914,
      "step": 8960
    },
    {
      "epoch": 2.7747273992730648,
      "grad_norm": 0.7504224181175232,
      "learning_rate": 2.2258663366336637e-05,
      "loss": 8.1251,
      "step": 8970
    },
    {
      "epoch": 2.777820740855309,
      "grad_norm": 0.5244613885879517,
      "learning_rate": 2.222772277227723e-05,
      "loss": 8.1172,
      "step": 8980
    },
    {
      "epoch": 2.780914082437553,
      "grad_norm": 0.3940989375114441,
      "learning_rate": 2.2196782178217824e-05,
      "loss": 8.115,
      "step": 8990
    },
    {
      "epoch": 2.7840074240197974,
      "grad_norm": 0.3459937572479248,
      "learning_rate": 2.2165841584158416e-05,
      "loss": 8.116,
      "step": 9000
    },
    {
      "epoch": 2.7871007656020415,
      "grad_norm": 0.41333797574043274,
      "learning_rate": 2.213490099009901e-05,
      "loss": 8.1065,
      "step": 9010
    },
    {
      "epoch": 2.790194107184286,
      "grad_norm": 0.4635697305202484,
      "learning_rate": 2.2103960396039607e-05,
      "loss": 8.1047,
      "step": 9020
    },
    {
      "epoch": 2.79328744876653,
      "grad_norm": 0.5662729740142822,
      "learning_rate": 2.20730198019802e-05,
      "loss": 8.1074,
      "step": 9030
    },
    {
      "epoch": 2.796380790348774,
      "grad_norm": 0.46692511439323425,
      "learning_rate": 2.2042079207920794e-05,
      "loss": 8.1119,
      "step": 9040
    },
    {
      "epoch": 2.7994741319310186,
      "grad_norm": 0.4131472408771515,
      "learning_rate": 2.2011138613861386e-05,
      "loss": 8.1114,
      "step": 9050
    },
    {
      "epoch": 2.8025674735132626,
      "grad_norm": 0.4273796081542969,
      "learning_rate": 2.198019801980198e-05,
      "loss": 8.1154,
      "step": 9060
    },
    {
      "epoch": 2.805660815095507,
      "grad_norm": 0.44634732604026794,
      "learning_rate": 2.1949257425742577e-05,
      "loss": 8.1124,
      "step": 9070
    },
    {
      "epoch": 2.808754156677751,
      "grad_norm": 0.5948656797409058,
      "learning_rate": 2.191831683168317e-05,
      "loss": 8.1121,
      "step": 9080
    },
    {
      "epoch": 2.8118474982599952,
      "grad_norm": 0.3740612268447876,
      "learning_rate": 2.1887376237623764e-05,
      "loss": 8.1057,
      "step": 9090
    },
    {
      "epoch": 2.8149408398422393,
      "grad_norm": 0.5075851082801819,
      "learning_rate": 2.1856435643564356e-05,
      "loss": 8.1032,
      "step": 9100
    },
    {
      "epoch": 2.818034181424484,
      "grad_norm": 0.3465963900089264,
      "learning_rate": 2.1825495049504952e-05,
      "loss": 8.0886,
      "step": 9110
    },
    {
      "epoch": 2.821127523006728,
      "grad_norm": 0.47802555561065674,
      "learning_rate": 2.1794554455445547e-05,
      "loss": 8.119,
      "step": 9120
    },
    {
      "epoch": 2.8242208645889724,
      "grad_norm": 0.42891445755958557,
      "learning_rate": 2.176361386138614e-05,
      "loss": 8.1041,
      "step": 9130
    },
    {
      "epoch": 2.8273142061712164,
      "grad_norm": 0.4049603343009949,
      "learning_rate": 2.1732673267326734e-05,
      "loss": 8.1028,
      "step": 9140
    },
    {
      "epoch": 2.8304075477534605,
      "grad_norm": 0.3554851710796356,
      "learning_rate": 2.1701732673267326e-05,
      "loss": 8.0901,
      "step": 9150
    },
    {
      "epoch": 2.833500889335705,
      "grad_norm": 0.43971046805381775,
      "learning_rate": 2.1670792079207922e-05,
      "loss": 8.0975,
      "step": 9160
    },
    {
      "epoch": 2.836594230917949,
      "grad_norm": 0.34304019808769226,
      "learning_rate": 2.1639851485148517e-05,
      "loss": 8.0988,
      "step": 9170
    },
    {
      "epoch": 2.8396875725001935,
      "grad_norm": 0.5888461470603943,
      "learning_rate": 2.160891089108911e-05,
      "loss": 8.1221,
      "step": 9180
    },
    {
      "epoch": 2.8427809140824376,
      "grad_norm": 0.39912310242652893,
      "learning_rate": 2.1577970297029705e-05,
      "loss": 8.1047,
      "step": 9190
    },
    {
      "epoch": 2.8458742556646817,
      "grad_norm": 0.3159428536891937,
      "learning_rate": 2.1547029702970297e-05,
      "loss": 8.1096,
      "step": 9200
    },
    {
      "epoch": 2.8489675972469257,
      "grad_norm": 0.37997597455978394,
      "learning_rate": 2.1516089108910892e-05,
      "loss": 8.1066,
      "step": 9210
    },
    {
      "epoch": 2.8520609388291702,
      "grad_norm": 0.49556225538253784,
      "learning_rate": 2.1485148514851487e-05,
      "loss": 8.1163,
      "step": 9220
    },
    {
      "epoch": 2.8551542804114143,
      "grad_norm": 0.4437481760978699,
      "learning_rate": 2.145420792079208e-05,
      "loss": 8.1289,
      "step": 9230
    },
    {
      "epoch": 2.858247621993659,
      "grad_norm": 0.5236688256263733,
      "learning_rate": 2.1423267326732675e-05,
      "loss": 8.1152,
      "step": 9240
    },
    {
      "epoch": 2.861340963575903,
      "grad_norm": 0.4161495268344879,
      "learning_rate": 2.1392326732673267e-05,
      "loss": 8.1069,
      "step": 9250
    },
    {
      "epoch": 2.864434305158147,
      "grad_norm": 0.3599238097667694,
      "learning_rate": 2.1361386138613862e-05,
      "loss": 8.0939,
      "step": 9260
    },
    {
      "epoch": 2.8675276467403914,
      "grad_norm": 0.40918344259262085,
      "learning_rate": 2.1330445544554458e-05,
      "loss": 8.1014,
      "step": 9270
    },
    {
      "epoch": 2.8706209883226355,
      "grad_norm": 0.27008455991744995,
      "learning_rate": 2.129950495049505e-05,
      "loss": 8.1182,
      "step": 9280
    },
    {
      "epoch": 2.87371432990488,
      "grad_norm": 0.5997000336647034,
      "learning_rate": 2.1268564356435645e-05,
      "loss": 8.1187,
      "step": 9290
    },
    {
      "epoch": 2.876807671487124,
      "grad_norm": 0.4420570433139801,
      "learning_rate": 2.1237623762376237e-05,
      "loss": 8.1169,
      "step": 9300
    },
    {
      "epoch": 2.879901013069368,
      "grad_norm": 0.44307079911231995,
      "learning_rate": 2.1206683168316832e-05,
      "loss": 8.1068,
      "step": 9310
    },
    {
      "epoch": 2.882994354651612,
      "grad_norm": 0.30560192465782166,
      "learning_rate": 2.1175742574257428e-05,
      "loss": 8.1036,
      "step": 9320
    },
    {
      "epoch": 2.8860876962338566,
      "grad_norm": 0.7062841653823853,
      "learning_rate": 2.114480198019802e-05,
      "loss": 8.1212,
      "step": 9330
    },
    {
      "epoch": 2.8891810378161007,
      "grad_norm": 0.46244415640830994,
      "learning_rate": 2.1113861386138615e-05,
      "loss": 8.1155,
      "step": 9340
    },
    {
      "epoch": 2.892274379398345,
      "grad_norm": 0.4661347270011902,
      "learning_rate": 2.1082920792079207e-05,
      "loss": 8.1094,
      "step": 9350
    },
    {
      "epoch": 2.8953677209805893,
      "grad_norm": 0.4260418713092804,
      "learning_rate": 2.1051980198019803e-05,
      "loss": 8.0876,
      "step": 9360
    },
    {
      "epoch": 2.8984610625628333,
      "grad_norm": 0.5367578268051147,
      "learning_rate": 2.1021039603960398e-05,
      "loss": 8.102,
      "step": 9370
    },
    {
      "epoch": 2.901554404145078,
      "grad_norm": 0.6018370985984802,
      "learning_rate": 2.099009900990099e-05,
      "loss": 8.0951,
      "step": 9380
    },
    {
      "epoch": 2.904647745727322,
      "grad_norm": 0.4417549967765808,
      "learning_rate": 2.0959158415841585e-05,
      "loss": 8.1003,
      "step": 9390
    },
    {
      "epoch": 2.9077410873095664,
      "grad_norm": 0.4422750473022461,
      "learning_rate": 2.0928217821782177e-05,
      "loss": 8.112,
      "step": 9400
    },
    {
      "epoch": 2.9108344288918104,
      "grad_norm": 0.3503737151622772,
      "learning_rate": 2.0897277227722773e-05,
      "loss": 8.0953,
      "step": 9410
    },
    {
      "epoch": 2.9139277704740545,
      "grad_norm": 0.27332603931427,
      "learning_rate": 2.0866336633663368e-05,
      "loss": 8.1081,
      "step": 9420
    },
    {
      "epoch": 2.9170211120562985,
      "grad_norm": 0.32986319065093994,
      "learning_rate": 2.083539603960396e-05,
      "loss": 8.1165,
      "step": 9430
    },
    {
      "epoch": 2.920114453638543,
      "grad_norm": 0.41897642612457275,
      "learning_rate": 2.0804455445544555e-05,
      "loss": 8.1146,
      "step": 9440
    },
    {
      "epoch": 2.923207795220787,
      "grad_norm": 0.6735855937004089,
      "learning_rate": 2.0773514851485147e-05,
      "loss": 8.1103,
      "step": 9450
    },
    {
      "epoch": 2.9263011368030316,
      "grad_norm": 0.435465931892395,
      "learning_rate": 2.0742574257425743e-05,
      "loss": 8.1095,
      "step": 9460
    },
    {
      "epoch": 2.9293944783852757,
      "grad_norm": 0.31395068764686584,
      "learning_rate": 2.0711633663366338e-05,
      "loss": 8.1063,
      "step": 9470
    },
    {
      "epoch": 2.9324878199675197,
      "grad_norm": 0.4898190200328827,
      "learning_rate": 2.068069306930693e-05,
      "loss": 8.1291,
      "step": 9480
    },
    {
      "epoch": 2.9355811615497642,
      "grad_norm": 0.46305903792381287,
      "learning_rate": 2.0649752475247526e-05,
      "loss": 8.1031,
      "step": 9490
    },
    {
      "epoch": 2.9386745031320083,
      "grad_norm": 0.37539705634117126,
      "learning_rate": 2.0618811881188118e-05,
      "loss": 8.1052,
      "step": 9500
    },
    {
      "epoch": 2.941767844714253,
      "grad_norm": 0.49637266993522644,
      "learning_rate": 2.0587871287128713e-05,
      "loss": 8.1025,
      "step": 9510
    },
    {
      "epoch": 2.944861186296497,
      "grad_norm": 0.32237479090690613,
      "learning_rate": 2.055693069306931e-05,
      "loss": 8.1097,
      "step": 9520
    },
    {
      "epoch": 2.947954527878741,
      "grad_norm": 0.33331596851348877,
      "learning_rate": 2.05259900990099e-05,
      "loss": 8.098,
      "step": 9530
    },
    {
      "epoch": 2.951047869460985,
      "grad_norm": 0.37394019961357117,
      "learning_rate": 2.0495049504950496e-05,
      "loss": 8.1002,
      "step": 9540
    },
    {
      "epoch": 2.9541412110432295,
      "grad_norm": 0.6005445718765259,
      "learning_rate": 2.0464108910891088e-05,
      "loss": 8.1152,
      "step": 9550
    },
    {
      "epoch": 2.9572345526254735,
      "grad_norm": 0.5652497410774231,
      "learning_rate": 2.0433168316831683e-05,
      "loss": 8.126,
      "step": 9560
    },
    {
      "epoch": 2.960327894207718,
      "grad_norm": 0.3823004364967346,
      "learning_rate": 2.040222772277228e-05,
      "loss": 8.1035,
      "step": 9570
    },
    {
      "epoch": 2.963421235789962,
      "grad_norm": 0.4654759466648102,
      "learning_rate": 2.037128712871287e-05,
      "loss": 8.0959,
      "step": 9580
    },
    {
      "epoch": 2.966514577372206,
      "grad_norm": 0.4632241129875183,
      "learning_rate": 2.0340346534653466e-05,
      "loss": 8.1251,
      "step": 9590
    },
    {
      "epoch": 2.9696079189544506,
      "grad_norm": 0.49755415320396423,
      "learning_rate": 2.030940594059406e-05,
      "loss": 8.111,
      "step": 9600
    },
    {
      "epoch": 2.9727012605366947,
      "grad_norm": 0.46184465289115906,
      "learning_rate": 2.0278465346534657e-05,
      "loss": 8.1125,
      "step": 9610
    },
    {
      "epoch": 2.975794602118939,
      "grad_norm": 0.6340774893760681,
      "learning_rate": 2.024752475247525e-05,
      "loss": 8.1093,
      "step": 9620
    },
    {
      "epoch": 2.9788879437011833,
      "grad_norm": 0.5468502044677734,
      "learning_rate": 2.0216584158415844e-05,
      "loss": 8.1122,
      "step": 9630
    },
    {
      "epoch": 2.9819812852834273,
      "grad_norm": 0.5222125053405762,
      "learning_rate": 2.018564356435644e-05,
      "loss": 8.0964,
      "step": 9640
    },
    {
      "epoch": 2.9850746268656714,
      "grad_norm": 0.3797357678413391,
      "learning_rate": 2.015470297029703e-05,
      "loss": 8.1108,
      "step": 9650
    },
    {
      "epoch": 2.988167968447916,
      "grad_norm": 0.5316882133483887,
      "learning_rate": 2.0123762376237627e-05,
      "loss": 8.1209,
      "step": 9660
    },
    {
      "epoch": 2.99126131003016,
      "grad_norm": 0.4953002333641052,
      "learning_rate": 2.009282178217822e-05,
      "loss": 8.1104,
      "step": 9670
    },
    {
      "epoch": 2.9943546516124044,
      "grad_norm": 0.8001145124435425,
      "learning_rate": 2.0061881188118814e-05,
      "loss": 8.0972,
      "step": 9680
    },
    {
      "epoch": 2.9974479931946485,
      "grad_norm": 0.34038370847702026,
      "learning_rate": 2.003094059405941e-05,
      "loss": 8.1195,
      "step": 9690
    },
    {
      "epoch": 3.0005413347768926,
      "grad_norm": 0.5133174061775208,
      "learning_rate": 2e-05,
      "loss": 8.0828,
      "step": 9700
    },
    {
      "epoch": 3.003634676359137,
      "grad_norm": 0.36918285489082336,
      "learning_rate": 1.9969059405940597e-05,
      "loss": 8.1145,
      "step": 9710
    },
    {
      "epoch": 3.006728017941381,
      "grad_norm": 0.46190744638442993,
      "learning_rate": 1.993811881188119e-05,
      "loss": 8.1113,
      "step": 9720
    },
    {
      "epoch": 3.0098213595236256,
      "grad_norm": 0.4492332935333252,
      "learning_rate": 1.9907178217821784e-05,
      "loss": 8.1016,
      "step": 9730
    },
    {
      "epoch": 3.0129147011058697,
      "grad_norm": 0.3267870545387268,
      "learning_rate": 1.987623762376238e-05,
      "loss": 8.1086,
      "step": 9740
    },
    {
      "epoch": 3.0160080426881137,
      "grad_norm": 0.411501944065094,
      "learning_rate": 1.9845297029702972e-05,
      "loss": 8.0901,
      "step": 9750
    },
    {
      "epoch": 3.0191013842703582,
      "grad_norm": 0.4272809624671936,
      "learning_rate": 1.9814356435643567e-05,
      "loss": 8.103,
      "step": 9760
    },
    {
      "epoch": 3.0221947258526023,
      "grad_norm": 0.48993706703186035,
      "learning_rate": 1.978341584158416e-05,
      "loss": 8.0882,
      "step": 9770
    },
    {
      "epoch": 3.0252880674348464,
      "grad_norm": 0.4848363995552063,
      "learning_rate": 1.9752475247524755e-05,
      "loss": 8.1152,
      "step": 9780
    },
    {
      "epoch": 3.028381409017091,
      "grad_norm": 0.34976232051849365,
      "learning_rate": 1.972153465346535e-05,
      "loss": 8.1152,
      "step": 9790
    },
    {
      "epoch": 3.031474750599335,
      "grad_norm": 0.44027823209762573,
      "learning_rate": 1.9690594059405942e-05,
      "loss": 8.1054,
      "step": 9800
    },
    {
      "epoch": 3.034568092181579,
      "grad_norm": 0.37200766801834106,
      "learning_rate": 1.9659653465346537e-05,
      "loss": 8.0938,
      "step": 9810
    },
    {
      "epoch": 3.0376614337638235,
      "grad_norm": 0.41305580735206604,
      "learning_rate": 1.963180693069307e-05,
      "loss": 8.1119,
      "step": 9820
    },
    {
      "epoch": 3.0407547753460675,
      "grad_norm": 0.6276663541793823,
      "learning_rate": 1.9603960396039607e-05,
      "loss": 8.1098,
      "step": 9830
    },
    {
      "epoch": 3.043848116928312,
      "grad_norm": 0.4224551022052765,
      "learning_rate": 1.95730198019802e-05,
      "loss": 8.1086,
      "step": 9840
    },
    {
      "epoch": 3.046941458510556,
      "grad_norm": 0.4541500210762024,
      "learning_rate": 1.9542079207920794e-05,
      "loss": 8.0982,
      "step": 9850
    },
    {
      "epoch": 3.0500348000928,
      "grad_norm": 0.5518205165863037,
      "learning_rate": 1.9511138613861386e-05,
      "loss": 8.1084,
      "step": 9860
    },
    {
      "epoch": 3.0531281416750446,
      "grad_norm": 0.30490946769714355,
      "learning_rate": 1.9480198019801982e-05,
      "loss": 8.0966,
      "step": 9870
    },
    {
      "epoch": 3.0562214832572887,
      "grad_norm": 0.4900627136230469,
      "learning_rate": 1.9449257425742577e-05,
      "loss": 8.1043,
      "step": 9880
    },
    {
      "epoch": 3.0593148248395328,
      "grad_norm": 0.37451571226119995,
      "learning_rate": 1.941831683168317e-05,
      "loss": 8.1015,
      "step": 9890
    },
    {
      "epoch": 3.0624081664217773,
      "grad_norm": 0.5845980048179626,
      "learning_rate": 1.9387376237623765e-05,
      "loss": 8.0982,
      "step": 9900
    },
    {
      "epoch": 3.0655015080040213,
      "grad_norm": 0.4052619934082031,
      "learning_rate": 1.9356435643564357e-05,
      "loss": 8.1137,
      "step": 9910
    },
    {
      "epoch": 3.0685948495862654,
      "grad_norm": 0.35062161087989807,
      "learning_rate": 1.9325495049504952e-05,
      "loss": 8.0973,
      "step": 9920
    },
    {
      "epoch": 3.07168819116851,
      "grad_norm": 0.4694761037826538,
      "learning_rate": 1.9294554455445547e-05,
      "loss": 8.1037,
      "step": 9930
    },
    {
      "epoch": 3.074781532750754,
      "grad_norm": 0.44860678911209106,
      "learning_rate": 1.926361386138614e-05,
      "loss": 8.112,
      "step": 9940
    },
    {
      "epoch": 3.0778748743329984,
      "grad_norm": 0.5666287541389465,
      "learning_rate": 1.9232673267326735e-05,
      "loss": 8.1111,
      "step": 9950
    },
    {
      "epoch": 3.0809682159152425,
      "grad_norm": 0.3398258090019226,
      "learning_rate": 1.9201732673267327e-05,
      "loss": 8.1059,
      "step": 9960
    },
    {
      "epoch": 3.0840615574974866,
      "grad_norm": 0.39188292622566223,
      "learning_rate": 1.9170792079207922e-05,
      "loss": 8.1096,
      "step": 9970
    },
    {
      "epoch": 3.087154899079731,
      "grad_norm": 0.46225929260253906,
      "learning_rate": 1.9139851485148517e-05,
      "loss": 8.1154,
      "step": 9980
    },
    {
      "epoch": 3.090248240661975,
      "grad_norm": 0.46131375432014465,
      "learning_rate": 1.910891089108911e-05,
      "loss": 8.1005,
      "step": 9990
    },
    {
      "epoch": 3.093341582244219,
      "grad_norm": 0.28739532828330994,
      "learning_rate": 1.9077970297029705e-05,
      "loss": 8.1037,
      "step": 10000
    },
    {
      "epoch": 3.0964349238264637,
      "grad_norm": 0.43261656165122986,
      "learning_rate": 1.9047029702970297e-05,
      "loss": 8.1134,
      "step": 10010
    },
    {
      "epoch": 3.0995282654087077,
      "grad_norm": 0.42065513134002686,
      "learning_rate": 1.9016089108910892e-05,
      "loss": 8.1082,
      "step": 10020
    },
    {
      "epoch": 3.102621606990952,
      "grad_norm": 0.45671650767326355,
      "learning_rate": 1.8985148514851488e-05,
      "loss": 8.1032,
      "step": 10030
    },
    {
      "epoch": 3.1057149485731963,
      "grad_norm": 0.5156928896903992,
      "learning_rate": 1.895420792079208e-05,
      "loss": 8.1066,
      "step": 10040
    },
    {
      "epoch": 3.1088082901554404,
      "grad_norm": 0.33724623918533325,
      "learning_rate": 1.8923267326732675e-05,
      "loss": 8.0965,
      "step": 10050
    },
    {
      "epoch": 3.111901631737685,
      "grad_norm": 0.3424505889415741,
      "learning_rate": 1.8892326732673267e-05,
      "loss": 8.1035,
      "step": 10060
    },
    {
      "epoch": 3.114994973319929,
      "grad_norm": 0.3008453845977783,
      "learning_rate": 1.8861386138613862e-05,
      "loss": 8.0918,
      "step": 10070
    },
    {
      "epoch": 3.118088314902173,
      "grad_norm": 0.3161577880382538,
      "learning_rate": 1.8830445544554458e-05,
      "loss": 8.114,
      "step": 10080
    },
    {
      "epoch": 3.1211816564844175,
      "grad_norm": 0.48389932513237,
      "learning_rate": 1.879950495049505e-05,
      "loss": 8.1183,
      "step": 10090
    },
    {
      "epoch": 3.1242749980666615,
      "grad_norm": 0.3970765173435211,
      "learning_rate": 1.8768564356435645e-05,
      "loss": 8.0955,
      "step": 10100
    },
    {
      "epoch": 3.1273683396489056,
      "grad_norm": 0.4142555594444275,
      "learning_rate": 1.8737623762376237e-05,
      "loss": 8.1093,
      "step": 10110
    },
    {
      "epoch": 3.13046168123115,
      "grad_norm": 0.6540219783782959,
      "learning_rate": 1.8706683168316833e-05,
      "loss": 8.1174,
      "step": 10120
    },
    {
      "epoch": 3.133555022813394,
      "grad_norm": 5.624925136566162,
      "learning_rate": 1.8675742574257428e-05,
      "loss": 8.0931,
      "step": 10130
    },
    {
      "epoch": 3.136648364395638,
      "grad_norm": 0.4318046569824219,
      "learning_rate": 1.864480198019802e-05,
      "loss": 8.1074,
      "step": 10140
    },
    {
      "epoch": 3.1397417059778827,
      "grad_norm": 0.4315359890460968,
      "learning_rate": 1.8613861386138615e-05,
      "loss": 8.0983,
      "step": 10150
    },
    {
      "epoch": 3.1428350475601268,
      "grad_norm": 0.4221060872077942,
      "learning_rate": 1.8582920792079207e-05,
      "loss": 8.0986,
      "step": 10160
    },
    {
      "epoch": 3.1459283891423713,
      "grad_norm": 0.5176987648010254,
      "learning_rate": 1.8551980198019803e-05,
      "loss": 8.1027,
      "step": 10170
    },
    {
      "epoch": 3.1490217307246153,
      "grad_norm": 0.3637983202934265,
      "learning_rate": 1.8521039603960398e-05,
      "loss": 8.086,
      "step": 10180
    },
    {
      "epoch": 3.1521150723068594,
      "grad_norm": 0.5128066539764404,
      "learning_rate": 1.849009900990099e-05,
      "loss": 8.1139,
      "step": 10190
    },
    {
      "epoch": 3.155208413889104,
      "grad_norm": 0.522601306438446,
      "learning_rate": 1.8459158415841586e-05,
      "loss": 8.1087,
      "step": 10200
    },
    {
      "epoch": 3.158301755471348,
      "grad_norm": 0.3401453495025635,
      "learning_rate": 1.8428217821782178e-05,
      "loss": 8.106,
      "step": 10210
    },
    {
      "epoch": 3.161395097053592,
      "grad_norm": 0.3356766700744629,
      "learning_rate": 1.8397277227722773e-05,
      "loss": 8.1082,
      "step": 10220
    },
    {
      "epoch": 3.1644884386358365,
      "grad_norm": 0.5813103318214417,
      "learning_rate": 1.8366336633663368e-05,
      "loss": 8.097,
      "step": 10230
    },
    {
      "epoch": 3.1675817802180806,
      "grad_norm": 0.4703655242919922,
      "learning_rate": 1.833539603960396e-05,
      "loss": 8.0887,
      "step": 10240
    },
    {
      "epoch": 3.1706751218003246,
      "grad_norm": 0.3524664044380188,
      "learning_rate": 1.8304455445544556e-05,
      "loss": 8.1071,
      "step": 10250
    },
    {
      "epoch": 3.173768463382569,
      "grad_norm": 0.5278324484825134,
      "learning_rate": 1.8273514851485148e-05,
      "loss": 8.0943,
      "step": 10260
    },
    {
      "epoch": 3.176861804964813,
      "grad_norm": 0.46975037455558777,
      "learning_rate": 1.8242574257425743e-05,
      "loss": 8.0944,
      "step": 10270
    },
    {
      "epoch": 3.1799551465470577,
      "grad_norm": 0.44407209753990173,
      "learning_rate": 1.821163366336634e-05,
      "loss": 8.108,
      "step": 10280
    },
    {
      "epoch": 3.1830484881293017,
      "grad_norm": 0.3815309405326843,
      "learning_rate": 1.818069306930693e-05,
      "loss": 8.1008,
      "step": 10290
    },
    {
      "epoch": 3.186141829711546,
      "grad_norm": 0.37426695227622986,
      "learning_rate": 1.8149752475247526e-05,
      "loss": 8.0927,
      "step": 10300
    },
    {
      "epoch": 3.1892351712937903,
      "grad_norm": 0.372979074716568,
      "learning_rate": 1.8118811881188118e-05,
      "loss": 8.1022,
      "step": 10310
    },
    {
      "epoch": 3.1923285128760344,
      "grad_norm": 0.45068565011024475,
      "learning_rate": 1.8087871287128713e-05,
      "loss": 8.1175,
      "step": 10320
    },
    {
      "epoch": 3.1954218544582784,
      "grad_norm": 0.429738849401474,
      "learning_rate": 1.805693069306931e-05,
      "loss": 8.1104,
      "step": 10330
    },
    {
      "epoch": 3.198515196040523,
      "grad_norm": 0.41066765785217285,
      "learning_rate": 1.80259900990099e-05,
      "loss": 8.1071,
      "step": 10340
    },
    {
      "epoch": 3.201608537622767,
      "grad_norm": 0.45860669016838074,
      "learning_rate": 1.7995049504950496e-05,
      "loss": 8.1056,
      "step": 10350
    },
    {
      "epoch": 3.204701879205011,
      "grad_norm": 0.5588765144348145,
      "learning_rate": 1.7964108910891088e-05,
      "loss": 8.1068,
      "step": 10360
    },
    {
      "epoch": 3.2077952207872555,
      "grad_norm": 0.36148351430892944,
      "learning_rate": 1.7933168316831683e-05,
      "loss": 8.1018,
      "step": 10370
    },
    {
      "epoch": 3.2108885623694996,
      "grad_norm": 0.30562177300453186,
      "learning_rate": 1.790222772277228e-05,
      "loss": 8.1018,
      "step": 10380
    },
    {
      "epoch": 3.213981903951744,
      "grad_norm": 0.2906149923801422,
      "learning_rate": 1.787128712871287e-05,
      "loss": 8.104,
      "step": 10390
    },
    {
      "epoch": 3.217075245533988,
      "grad_norm": 0.6387789249420166,
      "learning_rate": 1.7840346534653466e-05,
      "loss": 8.1152,
      "step": 10400
    },
    {
      "epoch": 3.220168587116232,
      "grad_norm": 0.7283279299736023,
      "learning_rate": 1.7809405940594058e-05,
      "loss": 8.1036,
      "step": 10410
    },
    {
      "epoch": 3.2232619286984767,
      "grad_norm": 0.4536355435848236,
      "learning_rate": 1.7778465346534654e-05,
      "loss": 8.1051,
      "step": 10420
    },
    {
      "epoch": 3.2263552702807208,
      "grad_norm": 0.48054268956184387,
      "learning_rate": 1.774752475247525e-05,
      "loss": 8.0987,
      "step": 10430
    },
    {
      "epoch": 3.229448611862965,
      "grad_norm": 0.5746442079544067,
      "learning_rate": 1.771658415841584e-05,
      "loss": 8.1044,
      "step": 10440
    },
    {
      "epoch": 3.2325419534452093,
      "grad_norm": 0.49931928515434265,
      "learning_rate": 1.7685643564356436e-05,
      "loss": 8.1056,
      "step": 10450
    },
    {
      "epoch": 3.2356352950274534,
      "grad_norm": 0.512105405330658,
      "learning_rate": 1.765470297029703e-05,
      "loss": 8.1028,
      "step": 10460
    },
    {
      "epoch": 3.2387286366096975,
      "grad_norm": 0.45315882563591003,
      "learning_rate": 1.7623762376237624e-05,
      "loss": 8.0924,
      "step": 10470
    },
    {
      "epoch": 3.241821978191942,
      "grad_norm": 0.55507493019104,
      "learning_rate": 1.759282178217822e-05,
      "loss": 8.1084,
      "step": 10480
    },
    {
      "epoch": 3.244915319774186,
      "grad_norm": 0.413985937833786,
      "learning_rate": 1.756188118811881e-05,
      "loss": 8.1061,
      "step": 10490
    },
    {
      "epoch": 3.2480086613564305,
      "grad_norm": 0.48644715547561646,
      "learning_rate": 1.7530940594059406e-05,
      "loss": 8.1001,
      "step": 10500
    },
    {
      "epoch": 3.2511020029386746,
      "grad_norm": 0.5440137386322021,
      "learning_rate": 1.75e-05,
      "loss": 8.1025,
      "step": 10510
    },
    {
      "epoch": 3.2541953445209186,
      "grad_norm": 0.41093987226486206,
      "learning_rate": 1.7469059405940594e-05,
      "loss": 8.112,
      "step": 10520
    },
    {
      "epoch": 3.257288686103163,
      "grad_norm": 0.4802369177341461,
      "learning_rate": 1.743811881188119e-05,
      "loss": 8.1063,
      "step": 10530
    },
    {
      "epoch": 3.260382027685407,
      "grad_norm": 0.40162938833236694,
      "learning_rate": 1.740717821782178e-05,
      "loss": 8.1105,
      "step": 10540
    },
    {
      "epoch": 3.2634753692676512,
      "grad_norm": 0.3690304756164551,
      "learning_rate": 1.7376237623762377e-05,
      "loss": 8.1136,
      "step": 10550
    },
    {
      "epoch": 3.2665687108498958,
      "grad_norm": 0.4402998387813568,
      "learning_rate": 1.734529702970297e-05,
      "loss": 8.1184,
      "step": 10560
    },
    {
      "epoch": 3.26966205243214,
      "grad_norm": 0.4958846867084503,
      "learning_rate": 1.7314356435643564e-05,
      "loss": 8.1119,
      "step": 10570
    },
    {
      "epoch": 3.272755394014384,
      "grad_norm": 0.4602234363555908,
      "learning_rate": 1.728341584158416e-05,
      "loss": 8.1183,
      "step": 10580
    },
    {
      "epoch": 3.2758487355966284,
      "grad_norm": 0.4221380650997162,
      "learning_rate": 1.725247524752475e-05,
      "loss": 8.123,
      "step": 10590
    },
    {
      "epoch": 3.2789420771788724,
      "grad_norm": 0.48136278986930847,
      "learning_rate": 1.7221534653465347e-05,
      "loss": 8.1141,
      "step": 10600
    },
    {
      "epoch": 3.282035418761117,
      "grad_norm": 0.4072864353656769,
      "learning_rate": 1.719059405940594e-05,
      "loss": 8.1018,
      "step": 10610
    },
    {
      "epoch": 3.285128760343361,
      "grad_norm": 0.5818206667900085,
      "learning_rate": 1.7159653465346538e-05,
      "loss": 8.1069,
      "step": 10620
    },
    {
      "epoch": 3.288222101925605,
      "grad_norm": 0.35283163189888,
      "learning_rate": 1.712871287128713e-05,
      "loss": 8.1117,
      "step": 10630
    },
    {
      "epoch": 3.2913154435078495,
      "grad_norm": 0.4196871221065521,
      "learning_rate": 1.7097772277227725e-05,
      "loss": 8.1031,
      "step": 10640
    },
    {
      "epoch": 3.2944087850900936,
      "grad_norm": 0.5096288323402405,
      "learning_rate": 1.706683168316832e-05,
      "loss": 8.1096,
      "step": 10650
    },
    {
      "epoch": 3.2975021266723377,
      "grad_norm": 0.3293718695640564,
      "learning_rate": 1.7035891089108912e-05,
      "loss": 8.0901,
      "step": 10660
    },
    {
      "epoch": 3.300595468254582,
      "grad_norm": 0.5032432079315186,
      "learning_rate": 1.7004950495049508e-05,
      "loss": 8.1157,
      "step": 10670
    },
    {
      "epoch": 3.3036888098368262,
      "grad_norm": 0.5033963322639465,
      "learning_rate": 1.69740099009901e-05,
      "loss": 8.0999,
      "step": 10680
    },
    {
      "epoch": 3.3067821514190703,
      "grad_norm": 0.4207122027873993,
      "learning_rate": 1.6943069306930695e-05,
      "loss": 8.114,
      "step": 10690
    },
    {
      "epoch": 3.309875493001315,
      "grad_norm": 0.35678520798683167,
      "learning_rate": 1.691212871287129e-05,
      "loss": 8.0962,
      "step": 10700
    },
    {
      "epoch": 3.312968834583559,
      "grad_norm": 0.5279456377029419,
      "learning_rate": 1.6881188118811882e-05,
      "loss": 8.1069,
      "step": 10710
    },
    {
      "epoch": 3.3160621761658033,
      "grad_norm": 0.48339685797691345,
      "learning_rate": 1.6850247524752478e-05,
      "loss": 8.1083,
      "step": 10720
    },
    {
      "epoch": 3.3191555177480474,
      "grad_norm": 0.32819077372550964,
      "learning_rate": 1.681930693069307e-05,
      "loss": 8.1026,
      "step": 10730
    },
    {
      "epoch": 3.3222488593302915,
      "grad_norm": 0.4149583876132965,
      "learning_rate": 1.6788366336633665e-05,
      "loss": 8.1153,
      "step": 10740
    },
    {
      "epoch": 3.3253422009125355,
      "grad_norm": 0.5064743757247925,
      "learning_rate": 1.675742574257426e-05,
      "loss": 8.0999,
      "step": 10750
    },
    {
      "epoch": 3.32843554249478,
      "grad_norm": 0.4764443337917328,
      "learning_rate": 1.6726485148514853e-05,
      "loss": 8.097,
      "step": 10760
    },
    {
      "epoch": 3.331528884077024,
      "grad_norm": 0.4955633878707886,
      "learning_rate": 1.6695544554455448e-05,
      "loss": 8.1023,
      "step": 10770
    },
    {
      "epoch": 3.3346222256592686,
      "grad_norm": 0.2490134984254837,
      "learning_rate": 1.666460396039604e-05,
      "loss": 8.1054,
      "step": 10780
    },
    {
      "epoch": 3.3377155672415126,
      "grad_norm": 0.5426995158195496,
      "learning_rate": 1.6633663366336635e-05,
      "loss": 8.0959,
      "step": 10790
    },
    {
      "epoch": 3.3408089088237567,
      "grad_norm": 0.45831364393234253,
      "learning_rate": 1.660272277227723e-05,
      "loss": 8.0923,
      "step": 10800
    },
    {
      "epoch": 3.343902250406001,
      "grad_norm": 0.5634242296218872,
      "learning_rate": 1.6571782178217823e-05,
      "loss": 8.1151,
      "step": 10810
    },
    {
      "epoch": 3.3469955919882453,
      "grad_norm": 0.4605768024921417,
      "learning_rate": 1.6540841584158418e-05,
      "loss": 8.0909,
      "step": 10820
    },
    {
      "epoch": 3.3500889335704898,
      "grad_norm": 0.49280890822410583,
      "learning_rate": 1.650990099009901e-05,
      "loss": 8.0981,
      "step": 10830
    },
    {
      "epoch": 3.353182275152734,
      "grad_norm": 0.3870653212070465,
      "learning_rate": 1.6478960396039606e-05,
      "loss": 8.1161,
      "step": 10840
    },
    {
      "epoch": 3.356275616734978,
      "grad_norm": 0.4076969027519226,
      "learning_rate": 1.64480198019802e-05,
      "loss": 8.1193,
      "step": 10850
    },
    {
      "epoch": 3.359368958317222,
      "grad_norm": 0.3480227589607239,
      "learning_rate": 1.6417079207920793e-05,
      "loss": 8.0931,
      "step": 10860
    },
    {
      "epoch": 3.3624622998994664,
      "grad_norm": 0.3324853479862213,
      "learning_rate": 1.638613861386139e-05,
      "loss": 8.099,
      "step": 10870
    },
    {
      "epoch": 3.3655556414817105,
      "grad_norm": 0.4290516674518585,
      "learning_rate": 1.635519801980198e-05,
      "loss": 8.1321,
      "step": 10880
    },
    {
      "epoch": 3.368648983063955,
      "grad_norm": 0.2965244948863983,
      "learning_rate": 1.6324257425742576e-05,
      "loss": 8.1012,
      "step": 10890
    },
    {
      "epoch": 3.371742324646199,
      "grad_norm": 0.5500494837760925,
      "learning_rate": 1.629331683168317e-05,
      "loss": 8.0965,
      "step": 10900
    },
    {
      "epoch": 3.374835666228443,
      "grad_norm": 0.4214031994342804,
      "learning_rate": 1.6262376237623763e-05,
      "loss": 8.0842,
      "step": 10910
    },
    {
      "epoch": 3.3779290078106876,
      "grad_norm": 0.33007165789604187,
      "learning_rate": 1.623143564356436e-05,
      "loss": 8.0863,
      "step": 10920
    },
    {
      "epoch": 3.3810223493929317,
      "grad_norm": 0.2914193570613861,
      "learning_rate": 1.620049504950495e-05,
      "loss": 8.1049,
      "step": 10930
    },
    {
      "epoch": 3.384115690975176,
      "grad_norm": 0.44208136200904846,
      "learning_rate": 1.6169554455445546e-05,
      "loss": 8.0941,
      "step": 10940
    },
    {
      "epoch": 3.3872090325574202,
      "grad_norm": 0.37348124384880066,
      "learning_rate": 1.613861386138614e-05,
      "loss": 8.0978,
      "step": 10950
    },
    {
      "epoch": 3.3903023741396643,
      "grad_norm": 0.36081328988075256,
      "learning_rate": 1.6107673267326733e-05,
      "loss": 8.1051,
      "step": 10960
    },
    {
      "epoch": 3.3933957157219083,
      "grad_norm": 0.599541425704956,
      "learning_rate": 1.607673267326733e-05,
      "loss": 8.1064,
      "step": 10970
    },
    {
      "epoch": 3.396489057304153,
      "grad_norm": 0.35763850808143616,
      "learning_rate": 1.604579207920792e-05,
      "loss": 8.1257,
      "step": 10980
    },
    {
      "epoch": 3.399582398886397,
      "grad_norm": 0.44651269912719727,
      "learning_rate": 1.6014851485148516e-05,
      "loss": 8.1045,
      "step": 10990
    },
    {
      "epoch": 3.4026757404686414,
      "grad_norm": 0.44913795590400696,
      "learning_rate": 1.598391089108911e-05,
      "loss": 8.1015,
      "step": 11000
    },
    {
      "epoch": 3.4057690820508855,
      "grad_norm": 0.47472280263900757,
      "learning_rate": 1.5952970297029703e-05,
      "loss": 8.0968,
      "step": 11010
    },
    {
      "epoch": 3.4088624236331295,
      "grad_norm": 0.4076017141342163,
      "learning_rate": 1.59220297029703e-05,
      "loss": 8.1042,
      "step": 11020
    },
    {
      "epoch": 3.411955765215374,
      "grad_norm": 0.3256297707557678,
      "learning_rate": 1.589108910891089e-05,
      "loss": 8.1088,
      "step": 11030
    },
    {
      "epoch": 3.415049106797618,
      "grad_norm": 0.3449499309062958,
      "learning_rate": 1.5860148514851486e-05,
      "loss": 8.1218,
      "step": 11040
    },
    {
      "epoch": 3.4181424483798626,
      "grad_norm": 0.5740675926208496,
      "learning_rate": 1.582920792079208e-05,
      "loss": 8.0936,
      "step": 11050
    },
    {
      "epoch": 3.4212357899621066,
      "grad_norm": 0.36710357666015625,
      "learning_rate": 1.5798267326732674e-05,
      "loss": 8.1,
      "step": 11060
    },
    {
      "epoch": 3.4243291315443507,
      "grad_norm": 0.5742743015289307,
      "learning_rate": 1.576732673267327e-05,
      "loss": 8.1099,
      "step": 11070
    },
    {
      "epoch": 3.4274224731265948,
      "grad_norm": 0.37264394760131836,
      "learning_rate": 1.573638613861386e-05,
      "loss": 8.1057,
      "step": 11080
    },
    {
      "epoch": 3.4305158147088393,
      "grad_norm": 0.4299846291542053,
      "learning_rate": 1.5705445544554456e-05,
      "loss": 8.1011,
      "step": 11090
    },
    {
      "epoch": 3.4336091562910833,
      "grad_norm": 0.21564996242523193,
      "learning_rate": 1.5674504950495052e-05,
      "loss": 8.106,
      "step": 11100
    },
    {
      "epoch": 3.436702497873328,
      "grad_norm": 0.46745267510414124,
      "learning_rate": 1.5643564356435644e-05,
      "loss": 8.0917,
      "step": 11110
    },
    {
      "epoch": 3.439795839455572,
      "grad_norm": 0.36090579628944397,
      "learning_rate": 1.561262376237624e-05,
      "loss": 8.093,
      "step": 11120
    },
    {
      "epoch": 3.442889181037816,
      "grad_norm": 0.3865288496017456,
      "learning_rate": 1.558168316831683e-05,
      "loss": 8.1092,
      "step": 11130
    },
    {
      "epoch": 3.4459825226200604,
      "grad_norm": 0.5071180462837219,
      "learning_rate": 1.5550742574257427e-05,
      "loss": 8.1062,
      "step": 11140
    },
    {
      "epoch": 3.4490758642023045,
      "grad_norm": 0.43497562408447266,
      "learning_rate": 1.5519801980198022e-05,
      "loss": 8.1106,
      "step": 11150
    },
    {
      "epoch": 3.452169205784549,
      "grad_norm": 0.4134635031223297,
      "learning_rate": 1.5488861386138614e-05,
      "loss": 8.1097,
      "step": 11160
    },
    {
      "epoch": 3.455262547366793,
      "grad_norm": 0.49336209893226624,
      "learning_rate": 1.545792079207921e-05,
      "loss": 8.0894,
      "step": 11170
    },
    {
      "epoch": 3.458355888949037,
      "grad_norm": 0.37034985423088074,
      "learning_rate": 1.54269801980198e-05,
      "loss": 8.1079,
      "step": 11180
    },
    {
      "epoch": 3.461449230531281,
      "grad_norm": 0.3840271830558777,
      "learning_rate": 1.5396039603960397e-05,
      "loss": 8.0998,
      "step": 11190
    },
    {
      "epoch": 3.4645425721135257,
      "grad_norm": 0.4728190004825592,
      "learning_rate": 1.5365099009900992e-05,
      "loss": 8.103,
      "step": 11200
    },
    {
      "epoch": 3.4676359136957697,
      "grad_norm": 0.29465457797050476,
      "learning_rate": 1.5334158415841584e-05,
      "loss": 8.1018,
      "step": 11210
    },
    {
      "epoch": 3.4707292552780142,
      "grad_norm": 0.33252790570259094,
      "learning_rate": 1.530321782178218e-05,
      "loss": 8.0851,
      "step": 11220
    },
    {
      "epoch": 3.4738225968602583,
      "grad_norm": 0.26807039976119995,
      "learning_rate": 1.527227722772277e-05,
      "loss": 8.105,
      "step": 11230
    },
    {
      "epoch": 3.4769159384425024,
      "grad_norm": 0.37959063053131104,
      "learning_rate": 1.5241336633663367e-05,
      "loss": 8.0924,
      "step": 11240
    },
    {
      "epoch": 3.480009280024747,
      "grad_norm": 0.41083046793937683,
      "learning_rate": 1.521039603960396e-05,
      "loss": 8.1159,
      "step": 11250
    },
    {
      "epoch": 3.483102621606991,
      "grad_norm": 0.4050012528896332,
      "learning_rate": 1.5179455445544554e-05,
      "loss": 8.1085,
      "step": 11260
    },
    {
      "epoch": 3.4861959631892354,
      "grad_norm": 0.3787955939769745,
      "learning_rate": 1.514851485148515e-05,
      "loss": 8.1149,
      "step": 11270
    },
    {
      "epoch": 3.4892893047714795,
      "grad_norm": 0.3537384569644928,
      "learning_rate": 1.5117574257425743e-05,
      "loss": 8.1164,
      "step": 11280
    },
    {
      "epoch": 3.4923826463537235,
      "grad_norm": 0.4673440456390381,
      "learning_rate": 1.5086633663366337e-05,
      "loss": 8.1021,
      "step": 11290
    },
    {
      "epoch": 3.4954759879359676,
      "grad_norm": 0.357994019985199,
      "learning_rate": 1.505569306930693e-05,
      "loss": 8.1116,
      "step": 11300
    },
    {
      "epoch": 3.498569329518212,
      "grad_norm": 0.4076803922653198,
      "learning_rate": 1.5024752475247524e-05,
      "loss": 8.093,
      "step": 11310
    },
    {
      "epoch": 3.501662671100456,
      "grad_norm": 0.34017065167427063,
      "learning_rate": 1.499381188118812e-05,
      "loss": 8.1139,
      "step": 11320
    },
    {
      "epoch": 3.5047560126827006,
      "grad_norm": 0.4401533603668213,
      "learning_rate": 1.4962871287128713e-05,
      "loss": 8.1185,
      "step": 11330
    },
    {
      "epoch": 3.5078493542649447,
      "grad_norm": 0.36438196897506714,
      "learning_rate": 1.4931930693069307e-05,
      "loss": 8.0997,
      "step": 11340
    },
    {
      "epoch": 3.5109426958471888,
      "grad_norm": 0.3833825886249542,
      "learning_rate": 1.4900990099009901e-05,
      "loss": 8.1061,
      "step": 11350
    },
    {
      "epoch": 3.5140360374294333,
      "grad_norm": 0.2591242492198944,
      "learning_rate": 1.4870049504950495e-05,
      "loss": 8.1044,
      "step": 11360
    },
    {
      "epoch": 3.5171293790116773,
      "grad_norm": 0.3583930432796478,
      "learning_rate": 1.483910891089109e-05,
      "loss": 8.1166,
      "step": 11370
    },
    {
      "epoch": 3.520222720593922,
      "grad_norm": 0.41947051882743835,
      "learning_rate": 1.4808168316831684e-05,
      "loss": 8.085,
      "step": 11380
    },
    {
      "epoch": 3.523316062176166,
      "grad_norm": 0.421604186296463,
      "learning_rate": 1.4777227722772277e-05,
      "loss": 8.1068,
      "step": 11390
    },
    {
      "epoch": 3.52640940375841,
      "grad_norm": 0.3165116012096405,
      "learning_rate": 1.4746287128712871e-05,
      "loss": 8.1061,
      "step": 11400
    },
    {
      "epoch": 3.529502745340654,
      "grad_norm": 0.433390736579895,
      "learning_rate": 1.4715346534653465e-05,
      "loss": 8.114,
      "step": 11410
    },
    {
      "epoch": 3.5325960869228985,
      "grad_norm": 0.3133591115474701,
      "learning_rate": 1.468440594059406e-05,
      "loss": 8.1108,
      "step": 11420
    },
    {
      "epoch": 3.5356894285051426,
      "grad_norm": 0.5469237565994263,
      "learning_rate": 1.4653465346534654e-05,
      "loss": 8.111,
      "step": 11430
    },
    {
      "epoch": 3.538782770087387,
      "grad_norm": 0.46641063690185547,
      "learning_rate": 1.4622524752475248e-05,
      "loss": 8.1124,
      "step": 11440
    },
    {
      "epoch": 3.541876111669631,
      "grad_norm": 0.4280323088169098,
      "learning_rate": 1.4591584158415841e-05,
      "loss": 8.0976,
      "step": 11450
    },
    {
      "epoch": 3.544969453251875,
      "grad_norm": 0.3588252365589142,
      "learning_rate": 1.4560643564356435e-05,
      "loss": 8.0799,
      "step": 11460
    },
    {
      "epoch": 3.5480627948341197,
      "grad_norm": 0.5382620096206665,
      "learning_rate": 1.4529702970297029e-05,
      "loss": 8.0971,
      "step": 11470
    },
    {
      "epoch": 3.5511561364163637,
      "grad_norm": 0.43373942375183105,
      "learning_rate": 1.4498762376237624e-05,
      "loss": 8.1115,
      "step": 11480
    },
    {
      "epoch": 3.5542494779986082,
      "grad_norm": 0.5428350567817688,
      "learning_rate": 1.4467821782178218e-05,
      "loss": 8.1106,
      "step": 11490
    },
    {
      "epoch": 3.5573428195808523,
      "grad_norm": 0.43552514910697937,
      "learning_rate": 1.4436881188118811e-05,
      "loss": 8.1066,
      "step": 11500
    },
    {
      "epoch": 3.5604361611630964,
      "grad_norm": 0.3100108504295349,
      "learning_rate": 1.4405940594059405e-05,
      "loss": 8.0991,
      "step": 11510
    },
    {
      "epoch": 3.5635295027453404,
      "grad_norm": 0.4565158486366272,
      "learning_rate": 1.4374999999999999e-05,
      "loss": 8.1129,
      "step": 11520
    },
    {
      "epoch": 3.566622844327585,
      "grad_norm": 0.39923667907714844,
      "learning_rate": 1.4344059405940594e-05,
      "loss": 8.1192,
      "step": 11530
    },
    {
      "epoch": 3.569716185909829,
      "grad_norm": 0.4239896535873413,
      "learning_rate": 1.4313118811881188e-05,
      "loss": 8.1181,
      "step": 11540
    },
    {
      "epoch": 3.5728095274920735,
      "grad_norm": 0.28871044516563416,
      "learning_rate": 1.4282178217821782e-05,
      "loss": 8.1035,
      "step": 11550
    },
    {
      "epoch": 3.5759028690743175,
      "grad_norm": 0.41867879033088684,
      "learning_rate": 1.4251237623762375e-05,
      "loss": 8.1021,
      "step": 11560
    },
    {
      "epoch": 3.5789962106565616,
      "grad_norm": 0.39196592569351196,
      "learning_rate": 1.4220297029702969e-05,
      "loss": 8.109,
      "step": 11570
    },
    {
      "epoch": 3.582089552238806,
      "grad_norm": 0.47929254174232483,
      "learning_rate": 1.4189356435643564e-05,
      "loss": 8.1134,
      "step": 11580
    },
    {
      "epoch": 3.58518289382105,
      "grad_norm": 0.5060728788375854,
      "learning_rate": 1.4158415841584158e-05,
      "loss": 8.1002,
      "step": 11590
    },
    {
      "epoch": 3.5882762354032947,
      "grad_norm": 0.3129682242870331,
      "learning_rate": 1.4127475247524752e-05,
      "loss": 8.1151,
      "step": 11600
    },
    {
      "epoch": 3.5913695769855387,
      "grad_norm": 0.3388754427433014,
      "learning_rate": 1.4096534653465345e-05,
      "loss": 8.0969,
      "step": 11610
    },
    {
      "epoch": 3.5944629185677828,
      "grad_norm": 0.4285489618778229,
      "learning_rate": 1.4065594059405939e-05,
      "loss": 8.0956,
      "step": 11620
    },
    {
      "epoch": 3.597556260150027,
      "grad_norm": 0.30442413687705994,
      "learning_rate": 1.4034653465346536e-05,
      "loss": 8.1065,
      "step": 11630
    },
    {
      "epoch": 3.6006496017322713,
      "grad_norm": 0.5260847210884094,
      "learning_rate": 1.4003712871287132e-05,
      "loss": 8.0978,
      "step": 11640
    },
    {
      "epoch": 3.6037429433145154,
      "grad_norm": 0.333693265914917,
      "learning_rate": 1.3972772277227725e-05,
      "loss": 8.1101,
      "step": 11650
    },
    {
      "epoch": 3.60683628489676,
      "grad_norm": 0.3874038755893707,
      "learning_rate": 1.3941831683168319e-05,
      "loss": 8.1041,
      "step": 11660
    },
    {
      "epoch": 3.609929626479004,
      "grad_norm": 0.4372905194759369,
      "learning_rate": 1.3910891089108913e-05,
      "loss": 8.1138,
      "step": 11670
    },
    {
      "epoch": 3.613022968061248,
      "grad_norm": 0.36207008361816406,
      "learning_rate": 1.3879950495049506e-05,
      "loss": 8.1103,
      "step": 11680
    },
    {
      "epoch": 3.6161163096434925,
      "grad_norm": 0.5028199553489685,
      "learning_rate": 1.3849009900990102e-05,
      "loss": 8.0988,
      "step": 11690
    },
    {
      "epoch": 3.6192096512257366,
      "grad_norm": 0.35599783062934875,
      "learning_rate": 1.3818069306930695e-05,
      "loss": 8.0986,
      "step": 11700
    },
    {
      "epoch": 3.622302992807981,
      "grad_norm": 0.31794866919517517,
      "learning_rate": 1.3787128712871289e-05,
      "loss": 8.0938,
      "step": 11710
    },
    {
      "epoch": 3.625396334390225,
      "grad_norm": 0.37773212790489197,
      "learning_rate": 1.3756188118811883e-05,
      "loss": 8.1186,
      "step": 11720
    },
    {
      "epoch": 3.628489675972469,
      "grad_norm": 0.36469292640686035,
      "learning_rate": 1.3725247524752476e-05,
      "loss": 8.0978,
      "step": 11730
    },
    {
      "epoch": 3.6315830175547132,
      "grad_norm": 0.4029846489429474,
      "learning_rate": 1.3694306930693072e-05,
      "loss": 8.1024,
      "step": 11740
    },
    {
      "epoch": 3.6346763591369577,
      "grad_norm": 0.3855964243412018,
      "learning_rate": 1.3663366336633666e-05,
      "loss": 8.1008,
      "step": 11750
    },
    {
      "epoch": 3.637769700719202,
      "grad_norm": 0.4206945598125458,
      "learning_rate": 1.363242574257426e-05,
      "loss": 8.1017,
      "step": 11760
    },
    {
      "epoch": 3.6408630423014463,
      "grad_norm": 0.6562212109565735,
      "learning_rate": 1.3601485148514853e-05,
      "loss": 8.1003,
      "step": 11770
    },
    {
      "epoch": 3.6439563838836904,
      "grad_norm": 0.37774574756622314,
      "learning_rate": 1.3570544554455447e-05,
      "loss": 8.0975,
      "step": 11780
    },
    {
      "epoch": 3.6470497254659344,
      "grad_norm": 0.4984301030635834,
      "learning_rate": 1.3539603960396042e-05,
      "loss": 8.0951,
      "step": 11790
    },
    {
      "epoch": 3.650143067048179,
      "grad_norm": 0.44583824276924133,
      "learning_rate": 1.3508663366336636e-05,
      "loss": 8.1173,
      "step": 11800
    },
    {
      "epoch": 3.653236408630423,
      "grad_norm": 0.41385045647621155,
      "learning_rate": 1.347772277227723e-05,
      "loss": 8.0978,
      "step": 11810
    },
    {
      "epoch": 3.6563297502126675,
      "grad_norm": 0.40406572818756104,
      "learning_rate": 1.3446782178217823e-05,
      "loss": 8.1039,
      "step": 11820
    },
    {
      "epoch": 3.6594230917949115,
      "grad_norm": 0.3647640645503998,
      "learning_rate": 1.3415841584158417e-05,
      "loss": 8.1037,
      "step": 11830
    },
    {
      "epoch": 3.6625164333771556,
      "grad_norm": 0.464382529258728,
      "learning_rate": 1.338799504950495e-05,
      "loss": 8.1154,
      "step": 11840
    },
    {
      "epoch": 3.6656097749593997,
      "grad_norm": 0.6339207291603088,
      "learning_rate": 1.3357054455445545e-05,
      "loss": 8.1111,
      "step": 11850
    },
    {
      "epoch": 3.668703116541644,
      "grad_norm": 0.3799396753311157,
      "learning_rate": 1.3326113861386139e-05,
      "loss": 8.1029,
      "step": 11860
    },
    {
      "epoch": 3.671796458123888,
      "grad_norm": 0.527326226234436,
      "learning_rate": 1.3295173267326733e-05,
      "loss": 8.107,
      "step": 11870
    },
    {
      "epoch": 3.6748897997061327,
      "grad_norm": 0.3180059790611267,
      "learning_rate": 1.3264232673267326e-05,
      "loss": 8.0925,
      "step": 11880
    },
    {
      "epoch": 3.677983141288377,
      "grad_norm": 0.5550610423088074,
      "learning_rate": 1.323329207920792e-05,
      "loss": 8.1057,
      "step": 11890
    },
    {
      "epoch": 3.681076482870621,
      "grad_norm": 0.38884061574935913,
      "learning_rate": 1.3202351485148515e-05,
      "loss": 8.1014,
      "step": 11900
    },
    {
      "epoch": 3.6841698244528653,
      "grad_norm": 0.31325098872184753,
      "learning_rate": 1.317141089108911e-05,
      "loss": 8.0993,
      "step": 11910
    },
    {
      "epoch": 3.6872631660351094,
      "grad_norm": 0.43336018919944763,
      "learning_rate": 1.3140470297029703e-05,
      "loss": 8.1098,
      "step": 11920
    },
    {
      "epoch": 3.690356507617354,
      "grad_norm": 0.4847187101840973,
      "learning_rate": 1.3109529702970297e-05,
      "loss": 8.1072,
      "step": 11930
    },
    {
      "epoch": 3.693449849199598,
      "grad_norm": 0.38891351222991943,
      "learning_rate": 1.307858910891089e-05,
      "loss": 8.0927,
      "step": 11940
    },
    {
      "epoch": 3.696543190781842,
      "grad_norm": 0.42333662509918213,
      "learning_rate": 1.3047648514851486e-05,
      "loss": 8.1218,
      "step": 11950
    },
    {
      "epoch": 3.699636532364086,
      "grad_norm": 0.4755714535713196,
      "learning_rate": 1.301670792079208e-05,
      "loss": 8.1002,
      "step": 11960
    },
    {
      "epoch": 3.7027298739463306,
      "grad_norm": 0.4375995993614197,
      "learning_rate": 1.2985767326732673e-05,
      "loss": 8.1136,
      "step": 11970
    },
    {
      "epoch": 3.7058232155285746,
      "grad_norm": 0.315766304731369,
      "learning_rate": 1.2954826732673267e-05,
      "loss": 8.1162,
      "step": 11980
    },
    {
      "epoch": 3.708916557110819,
      "grad_norm": 0.43180716037750244,
      "learning_rate": 1.292388613861386e-05,
      "loss": 8.1078,
      "step": 11990
    },
    {
      "epoch": 3.712009898693063,
      "grad_norm": 0.4966972768306732,
      "learning_rate": 1.2892945544554456e-05,
      "loss": 8.104,
      "step": 12000
    },
    {
      "epoch": 3.7151032402753073,
      "grad_norm": 0.3658710718154907,
      "learning_rate": 1.286200495049505e-05,
      "loss": 8.1102,
      "step": 12010
    },
    {
      "epoch": 3.7181965818575518,
      "grad_norm": 0.3355751633644104,
      "learning_rate": 1.2831064356435643e-05,
      "loss": 8.1089,
      "step": 12020
    },
    {
      "epoch": 3.721289923439796,
      "grad_norm": 0.4777666926383972,
      "learning_rate": 1.2800123762376237e-05,
      "loss": 8.1048,
      "step": 12030
    },
    {
      "epoch": 3.7243832650220403,
      "grad_norm": 0.528012216091156,
      "learning_rate": 1.276918316831683e-05,
      "loss": 8.1021,
      "step": 12040
    },
    {
      "epoch": 3.7274766066042844,
      "grad_norm": 0.46141040325164795,
      "learning_rate": 1.2738242574257426e-05,
      "loss": 8.0992,
      "step": 12050
    },
    {
      "epoch": 3.7305699481865284,
      "grad_norm": 0.3637773096561432,
      "learning_rate": 1.270730198019802e-05,
      "loss": 8.1025,
      "step": 12060
    },
    {
      "epoch": 3.7336632897687725,
      "grad_norm": 0.3270203769207001,
      "learning_rate": 1.2676361386138613e-05,
      "loss": 8.1015,
      "step": 12070
    },
    {
      "epoch": 3.736756631351017,
      "grad_norm": 0.42056170105934143,
      "learning_rate": 1.2645420792079207e-05,
      "loss": 8.1147,
      "step": 12080
    },
    {
      "epoch": 3.739849972933261,
      "grad_norm": 0.46362605690956116,
      "learning_rate": 1.26144801980198e-05,
      "loss": 8.1242,
      "step": 12090
    },
    {
      "epoch": 3.7429433145155055,
      "grad_norm": 0.41113027930259705,
      "learning_rate": 1.2583539603960396e-05,
      "loss": 8.1141,
      "step": 12100
    },
    {
      "epoch": 3.7460366560977496,
      "grad_norm": 0.3748932480812073,
      "learning_rate": 1.255259900990099e-05,
      "loss": 8.0996,
      "step": 12110
    },
    {
      "epoch": 3.7491299976799937,
      "grad_norm": 0.5087127089500427,
      "learning_rate": 1.2521658415841583e-05,
      "loss": 8.1115,
      "step": 12120
    },
    {
      "epoch": 3.752223339262238,
      "grad_norm": 0.3965197503566742,
      "learning_rate": 1.2490717821782179e-05,
      "loss": 8.1078,
      "step": 12130
    },
    {
      "epoch": 3.7553166808444822,
      "grad_norm": 0.34920966625213623,
      "learning_rate": 1.2459777227722773e-05,
      "loss": 8.0723,
      "step": 12140
    },
    {
      "epoch": 3.7584100224267267,
      "grad_norm": 0.5113892555236816,
      "learning_rate": 1.2428836633663366e-05,
      "loss": 8.1045,
      "step": 12150
    },
    {
      "epoch": 3.761503364008971,
      "grad_norm": 0.5669775605201721,
      "learning_rate": 1.2397896039603962e-05,
      "loss": 8.1139,
      "step": 12160
    },
    {
      "epoch": 3.764596705591215,
      "grad_norm": 0.40940672159194946,
      "learning_rate": 1.2366955445544555e-05,
      "loss": 8.114,
      "step": 12170
    },
    {
      "epoch": 3.767690047173459,
      "grad_norm": 0.406909704208374,
      "learning_rate": 1.2336014851485149e-05,
      "loss": 8.1117,
      "step": 12180
    },
    {
      "epoch": 3.7707833887557034,
      "grad_norm": 0.4225293695926666,
      "learning_rate": 1.2305074257425743e-05,
      "loss": 8.099,
      "step": 12190
    },
    {
      "epoch": 3.7738767303379475,
      "grad_norm": 0.4077262878417969,
      "learning_rate": 1.2274133663366336e-05,
      "loss": 8.1091,
      "step": 12200
    },
    {
      "epoch": 3.776970071920192,
      "grad_norm": 0.29766353964805603,
      "learning_rate": 1.2243193069306932e-05,
      "loss": 8.1085,
      "step": 12210
    },
    {
      "epoch": 3.780063413502436,
      "grad_norm": 0.4322611391544342,
      "learning_rate": 1.2212252475247526e-05,
      "loss": 8.1125,
      "step": 12220
    },
    {
      "epoch": 3.78315675508468,
      "grad_norm": 0.40750840306282043,
      "learning_rate": 1.218131188118812e-05,
      "loss": 8.1143,
      "step": 12230
    },
    {
      "epoch": 3.7862500966669246,
      "grad_norm": 0.4841535687446594,
      "learning_rate": 1.2150371287128713e-05,
      "loss": 8.1085,
      "step": 12240
    },
    {
      "epoch": 3.7893434382491686,
      "grad_norm": 0.41431429982185364,
      "learning_rate": 1.2119430693069307e-05,
      "loss": 8.1097,
      "step": 12250
    },
    {
      "epoch": 3.792436779831413,
      "grad_norm": 0.3474237620830536,
      "learning_rate": 1.2088490099009902e-05,
      "loss": 8.0998,
      "step": 12260
    },
    {
      "epoch": 3.795530121413657,
      "grad_norm": 0.34377312660217285,
      "learning_rate": 1.2057549504950496e-05,
      "loss": 8.106,
      "step": 12270
    },
    {
      "epoch": 3.7986234629959013,
      "grad_norm": 0.3838348984718323,
      "learning_rate": 1.202660891089109e-05,
      "loss": 8.1038,
      "step": 12280
    },
    {
      "epoch": 3.8017168045781453,
      "grad_norm": 0.5226777195930481,
      "learning_rate": 1.1995668316831683e-05,
      "loss": 8.1039,
      "step": 12290
    },
    {
      "epoch": 3.80481014616039,
      "grad_norm": 0.5079076290130615,
      "learning_rate": 1.1964727722772277e-05,
      "loss": 8.1022,
      "step": 12300
    },
    {
      "epoch": 3.807903487742634,
      "grad_norm": 0.46516600251197815,
      "learning_rate": 1.1933787128712872e-05,
      "loss": 8.1027,
      "step": 12310
    },
    {
      "epoch": 3.8109968293248784,
      "grad_norm": 0.36677253246307373,
      "learning_rate": 1.1902846534653466e-05,
      "loss": 8.0926,
      "step": 12320
    },
    {
      "epoch": 3.8140901709071224,
      "grad_norm": 0.3685280382633209,
      "learning_rate": 1.187190594059406e-05,
      "loss": 8.1002,
      "step": 12330
    },
    {
      "epoch": 3.8171835124893665,
      "grad_norm": 0.3298216164112091,
      "learning_rate": 1.1840965346534653e-05,
      "loss": 8.1109,
      "step": 12340
    },
    {
      "epoch": 3.820276854071611,
      "grad_norm": 0.3642781972885132,
      "learning_rate": 1.1810024752475247e-05,
      "loss": 8.1112,
      "step": 12350
    },
    {
      "epoch": 3.823370195653855,
      "grad_norm": 0.2822742760181427,
      "learning_rate": 1.1779084158415842e-05,
      "loss": 8.0857,
      "step": 12360
    },
    {
      "epoch": 3.8264635372360996,
      "grad_norm": 0.3740764856338501,
      "learning_rate": 1.1748143564356436e-05,
      "loss": 8.1139,
      "step": 12370
    },
    {
      "epoch": 3.8295568788183436,
      "grad_norm": 0.45429617166519165,
      "learning_rate": 1.1717202970297031e-05,
      "loss": 8.112,
      "step": 12380
    },
    {
      "epoch": 3.8326502204005877,
      "grad_norm": 0.5934394001960754,
      "learning_rate": 1.1686262376237625e-05,
      "loss": 8.1071,
      "step": 12390
    },
    {
      "epoch": 3.8357435619828317,
      "grad_norm": 0.41346925497055054,
      "learning_rate": 1.1655321782178219e-05,
      "loss": 8.1077,
      "step": 12400
    },
    {
      "epoch": 3.8388369035650762,
      "grad_norm": 0.6458012461662292,
      "learning_rate": 1.1624381188118812e-05,
      "loss": 8.0967,
      "step": 12410
    },
    {
      "epoch": 3.8419302451473203,
      "grad_norm": 0.4448651969432831,
      "learning_rate": 1.1593440594059408e-05,
      "loss": 8.0995,
      "step": 12420
    },
    {
      "epoch": 3.845023586729565,
      "grad_norm": 0.43399015069007874,
      "learning_rate": 1.1562500000000002e-05,
      "loss": 8.1084,
      "step": 12430
    },
    {
      "epoch": 3.848116928311809,
      "grad_norm": 0.5257229208946228,
      "learning_rate": 1.1531559405940595e-05,
      "loss": 8.0984,
      "step": 12440
    },
    {
      "epoch": 3.851210269894053,
      "grad_norm": 0.3766283690929413,
      "learning_rate": 1.1500618811881189e-05,
      "loss": 8.1058,
      "step": 12450
    },
    {
      "epoch": 3.8543036114762974,
      "grad_norm": 0.37777552008628845,
      "learning_rate": 1.1469678217821783e-05,
      "loss": 8.098,
      "step": 12460
    },
    {
      "epoch": 3.8573969530585415,
      "grad_norm": 0.2982431948184967,
      "learning_rate": 1.1438737623762378e-05,
      "loss": 8.1148,
      "step": 12470
    },
    {
      "epoch": 3.860490294640786,
      "grad_norm": 0.3095042109489441,
      "learning_rate": 1.1407797029702972e-05,
      "loss": 8.1042,
      "step": 12480
    },
    {
      "epoch": 3.86358363622303,
      "grad_norm": 0.36748334765434265,
      "learning_rate": 1.1376856435643565e-05,
      "loss": 8.112,
      "step": 12490
    },
    {
      "epoch": 3.866676977805274,
      "grad_norm": 0.4453704357147217,
      "learning_rate": 1.1345915841584159e-05,
      "loss": 8.0977,
      "step": 12500
    },
    {
      "epoch": 3.869770319387518,
      "grad_norm": 0.335215300321579,
      "learning_rate": 1.1314975247524753e-05,
      "loss": 8.1036,
      "step": 12510
    },
    {
      "epoch": 3.8728636609697626,
      "grad_norm": 0.5659045577049255,
      "learning_rate": 1.1284034653465348e-05,
      "loss": 8.0967,
      "step": 12520
    },
    {
      "epoch": 3.8759570025520067,
      "grad_norm": 0.3822405934333801,
      "learning_rate": 1.1253094059405942e-05,
      "loss": 8.1155,
      "step": 12530
    },
    {
      "epoch": 3.879050344134251,
      "grad_norm": 0.2948906719684601,
      "learning_rate": 1.1222153465346536e-05,
      "loss": 8.1105,
      "step": 12540
    },
    {
      "epoch": 3.8821436857164953,
      "grad_norm": 0.45879271626472473,
      "learning_rate": 1.119121287128713e-05,
      "loss": 8.0959,
      "step": 12550
    },
    {
      "epoch": 3.8852370272987393,
      "grad_norm": 0.32437336444854736,
      "learning_rate": 1.1160272277227723e-05,
      "loss": 8.0996,
      "step": 12560
    },
    {
      "epoch": 3.888330368880984,
      "grad_norm": 0.38903558254241943,
      "learning_rate": 1.1129331683168318e-05,
      "loss": 8.1027,
      "step": 12570
    },
    {
      "epoch": 3.891423710463228,
      "grad_norm": 0.398411363363266,
      "learning_rate": 1.1098391089108912e-05,
      "loss": 8.1117,
      "step": 12580
    },
    {
      "epoch": 3.8945170520454724,
      "grad_norm": 0.3728010952472687,
      "learning_rate": 1.1067450495049506e-05,
      "loss": 8.1,
      "step": 12590
    },
    {
      "epoch": 3.8976103936277164,
      "grad_norm": 0.26777175068855286,
      "learning_rate": 1.10365099009901e-05,
      "loss": 8.1049,
      "step": 12600
    },
    {
      "epoch": 3.9007037352099605,
      "grad_norm": 0.4046097695827484,
      "learning_rate": 1.1005569306930693e-05,
      "loss": 8.0851,
      "step": 12610
    },
    {
      "epoch": 3.9037970767922046,
      "grad_norm": 0.5357159376144409,
      "learning_rate": 1.0974628712871288e-05,
      "loss": 8.1079,
      "step": 12620
    },
    {
      "epoch": 3.906890418374449,
      "grad_norm": 0.3542024791240692,
      "learning_rate": 1.0943688118811882e-05,
      "loss": 8.103,
      "step": 12630
    },
    {
      "epoch": 3.909983759956693,
      "grad_norm": 0.29429346323013306,
      "learning_rate": 1.0912747524752476e-05,
      "loss": 8.11,
      "step": 12640
    },
    {
      "epoch": 3.9130771015389376,
      "grad_norm": 0.44802501797676086,
      "learning_rate": 1.088180693069307e-05,
      "loss": 8.1058,
      "step": 12650
    },
    {
      "epoch": 3.9161704431211817,
      "grad_norm": 0.4152248799800873,
      "learning_rate": 1.0850866336633663e-05,
      "loss": 8.1061,
      "step": 12660
    },
    {
      "epoch": 3.9192637847034257,
      "grad_norm": 0.33737531304359436,
      "learning_rate": 1.0819925742574259e-05,
      "loss": 8.1093,
      "step": 12670
    },
    {
      "epoch": 3.9223571262856702,
      "grad_norm": 0.5027520656585693,
      "learning_rate": 1.0788985148514852e-05,
      "loss": 8.1092,
      "step": 12680
    },
    {
      "epoch": 3.9254504678679143,
      "grad_norm": 0.5544971227645874,
      "learning_rate": 1.0758044554455446e-05,
      "loss": 8.1054,
      "step": 12690
    },
    {
      "epoch": 3.928543809450159,
      "grad_norm": 0.7236347198486328,
      "learning_rate": 1.072710396039604e-05,
      "loss": 8.1112,
      "step": 12700
    },
    {
      "epoch": 3.931637151032403,
      "grad_norm": 0.32161620259284973,
      "learning_rate": 1.0696163366336633e-05,
      "loss": 8.1027,
      "step": 12710
    },
    {
      "epoch": 3.934730492614647,
      "grad_norm": 0.31667324900627136,
      "learning_rate": 1.0665222772277229e-05,
      "loss": 8.1068,
      "step": 12720
    },
    {
      "epoch": 3.937823834196891,
      "grad_norm": 0.3488391935825348,
      "learning_rate": 1.0634282178217822e-05,
      "loss": 8.1019,
      "step": 12730
    },
    {
      "epoch": 3.9409171757791355,
      "grad_norm": 0.34023359417915344,
      "learning_rate": 1.0603341584158416e-05,
      "loss": 8.1062,
      "step": 12740
    },
    {
      "epoch": 3.9440105173613795,
      "grad_norm": 0.35203710198402405,
      "learning_rate": 1.057240099009901e-05,
      "loss": 8.114,
      "step": 12750
    },
    {
      "epoch": 3.947103858943624,
      "grad_norm": 0.4235670864582062,
      "learning_rate": 1.0541460396039604e-05,
      "loss": 8.0896,
      "step": 12760
    },
    {
      "epoch": 3.950197200525868,
      "grad_norm": 0.5901620388031006,
      "learning_rate": 1.0510519801980199e-05,
      "loss": 8.1038,
      "step": 12770
    },
    {
      "epoch": 3.953290542108112,
      "grad_norm": 0.42369896173477173,
      "learning_rate": 1.0479579207920793e-05,
      "loss": 8.0989,
      "step": 12780
    },
    {
      "epoch": 3.9563838836903567,
      "grad_norm": 0.3480532765388489,
      "learning_rate": 1.0448638613861386e-05,
      "loss": 8.0902,
      "step": 12790
    },
    {
      "epoch": 3.9594772252726007,
      "grad_norm": 0.35896116495132446,
      "learning_rate": 1.041769801980198e-05,
      "loss": 8.1144,
      "step": 12800
    },
    {
      "epoch": 3.962570566854845,
      "grad_norm": 0.4195922613143921,
      "learning_rate": 1.0386757425742574e-05,
      "loss": 8.1024,
      "step": 12810
    },
    {
      "epoch": 3.9656639084370893,
      "grad_norm": 0.3841812312602997,
      "learning_rate": 1.0355816831683169e-05,
      "loss": 8.1159,
      "step": 12820
    },
    {
      "epoch": 3.9687572500193333,
      "grad_norm": 0.42152583599090576,
      "learning_rate": 1.0324876237623763e-05,
      "loss": 8.1132,
      "step": 12830
    },
    {
      "epoch": 3.9718505916015774,
      "grad_norm": 0.5058812499046326,
      "learning_rate": 1.0293935643564356e-05,
      "loss": 8.1033,
      "step": 12840
    },
    {
      "epoch": 3.974943933183822,
      "grad_norm": 0.43944016098976135,
      "learning_rate": 1.026299504950495e-05,
      "loss": 8.094,
      "step": 12850
    },
    {
      "epoch": 3.978037274766066,
      "grad_norm": 0.5188089609146118,
      "learning_rate": 1.0232054455445544e-05,
      "loss": 8.102,
      "step": 12860
    },
    {
      "epoch": 3.9811306163483104,
      "grad_norm": 0.5888644456863403,
      "learning_rate": 1.020111386138614e-05,
      "loss": 8.1067,
      "step": 12870
    },
    {
      "epoch": 3.9842239579305545,
      "grad_norm": 0.6576205492019653,
      "learning_rate": 1.0170173267326733e-05,
      "loss": 8.0936,
      "step": 12880
    },
    {
      "epoch": 3.9873172995127986,
      "grad_norm": 0.42696404457092285,
      "learning_rate": 1.0139232673267328e-05,
      "loss": 8.1049,
      "step": 12890
    },
    {
      "epoch": 3.990410641095043,
      "grad_norm": 0.3262653648853302,
      "learning_rate": 1.0108292079207922e-05,
      "loss": 8.1223,
      "step": 12900
    },
    {
      "epoch": 3.993503982677287,
      "grad_norm": 0.5317575931549072,
      "learning_rate": 1.0077351485148516e-05,
      "loss": 8.1047,
      "step": 12910
    },
    {
      "epoch": 3.9965973242595316,
      "grad_norm": 0.5094757676124573,
      "learning_rate": 1.004641089108911e-05,
      "loss": 8.116,
      "step": 12920
    },
    {
      "epoch": 3.9996906658417757,
      "grad_norm": 0.545456051826477,
      "learning_rate": 1.0015470297029705e-05,
      "loss": 8.0945,
      "step": 12930
    },
    {
      "epoch": 4.00278400742402,
      "grad_norm": 0.3330582082271576,
      "learning_rate": 9.984529702970299e-06,
      "loss": 8.1106,
      "step": 12940
    },
    {
      "epoch": 4.005877349006264,
      "grad_norm": 0.38926154375076294,
      "learning_rate": 9.953589108910892e-06,
      "loss": 8.1128,
      "step": 12950
    },
    {
      "epoch": 4.008970690588508,
      "grad_norm": 0.2806829810142517,
      "learning_rate": 9.922648514851486e-06,
      "loss": 8.1038,
      "step": 12960
    },
    {
      "epoch": 4.012064032170753,
      "grad_norm": 0.352874755859375,
      "learning_rate": 9.89170792079208e-06,
      "loss": 8.0981,
      "step": 12970
    },
    {
      "epoch": 4.015157373752997,
      "grad_norm": 0.4834064245223999,
      "learning_rate": 9.860767326732675e-06,
      "loss": 8.0826,
      "step": 12980
    },
    {
      "epoch": 4.018250715335241,
      "grad_norm": 0.44703438878059387,
      "learning_rate": 9.829826732673269e-06,
      "loss": 8.116,
      "step": 12990
    },
    {
      "epoch": 4.021344056917485,
      "grad_norm": 0.29937657713890076,
      "learning_rate": 9.798886138613862e-06,
      "loss": 8.1168,
      "step": 13000
    },
    {
      "epoch": 4.024437398499729,
      "grad_norm": 0.4509787857532501,
      "learning_rate": 9.767945544554456e-06,
      "loss": 8.1027,
      "step": 13010
    },
    {
      "epoch": 4.027530740081974,
      "grad_norm": 0.3635117709636688,
      "learning_rate": 9.73700495049505e-06,
      "loss": 8.0982,
      "step": 13020
    },
    {
      "epoch": 4.030624081664218,
      "grad_norm": 0.3736985921859741,
      "learning_rate": 9.706064356435645e-06,
      "loss": 8.0967,
      "step": 13030
    },
    {
      "epoch": 4.033717423246462,
      "grad_norm": 0.7353485822677612,
      "learning_rate": 9.675123762376239e-06,
      "loss": 8.0802,
      "step": 13040
    },
    {
      "epoch": 4.036810764828706,
      "grad_norm": 0.40160706639289856,
      "learning_rate": 9.644183168316833e-06,
      "loss": 8.1029,
      "step": 13050
    },
    {
      "epoch": 4.03990410641095,
      "grad_norm": 0.2831556499004364,
      "learning_rate": 9.613242574257426e-06,
      "loss": 8.1067,
      "step": 13060
    },
    {
      "epoch": 4.042997447993194,
      "grad_norm": 0.3895133137702942,
      "learning_rate": 9.58230198019802e-06,
      "loss": 8.1234,
      "step": 13070
    },
    {
      "epoch": 4.046090789575439,
      "grad_norm": 0.39980563521385193,
      "learning_rate": 9.551361386138615e-06,
      "loss": 8.1174,
      "step": 13080
    },
    {
      "epoch": 4.049184131157683,
      "grad_norm": 0.3832123279571533,
      "learning_rate": 9.520420792079209e-06,
      "loss": 8.0992,
      "step": 13090
    },
    {
      "epoch": 4.052277472739927,
      "grad_norm": 0.3496878445148468,
      "learning_rate": 9.489480198019803e-06,
      "loss": 8.0918,
      "step": 13100
    },
    {
      "epoch": 4.055370814322171,
      "grad_norm": 0.33871644735336304,
      "learning_rate": 9.458539603960396e-06,
      "loss": 8.0958,
      "step": 13110
    },
    {
      "epoch": 4.0584641559044154,
      "grad_norm": 0.4557727873325348,
      "learning_rate": 9.42759900990099e-06,
      "loss": 8.0984,
      "step": 13120
    },
    {
      "epoch": 4.06155749748666,
      "grad_norm": 0.37860432267189026,
      "learning_rate": 9.396658415841585e-06,
      "loss": 8.1064,
      "step": 13130
    },
    {
      "epoch": 4.0646508390689045,
      "grad_norm": 0.4643920361995697,
      "learning_rate": 9.365717821782179e-06,
      "loss": 8.1105,
      "step": 13140
    },
    {
      "epoch": 4.0677441806511485,
      "grad_norm": 0.33611994981765747,
      "learning_rate": 9.334777227722773e-06,
      "loss": 8.1012,
      "step": 13150
    },
    {
      "epoch": 4.070837522233393,
      "grad_norm": 0.5409740209579468,
      "learning_rate": 9.303836633663367e-06,
      "loss": 8.1074,
      "step": 13160
    },
    {
      "epoch": 4.073930863815637,
      "grad_norm": 0.4531051814556122,
      "learning_rate": 9.27289603960396e-06,
      "loss": 8.1074,
      "step": 13170
    },
    {
      "epoch": 4.077024205397881,
      "grad_norm": 0.45378002524375916,
      "learning_rate": 9.241955445544556e-06,
      "loss": 8.1065,
      "step": 13180
    },
    {
      "epoch": 4.080117546980126,
      "grad_norm": 0.5324185490608215,
      "learning_rate": 9.21101485148515e-06,
      "loss": 8.1061,
      "step": 13190
    },
    {
      "epoch": 4.08321088856237,
      "grad_norm": 0.36936306953430176,
      "learning_rate": 9.180074257425743e-06,
      "loss": 8.1132,
      "step": 13200
    },
    {
      "epoch": 4.086304230144614,
      "grad_norm": 0.360364705324173,
      "learning_rate": 9.149133663366337e-06,
      "loss": 8.0958,
      "step": 13210
    },
    {
      "epoch": 4.089397571726858,
      "grad_norm": 0.35164687037467957,
      "learning_rate": 9.11819306930693e-06,
      "loss": 8.1081,
      "step": 13220
    },
    {
      "epoch": 4.092490913309102,
      "grad_norm": 0.33125174045562744,
      "learning_rate": 9.087252475247526e-06,
      "loss": 8.1109,
      "step": 13230
    },
    {
      "epoch": 4.095584254891347,
      "grad_norm": 0.44331666827201843,
      "learning_rate": 9.05631188118812e-06,
      "loss": 8.1055,
      "step": 13240
    },
    {
      "epoch": 4.098677596473591,
      "grad_norm": 0.29439619183540344,
      "learning_rate": 9.025371287128713e-06,
      "loss": 8.099,
      "step": 13250
    },
    {
      "epoch": 4.101770938055835,
      "grad_norm": 0.21846531331539154,
      "learning_rate": 8.994430693069307e-06,
      "loss": 8.1074,
      "step": 13260
    },
    {
      "epoch": 4.104864279638079,
      "grad_norm": 0.389622300863266,
      "learning_rate": 8.9634900990099e-06,
      "loss": 8.1108,
      "step": 13270
    },
    {
      "epoch": 4.107957621220323,
      "grad_norm": 0.40069881081581116,
      "learning_rate": 8.932549504950494e-06,
      "loss": 8.1207,
      "step": 13280
    },
    {
      "epoch": 4.111050962802567,
      "grad_norm": 0.4023802876472473,
      "learning_rate": 8.90160891089109e-06,
      "loss": 8.0808,
      "step": 13290
    },
    {
      "epoch": 4.114144304384812,
      "grad_norm": 0.4024971127510071,
      "learning_rate": 8.870668316831683e-06,
      "loss": 8.0854,
      "step": 13300
    },
    {
      "epoch": 4.117237645967056,
      "grad_norm": 0.32015496492385864,
      "learning_rate": 8.839727722772277e-06,
      "loss": 8.0974,
      "step": 13310
    },
    {
      "epoch": 4.1203309875493,
      "grad_norm": 0.44878289103507996,
      "learning_rate": 8.80878712871287e-06,
      "loss": 8.1009,
      "step": 13320
    },
    {
      "epoch": 4.123424329131544,
      "grad_norm": 0.35049518942832947,
      "learning_rate": 8.777846534653464e-06,
      "loss": 8.0961,
      "step": 13330
    },
    {
      "epoch": 4.126517670713788,
      "grad_norm": 0.3749077618122101,
      "learning_rate": 8.74690594059406e-06,
      "loss": 8.1123,
      "step": 13340
    },
    {
      "epoch": 4.129611012296033,
      "grad_norm": 0.3942984938621521,
      "learning_rate": 8.715965346534653e-06,
      "loss": 8.1283,
      "step": 13350
    },
    {
      "epoch": 4.132704353878277,
      "grad_norm": 0.367841511964798,
      "learning_rate": 8.685024752475247e-06,
      "loss": 8.1153,
      "step": 13360
    },
    {
      "epoch": 4.135797695460521,
      "grad_norm": 0.31665271520614624,
      "learning_rate": 8.654084158415841e-06,
      "loss": 8.1102,
      "step": 13370
    },
    {
      "epoch": 4.138891037042765,
      "grad_norm": 0.272122859954834,
      "learning_rate": 8.623143564356435e-06,
      "loss": 8.0998,
      "step": 13380
    },
    {
      "epoch": 4.1419843786250095,
      "grad_norm": 0.2882140576839447,
      "learning_rate": 8.592202970297032e-06,
      "loss": 8.0922,
      "step": 13390
    },
    {
      "epoch": 4.1450777202072535,
      "grad_norm": 0.31556177139282227,
      "learning_rate": 8.561262376237625e-06,
      "loss": 8.0882,
      "step": 13400
    },
    {
      "epoch": 4.1481710617894985,
      "grad_norm": 0.2653905749320984,
      "learning_rate": 8.530321782178219e-06,
      "loss": 8.1026,
      "step": 13410
    },
    {
      "epoch": 4.1512644033717425,
      "grad_norm": 0.4119306802749634,
      "learning_rate": 8.499381188118813e-06,
      "loss": 8.0965,
      "step": 13420
    },
    {
      "epoch": 4.154357744953987,
      "grad_norm": 0.2562706768512726,
      "learning_rate": 8.468440594059406e-06,
      "loss": 8.1008,
      "step": 13430
    },
    {
      "epoch": 4.157451086536231,
      "grad_norm": 0.4057457745075226,
      "learning_rate": 8.437500000000002e-06,
      "loss": 8.1078,
      "step": 13440
    },
    {
      "epoch": 4.160544428118475,
      "grad_norm": 0.43426284193992615,
      "learning_rate": 8.406559405940595e-06,
      "loss": 8.0828,
      "step": 13450
    },
    {
      "epoch": 4.163637769700719,
      "grad_norm": 0.3490391671657562,
      "learning_rate": 8.37561881188119e-06,
      "loss": 8.1167,
      "step": 13460
    },
    {
      "epoch": 4.166731111282964,
      "grad_norm": 0.4020681083202362,
      "learning_rate": 8.344678217821783e-06,
      "loss": 8.1055,
      "step": 13470
    },
    {
      "epoch": 4.169824452865208,
      "grad_norm": 0.4527534246444702,
      "learning_rate": 8.313737623762377e-06,
      "loss": 8.1077,
      "step": 13480
    },
    {
      "epoch": 4.172917794447452,
      "grad_norm": 0.36906033754348755,
      "learning_rate": 8.28279702970297e-06,
      "loss": 8.104,
      "step": 13490
    },
    {
      "epoch": 4.176011136029696,
      "grad_norm": 0.29580414295196533,
      "learning_rate": 8.251856435643566e-06,
      "loss": 8.11,
      "step": 13500
    },
    {
      "epoch": 4.17910447761194,
      "grad_norm": 0.3900734484195709,
      "learning_rate": 8.22091584158416e-06,
      "loss": 8.1033,
      "step": 13510
    },
    {
      "epoch": 4.182197819194185,
      "grad_norm": 0.5503886938095093,
      "learning_rate": 8.189975247524753e-06,
      "loss": 8.1061,
      "step": 13520
    },
    {
      "epoch": 4.185291160776429,
      "grad_norm": 0.2699277400970459,
      "learning_rate": 8.159034653465347e-06,
      "loss": 8.0837,
      "step": 13530
    },
    {
      "epoch": 4.188384502358673,
      "grad_norm": 0.3832526206970215,
      "learning_rate": 8.12809405940594e-06,
      "loss": 8.084,
      "step": 13540
    },
    {
      "epoch": 4.191477843940917,
      "grad_norm": 0.3395121991634369,
      "learning_rate": 8.097153465346536e-06,
      "loss": 8.108,
      "step": 13550
    },
    {
      "epoch": 4.194571185523161,
      "grad_norm": 0.3889562487602234,
      "learning_rate": 8.06621287128713e-06,
      "loss": 8.1085,
      "step": 13560
    },
    {
      "epoch": 4.197664527105406,
      "grad_norm": 0.45528531074523926,
      "learning_rate": 8.035272277227723e-06,
      "loss": 8.102,
      "step": 13570
    },
    {
      "epoch": 4.20075786868765,
      "grad_norm": 0.48383858799934387,
      "learning_rate": 8.004331683168317e-06,
      "loss": 8.0919,
      "step": 13580
    },
    {
      "epoch": 4.203851210269894,
      "grad_norm": 0.25893741846084595,
      "learning_rate": 7.97339108910891e-06,
      "loss": 8.1063,
      "step": 13590
    },
    {
      "epoch": 4.206944551852138,
      "grad_norm": 0.45256227254867554,
      "learning_rate": 7.942450495049506e-06,
      "loss": 8.0942,
      "step": 13600
    },
    {
      "epoch": 4.210037893434382,
      "grad_norm": 0.48350006341934204,
      "learning_rate": 7.9115099009901e-06,
      "loss": 8.0897,
      "step": 13610
    },
    {
      "epoch": 4.213131235016626,
      "grad_norm": 0.430148720741272,
      "learning_rate": 7.880569306930693e-06,
      "loss": 8.1004,
      "step": 13620
    },
    {
      "epoch": 4.216224576598871,
      "grad_norm": 0.41916242241859436,
      "learning_rate": 7.849628712871287e-06,
      "loss": 8.1124,
      "step": 13630
    },
    {
      "epoch": 4.219317918181115,
      "grad_norm": 0.3272339105606079,
      "learning_rate": 7.81868811881188e-06,
      "loss": 8.1,
      "step": 13640
    },
    {
      "epoch": 4.222411259763359,
      "grad_norm": 0.3179909884929657,
      "learning_rate": 7.787747524752476e-06,
      "loss": 8.0772,
      "step": 13650
    },
    {
      "epoch": 4.2255046013456035,
      "grad_norm": 0.4984106123447418,
      "learning_rate": 7.75680693069307e-06,
      "loss": 8.0933,
      "step": 13660
    },
    {
      "epoch": 4.2285979429278475,
      "grad_norm": 0.5079922676086426,
      "learning_rate": 7.725866336633664e-06,
      "loss": 8.1056,
      "step": 13670
    },
    {
      "epoch": 4.231691284510092,
      "grad_norm": 0.3747192621231079,
      "learning_rate": 7.694925742574257e-06,
      "loss": 8.0976,
      "step": 13680
    },
    {
      "epoch": 4.2347846260923365,
      "grad_norm": 0.39098912477493286,
      "learning_rate": 7.663985148514851e-06,
      "loss": 8.1001,
      "step": 13690
    },
    {
      "epoch": 4.237877967674581,
      "grad_norm": 0.434674471616745,
      "learning_rate": 7.633044554455446e-06,
      "loss": 8.1007,
      "step": 13700
    },
    {
      "epoch": 4.240971309256825,
      "grad_norm": 0.3793003559112549,
      "learning_rate": 7.60210396039604e-06,
      "loss": 8.0933,
      "step": 13710
    },
    {
      "epoch": 4.244064650839069,
      "grad_norm": 0.35241273045539856,
      "learning_rate": 7.571163366336634e-06,
      "loss": 8.0926,
      "step": 13720
    },
    {
      "epoch": 4.247157992421313,
      "grad_norm": 0.30514946579933167,
      "learning_rate": 7.540222772277228e-06,
      "loss": 8.0921,
      "step": 13730
    },
    {
      "epoch": 4.250251334003558,
      "grad_norm": 0.3492872714996338,
      "learning_rate": 7.509282178217822e-06,
      "loss": 8.1095,
      "step": 13740
    },
    {
      "epoch": 4.253344675585802,
      "grad_norm": 0.2812928259372711,
      "learning_rate": 7.478341584158416e-06,
      "loss": 8.1126,
      "step": 13750
    },
    {
      "epoch": 4.256438017168046,
      "grad_norm": 0.3047104775905609,
      "learning_rate": 7.44740099009901e-06,
      "loss": 8.1133,
      "step": 13760
    },
    {
      "epoch": 4.25953135875029,
      "grad_norm": 0.3279957175254822,
      "learning_rate": 7.416460396039604e-06,
      "loss": 8.1002,
      "step": 13770
    },
    {
      "epoch": 4.262624700332534,
      "grad_norm": 0.41789063811302185,
      "learning_rate": 7.385519801980198e-06,
      "loss": 8.1028,
      "step": 13780
    },
    {
      "epoch": 4.265718041914779,
      "grad_norm": 0.36249637603759766,
      "learning_rate": 7.354579207920792e-06,
      "loss": 8.1046,
      "step": 13790
    },
    {
      "epoch": 4.268811383497023,
      "grad_norm": 0.2327256202697754,
      "learning_rate": 7.323638613861386e-06,
      "loss": 8.1107,
      "step": 13800
    },
    {
      "epoch": 4.271904725079267,
      "grad_norm": 0.2927011549472809,
      "learning_rate": 7.29269801980198e-06,
      "loss": 8.0972,
      "step": 13810
    },
    {
      "epoch": 4.274998066661511,
      "grad_norm": 0.4007938802242279,
      "learning_rate": 7.261757425742574e-06,
      "loss": 8.1092,
      "step": 13820
    },
    {
      "epoch": 4.278091408243755,
      "grad_norm": 0.37015655636787415,
      "learning_rate": 7.2308168316831685e-06,
      "loss": 8.095,
      "step": 13830
    },
    {
      "epoch": 4.281184749825999,
      "grad_norm": 0.39993274211883545,
      "learning_rate": 7.199876237623762e-06,
      "loss": 8.1024,
      "step": 13840
    },
    {
      "epoch": 4.284278091408244,
      "grad_norm": 0.22308026254177094,
      "learning_rate": 7.168935643564356e-06,
      "loss": 8.0917,
      "step": 13850
    },
    {
      "epoch": 4.287371432990488,
      "grad_norm": 0.3430022895336151,
      "learning_rate": 7.1379950495049505e-06,
      "loss": 8.0894,
      "step": 13860
    },
    {
      "epoch": 4.290464774572732,
      "grad_norm": 0.2616811692714691,
      "learning_rate": 7.107054455445544e-06,
      "loss": 8.1129,
      "step": 13870
    },
    {
      "epoch": 4.293558116154976,
      "grad_norm": 4.08865213394165,
      "learning_rate": 7.076113861386139e-06,
      "loss": 8.1028,
      "step": 13880
    },
    {
      "epoch": 4.29665145773722,
      "grad_norm": 0.3867971897125244,
      "learning_rate": 7.045173267326732e-06,
      "loss": 8.1155,
      "step": 13890
    },
    {
      "epoch": 4.299744799319464,
      "grad_norm": 0.4421839416027069,
      "learning_rate": 7.014232673267328e-06,
      "loss": 8.0955,
      "step": 13900
    },
    {
      "epoch": 4.302838140901709,
      "grad_norm": 0.3562459647655487,
      "learning_rate": 6.9832920792079215e-06,
      "loss": 8.0923,
      "step": 13910
    },
    {
      "epoch": 4.305931482483953,
      "grad_norm": 0.5172577500343323,
      "learning_rate": 6.952351485148516e-06,
      "loss": 8.1202,
      "step": 13920
    },
    {
      "epoch": 4.3090248240661975,
      "grad_norm": 0.2964487671852112,
      "learning_rate": 6.92141089108911e-06,
      "loss": 8.1113,
      "step": 13930
    },
    {
      "epoch": 4.3121181656484415,
      "grad_norm": 0.48252785205841064,
      "learning_rate": 6.890470297029704e-06,
      "loss": 8.1026,
      "step": 13940
    },
    {
      "epoch": 4.315211507230686,
      "grad_norm": 0.3039126694202423,
      "learning_rate": 6.859529702970298e-06,
      "loss": 8.1017,
      "step": 13950
    },
    {
      "epoch": 4.3183048488129305,
      "grad_norm": 0.38424399495124817,
      "learning_rate": 6.828589108910892e-06,
      "loss": 8.0922,
      "step": 13960
    },
    {
      "epoch": 4.321398190395175,
      "grad_norm": 0.3259246051311493,
      "learning_rate": 6.797648514851486e-06,
      "loss": 8.0845,
      "step": 13970
    },
    {
      "epoch": 4.324491531977419,
      "grad_norm": 0.319842666387558,
      "learning_rate": 6.76670792079208e-06,
      "loss": 8.1135,
      "step": 13980
    },
    {
      "epoch": 4.327584873559663,
      "grad_norm": 0.31656137108802795,
      "learning_rate": 6.735767326732674e-06,
      "loss": 8.1136,
      "step": 13990
    },
    {
      "epoch": 4.330678215141907,
      "grad_norm": 0.43841493129730225,
      "learning_rate": 6.704826732673268e-06,
      "loss": 8.116,
      "step": 14000
    },
    {
      "epoch": 4.333771556724152,
      "grad_norm": 0.3447604775428772,
      "learning_rate": 6.673886138613862e-06,
      "loss": 8.0955,
      "step": 14010
    },
    {
      "epoch": 4.336864898306396,
      "grad_norm": 0.4411732852458954,
      "learning_rate": 6.642945544554456e-06,
      "loss": 8.1026,
      "step": 14020
    },
    {
      "epoch": 4.33995823988864,
      "grad_norm": 0.3775745928287506,
      "learning_rate": 6.61200495049505e-06,
      "loss": 8.1059,
      "step": 14030
    },
    {
      "epoch": 4.343051581470884,
      "grad_norm": 0.42800068855285645,
      "learning_rate": 6.5810643564356446e-06,
      "loss": 8.1139,
      "step": 14040
    },
    {
      "epoch": 4.346144923053128,
      "grad_norm": 0.26332762837409973,
      "learning_rate": 6.550123762376238e-06,
      "loss": 8.1143,
      "step": 14050
    },
    {
      "epoch": 4.349238264635372,
      "grad_norm": 0.4418105185031891,
      "learning_rate": 6.519183168316832e-06,
      "loss": 8.0822,
      "step": 14060
    },
    {
      "epoch": 4.352331606217617,
      "grad_norm": 0.3498707711696625,
      "learning_rate": 6.4882425742574265e-06,
      "loss": 8.1181,
      "step": 14070
    },
    {
      "epoch": 4.355424947799861,
      "grad_norm": 0.3995920419692993,
      "learning_rate": 6.45730198019802e-06,
      "loss": 8.1033,
      "step": 14080
    },
    {
      "epoch": 4.358518289382105,
      "grad_norm": 0.25043272972106934,
      "learning_rate": 6.426361386138615e-06,
      "loss": 8.1162,
      "step": 14090
    },
    {
      "epoch": 4.361611630964349,
      "grad_norm": 0.44372305274009705,
      "learning_rate": 6.395420792079208e-06,
      "loss": 8.1096,
      "step": 14100
    },
    {
      "epoch": 4.364704972546593,
      "grad_norm": 0.3079013228416443,
      "learning_rate": 6.364480198019802e-06,
      "loss": 8.1014,
      "step": 14110
    },
    {
      "epoch": 4.367798314128837,
      "grad_norm": 0.3124466836452484,
      "learning_rate": 6.333539603960397e-06,
      "loss": 8.1019,
      "step": 14120
    },
    {
      "epoch": 4.370891655711082,
      "grad_norm": 0.4900452196598053,
      "learning_rate": 6.30259900990099e-06,
      "loss": 8.1007,
      "step": 14130
    },
    {
      "epoch": 4.373984997293326,
      "grad_norm": 0.38545358180999756,
      "learning_rate": 6.271658415841585e-06,
      "loss": 8.1119,
      "step": 14140
    },
    {
      "epoch": 4.37707833887557,
      "grad_norm": 0.37000492215156555,
      "learning_rate": 6.240717821782179e-06,
      "loss": 8.1086,
      "step": 14150
    },
    {
      "epoch": 4.380171680457814,
      "grad_norm": 0.42429667711257935,
      "learning_rate": 6.209777227722772e-06,
      "loss": 8.0975,
      "step": 14160
    },
    {
      "epoch": 4.383265022040058,
      "grad_norm": 0.3165362477302551,
      "learning_rate": 6.178836633663367e-06,
      "loss": 8.1074,
      "step": 14170
    },
    {
      "epoch": 4.386358363622303,
      "grad_norm": 0.32675766944885254,
      "learning_rate": 6.1478960396039605e-06,
      "loss": 8.1049,
      "step": 14180
    },
    {
      "epoch": 4.389451705204547,
      "grad_norm": 0.5063733458518982,
      "learning_rate": 6.116955445544555e-06,
      "loss": 8.095,
      "step": 14190
    },
    {
      "epoch": 4.3925450467867915,
      "grad_norm": 0.5182270407676697,
      "learning_rate": 6.086014851485149e-06,
      "loss": 8.1012,
      "step": 14200
    },
    {
      "epoch": 4.3956383883690355,
      "grad_norm": 0.3907555639743805,
      "learning_rate": 6.0550742574257424e-06,
      "loss": 8.095,
      "step": 14210
    },
    {
      "epoch": 4.39873172995128,
      "grad_norm": 0.508646547794342,
      "learning_rate": 6.024133663366337e-06,
      "loss": 8.0989,
      "step": 14220
    },
    {
      "epoch": 4.4018250715335245,
      "grad_norm": 0.42055022716522217,
      "learning_rate": 5.993193069306931e-06,
      "loss": 8.1127,
      "step": 14230
    },
    {
      "epoch": 4.404918413115769,
      "grad_norm": 0.3775026798248291,
      "learning_rate": 5.962252475247525e-06,
      "loss": 8.1043,
      "step": 14240
    },
    {
      "epoch": 4.408011754698013,
      "grad_norm": 0.5511883497238159,
      "learning_rate": 5.931311881188119e-06,
      "loss": 8.1142,
      "step": 14250
    },
    {
      "epoch": 4.411105096280257,
      "grad_norm": 0.3768428862094879,
      "learning_rate": 5.900371287128713e-06,
      "loss": 8.111,
      "step": 14260
    },
    {
      "epoch": 4.414198437862501,
      "grad_norm": 0.4316994845867157,
      "learning_rate": 5.869430693069307e-06,
      "loss": 8.1139,
      "step": 14270
    },
    {
      "epoch": 4.417291779444745,
      "grad_norm": 0.41169580817222595,
      "learning_rate": 5.838490099009902e-06,
      "loss": 8.0997,
      "step": 14280
    },
    {
      "epoch": 4.42038512102699,
      "grad_norm": 0.36983466148376465,
      "learning_rate": 5.807549504950495e-06,
      "loss": 8.0957,
      "step": 14290
    },
    {
      "epoch": 4.423478462609234,
      "grad_norm": 0.40404367446899414,
      "learning_rate": 5.77660891089109e-06,
      "loss": 8.0989,
      "step": 14300
    },
    {
      "epoch": 4.426571804191478,
      "grad_norm": 0.36797359585762024,
      "learning_rate": 5.745668316831684e-06,
      "loss": 8.1001,
      "step": 14310
    },
    {
      "epoch": 4.429665145773722,
      "grad_norm": 0.3689989745616913,
      "learning_rate": 5.714727722772278e-06,
      "loss": 8.0882,
      "step": 14320
    },
    {
      "epoch": 4.432758487355966,
      "grad_norm": 0.43306365609169006,
      "learning_rate": 5.683787128712872e-06,
      "loss": 8.108,
      "step": 14330
    },
    {
      "epoch": 4.43585182893821,
      "grad_norm": 0.395702987909317,
      "learning_rate": 5.6528465346534655e-06,
      "loss": 8.094,
      "step": 14340
    },
    {
      "epoch": 4.438945170520455,
      "grad_norm": 0.376766562461853,
      "learning_rate": 5.62190594059406e-06,
      "loss": 8.1026,
      "step": 14350
    },
    {
      "epoch": 4.442038512102699,
      "grad_norm": 0.4830454885959625,
      "learning_rate": 5.590965346534654e-06,
      "loss": 8.0956,
      "step": 14360
    },
    {
      "epoch": 4.445131853684943,
      "grad_norm": 0.3248559832572937,
      "learning_rate": 5.560024752475248e-06,
      "loss": 8.1091,
      "step": 14370
    },
    {
      "epoch": 4.448225195267187,
      "grad_norm": 0.3358733654022217,
      "learning_rate": 5.529084158415842e-06,
      "loss": 8.1129,
      "step": 14380
    },
    {
      "epoch": 4.451318536849431,
      "grad_norm": 0.39254191517829895,
      "learning_rate": 5.498143564356436e-06,
      "loss": 8.1112,
      "step": 14390
    },
    {
      "epoch": 4.454411878431676,
      "grad_norm": 0.44720274209976196,
      "learning_rate": 5.46720297029703e-06,
      "loss": 8.1044,
      "step": 14400
    },
    {
      "epoch": 4.45750522001392,
      "grad_norm": 0.3640287220478058,
      "learning_rate": 5.436262376237624e-06,
      "loss": 8.1058,
      "step": 14410
    },
    {
      "epoch": 4.460598561596164,
      "grad_norm": 0.24021978676319122,
      "learning_rate": 5.405321782178218e-06,
      "loss": 8.1049,
      "step": 14420
    },
    {
      "epoch": 4.463691903178408,
      "grad_norm": 0.5031468272209167,
      "learning_rate": 5.374381188118812e-06,
      "loss": 8.0849,
      "step": 14430
    },
    {
      "epoch": 4.466785244760652,
      "grad_norm": 0.5306451916694641,
      "learning_rate": 5.343440594059406e-06,
      "loss": 8.1141,
      "step": 14440
    },
    {
      "epoch": 4.469878586342897,
      "grad_norm": 0.25650933384895325,
      "learning_rate": 5.3125e-06,
      "loss": 8.0961,
      "step": 14450
    },
    {
      "epoch": 4.472971927925141,
      "grad_norm": 0.3441239297389984,
      "learning_rate": 5.281559405940594e-06,
      "loss": 8.1078,
      "step": 14460
    },
    {
      "epoch": 4.4760652695073855,
      "grad_norm": 0.3289702832698822,
      "learning_rate": 5.250618811881188e-06,
      "loss": 8.1001,
      "step": 14470
    },
    {
      "epoch": 4.4791586110896295,
      "grad_norm": 0.3809906840324402,
      "learning_rate": 5.219678217821782e-06,
      "loss": 8.1001,
      "step": 14480
    },
    {
      "epoch": 4.482251952671874,
      "grad_norm": 0.490525484085083,
      "learning_rate": 5.188737623762376e-06,
      "loss": 8.1132,
      "step": 14490
    },
    {
      "epoch": 4.485345294254118,
      "grad_norm": 0.5233200788497925,
      "learning_rate": 5.1577970297029705e-06,
      "loss": 8.0967,
      "step": 14500
    },
    {
      "epoch": 4.488438635836363,
      "grad_norm": 0.5223960280418396,
      "learning_rate": 5.126856435643564e-06,
      "loss": 8.1129,
      "step": 14510
    },
    {
      "epoch": 4.491531977418607,
      "grad_norm": 0.3289199471473694,
      "learning_rate": 5.095915841584158e-06,
      "loss": 8.0859,
      "step": 14520
    },
    {
      "epoch": 4.494625319000851,
      "grad_norm": 0.40553632378578186,
      "learning_rate": 5.064975247524753e-06,
      "loss": 8.092,
      "step": 14530
    },
    {
      "epoch": 4.497718660583095,
      "grad_norm": 0.4002406597137451,
      "learning_rate": 5.034034653465347e-06,
      "loss": 8.1114,
      "step": 14540
    },
    {
      "epoch": 4.500812002165339,
      "grad_norm": 0.35580551624298096,
      "learning_rate": 5.003094059405941e-06,
      "loss": 8.0921,
      "step": 14550
    },
    {
      "epoch": 4.503905343747583,
      "grad_norm": 0.29819968342781067,
      "learning_rate": 4.972153465346535e-06,
      "loss": 8.1049,
      "step": 14560
    },
    {
      "epoch": 4.506998685329828,
      "grad_norm": 0.3006981313228607,
      "learning_rate": 4.941212871287129e-06,
      "loss": 8.0919,
      "step": 14570
    },
    {
      "epoch": 4.510092026912072,
      "grad_norm": 0.35150837898254395,
      "learning_rate": 4.9102722772277235e-06,
      "loss": 8.104,
      "step": 14580
    },
    {
      "epoch": 4.513185368494316,
      "grad_norm": 0.4638936221599579,
      "learning_rate": 4.879331683168317e-06,
      "loss": 8.107,
      "step": 14590
    },
    {
      "epoch": 4.51627871007656,
      "grad_norm": 0.41602230072021484,
      "learning_rate": 4.848391089108911e-06,
      "loss": 8.101,
      "step": 14600
    },
    {
      "epoch": 4.519372051658804,
      "grad_norm": 0.4434848129749298,
      "learning_rate": 4.817450495049505e-06,
      "loss": 8.0932,
      "step": 14610
    },
    {
      "epoch": 4.522465393241049,
      "grad_norm": 0.3882285952568054,
      "learning_rate": 4.786509900990099e-06,
      "loss": 8.1002,
      "step": 14620
    },
    {
      "epoch": 4.525558734823293,
      "grad_norm": 0.3686213493347168,
      "learning_rate": 4.755569306930694e-06,
      "loss": 8.1058,
      "step": 14630
    },
    {
      "epoch": 4.528652076405537,
      "grad_norm": 0.3190932869911194,
      "learning_rate": 4.727722772277228e-06,
      "loss": 8.0981,
      "step": 14640
    },
    {
      "epoch": 4.531745417987781,
      "grad_norm": 0.31913813948631287,
      "learning_rate": 4.696782178217822e-06,
      "loss": 8.1074,
      "step": 14650
    },
    {
      "epoch": 4.534838759570025,
      "grad_norm": 0.3473775088787079,
      "learning_rate": 4.665841584158416e-06,
      "loss": 8.102,
      "step": 14660
    },
    {
      "epoch": 4.53793210115227,
      "grad_norm": 0.27942389249801636,
      "learning_rate": 4.63490099009901e-06,
      "loss": 8.1067,
      "step": 14670
    },
    {
      "epoch": 4.541025442734514,
      "grad_norm": 0.442543089389801,
      "learning_rate": 4.603960396039604e-06,
      "loss": 8.1109,
      "step": 14680
    },
    {
      "epoch": 4.544118784316758,
      "grad_norm": 0.4095183312892914,
      "learning_rate": 4.573019801980198e-06,
      "loss": 8.1124,
      "step": 14690
    },
    {
      "epoch": 4.547212125899002,
      "grad_norm": 0.33327171206474304,
      "learning_rate": 4.542079207920792e-06,
      "loss": 8.1023,
      "step": 14700
    },
    {
      "epoch": 4.550305467481246,
      "grad_norm": 0.34773069620132446,
      "learning_rate": 4.511138613861386e-06,
      "loss": 8.1066,
      "step": 14710
    },
    {
      "epoch": 4.5533988090634905,
      "grad_norm": 0.4254034459590912,
      "learning_rate": 4.4801980198019806e-06,
      "loss": 8.1036,
      "step": 14720
    },
    {
      "epoch": 4.556492150645735,
      "grad_norm": 0.3119504749774933,
      "learning_rate": 4.449257425742574e-06,
      "loss": 8.1006,
      "step": 14730
    },
    {
      "epoch": 4.5595854922279795,
      "grad_norm": 0.33483684062957764,
      "learning_rate": 4.418316831683168e-06,
      "loss": 8.1046,
      "step": 14740
    },
    {
      "epoch": 4.5626788338102235,
      "grad_norm": 0.4237045347690582,
      "learning_rate": 4.3873762376237625e-06,
      "loss": 8.0895,
      "step": 14750
    },
    {
      "epoch": 4.565772175392468,
      "grad_norm": 0.3082759976387024,
      "learning_rate": 4.356435643564356e-06,
      "loss": 8.0887,
      "step": 14760
    },
    {
      "epoch": 4.568865516974712,
      "grad_norm": 0.39387044310569763,
      "learning_rate": 4.325495049504951e-06,
      "loss": 8.1067,
      "step": 14770
    },
    {
      "epoch": 4.571958858556956,
      "grad_norm": 0.48199987411499023,
      "learning_rate": 4.294554455445545e-06,
      "loss": 8.109,
      "step": 14780
    },
    {
      "epoch": 4.575052200139201,
      "grad_norm": 0.3959369361400604,
      "learning_rate": 4.263613861386139e-06,
      "loss": 8.103,
      "step": 14790
    },
    {
      "epoch": 4.578145541721445,
      "grad_norm": 0.3029645085334778,
      "learning_rate": 4.2326732673267335e-06,
      "loss": 8.1028,
      "step": 14800
    },
    {
      "epoch": 4.581238883303689,
      "grad_norm": 0.37423428893089294,
      "learning_rate": 4.201732673267327e-06,
      "loss": 8.1024,
      "step": 14810
    },
    {
      "epoch": 4.584332224885933,
      "grad_norm": 0.41209515929222107,
      "learning_rate": 4.170792079207921e-06,
      "loss": 8.1125,
      "step": 14820
    },
    {
      "epoch": 4.587425566468177,
      "grad_norm": 0.3616572618484497,
      "learning_rate": 4.139851485148515e-06,
      "loss": 8.1107,
      "step": 14830
    },
    {
      "epoch": 4.590518908050422,
      "grad_norm": 0.45969632267951965,
      "learning_rate": 4.108910891089109e-06,
      "loss": 8.1146,
      "step": 14840
    },
    {
      "epoch": 4.593612249632666,
      "grad_norm": 0.2642076909542084,
      "learning_rate": 4.077970297029704e-06,
      "loss": 8.0995,
      "step": 14850
    },
    {
      "epoch": 4.59670559121491,
      "grad_norm": 0.3880799412727356,
      "learning_rate": 4.047029702970297e-06,
      "loss": 8.1045,
      "step": 14860
    },
    {
      "epoch": 4.599798932797154,
      "grad_norm": 0.3535171151161194,
      "learning_rate": 4.016089108910891e-06,
      "loss": 8.1047,
      "step": 14870
    },
    {
      "epoch": 4.602892274379398,
      "grad_norm": 0.3712748885154724,
      "learning_rate": 3.9851485148514856e-06,
      "loss": 8.1114,
      "step": 14880
    },
    {
      "epoch": 4.605985615961643,
      "grad_norm": 0.45697107911109924,
      "learning_rate": 3.954207920792079e-06,
      "loss": 8.1016,
      "step": 14890
    },
    {
      "epoch": 4.609078957543887,
      "grad_norm": 0.34743160009384155,
      "learning_rate": 3.923267326732674e-06,
      "loss": 8.1057,
      "step": 14900
    },
    {
      "epoch": 4.612172299126131,
      "grad_norm": 0.5073808431625366,
      "learning_rate": 3.8923267326732675e-06,
      "loss": 8.0949,
      "step": 14910
    },
    {
      "epoch": 4.615265640708375,
      "grad_norm": 0.46063700318336487,
      "learning_rate": 3.861386138613861e-06,
      "loss": 8.1101,
      "step": 14920
    },
    {
      "epoch": 4.618358982290619,
      "grad_norm": 0.37062230706214905,
      "learning_rate": 3.830445544554456e-06,
      "loss": 8.1035,
      "step": 14930
    },
    {
      "epoch": 4.621452323872863,
      "grad_norm": 0.30693522095680237,
      "learning_rate": 3.7995049504950494e-06,
      "loss": 8.1095,
      "step": 14940
    },
    {
      "epoch": 4.624545665455108,
      "grad_norm": 0.36503949761390686,
      "learning_rate": 3.7685643564356435e-06,
      "loss": 8.0967,
      "step": 14950
    },
    {
      "epoch": 4.627639007037352,
      "grad_norm": 0.40320083498954773,
      "learning_rate": 3.7376237623762377e-06,
      "loss": 8.0925,
      "step": 14960
    },
    {
      "epoch": 4.630732348619596,
      "grad_norm": 0.3517049252986908,
      "learning_rate": 3.7066831683168318e-06,
      "loss": 8.0988,
      "step": 14970
    },
    {
      "epoch": 4.63382569020184,
      "grad_norm": 0.37394505739212036,
      "learning_rate": 3.675742574257426e-06,
      "loss": 8.1047,
      "step": 14980
    },
    {
      "epoch": 4.6369190317840845,
      "grad_norm": 0.5056451559066772,
      "learning_rate": 3.6448019801980196e-06,
      "loss": 8.1038,
      "step": 14990
    },
    {
      "epoch": 4.6400123733663285,
      "grad_norm": 0.5976082682609558,
      "learning_rate": 3.6138613861386137e-06,
      "loss": 8.1027,
      "step": 15000
    },
    {
      "epoch": 4.6431057149485735,
      "grad_norm": 0.42015063762664795,
      "learning_rate": 3.582920792079208e-06,
      "loss": 8.0944,
      "step": 15010
    },
    {
      "epoch": 4.6461990565308176,
      "grad_norm": 0.5027850866317749,
      "learning_rate": 3.551980198019802e-06,
      "loss": 8.0978,
      "step": 15020
    },
    {
      "epoch": 4.649292398113062,
      "grad_norm": 0.39751997590065,
      "learning_rate": 3.521039603960396e-06,
      "loss": 8.1138,
      "step": 15030
    },
    {
      "epoch": 4.652385739695306,
      "grad_norm": 0.28443509340286255,
      "learning_rate": 3.4900990099009906e-06,
      "loss": 8.1044,
      "step": 15040
    },
    {
      "epoch": 4.65547908127755,
      "grad_norm": 0.3344229757785797,
      "learning_rate": 3.4591584158415847e-06,
      "loss": 8.1055,
      "step": 15050
    },
    {
      "epoch": 4.658572422859795,
      "grad_norm": 0.39495596289634705,
      "learning_rate": 3.428217821782179e-06,
      "loss": 8.1063,
      "step": 15060
    },
    {
      "epoch": 4.661665764442039,
      "grad_norm": 0.2223936915397644,
      "learning_rate": 3.3972772277227725e-06,
      "loss": 8.0979,
      "step": 15070
    },
    {
      "epoch": 4.664759106024283,
      "grad_norm": 0.3078192174434662,
      "learning_rate": 3.3663366336633666e-06,
      "loss": 8.1092,
      "step": 15080
    },
    {
      "epoch": 4.667852447606527,
      "grad_norm": 0.507084846496582,
      "learning_rate": 3.3353960396039607e-06,
      "loss": 8.0989,
      "step": 15090
    },
    {
      "epoch": 4.670945789188771,
      "grad_norm": 0.46904057264328003,
      "learning_rate": 3.304455445544555e-06,
      "loss": 8.1136,
      "step": 15100
    },
    {
      "epoch": 4.674039130771016,
      "grad_norm": 0.45048579573631287,
      "learning_rate": 3.273514851485149e-06,
      "loss": 8.1042,
      "step": 15110
    },
    {
      "epoch": 4.67713247235326,
      "grad_norm": 0.47164881229400635,
      "learning_rate": 3.2425742574257427e-06,
      "loss": 8.1012,
      "step": 15120
    },
    {
      "epoch": 4.680225813935504,
      "grad_norm": 0.41999301314353943,
      "learning_rate": 3.211633663366337e-06,
      "loss": 8.1086,
      "step": 15130
    },
    {
      "epoch": 4.683319155517748,
      "grad_norm": 0.3988717198371887,
      "learning_rate": 3.180693069306931e-06,
      "loss": 8.1043,
      "step": 15140
    },
    {
      "epoch": 4.686412497099992,
      "grad_norm": 0.3425782024860382,
      "learning_rate": 3.149752475247525e-06,
      "loss": 8.1044,
      "step": 15150
    },
    {
      "epoch": 4.689505838682236,
      "grad_norm": 0.3227918744087219,
      "learning_rate": 3.118811881188119e-06,
      "loss": 8.112,
      "step": 15160
    },
    {
      "epoch": 4.692599180264481,
      "grad_norm": 0.4368152320384979,
      "learning_rate": 3.087871287128713e-06,
      "loss": 8.1012,
      "step": 15170
    },
    {
      "epoch": 4.695692521846725,
      "grad_norm": 0.39394834637641907,
      "learning_rate": 3.056930693069307e-06,
      "loss": 8.1027,
      "step": 15180
    },
    {
      "epoch": 4.698785863428969,
      "grad_norm": 0.26843369007110596,
      "learning_rate": 3.025990099009901e-06,
      "loss": 8.0955,
      "step": 15190
    },
    {
      "epoch": 4.701879205011213,
      "grad_norm": 0.36120814085006714,
      "learning_rate": 2.995049504950495e-06,
      "loss": 8.1072,
      "step": 15200
    },
    {
      "epoch": 4.704972546593457,
      "grad_norm": 0.43549367785453796,
      "learning_rate": 2.9641089108910893e-06,
      "loss": 8.112,
      "step": 15210
    },
    {
      "epoch": 4.708065888175701,
      "grad_norm": 0.2820917069911957,
      "learning_rate": 2.933168316831683e-06,
      "loss": 8.1002,
      "step": 15220
    },
    {
      "epoch": 4.711159229757946,
      "grad_norm": 0.37568527460098267,
      "learning_rate": 2.9022277227722775e-06,
      "loss": 8.0979,
      "step": 15230
    },
    {
      "epoch": 4.71425257134019,
      "grad_norm": 0.511873185634613,
      "learning_rate": 2.8712871287128717e-06,
      "loss": 8.0958,
      "step": 15240
    },
    {
      "epoch": 4.717345912922434,
      "grad_norm": 0.39788177609443665,
      "learning_rate": 2.8403465346534658e-06,
      "loss": 8.1,
      "step": 15250
    },
    {
      "epoch": 4.7204392545046785,
      "grad_norm": 0.4409356713294983,
      "learning_rate": 2.8094059405940595e-06,
      "loss": 8.0961,
      "step": 15260
    },
    {
      "epoch": 4.7235325960869226,
      "grad_norm": 0.44871512055397034,
      "learning_rate": 2.7784653465346536e-06,
      "loss": 8.1034,
      "step": 15270
    },
    {
      "epoch": 4.7266259376691675,
      "grad_norm": 0.22793926298618317,
      "learning_rate": 2.7475247524752477e-06,
      "loss": 8.104,
      "step": 15280
    },
    {
      "epoch": 4.729719279251412,
      "grad_norm": 0.31779271364212036,
      "learning_rate": 2.716584158415842e-06,
      "loss": 8.0945,
      "step": 15290
    },
    {
      "epoch": 4.732812620833656,
      "grad_norm": 0.39593371748924255,
      "learning_rate": 2.685643564356436e-06,
      "loss": 8.105,
      "step": 15300
    },
    {
      "epoch": 4.7359059624159,
      "grad_norm": 0.3095453083515167,
      "learning_rate": 2.6547029702970296e-06,
      "loss": 8.1146,
      "step": 15310
    },
    {
      "epoch": 4.738999303998144,
      "grad_norm": 0.30952420830726624,
      "learning_rate": 2.6237623762376237e-06,
      "loss": 8.1044,
      "step": 15320
    },
    {
      "epoch": 4.742092645580389,
      "grad_norm": 0.2949162423610687,
      "learning_rate": 2.592821782178218e-06,
      "loss": 8.0925,
      "step": 15330
    },
    {
      "epoch": 4.745185987162633,
      "grad_norm": 0.5842874050140381,
      "learning_rate": 2.561881188118812e-06,
      "loss": 8.1048,
      "step": 15340
    },
    {
      "epoch": 4.748279328744877,
      "grad_norm": 0.29191020131111145,
      "learning_rate": 2.530940594059406e-06,
      "loss": 8.096,
      "step": 15350
    },
    {
      "epoch": 4.751372670327121,
      "grad_norm": 0.45909035205841064,
      "learning_rate": 2.5e-06,
      "loss": 8.1168,
      "step": 15360
    },
    {
      "epoch": 4.754466011909365,
      "grad_norm": 0.36335286498069763,
      "learning_rate": 2.4690594059405943e-06,
      "loss": 8.1068,
      "step": 15370
    },
    {
      "epoch": 4.757559353491609,
      "grad_norm": 0.3498641550540924,
      "learning_rate": 2.4381188118811884e-06,
      "loss": 8.0978,
      "step": 15380
    },
    {
      "epoch": 4.760652695073854,
      "grad_norm": 0.3090795576572418,
      "learning_rate": 2.4071782178217826e-06,
      "loss": 8.1038,
      "step": 15390
    },
    {
      "epoch": 4.763746036656098,
      "grad_norm": 0.5186384320259094,
      "learning_rate": 2.3762376237623762e-06,
      "loss": 8.0994,
      "step": 15400
    },
    {
      "epoch": 4.766839378238342,
      "grad_norm": 0.3588573932647705,
      "learning_rate": 2.3452970297029704e-06,
      "loss": 8.0974,
      "step": 15410
    },
    {
      "epoch": 4.769932719820586,
      "grad_norm": 0.3856377601623535,
      "learning_rate": 2.3143564356435645e-06,
      "loss": 8.113,
      "step": 15420
    },
    {
      "epoch": 4.77302606140283,
      "grad_norm": 0.4533170759677887,
      "learning_rate": 2.2834158415841586e-06,
      "loss": 8.11,
      "step": 15430
    },
    {
      "epoch": 4.776119402985074,
      "grad_norm": 0.44566014409065247,
      "learning_rate": 2.2524752475247523e-06,
      "loss": 8.0859,
      "step": 15440
    },
    {
      "epoch": 4.779212744567319,
      "grad_norm": 0.2914019227027893,
      "learning_rate": 2.2215346534653464e-06,
      "loss": 8.081,
      "step": 15450
    },
    {
      "epoch": 4.782306086149563,
      "grad_norm": 0.4128001630306244,
      "learning_rate": 2.1905940594059405e-06,
      "loss": 8.115,
      "step": 15460
    },
    {
      "epoch": 4.785399427731807,
      "grad_norm": 0.3612712621688843,
      "learning_rate": 2.1596534653465346e-06,
      "loss": 8.0959,
      "step": 15470
    },
    {
      "epoch": 4.788492769314051,
      "grad_norm": 0.24110868573188782,
      "learning_rate": 2.128712871287129e-06,
      "loss": 8.0934,
      "step": 15480
    },
    {
      "epoch": 4.791586110896295,
      "grad_norm": 0.4558853209018707,
      "learning_rate": 2.097772277227723e-06,
      "loss": 8.1143,
      "step": 15490
    },
    {
      "epoch": 4.79467945247854,
      "grad_norm": 0.46932411193847656,
      "learning_rate": 2.066831683168317e-06,
      "loss": 8.1023,
      "step": 15500
    },
    {
      "epoch": 4.797772794060784,
      "grad_norm": 0.3829985558986664,
      "learning_rate": 2.035891089108911e-06,
      "loss": 8.0938,
      "step": 15510
    },
    {
      "epoch": 4.800866135643028,
      "grad_norm": 0.37854209542274475,
      "learning_rate": 2.0049504950495052e-06,
      "loss": 8.0972,
      "step": 15520
    },
    {
      "epoch": 4.8039594772252725,
      "grad_norm": 0.3430320918560028,
      "learning_rate": 1.974009900990099e-06,
      "loss": 8.1245,
      "step": 15530
    },
    {
      "epoch": 4.807052818807517,
      "grad_norm": 0.5030140280723572,
      "learning_rate": 1.943069306930693e-06,
      "loss": 8.1166,
      "step": 15540
    },
    {
      "epoch": 4.8101461603897615,
      "grad_norm": 0.28868839144706726,
      "learning_rate": 1.912128712871287e-06,
      "loss": 8.1121,
      "step": 15550
    },
    {
      "epoch": 4.813239501972006,
      "grad_norm": 0.2178889960050583,
      "learning_rate": 1.8811881188118813e-06,
      "loss": 8.1155,
      "step": 15560
    },
    {
      "epoch": 4.81633284355425,
      "grad_norm": 0.5226072072982788,
      "learning_rate": 1.8502475247524752e-06,
      "loss": 8.1049,
      "step": 15570
    },
    {
      "epoch": 4.819426185136494,
      "grad_norm": 0.3566581606864929,
      "learning_rate": 1.8193069306930693e-06,
      "loss": 8.1031,
      "step": 15580
    },
    {
      "epoch": 4.822519526718738,
      "grad_norm": 0.37152189016342163,
      "learning_rate": 1.7883663366336634e-06,
      "loss": 8.1061,
      "step": 15590
    },
    {
      "epoch": 4.825612868300982,
      "grad_norm": 0.3120175302028656,
      "learning_rate": 1.7574257425742577e-06,
      "loss": 8.1107,
      "step": 15600
    },
    {
      "epoch": 4.828706209883227,
      "grad_norm": 0.289459228515625,
      "learning_rate": 1.7264851485148516e-06,
      "loss": 8.0882,
      "step": 15610
    },
    {
      "epoch": 4.831799551465471,
      "grad_norm": 0.2865370810031891,
      "learning_rate": 1.6955445544554458e-06,
      "loss": 8.1066,
      "step": 15620
    },
    {
      "epoch": 4.834892893047715,
      "grad_norm": 0.39639410376548767,
      "learning_rate": 1.6646039603960399e-06,
      "loss": 8.1072,
      "step": 15630
    },
    {
      "epoch": 4.837986234629959,
      "grad_norm": 0.2856157720088959,
      "learning_rate": 1.6336633663366338e-06,
      "loss": 8.1117,
      "step": 15640
    },
    {
      "epoch": 4.841079576212203,
      "grad_norm": 0.2989204227924347,
      "learning_rate": 1.6027227722772279e-06,
      "loss": 8.107,
      "step": 15650
    },
    {
      "epoch": 4.844172917794447,
      "grad_norm": 0.2844051718711853,
      "learning_rate": 1.5717821782178218e-06,
      "loss": 8.0949,
      "step": 15660
    },
    {
      "epoch": 4.847266259376692,
      "grad_norm": 0.3701190948486328,
      "learning_rate": 1.540841584158416e-06,
      "loss": 8.1067,
      "step": 15670
    },
    {
      "epoch": 4.850359600958936,
      "grad_norm": 0.35381442308425903,
      "learning_rate": 1.50990099009901e-06,
      "loss": 8.1003,
      "step": 15680
    },
    {
      "epoch": 4.85345294254118,
      "grad_norm": 0.37805116176605225,
      "learning_rate": 1.478960396039604e-06,
      "loss": 8.0904,
      "step": 15690
    },
    {
      "epoch": 4.856546284123424,
      "grad_norm": 0.24650868773460388,
      "learning_rate": 1.4480198019801983e-06,
      "loss": 8.101,
      "step": 15700
    },
    {
      "epoch": 4.859639625705668,
      "grad_norm": 0.28535643219947815,
      "learning_rate": 1.4170792079207922e-06,
      "loss": 8.1086,
      "step": 15710
    },
    {
      "epoch": 4.862732967287913,
      "grad_norm": 0.3769356608390808,
      "learning_rate": 1.3861386138613863e-06,
      "loss": 8.1026,
      "step": 15720
    },
    {
      "epoch": 4.865826308870157,
      "grad_norm": 0.3968992531299591,
      "learning_rate": 1.3551980198019802e-06,
      "loss": 8.0967,
      "step": 15730
    },
    {
      "epoch": 4.868919650452401,
      "grad_norm": 0.4281098544597626,
      "learning_rate": 1.3242574257425743e-06,
      "loss": 8.1,
      "step": 15740
    },
    {
      "epoch": 4.872012992034645,
      "grad_norm": 0.38992205262184143,
      "learning_rate": 1.2933168316831682e-06,
      "loss": 8.0992,
      "step": 15750
    },
    {
      "epoch": 4.875106333616889,
      "grad_norm": 0.5514137744903564,
      "learning_rate": 1.2623762376237625e-06,
      "loss": 8.1049,
      "step": 15760
    },
    {
      "epoch": 4.878199675199134,
      "grad_norm": 0.385267049074173,
      "learning_rate": 1.2314356435643564e-06,
      "loss": 8.0994,
      "step": 15770
    },
    {
      "epoch": 4.881293016781378,
      "grad_norm": 0.36930593848228455,
      "learning_rate": 1.2004950495049506e-06,
      "loss": 8.113,
      "step": 15780
    },
    {
      "epoch": 4.8843863583636224,
      "grad_norm": 0.2964650094509125,
      "learning_rate": 1.1695544554455447e-06,
      "loss": 8.0743,
      "step": 15790
    },
    {
      "epoch": 4.8874796999458665,
      "grad_norm": 0.3893769085407257,
      "learning_rate": 1.1386138613861386e-06,
      "loss": 8.0938,
      "step": 15800
    },
    {
      "epoch": 4.890573041528111,
      "grad_norm": 0.4511466920375824,
      "learning_rate": 1.1076732673267327e-06,
      "loss": 8.1158,
      "step": 15810
    },
    {
      "epoch": 4.893666383110355,
      "grad_norm": 0.30164116621017456,
      "learning_rate": 1.0767326732673266e-06,
      "loss": 8.099,
      "step": 15820
    },
    {
      "epoch": 4.896759724692599,
      "grad_norm": 0.5734279155731201,
      "learning_rate": 1.045792079207921e-06,
      "loss": 8.1055,
      "step": 15830
    },
    {
      "epoch": 4.899853066274844,
      "grad_norm": 0.28530335426330566,
      "learning_rate": 1.0148514851485148e-06,
      "loss": 8.0922,
      "step": 15840
    },
    {
      "epoch": 4.902946407857088,
      "grad_norm": 0.3771764934062958,
      "learning_rate": 9.83910891089109e-07,
      "loss": 8.1215,
      "step": 15850
    },
    {
      "epoch": 4.906039749439332,
      "grad_norm": 0.3349776864051819,
      "learning_rate": 9.52970297029703e-07,
      "loss": 8.1064,
      "step": 15860
    },
    {
      "epoch": 4.909133091021576,
      "grad_norm": 0.34682318568229675,
      "learning_rate": 9.22029702970297e-07,
      "loss": 8.105,
      "step": 15870
    },
    {
      "epoch": 4.91222643260382,
      "grad_norm": 0.4844025671482086,
      "learning_rate": 8.910891089108911e-07,
      "loss": 8.1086,
      "step": 15880
    },
    {
      "epoch": 4.915319774186065,
      "grad_norm": 0.5155230760574341,
      "learning_rate": 8.601485148514852e-07,
      "loss": 8.1121,
      "step": 15890
    },
    {
      "epoch": 4.918413115768309,
      "grad_norm": 0.36933633685112,
      "learning_rate": 8.292079207920793e-07,
      "loss": 8.0887,
      "step": 15900
    },
    {
      "epoch": 4.921506457350553,
      "grad_norm": 0.5683713555335999,
      "learning_rate": 7.982673267326733e-07,
      "loss": 8.1104,
      "step": 15910
    },
    {
      "epoch": 4.924599798932797,
      "grad_norm": 0.3693365454673767,
      "learning_rate": 7.673267326732673e-07,
      "loss": 8.1097,
      "step": 15920
    },
    {
      "epoch": 4.927693140515041,
      "grad_norm": 0.5289320349693298,
      "learning_rate": 7.363861386138614e-07,
      "loss": 8.0669,
      "step": 15930
    },
    {
      "epoch": 4.930786482097286,
      "grad_norm": 0.4799935519695282,
      "learning_rate": 7.054455445544555e-07,
      "loss": 8.1069,
      "step": 15940
    },
    {
      "epoch": 4.93387982367953,
      "grad_norm": 0.328296959400177,
      "learning_rate": 6.745049504950495e-07,
      "loss": 8.0988,
      "step": 15950
    },
    {
      "epoch": 4.936973165261774,
      "grad_norm": 0.2903285324573517,
      "learning_rate": 6.435643564356436e-07,
      "loss": 8.1039,
      "step": 15960
    },
    {
      "epoch": 4.940066506844018,
      "grad_norm": 0.39832985401153564,
      "learning_rate": 6.126237623762377e-07,
      "loss": 8.1005,
      "step": 15970
    },
    {
      "epoch": 4.943159848426262,
      "grad_norm": 0.3706214427947998,
      "learning_rate": 5.816831683168317e-07,
      "loss": 8.0937,
      "step": 15980
    },
    {
      "epoch": 4.946253190008507,
      "grad_norm": 0.3363070487976074,
      "learning_rate": 5.507425742574257e-07,
      "loss": 8.1036,
      "step": 15990
    },
    {
      "epoch": 4.949346531590751,
      "grad_norm": 0.34539201855659485,
      "learning_rate": 5.198019801980199e-07,
      "loss": 8.1019,
      "step": 16000
    },
    {
      "epoch": 4.952439873172995,
      "grad_norm": 0.40287697315216064,
      "learning_rate": 4.888613861386139e-07,
      "loss": 8.1131,
      "step": 16010
    },
    {
      "epoch": 4.955533214755239,
      "grad_norm": 0.2991622984409332,
      "learning_rate": 4.5792079207920793e-07,
      "loss": 8.1076,
      "step": 16020
    },
    {
      "epoch": 4.958626556337483,
      "grad_norm": 0.3037008047103882,
      "learning_rate": 4.2698019801980205e-07,
      "loss": 8.0868,
      "step": 16030
    },
    {
      "epoch": 4.9617198979197275,
      "grad_norm": 0.33670574426651,
      "learning_rate": 3.9603960396039606e-07,
      "loss": 8.1034,
      "step": 16040
    },
    {
      "epoch": 4.9648132395019715,
      "grad_norm": 0.42219752073287964,
      "learning_rate": 3.650990099009901e-07,
      "loss": 8.1002,
      "step": 16050
    },
    {
      "epoch": 4.9679065810842165,
      "grad_norm": 0.2902373969554901,
      "learning_rate": 3.341584158415842e-07,
      "loss": 8.1086,
      "step": 16060
    },
    {
      "epoch": 4.9709999226664605,
      "grad_norm": 0.272007018327713,
      "learning_rate": 3.0321782178217825e-07,
      "loss": 8.1057,
      "step": 16070
    },
    {
      "epoch": 4.974093264248705,
      "grad_norm": 0.27069658041000366,
      "learning_rate": 2.7227722772277226e-07,
      "loss": 8.1062,
      "step": 16080
    },
    {
      "epoch": 4.977186605830949,
      "grad_norm": 0.2912357747554779,
      "learning_rate": 2.413366336633663e-07,
      "loss": 8.1011,
      "step": 16090
    },
    {
      "epoch": 4.980279947413193,
      "grad_norm": 0.33869531750679016,
      "learning_rate": 2.1039603960396041e-07,
      "loss": 8.0994,
      "step": 16100
    },
    {
      "epoch": 4.983373288995438,
      "grad_norm": 0.3845394551753998,
      "learning_rate": 1.7945544554455448e-07,
      "loss": 8.1302,
      "step": 16110
    },
    {
      "epoch": 4.986466630577682,
      "grad_norm": 0.3276771008968353,
      "learning_rate": 1.4851485148514852e-07,
      "loss": 8.0964,
      "step": 16120
    },
    {
      "epoch": 4.989559972159926,
      "grad_norm": 0.3773593604564667,
      "learning_rate": 1.1757425742574258e-07,
      "loss": 8.1077,
      "step": 16130
    },
    {
      "epoch": 4.99265331374217,
      "grad_norm": 0.43497416377067566,
      "learning_rate": 8.663366336633664e-08,
      "loss": 8.1188,
      "step": 16140
    },
    {
      "epoch": 4.995746655324414,
      "grad_norm": 0.30847489833831787,
      "learning_rate": 5.569306930693069e-08,
      "loss": 8.105,
      "step": 16150
    },
    {
      "epoch": 4.998839996906659,
      "grad_norm": 0.29649364948272705,
      "learning_rate": 2.4752475247524754e-08,
      "loss": 8.0928,
      "step": 16160
    }
  ],
  "logging_steps": 10,
  "max_steps": 16160,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 6.738821337317376e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
