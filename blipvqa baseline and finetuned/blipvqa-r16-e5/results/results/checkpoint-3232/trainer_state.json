{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9997679993813317,
  "eval_steps": 500,
  "global_step": 3232,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 1.2247768640518188,
      "learning_rate": 4.997524752475248e-05,
      "loss": 10.3209,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.124623417854309,
      "learning_rate": 4.9944306930693075e-05,
      "loss": 10.1005,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 1.021990418434143,
      "learning_rate": 4.9913366336633664e-05,
      "loss": 9.9043,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 0.7722101211547852,
      "learning_rate": 4.988242574257426e-05,
      "loss": 9.7366,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 0.6074748635292053,
      "learning_rate": 4.9851485148514855e-05,
      "loss": 9.5712,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 0.7219138145446777,
      "learning_rate": 4.982054455445545e-05,
      "loss": 9.4653,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 0.8722349405288696,
      "learning_rate": 4.978960396039604e-05,
      "loss": 9.3728,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.5499579310417175,
      "learning_rate": 4.975866336633664e-05,
      "loss": 9.2528,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.47239771485328674,
      "learning_rate": 4.972772277227723e-05,
      "loss": 9.1253,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.41896697878837585,
      "learning_rate": 4.9696782178217825e-05,
      "loss": 9.0156,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 0.4210807681083679,
      "learning_rate": 4.966584158415842e-05,
      "loss": 8.9628,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.4038824439048767,
      "learning_rate": 4.9634900990099016e-05,
      "loss": 8.8833,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.38877707719802856,
      "learning_rate": 4.9603960396039604e-05,
      "loss": 8.82,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.4252713918685913,
      "learning_rate": 4.95730198019802e-05,
      "loss": 8.7565,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.34656330943107605,
      "learning_rate": 4.9542079207920795e-05,
      "loss": 8.6942,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.4074937701225281,
      "learning_rate": 4.951113861386139e-05,
      "loss": 8.6437,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.37689608335494995,
      "learning_rate": 4.948019801980198e-05,
      "loss": 8.6368,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.47150588035583496,
      "learning_rate": 4.944925742574258e-05,
      "loss": 8.56,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.3021436631679535,
      "learning_rate": 4.941831683168317e-05,
      "loss": 8.5539,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 0.634679913520813,
      "learning_rate": 4.9387376237623765e-05,
      "loss": 8.521,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 0.43083757162094116,
      "learning_rate": 4.935643564356436e-05,
      "loss": 8.4952,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.3269064128398895,
      "learning_rate": 4.9325495049504956e-05,
      "loss": 8.4702,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 0.3242396414279938,
      "learning_rate": 4.9294554455445545e-05,
      "loss": 8.4377,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 0.42160049080848694,
      "learning_rate": 4.926361386138614e-05,
      "loss": 8.4423,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 0.7685427665710449,
      "learning_rate": 4.9232673267326735e-05,
      "loss": 8.4116,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 0.7840954065322876,
      "learning_rate": 4.920173267326733e-05,
      "loss": 8.3975,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.536022961139679,
      "learning_rate": 4.917079207920792e-05,
      "loss": 8.3935,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 0.43331119418144226,
      "learning_rate": 4.913985148514852e-05,
      "loss": 8.376,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 0.3564979135990143,
      "learning_rate": 4.910891089108911e-05,
      "loss": 8.3399,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 0.5969781279563904,
      "learning_rate": 4.9077970297029706e-05,
      "loss": 8.3449,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 1.029236912727356,
      "learning_rate": 4.90470297029703e-05,
      "loss": 8.3278,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 0.8592106699943542,
      "learning_rate": 4.9016089108910896e-05,
      "loss": 8.3073,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.37601804733276367,
      "learning_rate": 4.8985148514851485e-05,
      "loss": 8.3079,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.495095431804657,
      "learning_rate": 4.895420792079208e-05,
      "loss": 8.3113,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 0.5858795642852783,
      "learning_rate": 4.8923267326732676e-05,
      "loss": 8.291,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 0.7599595189094543,
      "learning_rate": 4.889232673267327e-05,
      "loss": 8.2882,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 0.5483265519142151,
      "learning_rate": 4.886138613861386e-05,
      "loss": 8.283,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 0.34301531314849854,
      "learning_rate": 4.883044554455446e-05,
      "loss": 8.2843,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 0.36520102620124817,
      "learning_rate": 4.879950495049505e-05,
      "loss": 8.2674,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 0.7266101241111755,
      "learning_rate": 4.8768564356435646e-05,
      "loss": 8.2721,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 0.8032565712928772,
      "learning_rate": 4.873762376237624e-05,
      "loss": 8.2506,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 0.9643657207489014,
      "learning_rate": 4.870668316831684e-05,
      "loss": 8.2656,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 1.3137482404708862,
      "learning_rate": 4.8675742574257425e-05,
      "loss": 8.2541,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 0.6304842233657837,
      "learning_rate": 4.864480198019802e-05,
      "loss": 8.2586,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 0.8513209223747253,
      "learning_rate": 4.8613861386138616e-05,
      "loss": 8.2515,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 0.35217440128326416,
      "learning_rate": 4.858292079207921e-05,
      "loss": 8.2625,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.4499775767326355,
      "learning_rate": 4.85519801980198e-05,
      "loss": 8.2373,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 0.37050744891166687,
      "learning_rate": 4.85210396039604e-05,
      "loss": 8.2332,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 0.5508533716201782,
      "learning_rate": 4.849009900990099e-05,
      "loss": 8.2356,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.7395459413528442,
      "learning_rate": 4.8459158415841586e-05,
      "loss": 8.2237,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 0.3201032280921936,
      "learning_rate": 4.842821782178218e-05,
      "loss": 8.2246,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 0.5138698816299438,
      "learning_rate": 4.839727722772278e-05,
      "loss": 8.2469,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 0.7068663835525513,
      "learning_rate": 4.8366336633663366e-05,
      "loss": 8.2411,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.3541112542152405,
      "learning_rate": 4.833539603960396e-05,
      "loss": 8.2452,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 0.28638482093811035,
      "learning_rate": 4.8304455445544556e-05,
      "loss": 8.239,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 0.49469882249832153,
      "learning_rate": 4.827351485148515e-05,
      "loss": 8.2383,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.36606070399284363,
      "learning_rate": 4.824257425742574e-05,
      "loss": 8.2376,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 0.3310873806476593,
      "learning_rate": 4.821163366336634e-05,
      "loss": 8.222,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 0.2834576964378357,
      "learning_rate": 4.818069306930693e-05,
      "loss": 8.2282,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 0.40760523080825806,
      "learning_rate": 4.8149752475247527e-05,
      "loss": 8.2198,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 0.4820593595504761,
      "learning_rate": 4.811881188118812e-05,
      "loss": 8.2187,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 0.32940781116485596,
      "learning_rate": 4.808787128712872e-05,
      "loss": 8.2253,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.7309168577194214,
      "learning_rate": 4.8056930693069306e-05,
      "loss": 8.2312,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 1.516921877861023,
      "learning_rate": 4.80259900990099e-05,
      "loss": 8.2177,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.6382942795753479,
      "learning_rate": 4.79950495049505e-05,
      "loss": 8.2123,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.31368473172187805,
      "learning_rate": 4.796410891089109e-05,
      "loss": 8.2271,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 0.5464399456977844,
      "learning_rate": 4.793316831683168e-05,
      "loss": 8.223,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 0.9966047406196594,
      "learning_rate": 4.790222772277228e-05,
      "loss": 8.2221,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 0.7671545743942261,
      "learning_rate": 4.787128712871287e-05,
      "loss": 8.2289,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 0.41397276520729065,
      "learning_rate": 4.784034653465347e-05,
      "loss": 8.2058,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 0.4239896237850189,
      "learning_rate": 4.780940594059406e-05,
      "loss": 8.2152,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 0.5347060561180115,
      "learning_rate": 4.777846534653466e-05,
      "loss": 8.2136,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 0.8777751326560974,
      "learning_rate": 4.7747524752475246e-05,
      "loss": 8.2308,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 0.7064963579177856,
      "learning_rate": 4.771658415841584e-05,
      "loss": 8.2296,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 0.6401854157447815,
      "learning_rate": 4.768564356435644e-05,
      "loss": 8.2099,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 0.5358422994613647,
      "learning_rate": 4.765470297029703e-05,
      "loss": 8.1989,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 1.2079377174377441,
      "learning_rate": 4.762376237623762e-05,
      "loss": 8.2005,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 0.5266783833503723,
      "learning_rate": 4.759282178217822e-05,
      "loss": 8.2206,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 1.395314335823059,
      "learning_rate": 4.756188118811881e-05,
      "loss": 8.2298,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 0.30735427141189575,
      "learning_rate": 4.753094059405941e-05,
      "loss": 8.2272,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 0.37388095259666443,
      "learning_rate": 4.75e-05,
      "loss": 8.2045,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.8053635954856873,
      "learning_rate": 4.74690594059406e-05,
      "loss": 8.216,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.5549759268760681,
      "learning_rate": 4.743811881188119e-05,
      "loss": 8.2045,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 0.41379955410957336,
      "learning_rate": 4.740717821782178e-05,
      "loss": 8.2138,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 0.49880656599998474,
      "learning_rate": 4.737623762376238e-05,
      "loss": 8.2189,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.8625946044921875,
      "learning_rate": 4.734529702970297e-05,
      "loss": 8.2007,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.6511316299438477,
      "learning_rate": 4.731435643564356e-05,
      "loss": 8.216,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 0.6263079643249512,
      "learning_rate": 4.7283415841584164e-05,
      "loss": 8.1913,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 0.4104655981063843,
      "learning_rate": 4.725247524752475e-05,
      "loss": 8.1926,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 0.6455371975898743,
      "learning_rate": 4.722153465346535e-05,
      "loss": 8.2126,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.42285627126693726,
      "learning_rate": 4.719059405940594e-05,
      "loss": 8.2182,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 0.3862406611442566,
      "learning_rate": 4.715965346534654e-05,
      "loss": 8.2198,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 0.7337411642074585,
      "learning_rate": 4.712871287128713e-05,
      "loss": 8.2039,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 0.5963287353515625,
      "learning_rate": 4.709777227722772e-05,
      "loss": 8.2107,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 1.2074090242385864,
      "learning_rate": 4.706683168316832e-05,
      "loss": 8.199,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 0.7971219420433044,
      "learning_rate": 4.703589108910891e-05,
      "loss": 8.198,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 1.079160451889038,
      "learning_rate": 4.70049504950495e-05,
      "loss": 8.216,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 0.9745631217956543,
      "learning_rate": 4.6974009900990104e-05,
      "loss": 8.205,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.5134568810462952,
      "learning_rate": 4.694306930693069e-05,
      "loss": 8.2127,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 0.5138888359069824,
      "learning_rate": 4.691212871287129e-05,
      "loss": 8.2052,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 0.40537601709365845,
      "learning_rate": 4.688118811881188e-05,
      "loss": 8.2065,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 0.48841580748558044,
      "learning_rate": 4.685024752475248e-05,
      "loss": 8.197,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 0.7114644646644592,
      "learning_rate": 4.6819306930693074e-05,
      "loss": 8.2106,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 0.5906206965446472,
      "learning_rate": 4.678836633663366e-05,
      "loss": 8.2063,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 0.5407156944274902,
      "learning_rate": 4.6757425742574265e-05,
      "loss": 8.1951,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 0.6469225287437439,
      "learning_rate": 4.6726485148514853e-05,
      "loss": 8.2108,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 0.7905026078224182,
      "learning_rate": 4.669554455445545e-05,
      "loss": 8.2066,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 1.1467782258987427,
      "learning_rate": 4.6664603960396044e-05,
      "loss": 8.1957,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.5464600920677185,
      "learning_rate": 4.663366336633664e-05,
      "loss": 8.2019,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.2692526578903198,
      "learning_rate": 4.660272277227723e-05,
      "loss": 8.1918,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 1.0630919933319092,
      "learning_rate": 4.6571782178217824e-05,
      "loss": 8.2114,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 0.7366971969604492,
      "learning_rate": 4.654084158415842e-05,
      "loss": 8.2015,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 0.9527750015258789,
      "learning_rate": 4.6509900990099014e-05,
      "loss": 8.1882,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 0.5087741613388062,
      "learning_rate": 4.64789603960396e-05,
      "loss": 8.1986,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 0.36688852310180664,
      "learning_rate": 4.6448019801980205e-05,
      "loss": 8.1762,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 0.4138261079788208,
      "learning_rate": 4.6417079207920794e-05,
      "loss": 8.189,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 0.9040499925613403,
      "learning_rate": 4.638613861386139e-05,
      "loss": 8.1979,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 0.46051475405693054,
      "learning_rate": 4.6355198019801985e-05,
      "loss": 8.1926,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 0.7377279996871948,
      "learning_rate": 4.632425742574258e-05,
      "loss": 8.1965,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 0.35922303795814514,
      "learning_rate": 4.629331683168317e-05,
      "loss": 8.2081,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 0.7749016880989075,
      "learning_rate": 4.6262376237623764e-05,
      "loss": 8.1848,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 0.5312947034835815,
      "learning_rate": 4.623143564356436e-05,
      "loss": 8.1993,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 0.4701521396636963,
      "learning_rate": 4.6200495049504955e-05,
      "loss": 8.1961,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 0.5874415636062622,
      "learning_rate": 4.616955445544554e-05,
      "loss": 8.1919,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 0.5899882316589355,
      "learning_rate": 4.6138613861386145e-05,
      "loss": 8.1866,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 0.6891916394233704,
      "learning_rate": 4.6107673267326734e-05,
      "loss": 8.1861,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 0.45299622416496277,
      "learning_rate": 4.607673267326733e-05,
      "loss": 8.1874,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 1.070456862449646,
      "learning_rate": 4.6045792079207925e-05,
      "loss": 8.1963,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 0.5316533446311951,
      "learning_rate": 4.601485148514852e-05,
      "loss": 8.1986,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 0.5455809831619263,
      "learning_rate": 4.598391089108911e-05,
      "loss": 8.2036,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 0.6389105319976807,
      "learning_rate": 4.5952970297029704e-05,
      "loss": 8.1907,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 0.4612026512622833,
      "learning_rate": 4.59220297029703e-05,
      "loss": 8.2057,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 0.5475612282752991,
      "learning_rate": 4.5891089108910895e-05,
      "loss": 8.1943,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 0.7979524731636047,
      "learning_rate": 4.5860148514851484e-05,
      "loss": 8.1736,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 0.8579632043838501,
      "learning_rate": 4.5829207920792086e-05,
      "loss": 8.1832,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 0.39259180426597595,
      "learning_rate": 4.5798267326732674e-05,
      "loss": 8.1819,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 0.6323119401931763,
      "learning_rate": 4.576732673267327e-05,
      "loss": 8.173,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.39811235666275024,
      "learning_rate": 4.5736386138613865e-05,
      "loss": 8.1844,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.238008737564087,
      "learning_rate": 4.570544554455446e-05,
      "loss": 8.1994,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 0.7153070569038391,
      "learning_rate": 4.567450495049505e-05,
      "loss": 8.1717,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 0.7053283452987671,
      "learning_rate": 4.5643564356435645e-05,
      "loss": 8.1978,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 1.0989347696304321,
      "learning_rate": 4.561262376237624e-05,
      "loss": 8.1986,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 0.5493602156639099,
      "learning_rate": 4.5581683168316835e-05,
      "loss": 8.1952,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 0.639211893081665,
      "learning_rate": 4.5550742574257424e-05,
      "loss": 8.195,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 0.6570772528648376,
      "learning_rate": 4.5519801980198026e-05,
      "loss": 8.194,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 0.7170979380607605,
      "learning_rate": 4.5488861386138615e-05,
      "loss": 8.1849,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 0.3791988790035248,
      "learning_rate": 4.545792079207921e-05,
      "loss": 8.1876,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 0.5699512362480164,
      "learning_rate": 4.5426980198019805e-05,
      "loss": 8.1877,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 0.5981238484382629,
      "learning_rate": 4.53960396039604e-05,
      "loss": 8.2093,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 0.32610607147216797,
      "learning_rate": 4.536509900990099e-05,
      "loss": 8.1761,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 1.279944896697998,
      "learning_rate": 4.5334158415841585e-05,
      "loss": 8.1933,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 0.6262332201004028,
      "learning_rate": 4.530321782178218e-05,
      "loss": 8.1985,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 0.40352341532707214,
      "learning_rate": 4.5272277227722776e-05,
      "loss": 8.1891,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 0.7490780353546143,
      "learning_rate": 4.5241336633663364e-05,
      "loss": 8.2028,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 0.5756144523620605,
      "learning_rate": 4.5210396039603966e-05,
      "loss": 8.1738,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.49029240012168884,
      "learning_rate": 4.5179455445544555e-05,
      "loss": 8.1631,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 0.5849631428718567,
      "learning_rate": 4.514851485148515e-05,
      "loss": 8.2034,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 1.2421512603759766,
      "learning_rate": 4.5117574257425746e-05,
      "loss": 8.1833,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 0.7801458835601807,
      "learning_rate": 4.508663366336634e-05,
      "loss": 8.1783,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 0.5695696473121643,
      "learning_rate": 4.505569306930693e-05,
      "loss": 8.1896,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 0.6546632051467896,
      "learning_rate": 4.5024752475247525e-05,
      "loss": 8.1863,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 1.2603425979614258,
      "learning_rate": 4.499381188118812e-05,
      "loss": 8.1848,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 1.300681233406067,
      "learning_rate": 4.4962871287128716e-05,
      "loss": 8.164,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 0.5545843839645386,
      "learning_rate": 4.4931930693069305e-05,
      "loss": 8.1922,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 0.6221284866333008,
      "learning_rate": 4.490099009900991e-05,
      "loss": 8.1893,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 0.3282677233219147,
      "learning_rate": 4.4870049504950495e-05,
      "loss": 8.1813,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 0.9480931162834167,
      "learning_rate": 4.483910891089109e-05,
      "loss": 8.1902,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 0.5979871153831482,
      "learning_rate": 4.4808168316831686e-05,
      "loss": 8.1678,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 0.49511781334877014,
      "learning_rate": 4.477722772277228e-05,
      "loss": 8.1751,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 0.34118643403053284,
      "learning_rate": 4.474628712871287e-05,
      "loss": 8.182,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 1.4929418563842773,
      "learning_rate": 4.4715346534653465e-05,
      "loss": 8.1935,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 0.4099706709384918,
      "learning_rate": 4.468440594059406e-05,
      "loss": 8.1847,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 0.5727508068084717,
      "learning_rate": 4.4653465346534656e-05,
      "loss": 8.1765,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 1.2658605575561523,
      "learning_rate": 4.4622524752475245e-05,
      "loss": 8.185,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 0.8700957894325256,
      "learning_rate": 4.459158415841585e-05,
      "loss": 8.1904,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 0.5159096717834473,
      "learning_rate": 4.4560643564356436e-05,
      "loss": 8.196,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 1.5243291854858398,
      "learning_rate": 4.452970297029703e-05,
      "loss": 8.1689,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 1.0142402648925781,
      "learning_rate": 4.4498762376237626e-05,
      "loss": 8.1918,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 1.3960062265396118,
      "learning_rate": 4.446782178217822e-05,
      "loss": 8.1873,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 1.2078458070755005,
      "learning_rate": 4.443688118811881e-05,
      "loss": 8.1798,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 1.5516475439071655,
      "learning_rate": 4.4405940594059406e-05,
      "loss": 8.1711,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 0.869965672492981,
      "learning_rate": 4.4375e-05,
      "loss": 8.1954,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 1.2989387512207031,
      "learning_rate": 4.4344059405940597e-05,
      "loss": 8.1859,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 0.4227568209171295,
      "learning_rate": 4.4313118811881185e-05,
      "loss": 8.1799,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 0.47840455174446106,
      "learning_rate": 4.428217821782179e-05,
      "loss": 8.1762,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 0.6492992043495178,
      "learning_rate": 4.4251237623762376e-05,
      "loss": 8.1945,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 0.4004427194595337,
      "learning_rate": 4.422029702970297e-05,
      "loss": 8.2049,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 0.8578277230262756,
      "learning_rate": 4.418935643564357e-05,
      "loss": 8.182,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.9048387408256531,
      "learning_rate": 4.415841584158416e-05,
      "loss": 8.1799,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 0.46773746609687805,
      "learning_rate": 4.412747524752475e-05,
      "loss": 8.1909,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 0.33044618368148804,
      "learning_rate": 4.4096534653465346e-05,
      "loss": 8.1837,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 1.1874784231185913,
      "learning_rate": 4.406559405940594e-05,
      "loss": 8.1886,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.4573058784008026,
      "learning_rate": 4.403465346534654e-05,
      "loss": 8.1852,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 1.0154377222061157,
      "learning_rate": 4.4003712871287126e-05,
      "loss": 8.1911,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 0.5081018805503845,
      "learning_rate": 4.397277227722773e-05,
      "loss": 8.1769,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 0.48365461826324463,
      "learning_rate": 4.3941831683168316e-05,
      "loss": 8.1715,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 0.49079352617263794,
      "learning_rate": 4.391089108910891e-05,
      "loss": 8.1659,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 0.9408277273178101,
      "learning_rate": 4.387995049504951e-05,
      "loss": 8.176,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 0.7148870229721069,
      "learning_rate": 4.38490099009901e-05,
      "loss": 8.1861,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 0.42110419273376465,
      "learning_rate": 4.381806930693069e-05,
      "loss": 8.1883,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.34316784143447876,
      "learning_rate": 4.3787128712871286e-05,
      "loss": 8.181,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 0.4234250783920288,
      "learning_rate": 4.375618811881188e-05,
      "loss": 8.2,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 0.3691387176513672,
      "learning_rate": 4.372524752475248e-05,
      "loss": 8.1787,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 0.44123542308807373,
      "learning_rate": 4.369430693069307e-05,
      "loss": 8.1924,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 0.45960983633995056,
      "learning_rate": 4.366336633663367e-05,
      "loss": 8.1876,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 0.43252694606781006,
      "learning_rate": 4.363242574257426e-05,
      "loss": 8.1805,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 0.3911884129047394,
      "learning_rate": 4.360148514851485e-05,
      "loss": 8.1835,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 0.3393804430961609,
      "learning_rate": 4.357054455445545e-05,
      "loss": 8.1993,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 0.5701378583908081,
      "learning_rate": 4.353960396039604e-05,
      "loss": 8.1902,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 0.4500882625579834,
      "learning_rate": 4.350866336633664e-05,
      "loss": 8.1975,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 0.36670684814453125,
      "learning_rate": 4.347772277227723e-05,
      "loss": 8.1825,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 0.4888262450695038,
      "learning_rate": 4.344678217821783e-05,
      "loss": 8.1775,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 0.575025200843811,
      "learning_rate": 4.341584158415842e-05,
      "loss": 8.1913,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 1.005854845046997,
      "learning_rate": 4.338490099009901e-05,
      "loss": 8.1912,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 1.063337802886963,
      "learning_rate": 4.335396039603961e-05,
      "loss": 8.2002,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 0.5566020607948303,
      "learning_rate": 4.3323019801980204e-05,
      "loss": 8.1743,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 0.6357192993164062,
      "learning_rate": 4.329207920792079e-05,
      "loss": 8.1795,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 0.46208086609840393,
      "learning_rate": 4.326113861386139e-05,
      "loss": 8.1783,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 0.5193877220153809,
      "learning_rate": 4.323019801980198e-05,
      "loss": 8.1738,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 0.5517786145210266,
      "learning_rate": 4.319925742574258e-05,
      "loss": 8.191,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 0.4851343631744385,
      "learning_rate": 4.316831683168317e-05,
      "loss": 8.18,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 0.4999909996986389,
      "learning_rate": 4.313737623762377e-05,
      "loss": 8.1893,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 1.0605361461639404,
      "learning_rate": 4.310643564356436e-05,
      "loss": 8.1847,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 1.3921605348587036,
      "learning_rate": 4.307549504950495e-05,
      "loss": 8.181,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.6739130616188049,
      "learning_rate": 4.304455445544555e-05,
      "loss": 8.202,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 0.7772232294082642,
      "learning_rate": 4.3013613861386144e-05,
      "loss": 8.1742,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 0.41060587763786316,
      "learning_rate": 4.298267326732673e-05,
      "loss": 8.1627,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 0.37843480706214905,
      "learning_rate": 4.295173267326733e-05,
      "loss": 8.1715,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 1.0329080820083618,
      "learning_rate": 4.2920792079207923e-05,
      "loss": 8.1796,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 0.31238245964050293,
      "learning_rate": 4.289294554455446e-05,
      "loss": 8.1815,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 0.4785993695259094,
      "learning_rate": 4.286200495049505e-05,
      "loss": 8.1691,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.8877915143966675,
      "learning_rate": 4.2831064356435644e-05,
      "loss": 8.1794,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 0.8918452858924866,
      "learning_rate": 4.280012376237624e-05,
      "loss": 8.176,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 0.9933529496192932,
      "learning_rate": 4.2769183168316835e-05,
      "loss": 8.1888,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 0.4408724904060364,
      "learning_rate": 4.273824257425742e-05,
      "loss": 8.1638,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 0.3677043616771698,
      "learning_rate": 4.2707301980198025e-05,
      "loss": 8.1954,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 0.8111699223518372,
      "learning_rate": 4.2676361386138614e-05,
      "loss": 8.1806,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 0.732003927230835,
      "learning_rate": 4.264542079207921e-05,
      "loss": 8.1785,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 0.7386962175369263,
      "learning_rate": 4.2614480198019805e-05,
      "loss": 8.1704,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 0.35727474093437195,
      "learning_rate": 4.25835396039604e-05,
      "loss": 8.1704,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 0.5202633738517761,
      "learning_rate": 4.255259900990099e-05,
      "loss": 8.1765,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 0.7533573508262634,
      "learning_rate": 4.2521658415841584e-05,
      "loss": 8.1878,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 0.5169065594673157,
      "learning_rate": 4.249071782178218e-05,
      "loss": 8.1817,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 0.3747626543045044,
      "learning_rate": 4.2459777227722775e-05,
      "loss": 8.1904,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 0.8353764414787292,
      "learning_rate": 4.2428836633663364e-05,
      "loss": 8.1847,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.47149935364723206,
      "learning_rate": 4.2397896039603966e-05,
      "loss": 8.1857,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.5039201378822327,
      "learning_rate": 4.2366955445544554e-05,
      "loss": 8.1584,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 0.39962735772132874,
      "learning_rate": 4.233601485148515e-05,
      "loss": 8.1755,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 0.4213845729827881,
      "learning_rate": 4.2305074257425745e-05,
      "loss": 8.1911,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 0.7187732458114624,
      "learning_rate": 4.227413366336634e-05,
      "loss": 8.1729,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.3765415549278259,
      "learning_rate": 4.224319306930693e-05,
      "loss": 8.1917,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 0.4099287986755371,
      "learning_rate": 4.2212252475247525e-05,
      "loss": 8.1763,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 0.5655828714370728,
      "learning_rate": 4.218131188118813e-05,
      "loss": 8.1521,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 0.3301551640033722,
      "learning_rate": 4.2150371287128715e-05,
      "loss": 8.1814,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 1.0062203407287598,
      "learning_rate": 4.211943069306931e-05,
      "loss": 8.17,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 0.3695231080055237,
      "learning_rate": 4.2088490099009906e-05,
      "loss": 8.1822,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 0.5440958738327026,
      "learning_rate": 4.20575495049505e-05,
      "loss": 8.1831,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 0.5230558514595032,
      "learning_rate": 4.202660891089109e-05,
      "loss": 8.1775,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 0.7811059951782227,
      "learning_rate": 4.1995668316831686e-05,
      "loss": 8.1838,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 0.7115465402603149,
      "learning_rate": 4.196472772277228e-05,
      "loss": 8.184,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 0.5242425799369812,
      "learning_rate": 4.1933787128712876e-05,
      "loss": 8.172,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 0.6848767399787903,
      "learning_rate": 4.1902846534653465e-05,
      "loss": 8.1575,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 0.9786973595619202,
      "learning_rate": 4.187190594059407e-05,
      "loss": 8.1607,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 0.40516024827957153,
      "learning_rate": 4.1840965346534656e-05,
      "loss": 8.1797,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 0.4008958339691162,
      "learning_rate": 4.181002475247525e-05,
      "loss": 8.1882,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 0.9412573575973511,
      "learning_rate": 4.1779084158415846e-05,
      "loss": 8.1745,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 0.5629944205284119,
      "learning_rate": 4.174814356435644e-05,
      "loss": 8.1801,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 0.6402981877326965,
      "learning_rate": 4.171720297029703e-05,
      "loss": 8.1934,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 0.9342613816261292,
      "learning_rate": 4.1686262376237626e-05,
      "loss": 8.177,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 1.2738996744155884,
      "learning_rate": 4.165532178217822e-05,
      "loss": 8.1711,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 0.5073431134223938,
      "learning_rate": 4.1624381188118817e-05,
      "loss": 8.1783,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 0.41454824805259705,
      "learning_rate": 4.1593440594059405e-05,
      "loss": 8.1973,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 1.1496150493621826,
      "learning_rate": 4.156250000000001e-05,
      "loss": 8.1858,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 0.4803610146045685,
      "learning_rate": 4.1531559405940596e-05,
      "loss": 8.1708,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.34687677025794983,
      "learning_rate": 4.150061881188119e-05,
      "loss": 8.1863,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 0.5241560339927673,
      "learning_rate": 4.146967821782179e-05,
      "loss": 8.1912,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 0.7502440810203552,
      "learning_rate": 4.143873762376238e-05,
      "loss": 8.1845,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 0.9989693760871887,
      "learning_rate": 4.140779702970297e-05,
      "loss": 8.1773,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.6086286902427673,
      "learning_rate": 4.1376856435643566e-05,
      "loss": 8.1675,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.40269002318382263,
      "learning_rate": 4.134591584158416e-05,
      "loss": 8.1915,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.3288612365722656,
      "learning_rate": 4.131497524752476e-05,
      "loss": 8.1653,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 0.5025574564933777,
      "learning_rate": 4.1284034653465346e-05,
      "loss": 8.1629,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 0.3438236117362976,
      "learning_rate": 4.125309405940595e-05,
      "loss": 8.2046,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.5780764818191528,
      "learning_rate": 4.1222153465346536e-05,
      "loss": 8.1778,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.39652180671691895,
      "learning_rate": 4.119121287128713e-05,
      "loss": 8.1823,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 0.6715706586837769,
      "learning_rate": 4.116027227722773e-05,
      "loss": 8.1738,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.3920033276081085,
      "learning_rate": 4.112933168316832e-05,
      "loss": 8.1781,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 0.39238977432250977,
      "learning_rate": 4.109839108910891e-05,
      "loss": 8.1748,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 0.3157040774822235,
      "learning_rate": 4.1067450495049506e-05,
      "loss": 8.1726,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 0.7599718570709229,
      "learning_rate": 4.10365099009901e-05,
      "loss": 8.1836,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 1.2285321950912476,
      "learning_rate": 4.10055693069307e-05,
      "loss": 8.1901,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 0.9450066089630127,
      "learning_rate": 4.0974628712871286e-05,
      "loss": 8.1854,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 1.1108378171920776,
      "learning_rate": 4.094368811881189e-05,
      "loss": 8.1845,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 0.4957656264305115,
      "learning_rate": 4.091274752475248e-05,
      "loss": 8.1684,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 0.7190476059913635,
      "learning_rate": 4.088180693069307e-05,
      "loss": 8.1668,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 0.3910123109817505,
      "learning_rate": 4.085086633663367e-05,
      "loss": 8.1671,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 0.6209633946418762,
      "learning_rate": 4.081992574257426e-05,
      "loss": 8.1741,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 0.6628877520561218,
      "learning_rate": 4.078898514851485e-05,
      "loss": 8.1724,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 1.0487492084503174,
      "learning_rate": 4.075804455445545e-05,
      "loss": 8.1865,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 0.7827565670013428,
      "learning_rate": 4.072710396039604e-05,
      "loss": 8.1843,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.3538581430912018,
      "learning_rate": 4.069616336633664e-05,
      "loss": 8.1539,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.47143933176994324,
      "learning_rate": 4.0665222772277226e-05,
      "loss": 8.1788,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.5566879510879517,
      "learning_rate": 4.063428217821783e-05,
      "loss": 8.175,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 0.7525437474250793,
      "learning_rate": 4.060334158415842e-05,
      "loss": 8.1687,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 1.0151294469833374,
      "learning_rate": 4.057240099009901e-05,
      "loss": 8.1862,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 0.4681243896484375,
      "learning_rate": 4.054146039603961e-05,
      "loss": 8.1858,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 0.8434861898422241,
      "learning_rate": 4.05105198019802e-05,
      "loss": 8.1885,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 0.3857128918170929,
      "learning_rate": 4.047957920792079e-05,
      "loss": 8.1875,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 0.7138345241546631,
      "learning_rate": 4.044863861386139e-05,
      "loss": 8.1638,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 0.500750720500946,
      "learning_rate": 4.041769801980198e-05,
      "loss": 8.1765,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 0.5411683917045593,
      "learning_rate": 4.038675742574258e-05,
      "loss": 8.1828,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 0.559773325920105,
      "learning_rate": 4.0355816831683166e-05,
      "loss": 8.1659,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 0.5506566166877747,
      "learning_rate": 4.032487623762377e-05,
      "loss": 8.1777,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 1.041272521018982,
      "learning_rate": 4.029393564356436e-05,
      "loss": 8.1708,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 0.5980176329612732,
      "learning_rate": 4.0266089108910894e-05,
      "loss": 8.1868,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.670759916305542,
      "learning_rate": 4.023514851485149e-05,
      "loss": 8.1805,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 0.4230586588382721,
      "learning_rate": 4.020420792079208e-05,
      "loss": 8.1705,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 0.525131106376648,
      "learning_rate": 4.017326732673268e-05,
      "loss": 8.1747,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.4320549964904785,
      "learning_rate": 4.014232673267327e-05,
      "loss": 8.1686,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 0.784473717212677,
      "learning_rate": 4.0111386138613864e-05,
      "loss": 8.1712,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 0.6629128456115723,
      "learning_rate": 4.008044554455446e-05,
      "loss": 8.175,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.6317083835601807,
      "learning_rate": 4.0049504950495055e-05,
      "loss": 8.17,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 0.9099751114845276,
      "learning_rate": 4.001856435643564e-05,
      "loss": 8.1644,
      "step": 3230
    }
  ],
  "logging_steps": 10,
  "max_steps": 16160,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 1.3480770221666304e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
