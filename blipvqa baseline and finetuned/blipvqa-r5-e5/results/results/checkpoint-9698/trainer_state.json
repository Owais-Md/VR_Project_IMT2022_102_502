{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.999922666460444,
  "eval_steps": 500,
  "global_step": 9698,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 2.283881425857544,
      "learning_rate": 4.997524752475248e-05,
      "loss": 10.3057,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.7015577554702759,
      "learning_rate": 4.9944306930693075e-05,
      "loss": 10.0835,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 1.8396601676940918,
      "learning_rate": 4.9913366336633664e-05,
      "loss": 9.8731,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 1.0070555210113525,
      "learning_rate": 4.988242574257426e-05,
      "loss": 9.6963,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 1.0936119556427002,
      "learning_rate": 4.9851485148514855e-05,
      "loss": 9.5427,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 0.9463545083999634,
      "learning_rate": 4.982054455445545e-05,
      "loss": 9.4026,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 1.0548697710037231,
      "learning_rate": 4.978960396039604e-05,
      "loss": 9.2535,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.846869170665741,
      "learning_rate": 4.975866336633664e-05,
      "loss": 9.1537,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.7546383142471313,
      "learning_rate": 4.972772277227723e-05,
      "loss": 9.0376,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.7686005234718323,
      "learning_rate": 4.9696782178217825e-05,
      "loss": 8.9299,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 0.7528889775276184,
      "learning_rate": 4.966584158415842e-05,
      "loss": 8.869,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.7256690859794617,
      "learning_rate": 4.963799504950496e-05,
      "loss": 8.7791,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.6986427903175354,
      "learning_rate": 4.9607054455445546e-05,
      "loss": 8.733,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.6804739236831665,
      "learning_rate": 4.957611386138614e-05,
      "loss": 8.6765,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.7127686142921448,
      "learning_rate": 4.9545173267326736e-05,
      "loss": 8.6174,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.6702990531921387,
      "learning_rate": 4.951423267326733e-05,
      "loss": 8.5714,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.7780759334564209,
      "learning_rate": 4.948329207920792e-05,
      "loss": 8.5676,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.7366856932640076,
      "learning_rate": 4.9452351485148516e-05,
      "loss": 8.497,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.7136861681938171,
      "learning_rate": 4.942141089108911e-05,
      "loss": 8.4928,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 1.917746663093567,
      "learning_rate": 4.9390470297029706e-05,
      "loss": 8.4637,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 1.148116111755371,
      "learning_rate": 4.9359529702970295e-05,
      "loss": 8.4424,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.5712763667106628,
      "learning_rate": 4.93285891089109e-05,
      "loss": 8.4241,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 0.6743816137313843,
      "learning_rate": 4.9297648514851486e-05,
      "loss": 8.3973,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 0.6497937440872192,
      "learning_rate": 4.926670792079208e-05,
      "loss": 8.4058,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 1.2516216039657593,
      "learning_rate": 4.9235767326732677e-05,
      "loss": 8.3807,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 1.2762138843536377,
      "learning_rate": 4.920482673267327e-05,
      "loss": 8.3691,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.6043147444725037,
      "learning_rate": 4.917388613861386e-05,
      "loss": 8.367,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 1.278912901878357,
      "learning_rate": 4.9142945544554456e-05,
      "loss": 8.3501,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 1.0998435020446777,
      "learning_rate": 4.911200495049505e-05,
      "loss": 8.3126,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 1.0249993801116943,
      "learning_rate": 4.908106435643565e-05,
      "loss": 8.3197,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 1.9351379871368408,
      "learning_rate": 4.9050123762376235e-05,
      "loss": 8.307,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 0.9796373844146729,
      "learning_rate": 4.901918316831684e-05,
      "loss": 8.2893,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.837062656879425,
      "learning_rate": 4.8988242574257426e-05,
      "loss": 8.2921,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.7073545455932617,
      "learning_rate": 4.895730198019802e-05,
      "loss": 8.2982,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 1.0989118814468384,
      "learning_rate": 4.892636138613862e-05,
      "loss": 8.2793,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 1.3417258262634277,
      "learning_rate": 4.889542079207921e-05,
      "loss": 8.2782,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 1.1105343103408813,
      "learning_rate": 4.88644801980198e-05,
      "loss": 8.2753,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 1.1419422626495361,
      "learning_rate": 4.8833539603960396e-05,
      "loss": 8.2766,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 1.4925583600997925,
      "learning_rate": 4.880259900990099e-05,
      "loss": 8.2618,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 1.426761507987976,
      "learning_rate": 4.877165841584159e-05,
      "loss": 8.2665,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 1.2589497566223145,
      "learning_rate": 4.8740717821782176e-05,
      "loss": 8.2462,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 3.0927257537841797,
      "learning_rate": 4.870977722772278e-05,
      "loss": 8.2647,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 1.1971580982208252,
      "learning_rate": 4.8678836633663366e-05,
      "loss": 8.2508,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 1.6362168788909912,
      "learning_rate": 4.864789603960396e-05,
      "loss": 8.2566,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 1.0588487386703491,
      "learning_rate": 4.861695544554456e-05,
      "loss": 8.2492,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 0.757152795791626,
      "learning_rate": 4.858601485148515e-05,
      "loss": 8.2621,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.8795488476753235,
      "learning_rate": 4.855507425742574e-05,
      "loss": 8.2369,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 0.695861279964447,
      "learning_rate": 4.852413366336634e-05,
      "loss": 8.2328,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 1.3076118230819702,
      "learning_rate": 4.849319306930693e-05,
      "loss": 8.2352,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.7781069874763489,
      "learning_rate": 4.846225247524753e-05,
      "loss": 8.2228,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 1.1994925737380981,
      "learning_rate": 4.843131188118812e-05,
      "loss": 8.2235,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 1.3085798025131226,
      "learning_rate": 4.840037128712872e-05,
      "loss": 8.2474,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 2.003173589706421,
      "learning_rate": 4.8369430693069314e-05,
      "loss": 8.2414,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.8361700773239136,
      "learning_rate": 4.83384900990099e-05,
      "loss": 8.2453,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 1.8430653810501099,
      "learning_rate": 4.83075495049505e-05,
      "loss": 8.2399,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 1.4458892345428467,
      "learning_rate": 4.827660891089109e-05,
      "loss": 8.2388,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.6414463520050049,
      "learning_rate": 4.824566831683169e-05,
      "loss": 8.2377,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 0.614952802658081,
      "learning_rate": 4.821472772277228e-05,
      "loss": 8.2242,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 2.2713072299957275,
      "learning_rate": 4.818378712871288e-05,
      "loss": 8.2297,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 1.6645755767822266,
      "learning_rate": 4.815284653465347e-05,
      "loss": 8.2216,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 1.319783091545105,
      "learning_rate": 4.812190594059406e-05,
      "loss": 8.22,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 0.7334883213043213,
      "learning_rate": 4.809096534653466e-05,
      "loss": 8.2276,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.5590004324913025,
      "learning_rate": 4.8060024752475254e-05,
      "loss": 8.2329,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 1.0776251554489136,
      "learning_rate": 4.802908415841584e-05,
      "loss": 8.2187,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.783419668674469,
      "learning_rate": 4.799814356435644e-05,
      "loss": 8.214,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.5639674663543701,
      "learning_rate": 4.796720297029703e-05,
      "loss": 8.2288,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 1.6762508153915405,
      "learning_rate": 4.793626237623763e-05,
      "loss": 8.2244,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 1.8065810203552246,
      "learning_rate": 4.790532178217822e-05,
      "loss": 8.2238,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 2.1397507190704346,
      "learning_rate": 4.787438118811882e-05,
      "loss": 8.2311,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 1.9543536901474,
      "learning_rate": 4.784344059405941e-05,
      "loss": 8.2082,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 1.0264626741409302,
      "learning_rate": 4.7812500000000003e-05,
      "loss": 8.218,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 1.7906293869018555,
      "learning_rate": 4.77815594059406e-05,
      "loss": 8.2161,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 0.523497462272644,
      "learning_rate": 4.7750618811881194e-05,
      "loss": 8.2333,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 0.7396768927574158,
      "learning_rate": 4.771967821782178e-05,
      "loss": 8.2318,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 1.0711976289749146,
      "learning_rate": 4.768873762376238e-05,
      "loss": 8.212,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 1.1562745571136475,
      "learning_rate": 4.7657797029702974e-05,
      "loss": 8.2018,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 0.9633840322494507,
      "learning_rate": 4.762685643564357e-05,
      "loss": 8.2027,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 1.9501839876174927,
      "learning_rate": 4.759591584158416e-05,
      "loss": 8.2235,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 0.6358993053436279,
      "learning_rate": 4.756497524752476e-05,
      "loss": 8.2305,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 1.3217599391937256,
      "learning_rate": 4.753403465346535e-05,
      "loss": 8.229,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 1.5870161056518555,
      "learning_rate": 4.7503094059405944e-05,
      "loss": 8.2073,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.7626834511756897,
      "learning_rate": 4.747215346534654e-05,
      "loss": 8.2184,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.9049161672592163,
      "learning_rate": 4.7441212871287135e-05,
      "loss": 8.2069,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 0.767512321472168,
      "learning_rate": 4.741027227722772e-05,
      "loss": 8.2177,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 1.644620418548584,
      "learning_rate": 4.737933168316832e-05,
      "loss": 8.2219,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.6423130035400391,
      "learning_rate": 4.7348391089108914e-05,
      "loss": 8.203,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.9769448041915894,
      "learning_rate": 4.731745049504951e-05,
      "loss": 8.2188,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 0.7573779821395874,
      "learning_rate": 4.72865099009901e-05,
      "loss": 8.1944,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 0.7785887718200684,
      "learning_rate": 4.72555693069307e-05,
      "loss": 8.1956,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 1.4564820528030396,
      "learning_rate": 4.722462871287129e-05,
      "loss": 8.2146,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.9155580997467041,
      "learning_rate": 4.7193688118811884e-05,
      "loss": 8.2203,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 1.7754689455032349,
      "learning_rate": 4.716274752475248e-05,
      "loss": 8.2226,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 1.523375391960144,
      "learning_rate": 4.7131806930693075e-05,
      "loss": 8.2055,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 2.337571620941162,
      "learning_rate": 4.7100866336633663e-05,
      "loss": 8.2128,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 2.301279306411743,
      "learning_rate": 4.706992574257426e-05,
      "loss": 8.2004,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 1.6187915802001953,
      "learning_rate": 4.7038985148514854e-05,
      "loss": 8.1995,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 1.382105827331543,
      "learning_rate": 4.700804455445545e-05,
      "loss": 8.218,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 0.6750116348266602,
      "learning_rate": 4.697710396039604e-05,
      "loss": 8.2066,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.8816543817520142,
      "learning_rate": 4.694616336633664e-05,
      "loss": 8.2152,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 1.4862397909164429,
      "learning_rate": 4.691522277227723e-05,
      "loss": 8.2067,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 2.049319267272949,
      "learning_rate": 4.6884282178217824e-05,
      "loss": 8.2084,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 0.9031298160552979,
      "learning_rate": 4.685334158415842e-05,
      "loss": 8.1988,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 1.8627923727035522,
      "learning_rate": 4.6822400990099015e-05,
      "loss": 8.2123,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 2.3312253952026367,
      "learning_rate": 4.6791460396039604e-05,
      "loss": 8.2085,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 1.4772182703018188,
      "learning_rate": 4.67605198019802e-05,
      "loss": 8.1977,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 0.7699053287506104,
      "learning_rate": 4.6729579207920795e-05,
      "loss": 8.214,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 1.258692741394043,
      "learning_rate": 4.669863861386139e-05,
      "loss": 8.2091,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 1.3253406286239624,
      "learning_rate": 4.666769801980198e-05,
      "loss": 8.1978,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.686968982219696,
      "learning_rate": 4.663675742574258e-05,
      "loss": 8.2043,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.0489264726638794,
      "learning_rate": 4.660581683168317e-05,
      "loss": 8.1951,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 0.6969893574714661,
      "learning_rate": 4.6574876237623765e-05,
      "loss": 8.2131,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 0.8512948155403137,
      "learning_rate": 4.654393564356436e-05,
      "loss": 8.2043,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 1.1636050939559937,
      "learning_rate": 4.6512995049504955e-05,
      "loss": 8.1909,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 2.8269481658935547,
      "learning_rate": 4.6482054455445544e-05,
      "loss": 8.2012,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 1.2299357652664185,
      "learning_rate": 4.645111386138614e-05,
      "loss": 8.1793,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 0.7432317733764648,
      "learning_rate": 4.6420173267326735e-05,
      "loss": 8.1917,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 1.4997855424880981,
      "learning_rate": 4.638923267326733e-05,
      "loss": 8.2003,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 1.17046320438385,
      "learning_rate": 4.635829207920792e-05,
      "loss": 8.194,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 0.8377035856246948,
      "learning_rate": 4.632735148514852e-05,
      "loss": 8.1987,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 1.1011815071105957,
      "learning_rate": 4.629641089108911e-05,
      "loss": 8.2105,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 2.4605493545532227,
      "learning_rate": 4.6265470297029705e-05,
      "loss": 8.1883,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 2.327408790588379,
      "learning_rate": 4.62345297029703e-05,
      "loss": 8.2033,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 0.8229179978370667,
      "learning_rate": 4.6203589108910896e-05,
      "loss": 8.1983,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 0.7746541500091553,
      "learning_rate": 4.6172648514851484e-05,
      "loss": 8.1951,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 1.7976148128509521,
      "learning_rate": 4.614170792079208e-05,
      "loss": 8.1896,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 0.8158743977546692,
      "learning_rate": 4.6110767326732675e-05,
      "loss": 8.1881,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 1.2931594848632812,
      "learning_rate": 4.607982673267327e-05,
      "loss": 8.1896,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 1.3114842176437378,
      "learning_rate": 4.604888613861386e-05,
      "loss": 8.1987,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 1.2831923961639404,
      "learning_rate": 4.601794554455446e-05,
      "loss": 8.1997,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 2.43292236328125,
      "learning_rate": 4.598700495049505e-05,
      "loss": 8.2048,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 1.5269575119018555,
      "learning_rate": 4.5956064356435645e-05,
      "loss": 8.194,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 2.0795676708221436,
      "learning_rate": 4.592512376237624e-05,
      "loss": 8.2075,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 2.4864180088043213,
      "learning_rate": 4.5894183168316836e-05,
      "loss": 8.197,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 1.2086435556411743,
      "learning_rate": 4.5863242574257425e-05,
      "loss": 8.1758,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 1.5092233419418335,
      "learning_rate": 4.583230198019802e-05,
      "loss": 8.1855,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 1.1152379512786865,
      "learning_rate": 4.5801361386138616e-05,
      "loss": 8.1831,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 0.8014148473739624,
      "learning_rate": 4.577042079207921e-05,
      "loss": 8.175,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.7414972186088562,
      "learning_rate": 4.57394801980198e-05,
      "loss": 8.1869,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.085468053817749,
      "learning_rate": 4.57085396039604e-05,
      "loss": 8.2012,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 1.602766752243042,
      "learning_rate": 4.567759900990099e-05,
      "loss": 8.1717,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 0.7542450428009033,
      "learning_rate": 4.5646658415841586e-05,
      "loss": 8.1987,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 0.933017909526825,
      "learning_rate": 4.561571782178218e-05,
      "loss": 8.1989,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 2.079925775527954,
      "learning_rate": 4.5584777227722776e-05,
      "loss": 8.1969,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 1.0393853187561035,
      "learning_rate": 4.5553836633663365e-05,
      "loss": 8.1958,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 0.9792965650558472,
      "learning_rate": 4.552289603960396e-05,
      "loss": 8.1952,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 0.6396719813346863,
      "learning_rate": 4.5491955445544556e-05,
      "loss": 8.1849,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 1.0150868892669678,
      "learning_rate": 4.546101485148515e-05,
      "loss": 8.1889,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 1.6108311414718628,
      "learning_rate": 4.543007425742574e-05,
      "loss": 8.1892,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 1.2107347249984741,
      "learning_rate": 4.539913366336634e-05,
      "loss": 8.2103,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 2.3177919387817383,
      "learning_rate": 4.536819306930693e-05,
      "loss": 8.1775,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 2.523088216781616,
      "learning_rate": 4.5337252475247526e-05,
      "loss": 8.1951,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 1.209039330482483,
      "learning_rate": 4.530631188118812e-05,
      "loss": 8.1994,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 0.9921939969062805,
      "learning_rate": 4.527537128712872e-05,
      "loss": 8.1897,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 1.8710671663284302,
      "learning_rate": 4.524443069306931e-05,
      "loss": 8.2037,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 1.856017827987671,
      "learning_rate": 4.52134900990099e-05,
      "loss": 8.1749,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.9681246876716614,
      "learning_rate": 4.51825495049505e-05,
      "loss": 8.1651,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 1.9962009191513062,
      "learning_rate": 4.515160891089109e-05,
      "loss": 8.2048,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 0.6548168063163757,
      "learning_rate": 4.512066831683169e-05,
      "loss": 8.1847,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 1.0097074508666992,
      "learning_rate": 4.508972772277228e-05,
      "loss": 8.1786,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 3.913090705871582,
      "learning_rate": 4.505878712871288e-05,
      "loss": 8.1914,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 2.477466583251953,
      "learning_rate": 4.5027846534653466e-05,
      "loss": 8.1868,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 1.6980576515197754,
      "learning_rate": 4.499690594059406e-05,
      "loss": 8.1864,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 0.7103257775306702,
      "learning_rate": 4.496596534653466e-05,
      "loss": 8.1642,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 1.0004551410675049,
      "learning_rate": 4.493502475247525e-05,
      "loss": 8.1928,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 0.7323936223983765,
      "learning_rate": 4.490408415841584e-05,
      "loss": 8.19,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 1.083060383796692,
      "learning_rate": 4.487314356435644e-05,
      "loss": 8.1821,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 1.411689281463623,
      "learning_rate": 4.484220297029703e-05,
      "loss": 8.1909,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 1.8345826864242554,
      "learning_rate": 4.481126237623763e-05,
      "loss": 8.1705,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 0.8988075256347656,
      "learning_rate": 4.478032178217822e-05,
      "loss": 8.1757,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 2.0011909008026123,
      "learning_rate": 4.474938118811882e-05,
      "loss": 8.1819,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 1.4574311971664429,
      "learning_rate": 4.471844059405941e-05,
      "loss": 8.1937,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 1.8036798238754272,
      "learning_rate": 4.46875e-05,
      "loss": 8.1856,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 1.1992884874343872,
      "learning_rate": 4.46565594059406e-05,
      "loss": 8.1782,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 1.0265569686889648,
      "learning_rate": 4.462561881188119e-05,
      "loss": 8.1857,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 2.0533766746520996,
      "learning_rate": 4.459467821782178e-05,
      "loss": 8.1917,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 1.6524206399917603,
      "learning_rate": 4.4563737623762384e-05,
      "loss": 8.1967,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 1.3807529211044312,
      "learning_rate": 4.453279702970297e-05,
      "loss": 8.1698,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 2.298870086669922,
      "learning_rate": 4.450185643564357e-05,
      "loss": 8.1933,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 0.7895309329032898,
      "learning_rate": 4.447091584158416e-05,
      "loss": 8.1897,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 0.6869894862174988,
      "learning_rate": 4.443997524752476e-05,
      "loss": 8.181,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 1.9506685733795166,
      "learning_rate": 4.440903465346535e-05,
      "loss": 8.1712,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 1.4609322547912598,
      "learning_rate": 4.437809405940594e-05,
      "loss": 8.1963,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 2.905377149581909,
      "learning_rate": 4.434715346534654e-05,
      "loss": 8.186,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 1.0420995950698853,
      "learning_rate": 4.431621287128713e-05,
      "loss": 8.1818,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 1.0503602027893066,
      "learning_rate": 4.428527227722772e-05,
      "loss": 8.1772,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 1.0495920181274414,
      "learning_rate": 4.4254331683168324e-05,
      "loss": 8.1953,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 1.1845871210098267,
      "learning_rate": 4.422339108910891e-05,
      "loss": 8.2054,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 0.8258082270622253,
      "learning_rate": 4.419245049504951e-05,
      "loss": 8.1825,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.954254686832428,
      "learning_rate": 4.41615099009901e-05,
      "loss": 8.1811,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 0.9058201313018799,
      "learning_rate": 4.41305693069307e-05,
      "loss": 8.1925,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 1.8257588148117065,
      "learning_rate": 4.409962871287129e-05,
      "loss": 8.1849,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 2.6599490642547607,
      "learning_rate": 4.406868811881188e-05,
      "loss": 8.1895,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 3.222787380218506,
      "learning_rate": 4.403774752475248e-05,
      "loss": 8.1875,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 1.5086697340011597,
      "learning_rate": 4.4006806930693073e-05,
      "loss": 8.1931,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 2.020599842071533,
      "learning_rate": 4.397586633663366e-05,
      "loss": 8.1783,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 3.1900675296783447,
      "learning_rate": 4.3944925742574264e-05,
      "loss": 8.1733,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 2.096738576889038,
      "learning_rate": 4.391398514851485e-05,
      "loss": 8.1673,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 1.2224245071411133,
      "learning_rate": 4.388304455445545e-05,
      "loss": 8.1777,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 1.1820451021194458,
      "learning_rate": 4.3852103960396044e-05,
      "loss": 8.1868,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 0.6829479336738586,
      "learning_rate": 4.382116336633664e-05,
      "loss": 8.1895,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.9388048052787781,
      "learning_rate": 4.379022277227723e-05,
      "loss": 8.1835,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 1.8816661834716797,
      "learning_rate": 4.375928217821782e-05,
      "loss": 8.2014,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 2.058828592300415,
      "learning_rate": 4.372834158415842e-05,
      "loss": 8.1804,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 0.6706216931343079,
      "learning_rate": 4.3697400990099014e-05,
      "loss": 8.194,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 1.2952839136123657,
      "learning_rate": 4.36664603960396e-05,
      "loss": 8.1882,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 1.1358423233032227,
      "learning_rate": 4.3635519801980205e-05,
      "loss": 8.1826,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 2.457620620727539,
      "learning_rate": 4.360457920792079e-05,
      "loss": 8.1862,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 0.9240761399269104,
      "learning_rate": 4.357363861386139e-05,
      "loss": 8.2007,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 1.1774667501449585,
      "learning_rate": 4.3542698019801984e-05,
      "loss": 8.192,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 1.1870375871658325,
      "learning_rate": 4.351175742574258e-05,
      "loss": 8.2006,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 0.641690194606781,
      "learning_rate": 4.348081683168317e-05,
      "loss": 8.1851,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 1.9188201427459717,
      "learning_rate": 4.344987623762376e-05,
      "loss": 8.1785,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 1.383307933807373,
      "learning_rate": 4.341893564356436e-05,
      "loss": 8.1925,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 2.127551794052124,
      "learning_rate": 4.3387995049504954e-05,
      "loss": 8.1923,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 1.263771891593933,
      "learning_rate": 4.335705445544554e-05,
      "loss": 8.2007,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 1.2934004068374634,
      "learning_rate": 4.3326113861386145e-05,
      "loss": 8.1754,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 1.2981380224227905,
      "learning_rate": 4.3295173267326733e-05,
      "loss": 8.1811,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 0.7973347902297974,
      "learning_rate": 4.326423267326733e-05,
      "loss": 8.1797,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 2.5965218544006348,
      "learning_rate": 4.3233292079207924e-05,
      "loss": 8.1762,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 1.4685229063034058,
      "learning_rate": 4.320235148514852e-05,
      "loss": 8.1916,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 1.339370846748352,
      "learning_rate": 4.317141089108911e-05,
      "loss": 8.1817,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 1.7123206853866577,
      "learning_rate": 4.3140470297029704e-05,
      "loss": 8.1901,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 1.1629844903945923,
      "learning_rate": 4.31095297029703e-05,
      "loss": 8.1858,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 1.1125881671905518,
      "learning_rate": 4.3078589108910894e-05,
      "loss": 8.1818,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.8006672263145447,
      "learning_rate": 4.304764851485148e-05,
      "loss": 8.2041,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 0.8128392100334167,
      "learning_rate": 4.3016707920792085e-05,
      "loss": 8.1749,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 1.392057180404663,
      "learning_rate": 4.2985767326732674e-05,
      "loss": 8.1642,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 0.7961410284042358,
      "learning_rate": 4.295482673267327e-05,
      "loss": 8.1724,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 0.8821604251861572,
      "learning_rate": 4.2923886138613865e-05,
      "loss": 8.1812,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 2.1213696002960205,
      "learning_rate": 4.289294554455446e-05,
      "loss": 8.1812,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 2.049058198928833,
      "learning_rate": 4.286200495049505e-05,
      "loss": 8.1706,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.8438975811004639,
      "learning_rate": 4.2831064356435644e-05,
      "loss": 8.1814,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 1.5994082689285278,
      "learning_rate": 4.280012376237624e-05,
      "loss": 8.1773,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 1.6760908365249634,
      "learning_rate": 4.2769183168316835e-05,
      "loss": 8.1915,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 1.1761233806610107,
      "learning_rate": 4.273824257425742e-05,
      "loss": 8.1648,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 0.8909958004951477,
      "learning_rate": 4.2707301980198025e-05,
      "loss": 8.1975,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 1.8550859689712524,
      "learning_rate": 4.2676361386138614e-05,
      "loss": 8.1823,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 2.531545877456665,
      "learning_rate": 4.264542079207921e-05,
      "loss": 8.1802,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 1.5944923162460327,
      "learning_rate": 4.2614480198019805e-05,
      "loss": 8.1732,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 1.9887875318527222,
      "learning_rate": 4.25835396039604e-05,
      "loss": 8.1727,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 2.872790575027466,
      "learning_rate": 4.255259900990099e-05,
      "loss": 8.1794,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 0.7231345176696777,
      "learning_rate": 4.2521658415841584e-05,
      "loss": 8.1898,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 1.993692398071289,
      "learning_rate": 4.249071782178218e-05,
      "loss": 8.1832,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 2.00915265083313,
      "learning_rate": 4.2459777227722775e-05,
      "loss": 8.1933,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 1.4420005083084106,
      "learning_rate": 4.2428836633663364e-05,
      "loss": 8.1866,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.6248858571052551,
      "learning_rate": 4.2397896039603966e-05,
      "loss": 8.1862,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.8173538446426392,
      "learning_rate": 4.2366955445544554e-05,
      "loss": 8.1595,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 0.9942618012428284,
      "learning_rate": 4.233601485148515e-05,
      "loss": 8.1785,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 0.8387640118598938,
      "learning_rate": 4.2305074257425745e-05,
      "loss": 8.1929,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 1.4144275188446045,
      "learning_rate": 4.227413366336634e-05,
      "loss": 8.1745,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.7667526006698608,
      "learning_rate": 4.224319306930693e-05,
      "loss": 8.1941,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 1.2658334970474243,
      "learning_rate": 4.2212252475247525e-05,
      "loss": 8.1778,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 1.2555665969848633,
      "learning_rate": 4.218131188118813e-05,
      "loss": 8.1546,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 1.0784926414489746,
      "learning_rate": 4.2150371287128715e-05,
      "loss": 8.1839,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 0.7848606109619141,
      "learning_rate": 4.211943069306931e-05,
      "loss": 8.1716,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 0.8297044634819031,
      "learning_rate": 4.2088490099009906e-05,
      "loss": 8.1839,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 1.2344859838485718,
      "learning_rate": 4.20575495049505e-05,
      "loss": 8.1839,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 1.8767091035842896,
      "learning_rate": 4.202660891089109e-05,
      "loss": 8.18,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 2.172985792160034,
      "learning_rate": 4.1995668316831686e-05,
      "loss": 8.1864,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 0.9894881844520569,
      "learning_rate": 4.196472772277228e-05,
      "loss": 8.1868,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 2.2789270877838135,
      "learning_rate": 4.1933787128712876e-05,
      "loss": 8.1741,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 0.9330989122390747,
      "learning_rate": 4.1902846534653465e-05,
      "loss": 8.1596,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 1.5242124795913696,
      "learning_rate": 4.187190594059407e-05,
      "loss": 8.1624,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 1.4509238004684448,
      "learning_rate": 4.1840965346534656e-05,
      "loss": 8.1812,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 1.110629916191101,
      "learning_rate": 4.181002475247525e-05,
      "loss": 8.1899,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 1.2923803329467773,
      "learning_rate": 4.1779084158415846e-05,
      "loss": 8.1763,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 0.8786554336547852,
      "learning_rate": 4.174814356435644e-05,
      "loss": 8.1813,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 1.402254343032837,
      "learning_rate": 4.171720297029703e-05,
      "loss": 8.1962,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 1.2103198766708374,
      "learning_rate": 4.1686262376237626e-05,
      "loss": 8.1784,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 1.835925579071045,
      "learning_rate": 4.165532178217822e-05,
      "loss": 8.1734,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 1.5465960502624512,
      "learning_rate": 4.1624381188118817e-05,
      "loss": 8.1802,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 1.3195463418960571,
      "learning_rate": 4.1593440594059405e-05,
      "loss": 8.1999,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 1.1356457471847534,
      "learning_rate": 4.156250000000001e-05,
      "loss": 8.1874,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 1.2040010690689087,
      "learning_rate": 4.1531559405940596e-05,
      "loss": 8.1718,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.6268787980079651,
      "learning_rate": 4.150061881188119e-05,
      "loss": 8.1883,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 1.3542238473892212,
      "learning_rate": 4.146967821782179e-05,
      "loss": 8.1929,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 1.0542324781417847,
      "learning_rate": 4.143873762376238e-05,
      "loss": 8.1867,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 0.959769070148468,
      "learning_rate": 4.140779702970297e-05,
      "loss": 8.1805,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.8823913931846619,
      "learning_rate": 4.1376856435643566e-05,
      "loss": 8.1691,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.993774950504303,
      "learning_rate": 4.134591584158416e-05,
      "loss": 8.1943,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.6663392186164856,
      "learning_rate": 4.131497524752476e-05,
      "loss": 8.1682,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 1.050313115119934,
      "learning_rate": 4.1284034653465346e-05,
      "loss": 8.1664,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 0.6561669111251831,
      "learning_rate": 4.125309405940595e-05,
      "loss": 8.2059,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.5854478478431702,
      "learning_rate": 4.1222153465346536e-05,
      "loss": 8.1804,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.6190448999404907,
      "learning_rate": 4.119121287128713e-05,
      "loss": 8.1843,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 0.8077029585838318,
      "learning_rate": 4.116027227722773e-05,
      "loss": 8.1768,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.6319847106933594,
      "learning_rate": 4.112933168316832e-05,
      "loss": 8.1793,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 0.6598100662231445,
      "learning_rate": 4.109839108910891e-05,
      "loss": 8.1773,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 0.6498719453811646,
      "learning_rate": 4.1067450495049506e-05,
      "loss": 8.1743,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 1.3095574378967285,
      "learning_rate": 4.10365099009901e-05,
      "loss": 8.1851,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 2.136505126953125,
      "learning_rate": 4.10055693069307e-05,
      "loss": 8.1921,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 0.8504270315170288,
      "learning_rate": 4.0974628712871286e-05,
      "loss": 8.1865,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 1.7754770517349243,
      "learning_rate": 4.094368811881189e-05,
      "loss": 8.1855,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 1.306828260421753,
      "learning_rate": 4.091274752475248e-05,
      "loss": 8.1704,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 1.1314023733139038,
      "learning_rate": 4.088180693069307e-05,
      "loss": 8.1697,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 2.4816794395446777,
      "learning_rate": 4.085086633663367e-05,
      "loss": 8.1709,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 1.2383394241333008,
      "learning_rate": 4.081992574257426e-05,
      "loss": 8.1762,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 0.8009899258613586,
      "learning_rate": 4.078898514851485e-05,
      "loss": 8.1734,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 1.5692912340164185,
      "learning_rate": 4.075804455445545e-05,
      "loss": 8.188,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 1.0885016918182373,
      "learning_rate": 4.072710396039604e-05,
      "loss": 8.1867,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.7706090807914734,
      "learning_rate": 4.069616336633664e-05,
      "loss": 8.1562,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.8732590675354004,
      "learning_rate": 4.0665222772277226e-05,
      "loss": 8.1804,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.9803152680397034,
      "learning_rate": 4.063428217821783e-05,
      "loss": 8.177,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 0.7919334769248962,
      "learning_rate": 4.060334158415842e-05,
      "loss": 8.1704,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 1.1393951177597046,
      "learning_rate": 4.057240099009901e-05,
      "loss": 8.1884,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 1.2807313203811646,
      "learning_rate": 4.054146039603961e-05,
      "loss": 8.1877,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 1.3348079919815063,
      "learning_rate": 4.05105198019802e-05,
      "loss": 8.1906,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 0.723763644695282,
      "learning_rate": 4.047957920792079e-05,
      "loss": 8.1892,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 0.6793975830078125,
      "learning_rate": 4.044863861386139e-05,
      "loss": 8.1648,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 1.0764813423156738,
      "learning_rate": 4.041769801980198e-05,
      "loss": 8.1786,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 1.1117558479309082,
      "learning_rate": 4.038675742574258e-05,
      "loss": 8.185,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 2.068308115005493,
      "learning_rate": 4.0355816831683166e-05,
      "loss": 8.1686,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 1.702989935874939,
      "learning_rate": 4.032487623762377e-05,
      "loss": 8.18,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 1.5032709836959839,
      "learning_rate": 4.029393564356436e-05,
      "loss": 8.1727,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 1.2828290462493896,
      "learning_rate": 4.026299504950495e-05,
      "loss": 8.1885,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.9744423031806946,
      "learning_rate": 4.023205445544555e-05,
      "loss": 8.1824,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 1.3720958232879639,
      "learning_rate": 4.0201113861386143e-05,
      "loss": 8.1716,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 1.1993892192840576,
      "learning_rate": 4.017017326732673e-05,
      "loss": 8.1774,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.7784874439239502,
      "learning_rate": 4.013923267326733e-05,
      "loss": 8.1701,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 1.4290626049041748,
      "learning_rate": 4.010829207920792e-05,
      "loss": 8.1728,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 1.2606793642044067,
      "learning_rate": 4.007735148514852e-05,
      "loss": 8.1773,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.9906719326972961,
      "learning_rate": 4.004641089108911e-05,
      "loss": 8.1718,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 0.9914515018463135,
      "learning_rate": 4.001547029702971e-05,
      "loss": 8.1653,
      "step": 3230
    },
    {
      "epoch": 1.002242672647127,
      "grad_norm": 1.2213011980056763,
      "learning_rate": 3.99845297029703e-05,
      "loss": 8.1681,
      "step": 3240
    },
    {
      "epoch": 1.0053360142293712,
      "grad_norm": 1.1975106000900269,
      "learning_rate": 3.995358910891089e-05,
      "loss": 8.185,
      "step": 3250
    },
    {
      "epoch": 1.0084293558116155,
      "grad_norm": 1.2800188064575195,
      "learning_rate": 3.992264851485149e-05,
      "loss": 8.1651,
      "step": 3260
    },
    {
      "epoch": 1.0115226973938598,
      "grad_norm": 1.2291233539581299,
      "learning_rate": 3.9891707920792084e-05,
      "loss": 8.1594,
      "step": 3270
    },
    {
      "epoch": 1.0146160389761039,
      "grad_norm": 1.1720836162567139,
      "learning_rate": 3.986076732673267e-05,
      "loss": 8.1664,
      "step": 3280
    },
    {
      "epoch": 1.0177093805583481,
      "grad_norm": 1.567177414894104,
      "learning_rate": 3.982982673267327e-05,
      "loss": 8.1677,
      "step": 3290
    },
    {
      "epoch": 1.0208027221405924,
      "grad_norm": 0.5762407183647156,
      "learning_rate": 3.979888613861386e-05,
      "loss": 8.1839,
      "step": 3300
    },
    {
      "epoch": 1.0238960637228367,
      "grad_norm": 1.0825451612472534,
      "learning_rate": 3.976794554455446e-05,
      "loss": 8.1849,
      "step": 3310
    },
    {
      "epoch": 1.0269894053050808,
      "grad_norm": 0.9254522919654846,
      "learning_rate": 3.973700495049505e-05,
      "loss": 8.1834,
      "step": 3320
    },
    {
      "epoch": 1.030082746887325,
      "grad_norm": 1.2102525234222412,
      "learning_rate": 3.970606435643565e-05,
      "loss": 8.167,
      "step": 3330
    },
    {
      "epoch": 1.0331760884695693,
      "grad_norm": 0.8051912188529968,
      "learning_rate": 3.967512376237624e-05,
      "loss": 8.1812,
      "step": 3340
    },
    {
      "epoch": 1.0362694300518134,
      "grad_norm": 0.5957062244415283,
      "learning_rate": 3.964418316831683e-05,
      "loss": 8.1632,
      "step": 3350
    },
    {
      "epoch": 1.0393627716340577,
      "grad_norm": 0.9674434661865234,
      "learning_rate": 3.961324257425743e-05,
      "loss": 8.1782,
      "step": 3360
    },
    {
      "epoch": 1.042456113216302,
      "grad_norm": 0.8710668683052063,
      "learning_rate": 3.9582301980198024e-05,
      "loss": 8.1824,
      "step": 3370
    },
    {
      "epoch": 1.0455494547985462,
      "grad_norm": 0.7723981738090515,
      "learning_rate": 3.955136138613861e-05,
      "loss": 8.1612,
      "step": 3380
    },
    {
      "epoch": 1.0486427963807903,
      "grad_norm": 0.7678624987602234,
      "learning_rate": 3.952042079207921e-05,
      "loss": 8.1651,
      "step": 3390
    },
    {
      "epoch": 1.0517361379630346,
      "grad_norm": 1.9644544124603271,
      "learning_rate": 3.9489480198019803e-05,
      "loss": 8.1778,
      "step": 3400
    },
    {
      "epoch": 1.0548294795452788,
      "grad_norm": 1.7867558002471924,
      "learning_rate": 3.94585396039604e-05,
      "loss": 8.1736,
      "step": 3410
    },
    {
      "epoch": 1.057922821127523,
      "grad_norm": 0.6512953042984009,
      "learning_rate": 3.942759900990099e-05,
      "loss": 8.1605,
      "step": 3420
    },
    {
      "epoch": 1.0610161627097672,
      "grad_norm": 0.7946992516517639,
      "learning_rate": 3.939665841584159e-05,
      "loss": 8.1768,
      "step": 3430
    },
    {
      "epoch": 1.0641095042920115,
      "grad_norm": 0.8331141471862793,
      "learning_rate": 3.936571782178218e-05,
      "loss": 8.1719,
      "step": 3440
    },
    {
      "epoch": 1.0672028458742557,
      "grad_norm": 0.7313192486763,
      "learning_rate": 3.9334777227722774e-05,
      "loss": 8.1841,
      "step": 3450
    },
    {
      "epoch": 1.0702961874564998,
      "grad_norm": 1.2313882112503052,
      "learning_rate": 3.930383663366337e-05,
      "loss": 8.187,
      "step": 3460
    },
    {
      "epoch": 1.073389529038744,
      "grad_norm": 1.111137866973877,
      "learning_rate": 3.9272896039603964e-05,
      "loss": 8.1685,
      "step": 3470
    },
    {
      "epoch": 1.0764828706209884,
      "grad_norm": 1.3812202215194702,
      "learning_rate": 3.924195544554455e-05,
      "loss": 8.1836,
      "step": 3480
    },
    {
      "epoch": 1.0795762122032326,
      "grad_norm": 1.4169095754623413,
      "learning_rate": 3.921101485148515e-05,
      "loss": 8.1962,
      "step": 3490
    },
    {
      "epoch": 1.0826695537854767,
      "grad_norm": 1.0171616077423096,
      "learning_rate": 3.9180074257425744e-05,
      "loss": 8.1668,
      "step": 3500
    },
    {
      "epoch": 1.085762895367721,
      "grad_norm": 0.7996727824211121,
      "learning_rate": 3.914913366336634e-05,
      "loss": 8.1649,
      "step": 3510
    },
    {
      "epoch": 1.0888562369499653,
      "grad_norm": 1.4300481081008911,
      "learning_rate": 3.911819306930693e-05,
      "loss": 8.1738,
      "step": 3520
    },
    {
      "epoch": 1.0919495785322093,
      "grad_norm": 1.0143523216247559,
      "learning_rate": 3.908725247524753e-05,
      "loss": 8.1888,
      "step": 3530
    },
    {
      "epoch": 1.0950429201144536,
      "grad_norm": 1.0000865459442139,
      "learning_rate": 3.9056311881188125e-05,
      "loss": 8.1781,
      "step": 3540
    },
    {
      "epoch": 1.0981362616966979,
      "grad_norm": 1.3436375856399536,
      "learning_rate": 3.9025371287128714e-05,
      "loss": 8.1825,
      "step": 3550
    },
    {
      "epoch": 1.1012296032789421,
      "grad_norm": 1.0800632238388062,
      "learning_rate": 3.899443069306931e-05,
      "loss": 8.1661,
      "step": 3560
    },
    {
      "epoch": 1.1043229448611862,
      "grad_norm": 1.7473244667053223,
      "learning_rate": 3.8963490099009905e-05,
      "loss": 8.1815,
      "step": 3570
    },
    {
      "epoch": 1.1074162864434305,
      "grad_norm": 1.321324110031128,
      "learning_rate": 3.89325495049505e-05,
      "loss": 8.1679,
      "step": 3580
    },
    {
      "epoch": 1.1105096280256748,
      "grad_norm": 0.7825003862380981,
      "learning_rate": 3.890160891089109e-05,
      "loss": 8.1671,
      "step": 3590
    },
    {
      "epoch": 1.113602969607919,
      "grad_norm": 0.8835910558700562,
      "learning_rate": 3.887066831683169e-05,
      "loss": 8.1803,
      "step": 3600
    },
    {
      "epoch": 1.116696311190163,
      "grad_norm": 0.986366331577301,
      "learning_rate": 3.883972772277228e-05,
      "loss": 8.1777,
      "step": 3610
    },
    {
      "epoch": 1.1197896527724074,
      "grad_norm": 0.8880130052566528,
      "learning_rate": 3.8808787128712875e-05,
      "loss": 8.1713,
      "step": 3620
    },
    {
      "epoch": 1.1228829943546517,
      "grad_norm": 0.9621564745903015,
      "learning_rate": 3.877784653465347e-05,
      "loss": 8.1945,
      "step": 3630
    },
    {
      "epoch": 1.1259763359368957,
      "grad_norm": 1.0133496522903442,
      "learning_rate": 3.8746905940594066e-05,
      "loss": 8.1723,
      "step": 3640
    },
    {
      "epoch": 1.12906967751914,
      "grad_norm": 0.8424130082130432,
      "learning_rate": 3.8715965346534654e-05,
      "loss": 8.1794,
      "step": 3650
    },
    {
      "epoch": 1.1321630191013843,
      "grad_norm": 0.6788762807846069,
      "learning_rate": 3.868502475247525e-05,
      "loss": 8.1831,
      "step": 3660
    },
    {
      "epoch": 1.1352563606836286,
      "grad_norm": 0.9400535225868225,
      "learning_rate": 3.8654084158415845e-05,
      "loss": 8.1648,
      "step": 3670
    },
    {
      "epoch": 1.1383497022658726,
      "grad_norm": 1.319170594215393,
      "learning_rate": 3.862314356435644e-05,
      "loss": 8.1676,
      "step": 3680
    },
    {
      "epoch": 1.141443043848117,
      "grad_norm": 0.5681795477867126,
      "learning_rate": 3.859220297029703e-05,
      "loss": 8.162,
      "step": 3690
    },
    {
      "epoch": 1.1445363854303612,
      "grad_norm": 0.6263239979743958,
      "learning_rate": 3.856126237623763e-05,
      "loss": 8.1723,
      "step": 3700
    },
    {
      "epoch": 1.1476297270126055,
      "grad_norm": 1.3395272493362427,
      "learning_rate": 3.853032178217822e-05,
      "loss": 8.1587,
      "step": 3710
    },
    {
      "epoch": 1.1507230685948495,
      "grad_norm": 0.7281138300895691,
      "learning_rate": 3.8499381188118815e-05,
      "loss": 8.1718,
      "step": 3720
    },
    {
      "epoch": 1.1538164101770938,
      "grad_norm": 1.0737323760986328,
      "learning_rate": 3.846844059405941e-05,
      "loss": 8.167,
      "step": 3730
    },
    {
      "epoch": 1.156909751759338,
      "grad_norm": 0.8106751441955566,
      "learning_rate": 3.8437500000000006e-05,
      "loss": 8.1812,
      "step": 3740
    },
    {
      "epoch": 1.1600030933415821,
      "grad_norm": 1.8575725555419922,
      "learning_rate": 3.8406559405940595e-05,
      "loss": 8.1656,
      "step": 3750
    },
    {
      "epoch": 1.1630964349238264,
      "grad_norm": 0.621423602104187,
      "learning_rate": 3.837561881188119e-05,
      "loss": 8.1779,
      "step": 3760
    },
    {
      "epoch": 1.1661897765060707,
      "grad_norm": 0.6427802443504333,
      "learning_rate": 3.8344678217821785e-05,
      "loss": 8.1687,
      "step": 3770
    },
    {
      "epoch": 1.169283118088315,
      "grad_norm": 0.7275810241699219,
      "learning_rate": 3.831373762376238e-05,
      "loss": 8.1681,
      "step": 3780
    },
    {
      "epoch": 1.172376459670559,
      "grad_norm": 0.5198794007301331,
      "learning_rate": 3.828279702970297e-05,
      "loss": 8.1799,
      "step": 3790
    },
    {
      "epoch": 1.1754698012528033,
      "grad_norm": 1.0406533479690552,
      "learning_rate": 3.825185643564357e-05,
      "loss": 8.1835,
      "step": 3800
    },
    {
      "epoch": 1.1785631428350476,
      "grad_norm": 0.6469219923019409,
      "learning_rate": 3.822091584158416e-05,
      "loss": 8.1766,
      "step": 3810
    },
    {
      "epoch": 1.1816564844172919,
      "grad_norm": 2.0886242389678955,
      "learning_rate": 3.8189975247524755e-05,
      "loss": 8.1786,
      "step": 3820
    },
    {
      "epoch": 1.184749825999536,
      "grad_norm": 0.8466972708702087,
      "learning_rate": 3.815903465346535e-05,
      "loss": 8.1562,
      "step": 3830
    },
    {
      "epoch": 1.1878431675817802,
      "grad_norm": 1.4182608127593994,
      "learning_rate": 3.8128094059405946e-05,
      "loss": 8.1781,
      "step": 3840
    },
    {
      "epoch": 1.1909365091640245,
      "grad_norm": 0.9937980771064758,
      "learning_rate": 3.8097153465346535e-05,
      "loss": 8.1705,
      "step": 3850
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 0.7916375398635864,
      "learning_rate": 3.806621287128713e-05,
      "loss": 8.1759,
      "step": 3860
    },
    {
      "epoch": 1.1971231923285128,
      "grad_norm": 0.8437491059303284,
      "learning_rate": 3.8035272277227726e-05,
      "loss": 8.1652,
      "step": 3870
    },
    {
      "epoch": 1.200216533910757,
      "grad_norm": 1.6076982021331787,
      "learning_rate": 3.800433168316832e-05,
      "loss": 8.1831,
      "step": 3880
    },
    {
      "epoch": 1.2033098754930014,
      "grad_norm": 1.6538817882537842,
      "learning_rate": 3.797339108910891e-05,
      "loss": 8.1673,
      "step": 3890
    },
    {
      "epoch": 1.2064032170752454,
      "grad_norm": 1.3117600679397583,
      "learning_rate": 3.794245049504951e-05,
      "loss": 8.171,
      "step": 3900
    },
    {
      "epoch": 1.2094965586574897,
      "grad_norm": 0.7339670658111572,
      "learning_rate": 3.79115099009901e-05,
      "loss": 8.1719,
      "step": 3910
    },
    {
      "epoch": 1.212589900239734,
      "grad_norm": 1.092329502105713,
      "learning_rate": 3.7880569306930696e-05,
      "loss": 8.1698,
      "step": 3920
    },
    {
      "epoch": 1.2156832418219783,
      "grad_norm": 1.4451707601547241,
      "learning_rate": 3.784962871287129e-05,
      "loss": 8.1575,
      "step": 3930
    },
    {
      "epoch": 1.2187765834042223,
      "grad_norm": 0.7700849771499634,
      "learning_rate": 3.7818688118811887e-05,
      "loss": 8.1833,
      "step": 3940
    },
    {
      "epoch": 1.2218699249864666,
      "grad_norm": 0.6883994936943054,
      "learning_rate": 3.7787747524752475e-05,
      "loss": 8.1667,
      "step": 3950
    },
    {
      "epoch": 1.224963266568711,
      "grad_norm": 1.1646314859390259,
      "learning_rate": 3.775680693069307e-05,
      "loss": 8.1717,
      "step": 3960
    },
    {
      "epoch": 1.228056608150955,
      "grad_norm": 1.423634648323059,
      "learning_rate": 3.7725866336633666e-05,
      "loss": 8.173,
      "step": 3970
    },
    {
      "epoch": 1.2311499497331992,
      "grad_norm": 2.0301990509033203,
      "learning_rate": 3.769492574257426e-05,
      "loss": 8.1608,
      "step": 3980
    },
    {
      "epoch": 1.2342432913154435,
      "grad_norm": 1.3395417928695679,
      "learning_rate": 3.766398514851485e-05,
      "loss": 8.1734,
      "step": 3990
    },
    {
      "epoch": 1.2373366328976878,
      "grad_norm": 0.6827284097671509,
      "learning_rate": 3.763304455445545e-05,
      "loss": 8.1768,
      "step": 4000
    },
    {
      "epoch": 1.2404299744799319,
      "grad_norm": 0.8821004629135132,
      "learning_rate": 3.760210396039604e-05,
      "loss": 8.1767,
      "step": 4010
    },
    {
      "epoch": 1.2435233160621761,
      "grad_norm": 0.6663477420806885,
      "learning_rate": 3.7571163366336636e-05,
      "loss": 8.1713,
      "step": 4020
    },
    {
      "epoch": 1.2466166576444204,
      "grad_norm": 0.6850157380104065,
      "learning_rate": 3.754022277227723e-05,
      "loss": 8.1731,
      "step": 4030
    },
    {
      "epoch": 1.2497099992266647,
      "grad_norm": 0.708370566368103,
      "learning_rate": 3.750928217821783e-05,
      "loss": 8.174,
      "step": 4040
    },
    {
      "epoch": 1.2528033408089088,
      "grad_norm": 0.8563492298126221,
      "learning_rate": 3.7478341584158416e-05,
      "loss": 8.1591,
      "step": 4050
    },
    {
      "epoch": 1.255896682391153,
      "grad_norm": 0.658682644367218,
      "learning_rate": 3.744740099009901e-05,
      "loss": 8.1641,
      "step": 4060
    },
    {
      "epoch": 1.2589900239733973,
      "grad_norm": 0.7775496244430542,
      "learning_rate": 3.7416460396039606e-05,
      "loss": 8.1602,
      "step": 4070
    },
    {
      "epoch": 1.2620833655556414,
      "grad_norm": 1.0847424268722534,
      "learning_rate": 3.73855198019802e-05,
      "loss": 8.1931,
      "step": 4080
    },
    {
      "epoch": 1.2651767071378857,
      "grad_norm": 1.591184139251709,
      "learning_rate": 3.735457920792079e-05,
      "loss": 8.1891,
      "step": 4090
    },
    {
      "epoch": 1.26827004872013,
      "grad_norm": 1.4072344303131104,
      "learning_rate": 3.732363861386139e-05,
      "loss": 8.1626,
      "step": 4100
    },
    {
      "epoch": 1.2713633903023742,
      "grad_norm": 0.8713261485099792,
      "learning_rate": 3.729269801980198e-05,
      "loss": 8.1585,
      "step": 4110
    },
    {
      "epoch": 1.2744567318846183,
      "grad_norm": 0.9413397312164307,
      "learning_rate": 3.7261757425742576e-05,
      "loss": 8.1543,
      "step": 4120
    },
    {
      "epoch": 1.2775500734668626,
      "grad_norm": 1.0500810146331787,
      "learning_rate": 3.723081683168317e-05,
      "loss": 8.1732,
      "step": 4130
    },
    {
      "epoch": 1.2806434150491068,
      "grad_norm": 1.8383299112319946,
      "learning_rate": 3.719987623762377e-05,
      "loss": 8.1751,
      "step": 4140
    },
    {
      "epoch": 1.2837367566313511,
      "grad_norm": 1.2703605890274048,
      "learning_rate": 3.7168935643564356e-05,
      "loss": 8.1746,
      "step": 4150
    },
    {
      "epoch": 1.2868300982135952,
      "grad_norm": 0.8107616901397705,
      "learning_rate": 3.713799504950495e-05,
      "loss": 8.1797,
      "step": 4160
    },
    {
      "epoch": 1.2899234397958395,
      "grad_norm": 1.1127413511276245,
      "learning_rate": 3.710705445544555e-05,
      "loss": 8.1818,
      "step": 4170
    },
    {
      "epoch": 1.2930167813780837,
      "grad_norm": 1.1264545917510986,
      "learning_rate": 3.707611386138614e-05,
      "loss": 8.1755,
      "step": 4180
    },
    {
      "epoch": 1.2961101229603278,
      "grad_norm": 0.8028554320335388,
      "learning_rate": 3.704517326732673e-05,
      "loss": 8.1634,
      "step": 4190
    },
    {
      "epoch": 1.299203464542572,
      "grad_norm": 1.2072283029556274,
      "learning_rate": 3.701423267326733e-05,
      "loss": 8.1653,
      "step": 4200
    },
    {
      "epoch": 1.3022968061248164,
      "grad_norm": 1.267712116241455,
      "learning_rate": 3.698329207920792e-05,
      "loss": 8.175,
      "step": 4210
    },
    {
      "epoch": 1.3053901477070606,
      "grad_norm": 0.8405670523643494,
      "learning_rate": 3.695235148514852e-05,
      "loss": 8.18,
      "step": 4220
    },
    {
      "epoch": 1.3084834892893047,
      "grad_norm": 1.5811591148376465,
      "learning_rate": 3.692141089108911e-05,
      "loss": 8.167,
      "step": 4230
    },
    {
      "epoch": 1.311576830871549,
      "grad_norm": 0.7154774069786072,
      "learning_rate": 3.689047029702971e-05,
      "loss": 8.1731,
      "step": 4240
    },
    {
      "epoch": 1.3146701724537933,
      "grad_norm": 0.5312008261680603,
      "learning_rate": 3.6859529702970296e-05,
      "loss": 8.1673,
      "step": 4250
    },
    {
      "epoch": 1.3177635140360375,
      "grad_norm": 0.9514366388320923,
      "learning_rate": 3.682858910891089e-05,
      "loss": 8.158,
      "step": 4260
    },
    {
      "epoch": 1.3208568556182816,
      "grad_norm": 0.7574382424354553,
      "learning_rate": 3.679764851485149e-05,
      "loss": 8.174,
      "step": 4270
    },
    {
      "epoch": 1.3239501972005259,
      "grad_norm": 0.7057363986968994,
      "learning_rate": 3.676670792079208e-05,
      "loss": 8.167,
      "step": 4280
    },
    {
      "epoch": 1.3270435387827701,
      "grad_norm": 0.7980016469955444,
      "learning_rate": 3.673576732673267e-05,
      "loss": 8.1763,
      "step": 4290
    },
    {
      "epoch": 1.3301368803650142,
      "grad_norm": 0.6745693683624268,
      "learning_rate": 3.670482673267327e-05,
      "loss": 8.1637,
      "step": 4300
    },
    {
      "epoch": 1.3332302219472585,
      "grad_norm": 1.2513898611068726,
      "learning_rate": 3.667388613861386e-05,
      "loss": 8.1529,
      "step": 4310
    },
    {
      "epoch": 1.3363235635295028,
      "grad_norm": 1.2123066186904907,
      "learning_rate": 3.664294554455446e-05,
      "loss": 8.1717,
      "step": 4320
    },
    {
      "epoch": 1.339416905111747,
      "grad_norm": 1.742858648300171,
      "learning_rate": 3.6612004950495046e-05,
      "loss": 8.1762,
      "step": 4330
    },
    {
      "epoch": 1.342510246693991,
      "grad_norm": 0.7529677748680115,
      "learning_rate": 3.658106435643565e-05,
      "loss": 8.1643,
      "step": 4340
    },
    {
      "epoch": 1.3456035882762354,
      "grad_norm": 1.1175521612167358,
      "learning_rate": 3.6550123762376236e-05,
      "loss": 8.1689,
      "step": 4350
    },
    {
      "epoch": 1.3486969298584797,
      "grad_norm": 0.8537497520446777,
      "learning_rate": 3.651918316831683e-05,
      "loss": 8.1774,
      "step": 4360
    },
    {
      "epoch": 1.351790271440724,
      "grad_norm": 0.7117443084716797,
      "learning_rate": 3.648824257425743e-05,
      "loss": 8.178,
      "step": 4370
    },
    {
      "epoch": 1.354883613022968,
      "grad_norm": 0.9373490810394287,
      "learning_rate": 3.645730198019802e-05,
      "loss": 8.1835,
      "step": 4380
    },
    {
      "epoch": 1.3579769546052123,
      "grad_norm": 1.2221544981002808,
      "learning_rate": 3.642636138613861e-05,
      "loss": 8.1704,
      "step": 4390
    },
    {
      "epoch": 1.3610702961874566,
      "grad_norm": 0.8514627814292908,
      "learning_rate": 3.6395420792079213e-05,
      "loss": 8.1624,
      "step": 4400
    },
    {
      "epoch": 1.3641636377697006,
      "grad_norm": 0.6535224318504333,
      "learning_rate": 3.63644801980198e-05,
      "loss": 8.1681,
      "step": 4410
    },
    {
      "epoch": 1.367256979351945,
      "grad_norm": 0.9356518387794495,
      "learning_rate": 3.63335396039604e-05,
      "loss": 8.1797,
      "step": 4420
    },
    {
      "epoch": 1.3703503209341892,
      "grad_norm": 0.5273188948631287,
      "learning_rate": 3.6302599009900986e-05,
      "loss": 8.1891,
      "step": 4430
    },
    {
      "epoch": 1.3734436625164332,
      "grad_norm": 1.5884488821029663,
      "learning_rate": 3.627165841584159e-05,
      "loss": 8.1714,
      "step": 4440
    },
    {
      "epoch": 1.3765370040986775,
      "grad_norm": 0.8049890995025635,
      "learning_rate": 3.624071782178218e-05,
      "loss": 8.1633,
      "step": 4450
    },
    {
      "epoch": 1.3796303456809218,
      "grad_norm": 0.8034205436706543,
      "learning_rate": 3.620977722772277e-05,
      "loss": 8.1739,
      "step": 4460
    },
    {
      "epoch": 1.382723687263166,
      "grad_norm": 0.9518395066261292,
      "learning_rate": 3.617883663366337e-05,
      "loss": 8.1705,
      "step": 4470
    },
    {
      "epoch": 1.3858170288454104,
      "grad_norm": 0.7396552562713623,
      "learning_rate": 3.614789603960396e-05,
      "loss": 8.1652,
      "step": 4480
    },
    {
      "epoch": 1.3889103704276544,
      "grad_norm": 0.6566534638404846,
      "learning_rate": 3.611695544554455e-05,
      "loss": 8.1705,
      "step": 4490
    },
    {
      "epoch": 1.3920037120098987,
      "grad_norm": 0.8734368681907654,
      "learning_rate": 3.6086014851485154e-05,
      "loss": 8.1588,
      "step": 4500
    },
    {
      "epoch": 1.395097053592143,
      "grad_norm": 1.3763115406036377,
      "learning_rate": 3.605507425742574e-05,
      "loss": 8.1649,
      "step": 4510
    },
    {
      "epoch": 1.398190395174387,
      "grad_norm": 0.5389499068260193,
      "learning_rate": 3.602413366336634e-05,
      "loss": 8.1608,
      "step": 4520
    },
    {
      "epoch": 1.4012837367566313,
      "grad_norm": 0.9031329154968262,
      "learning_rate": 3.5993193069306926e-05,
      "loss": 8.1834,
      "step": 4530
    },
    {
      "epoch": 1.4043770783388756,
      "grad_norm": 0.726360559463501,
      "learning_rate": 3.596225247524753e-05,
      "loss": 8.1831,
      "step": 4540
    },
    {
      "epoch": 1.4074704199211197,
      "grad_norm": 0.987770676612854,
      "learning_rate": 3.5931311881188124e-05,
      "loss": 8.1612,
      "step": 4550
    },
    {
      "epoch": 1.410563761503364,
      "grad_norm": 0.9864580631256104,
      "learning_rate": 3.590037128712871e-05,
      "loss": 8.1818,
      "step": 4560
    },
    {
      "epoch": 1.4136571030856082,
      "grad_norm": 0.8496710658073425,
      "learning_rate": 3.5869430693069315e-05,
      "loss": 8.1829,
      "step": 4570
    },
    {
      "epoch": 1.4167504446678525,
      "grad_norm": 1.5568605661392212,
      "learning_rate": 3.58384900990099e-05,
      "loss": 8.1683,
      "step": 4580
    },
    {
      "epoch": 1.4198437862500968,
      "grad_norm": 1.9154937267303467,
      "learning_rate": 3.58075495049505e-05,
      "loss": 8.1567,
      "step": 4590
    },
    {
      "epoch": 1.4229371278323408,
      "grad_norm": 2.359445095062256,
      "learning_rate": 3.5776608910891094e-05,
      "loss": 8.1525,
      "step": 4600
    },
    {
      "epoch": 1.4260304694145851,
      "grad_norm": 1.1888504028320312,
      "learning_rate": 3.574566831683169e-05,
      "loss": 8.1612,
      "step": 4610
    },
    {
      "epoch": 1.4291238109968294,
      "grad_norm": 0.7650144100189209,
      "learning_rate": 3.571472772277228e-05,
      "loss": 8.1683,
      "step": 4620
    },
    {
      "epoch": 1.4322171525790734,
      "grad_norm": 0.773417592048645,
      "learning_rate": 3.5683787128712873e-05,
      "loss": 8.1754,
      "step": 4630
    },
    {
      "epoch": 1.4353104941613177,
      "grad_norm": 0.7749471664428711,
      "learning_rate": 3.565284653465347e-05,
      "loss": 8.173,
      "step": 4640
    },
    {
      "epoch": 1.438403835743562,
      "grad_norm": 0.6494299173355103,
      "learning_rate": 3.5621905940594064e-05,
      "loss": 8.1753,
      "step": 4650
    },
    {
      "epoch": 1.441497177325806,
      "grad_norm": 0.673133134841919,
      "learning_rate": 3.559096534653465e-05,
      "loss": 8.1666,
      "step": 4660
    },
    {
      "epoch": 1.4445905189080503,
      "grad_norm": 0.5829264521598816,
      "learning_rate": 3.5560024752475255e-05,
      "loss": 8.1766,
      "step": 4670
    },
    {
      "epoch": 1.4476838604902946,
      "grad_norm": 0.7950640916824341,
      "learning_rate": 3.5529084158415844e-05,
      "loss": 8.1733,
      "step": 4680
    },
    {
      "epoch": 1.450777202072539,
      "grad_norm": 1.028467059135437,
      "learning_rate": 3.549814356435644e-05,
      "loss": 8.1629,
      "step": 4690
    },
    {
      "epoch": 1.4538705436547832,
      "grad_norm": 0.8920987844467163,
      "learning_rate": 3.5467202970297034e-05,
      "loss": 8.1721,
      "step": 4700
    },
    {
      "epoch": 1.4569638852370272,
      "grad_norm": 1.1075754165649414,
      "learning_rate": 3.543626237623763e-05,
      "loss": 8.1715,
      "step": 4710
    },
    {
      "epoch": 1.4600572268192715,
      "grad_norm": 0.9275581240653992,
      "learning_rate": 3.540532178217822e-05,
      "loss": 8.1765,
      "step": 4720
    },
    {
      "epoch": 1.4631505684015158,
      "grad_norm": 0.6811912059783936,
      "learning_rate": 3.5374381188118814e-05,
      "loss": 8.1715,
      "step": 4730
    },
    {
      "epoch": 1.4662439099837599,
      "grad_norm": 1.149160385131836,
      "learning_rate": 3.534344059405941e-05,
      "loss": 8.1683,
      "step": 4740
    },
    {
      "epoch": 1.4693372515660041,
      "grad_norm": 0.5867406129837036,
      "learning_rate": 3.5312500000000005e-05,
      "loss": 8.1667,
      "step": 4750
    },
    {
      "epoch": 1.4724305931482484,
      "grad_norm": 0.9364137053489685,
      "learning_rate": 3.528155940594059e-05,
      "loss": 8.1744,
      "step": 4760
    },
    {
      "epoch": 1.4755239347304925,
      "grad_norm": 0.628791332244873,
      "learning_rate": 3.5250618811881195e-05,
      "loss": 8.1709,
      "step": 4770
    },
    {
      "epoch": 1.4786172763127368,
      "grad_norm": 0.8404129147529602,
      "learning_rate": 3.5219678217821784e-05,
      "loss": 8.179,
      "step": 4780
    },
    {
      "epoch": 1.481710617894981,
      "grad_norm": 1.3416937589645386,
      "learning_rate": 3.518873762376238e-05,
      "loss": 8.1721,
      "step": 4790
    },
    {
      "epoch": 1.4848039594772253,
      "grad_norm": 0.6549462080001831,
      "learning_rate": 3.515779702970297e-05,
      "loss": 8.156,
      "step": 4800
    },
    {
      "epoch": 1.4878973010594696,
      "grad_norm": 1.0403521060943604,
      "learning_rate": 3.512685643564357e-05,
      "loss": 8.1714,
      "step": 4810
    },
    {
      "epoch": 1.4909906426417137,
      "grad_norm": 0.7095990180969238,
      "learning_rate": 3.509591584158416e-05,
      "loss": 8.168,
      "step": 4820
    },
    {
      "epoch": 1.494083984223958,
      "grad_norm": 0.7644259929656982,
      "learning_rate": 3.5064975247524754e-05,
      "loss": 8.1823,
      "step": 4830
    },
    {
      "epoch": 1.4971773258062022,
      "grad_norm": 0.5413919687271118,
      "learning_rate": 3.503403465346535e-05,
      "loss": 8.1622,
      "step": 4840
    },
    {
      "epoch": 1.5002706673884463,
      "grad_norm": 0.6657000184059143,
      "learning_rate": 3.5003094059405945e-05,
      "loss": 8.1561,
      "step": 4850
    },
    {
      "epoch": 1.5033640089706906,
      "grad_norm": 1.2642089128494263,
      "learning_rate": 3.4972153465346533e-05,
      "loss": 8.1566,
      "step": 4860
    },
    {
      "epoch": 1.5064573505529348,
      "grad_norm": 0.7570921778678894,
      "learning_rate": 3.4941212871287136e-05,
      "loss": 8.1686,
      "step": 4870
    },
    {
      "epoch": 1.509550692135179,
      "grad_norm": 0.6579694151878357,
      "learning_rate": 3.4910272277227724e-05,
      "loss": 8.1738,
      "step": 4880
    },
    {
      "epoch": 1.5126440337174234,
      "grad_norm": 1.0187548398971558,
      "learning_rate": 3.487933168316832e-05,
      "loss": 8.1708,
      "step": 4890
    },
    {
      "epoch": 1.5157373752996675,
      "grad_norm": 0.6494972705841064,
      "learning_rate": 3.484839108910891e-05,
      "loss": 8.1735,
      "step": 4900
    },
    {
      "epoch": 1.5188307168819117,
      "grad_norm": 1.3628088235855103,
      "learning_rate": 3.481745049504951e-05,
      "loss": 8.1639,
      "step": 4910
    },
    {
      "epoch": 1.521924058464156,
      "grad_norm": 1.2158830165863037,
      "learning_rate": 3.47865099009901e-05,
      "loss": 8.1749,
      "step": 4920
    },
    {
      "epoch": 1.5250174000464,
      "grad_norm": 0.6442435383796692,
      "learning_rate": 3.4755569306930694e-05,
      "loss": 8.1531,
      "step": 4930
    },
    {
      "epoch": 1.5281107416286444,
      "grad_norm": 0.6110523343086243,
      "learning_rate": 3.472462871287129e-05,
      "loss": 8.1767,
      "step": 4940
    },
    {
      "epoch": 1.5312040832108886,
      "grad_norm": 1.036623239517212,
      "learning_rate": 3.4693688118811885e-05,
      "loss": 8.1685,
      "step": 4950
    },
    {
      "epoch": 1.5342974247931327,
      "grad_norm": 1.135884404182434,
      "learning_rate": 3.4662747524752474e-05,
      "loss": 8.1831,
      "step": 4960
    },
    {
      "epoch": 1.537390766375377,
      "grad_norm": 0.7258022427558899,
      "learning_rate": 3.4631806930693076e-05,
      "loss": 8.1704,
      "step": 4970
    },
    {
      "epoch": 1.5404841079576213,
      "grad_norm": 0.7321982979774475,
      "learning_rate": 3.4600866336633665e-05,
      "loss": 8.1783,
      "step": 4980
    },
    {
      "epoch": 1.5435774495398653,
      "grad_norm": 1.108925700187683,
      "learning_rate": 3.456992574257426e-05,
      "loss": 8.1712,
      "step": 4990
    },
    {
      "epoch": 1.5466707911221098,
      "grad_norm": 0.5284704566001892,
      "learning_rate": 3.453898514851485e-05,
      "loss": 8.159,
      "step": 5000
    },
    {
      "epoch": 1.5497641327043539,
      "grad_norm": 1.830784797668457,
      "learning_rate": 3.450804455445545e-05,
      "loss": 8.1824,
      "step": 5010
    },
    {
      "epoch": 1.5528574742865981,
      "grad_norm": 1.6082335710525513,
      "learning_rate": 3.447710396039604e-05,
      "loss": 8.1504,
      "step": 5020
    },
    {
      "epoch": 1.5559508158688424,
      "grad_norm": 0.9002389907836914,
      "learning_rate": 3.4446163366336635e-05,
      "loss": 8.169,
      "step": 5030
    },
    {
      "epoch": 1.5590441574510865,
      "grad_norm": 0.7099100351333618,
      "learning_rate": 3.441522277227723e-05,
      "loss": 8.1614,
      "step": 5040
    },
    {
      "epoch": 1.5621374990333308,
      "grad_norm": 0.91901034116745,
      "learning_rate": 3.4384282178217825e-05,
      "loss": 8.1562,
      "step": 5050
    },
    {
      "epoch": 1.565230840615575,
      "grad_norm": 0.7427526116371155,
      "learning_rate": 3.4353341584158414e-05,
      "loss": 8.1818,
      "step": 5060
    },
    {
      "epoch": 1.568324182197819,
      "grad_norm": 0.7421169281005859,
      "learning_rate": 3.4322400990099016e-05,
      "loss": 8.1479,
      "step": 5070
    },
    {
      "epoch": 1.5714175237800634,
      "grad_norm": 0.693516194820404,
      "learning_rate": 3.4291460396039605e-05,
      "loss": 8.1683,
      "step": 5080
    },
    {
      "epoch": 1.5745108653623077,
      "grad_norm": 0.6199151277542114,
      "learning_rate": 3.42605198019802e-05,
      "loss": 8.1772,
      "step": 5090
    },
    {
      "epoch": 1.5776042069445517,
      "grad_norm": 0.8506600856781006,
      "learning_rate": 3.422957920792079e-05,
      "loss": 8.1751,
      "step": 5100
    },
    {
      "epoch": 1.5806975485267962,
      "grad_norm": 0.8408052325248718,
      "learning_rate": 3.419863861386139e-05,
      "loss": 8.1779,
      "step": 5110
    },
    {
      "epoch": 1.5837908901090403,
      "grad_norm": 0.6840189099311829,
      "learning_rate": 3.416769801980198e-05,
      "loss": 8.1611,
      "step": 5120
    },
    {
      "epoch": 1.5868842316912846,
      "grad_norm": 0.624847412109375,
      "learning_rate": 3.4136757425742575e-05,
      "loss": 8.1609,
      "step": 5130
    },
    {
      "epoch": 1.5899775732735288,
      "grad_norm": 0.6341762542724609,
      "learning_rate": 3.410581683168317e-05,
      "loss": 8.1673,
      "step": 5140
    },
    {
      "epoch": 1.593070914855773,
      "grad_norm": 1.453444004058838,
      "learning_rate": 3.4074876237623766e-05,
      "loss": 8.1828,
      "step": 5150
    },
    {
      "epoch": 1.5961642564380172,
      "grad_norm": 1.1536788940429688,
      "learning_rate": 3.4043935643564354e-05,
      "loss": 8.1663,
      "step": 5160
    },
    {
      "epoch": 1.5992575980202615,
      "grad_norm": 0.6205885410308838,
      "learning_rate": 3.401299504950495e-05,
      "loss": 8.1555,
      "step": 5170
    },
    {
      "epoch": 1.6023509396025055,
      "grad_norm": 0.7434297204017639,
      "learning_rate": 3.3982054455445545e-05,
      "loss": 8.1773,
      "step": 5180
    },
    {
      "epoch": 1.6054442811847498,
      "grad_norm": 0.7958365082740784,
      "learning_rate": 3.395111386138614e-05,
      "loss": 8.1703,
      "step": 5190
    },
    {
      "epoch": 1.608537622766994,
      "grad_norm": 1.394105076789856,
      "learning_rate": 3.392017326732673e-05,
      "loss": 8.1645,
      "step": 5200
    },
    {
      "epoch": 1.6116309643492381,
      "grad_norm": 0.7183541059494019,
      "learning_rate": 3.388923267326733e-05,
      "loss": 8.175,
      "step": 5210
    },
    {
      "epoch": 1.6147243059314826,
      "grad_norm": 1.2752171754837036,
      "learning_rate": 3.385829207920792e-05,
      "loss": 8.1518,
      "step": 5220
    },
    {
      "epoch": 1.6178176475137267,
      "grad_norm": 0.8980184197425842,
      "learning_rate": 3.3827351485148515e-05,
      "loss": 8.1762,
      "step": 5230
    },
    {
      "epoch": 1.620910989095971,
      "grad_norm": 0.9900051951408386,
      "learning_rate": 3.379641089108911e-05,
      "loss": 8.1836,
      "step": 5240
    },
    {
      "epoch": 1.6240043306782153,
      "grad_norm": 0.9499492645263672,
      "learning_rate": 3.3765470297029706e-05,
      "loss": 8.1739,
      "step": 5250
    },
    {
      "epoch": 1.6270976722604593,
      "grad_norm": 1.594128131866455,
      "learning_rate": 3.3734529702970295e-05,
      "loss": 8.1754,
      "step": 5260
    },
    {
      "epoch": 1.6301910138427036,
      "grad_norm": 0.9980596303939819,
      "learning_rate": 3.370358910891089e-05,
      "loss": 8.1728,
      "step": 5270
    },
    {
      "epoch": 1.6332843554249479,
      "grad_norm": 1.5016725063323975,
      "learning_rate": 3.3672648514851486e-05,
      "loss": 8.1959,
      "step": 5280
    },
    {
      "epoch": 1.636377697007192,
      "grad_norm": 0.5902643799781799,
      "learning_rate": 3.364170792079208e-05,
      "loss": 8.162,
      "step": 5290
    },
    {
      "epoch": 1.6394710385894362,
      "grad_norm": 0.7083823084831238,
      "learning_rate": 3.361076732673267e-05,
      "loss": 8.1684,
      "step": 5300
    },
    {
      "epoch": 1.6425643801716805,
      "grad_norm": 0.9555543661117554,
      "learning_rate": 3.357982673267327e-05,
      "loss": 8.1653,
      "step": 5310
    },
    {
      "epoch": 1.6456577217539246,
      "grad_norm": 0.49654459953308105,
      "learning_rate": 3.354888613861386e-05,
      "loss": 8.1635,
      "step": 5320
    },
    {
      "epoch": 1.648751063336169,
      "grad_norm": 1.0117039680480957,
      "learning_rate": 3.3517945544554456e-05,
      "loss": 8.1552,
      "step": 5330
    },
    {
      "epoch": 1.6518444049184131,
      "grad_norm": 0.8782178163528442,
      "learning_rate": 3.348700495049505e-05,
      "loss": 8.1592,
      "step": 5340
    },
    {
      "epoch": 1.6549377465006572,
      "grad_norm": 0.7171444296836853,
      "learning_rate": 3.3456064356435646e-05,
      "loss": 8.166,
      "step": 5350
    },
    {
      "epoch": 1.6580310880829017,
      "grad_norm": 0.9967385530471802,
      "learning_rate": 3.3425123762376235e-05,
      "loss": 8.1615,
      "step": 5360
    },
    {
      "epoch": 1.6611244296651457,
      "grad_norm": 0.9756525158882141,
      "learning_rate": 3.339418316831683e-05,
      "loss": 8.1733,
      "step": 5370
    },
    {
      "epoch": 1.66421777124739,
      "grad_norm": 1.3286625146865845,
      "learning_rate": 3.3363242574257426e-05,
      "loss": 8.1769,
      "step": 5380
    },
    {
      "epoch": 1.6673111128296343,
      "grad_norm": 1.2493183612823486,
      "learning_rate": 3.333230198019802e-05,
      "loss": 8.1651,
      "step": 5390
    },
    {
      "epoch": 1.6704044544118783,
      "grad_norm": 0.650644063949585,
      "learning_rate": 3.330136138613861e-05,
      "loss": 8.1739,
      "step": 5400
    },
    {
      "epoch": 1.6734977959941226,
      "grad_norm": 0.6978752017021179,
      "learning_rate": 3.327042079207921e-05,
      "loss": 8.1537,
      "step": 5410
    },
    {
      "epoch": 1.676591137576367,
      "grad_norm": 0.9390348792076111,
      "learning_rate": 3.32394801980198e-05,
      "loss": 8.1416,
      "step": 5420
    },
    {
      "epoch": 1.679684479158611,
      "grad_norm": 0.7109711766242981,
      "learning_rate": 3.3208539603960396e-05,
      "loss": 8.1503,
      "step": 5430
    },
    {
      "epoch": 1.6827778207408555,
      "grad_norm": 1.0785995721817017,
      "learning_rate": 3.317759900990099e-05,
      "loss": 8.1772,
      "step": 5440
    },
    {
      "epoch": 1.6858711623230995,
      "grad_norm": 1.2778241634368896,
      "learning_rate": 3.314665841584159e-05,
      "loss": 8.178,
      "step": 5450
    },
    {
      "epoch": 1.6889645039053436,
      "grad_norm": 0.6923041343688965,
      "learning_rate": 3.3115717821782175e-05,
      "loss": 8.1725,
      "step": 5460
    },
    {
      "epoch": 1.692057845487588,
      "grad_norm": 1.0062930583953857,
      "learning_rate": 3.308477722772277e-05,
      "loss": 8.179,
      "step": 5470
    },
    {
      "epoch": 1.6951511870698321,
      "grad_norm": 1.0806899070739746,
      "learning_rate": 3.3053836633663366e-05,
      "loss": 8.176,
      "step": 5480
    },
    {
      "epoch": 1.6982445286520764,
      "grad_norm": 0.7845777273178101,
      "learning_rate": 3.302289603960396e-05,
      "loss": 8.176,
      "step": 5490
    },
    {
      "epoch": 1.7013378702343207,
      "grad_norm": 0.81512850522995,
      "learning_rate": 3.299195544554455e-05,
      "loss": 8.1601,
      "step": 5500
    },
    {
      "epoch": 1.7044312118165648,
      "grad_norm": 0.7820336818695068,
      "learning_rate": 3.296101485148515e-05,
      "loss": 8.1617,
      "step": 5510
    },
    {
      "epoch": 1.707524553398809,
      "grad_norm": 0.8367799520492554,
      "learning_rate": 3.293007425742574e-05,
      "loss": 8.1832,
      "step": 5520
    },
    {
      "epoch": 1.7106178949810533,
      "grad_norm": 0.9358869194984436,
      "learning_rate": 3.2899133663366336e-05,
      "loss": 8.1586,
      "step": 5530
    },
    {
      "epoch": 1.7137112365632974,
      "grad_norm": 0.6722541451454163,
      "learning_rate": 3.286819306930693e-05,
      "loss": 8.1655,
      "step": 5540
    },
    {
      "epoch": 1.7168045781455419,
      "grad_norm": 0.6855424642562866,
      "learning_rate": 3.283725247524753e-05,
      "loss": 8.1733,
      "step": 5550
    },
    {
      "epoch": 1.719897919727786,
      "grad_norm": 1.004778504371643,
      "learning_rate": 3.280631188118812e-05,
      "loss": 8.161,
      "step": 5560
    },
    {
      "epoch": 1.72299126131003,
      "grad_norm": 0.9205971956253052,
      "learning_rate": 3.277537128712871e-05,
      "loss": 8.1722,
      "step": 5570
    },
    {
      "epoch": 1.7260846028922745,
      "grad_norm": 0.572397768497467,
      "learning_rate": 3.274443069306931e-05,
      "loss": 8.17,
      "step": 5580
    },
    {
      "epoch": 1.7291779444745186,
      "grad_norm": 1.0157551765441895,
      "learning_rate": 3.27134900990099e-05,
      "loss": 8.1613,
      "step": 5590
    },
    {
      "epoch": 1.7322712860567628,
      "grad_norm": 1.26455819606781,
      "learning_rate": 3.26825495049505e-05,
      "loss": 8.1859,
      "step": 5600
    },
    {
      "epoch": 1.7353646276390071,
      "grad_norm": 0.8484792709350586,
      "learning_rate": 3.265160891089109e-05,
      "loss": 8.1688,
      "step": 5610
    },
    {
      "epoch": 1.7384579692212512,
      "grad_norm": 1.1313810348510742,
      "learning_rate": 3.262066831683169e-05,
      "loss": 8.1737,
      "step": 5620
    },
    {
      "epoch": 1.7415513108034955,
      "grad_norm": 0.6978829503059387,
      "learning_rate": 3.259282178217822e-05,
      "loss": 8.1746,
      "step": 5630
    },
    {
      "epoch": 1.7446446523857397,
      "grad_norm": 1.0052316188812256,
      "learning_rate": 3.256188118811881e-05,
      "loss": 8.1788,
      "step": 5640
    },
    {
      "epoch": 1.7477379939679838,
      "grad_norm": 0.8323175311088562,
      "learning_rate": 3.253094059405941e-05,
      "loss": 8.1659,
      "step": 5650
    },
    {
      "epoch": 1.7508313355502283,
      "grad_norm": 0.907998263835907,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 8.1722,
      "step": 5660
    },
    {
      "epoch": 1.7539246771324724,
      "grad_norm": 0.90989089012146,
      "learning_rate": 3.246905940594059e-05,
      "loss": 8.18,
      "step": 5670
    },
    {
      "epoch": 1.7570180187147164,
      "grad_norm": 1.2065491676330566,
      "learning_rate": 3.243811881188119e-05,
      "loss": 8.1813,
      "step": 5680
    },
    {
      "epoch": 1.760111360296961,
      "grad_norm": 0.7596132755279541,
      "learning_rate": 3.240717821782178e-05,
      "loss": 8.1716,
      "step": 5690
    },
    {
      "epoch": 1.763204701879205,
      "grad_norm": 0.8433894515037537,
      "learning_rate": 3.237623762376238e-05,
      "loss": 8.1656,
      "step": 5700
    },
    {
      "epoch": 1.7662980434614493,
      "grad_norm": 0.9066236019134521,
      "learning_rate": 3.234529702970297e-05,
      "loss": 8.1789,
      "step": 5710
    },
    {
      "epoch": 1.7693913850436935,
      "grad_norm": 0.6621925234794617,
      "learning_rate": 3.231435643564357e-05,
      "loss": 8.1674,
      "step": 5720
    },
    {
      "epoch": 1.7724847266259376,
      "grad_norm": 0.7098196744918823,
      "learning_rate": 3.228341584158416e-05,
      "loss": 8.1626,
      "step": 5730
    },
    {
      "epoch": 1.7755780682081819,
      "grad_norm": 0.497574120759964,
      "learning_rate": 3.2252475247524753e-05,
      "loss": 8.173,
      "step": 5740
    },
    {
      "epoch": 1.7786714097904262,
      "grad_norm": 0.7098990678787231,
      "learning_rate": 3.222153465346535e-05,
      "loss": 8.1546,
      "step": 5750
    },
    {
      "epoch": 1.7817647513726702,
      "grad_norm": 0.9132131338119507,
      "learning_rate": 3.2190594059405944e-05,
      "loss": 8.1604,
      "step": 5760
    },
    {
      "epoch": 1.7848580929549147,
      "grad_norm": 0.8173969388008118,
      "learning_rate": 3.215965346534653e-05,
      "loss": 8.1703,
      "step": 5770
    },
    {
      "epoch": 1.7879514345371588,
      "grad_norm": 0.676900327205658,
      "learning_rate": 3.212871287128713e-05,
      "loss": 8.1643,
      "step": 5780
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 0.8337453007698059,
      "learning_rate": 3.2097772277227724e-05,
      "loss": 8.1678,
      "step": 5790
    },
    {
      "epoch": 1.7941381177016473,
      "grad_norm": 0.9127116799354553,
      "learning_rate": 3.206683168316832e-05,
      "loss": 8.1652,
      "step": 5800
    },
    {
      "epoch": 1.7972314592838914,
      "grad_norm": 0.5822204947471619,
      "learning_rate": 3.203589108910891e-05,
      "loss": 8.1677,
      "step": 5810
    },
    {
      "epoch": 1.8003248008661357,
      "grad_norm": 0.7787448167800903,
      "learning_rate": 3.200495049504951e-05,
      "loss": 8.1672,
      "step": 5820
    },
    {
      "epoch": 1.80341814244838,
      "grad_norm": 0.5487079620361328,
      "learning_rate": 3.19740099009901e-05,
      "loss": 8.1581,
      "step": 5830
    },
    {
      "epoch": 1.806511484030624,
      "grad_norm": 0.7901445031166077,
      "learning_rate": 3.1943069306930694e-05,
      "loss": 8.1562,
      "step": 5840
    },
    {
      "epoch": 1.8096048256128683,
      "grad_norm": 0.6084808707237244,
      "learning_rate": 3.191212871287129e-05,
      "loss": 8.1638,
      "step": 5850
    },
    {
      "epoch": 1.8126981671951126,
      "grad_norm": 0.6935718059539795,
      "learning_rate": 3.1881188118811885e-05,
      "loss": 8.1782,
      "step": 5860
    },
    {
      "epoch": 1.8157915087773566,
      "grad_norm": 1.0455889701843262,
      "learning_rate": 3.185024752475247e-05,
      "loss": 8.1525,
      "step": 5870
    },
    {
      "epoch": 1.8188848503596011,
      "grad_norm": 1.3935803174972534,
      "learning_rate": 3.181930693069307e-05,
      "loss": 8.1758,
      "step": 5880
    },
    {
      "epoch": 1.8219781919418452,
      "grad_norm": 1.8674882650375366,
      "learning_rate": 3.1788366336633664e-05,
      "loss": 8.1761,
      "step": 5890
    },
    {
      "epoch": 1.8250715335240892,
      "grad_norm": 1.8687641620635986,
      "learning_rate": 3.175742574257426e-05,
      "loss": 8.1575,
      "step": 5900
    },
    {
      "epoch": 1.8281648751063337,
      "grad_norm": 1.4557090997695923,
      "learning_rate": 3.172648514851485e-05,
      "loss": 8.1539,
      "step": 5910
    },
    {
      "epoch": 1.8312582166885778,
      "grad_norm": 1.0592007637023926,
      "learning_rate": 3.169554455445545e-05,
      "loss": 8.1688,
      "step": 5920
    },
    {
      "epoch": 1.834351558270822,
      "grad_norm": 1.8422770500183105,
      "learning_rate": 3.166460396039604e-05,
      "loss": 8.1682,
      "step": 5930
    },
    {
      "epoch": 1.8374448998530664,
      "grad_norm": 1.6825525760650635,
      "learning_rate": 3.1633663366336634e-05,
      "loss": 8.1757,
      "step": 5940
    },
    {
      "epoch": 1.8405382414353104,
      "grad_norm": 0.7014016509056091,
      "learning_rate": 3.160272277227723e-05,
      "loss": 8.1785,
      "step": 5950
    },
    {
      "epoch": 1.8436315830175547,
      "grad_norm": 0.8315780162811279,
      "learning_rate": 3.1571782178217825e-05,
      "loss": 8.1707,
      "step": 5960
    },
    {
      "epoch": 1.846724924599799,
      "grad_norm": 0.673385739326477,
      "learning_rate": 3.1540841584158413e-05,
      "loss": 8.1694,
      "step": 5970
    },
    {
      "epoch": 1.849818266182043,
      "grad_norm": 0.7577770352363586,
      "learning_rate": 3.150990099009901e-05,
      "loss": 8.156,
      "step": 5980
    },
    {
      "epoch": 1.8529116077642873,
      "grad_norm": 1.0061264038085938,
      "learning_rate": 3.1478960396039604e-05,
      "loss": 8.1631,
      "step": 5990
    },
    {
      "epoch": 1.8560049493465316,
      "grad_norm": 1.0586001873016357,
      "learning_rate": 3.14480198019802e-05,
      "loss": 8.1502,
      "step": 6000
    },
    {
      "epoch": 1.8590982909287757,
      "grad_norm": 0.6203423738479614,
      "learning_rate": 3.141707920792079e-05,
      "loss": 8.1797,
      "step": 6010
    },
    {
      "epoch": 1.8621916325110202,
      "grad_norm": 0.9820427894592285,
      "learning_rate": 3.138613861386139e-05,
      "loss": 8.1648,
      "step": 6020
    },
    {
      "epoch": 1.8652849740932642,
      "grad_norm": 0.8698956966400146,
      "learning_rate": 3.135519801980198e-05,
      "loss": 8.1695,
      "step": 6030
    },
    {
      "epoch": 1.8683783156755085,
      "grad_norm": 1.0272184610366821,
      "learning_rate": 3.1324257425742574e-05,
      "loss": 8.1791,
      "step": 6040
    },
    {
      "epoch": 1.8714716572577528,
      "grad_norm": 0.8092179894447327,
      "learning_rate": 3.129331683168317e-05,
      "loss": 8.1563,
      "step": 6050
    },
    {
      "epoch": 1.8745649988399968,
      "grad_norm": 0.8402934670448303,
      "learning_rate": 3.1262376237623765e-05,
      "loss": 8.1743,
      "step": 6060
    },
    {
      "epoch": 1.8776583404222411,
      "grad_norm": 1.0219727754592896,
      "learning_rate": 3.123143564356436e-05,
      "loss": 8.1753,
      "step": 6070
    },
    {
      "epoch": 1.8807516820044854,
      "grad_norm": 0.9831115007400513,
      "learning_rate": 3.120049504950495e-05,
      "loss": 8.1766,
      "step": 6080
    },
    {
      "epoch": 1.8838450235867295,
      "grad_norm": 0.42784062027931213,
      "learning_rate": 3.116955445544555e-05,
      "loss": 8.1617,
      "step": 6090
    },
    {
      "epoch": 1.8869383651689737,
      "grad_norm": 0.6545125842094421,
      "learning_rate": 3.113861386138614e-05,
      "loss": 8.1467,
      "step": 6100
    },
    {
      "epoch": 1.890031706751218,
      "grad_norm": 0.6908999085426331,
      "learning_rate": 3.1107673267326735e-05,
      "loss": 8.1599,
      "step": 6110
    },
    {
      "epoch": 1.893125048333462,
      "grad_norm": 0.546354353427887,
      "learning_rate": 3.107673267326733e-05,
      "loss": 8.1741,
      "step": 6120
    },
    {
      "epoch": 1.8962183899157066,
      "grad_norm": 0.6718780994415283,
      "learning_rate": 3.1045792079207926e-05,
      "loss": 8.1536,
      "step": 6130
    },
    {
      "epoch": 1.8993117314979506,
      "grad_norm": 0.8059448003768921,
      "learning_rate": 3.1014851485148515e-05,
      "loss": 8.1708,
      "step": 6140
    },
    {
      "epoch": 1.902405073080195,
      "grad_norm": 0.6182971596717834,
      "learning_rate": 3.098391089108911e-05,
      "loss": 8.1726,
      "step": 6150
    },
    {
      "epoch": 1.9054984146624392,
      "grad_norm": 0.7981633543968201,
      "learning_rate": 3.0952970297029706e-05,
      "loss": 8.1542,
      "step": 6160
    },
    {
      "epoch": 1.9085917562446832,
      "grad_norm": 0.6761003136634827,
      "learning_rate": 3.09220297029703e-05,
      "loss": 8.1601,
      "step": 6170
    },
    {
      "epoch": 1.9116850978269275,
      "grad_norm": 0.79880291223526,
      "learning_rate": 3.089108910891089e-05,
      "loss": 8.1725,
      "step": 6180
    },
    {
      "epoch": 1.9147784394091718,
      "grad_norm": 0.6769899725914001,
      "learning_rate": 3.086014851485149e-05,
      "loss": 8.1708,
      "step": 6190
    },
    {
      "epoch": 1.9178717809914159,
      "grad_norm": 0.5824911594390869,
      "learning_rate": 3.082920792079208e-05,
      "loss": 8.1796,
      "step": 6200
    },
    {
      "epoch": 1.9209651225736601,
      "grad_norm": 0.7731926441192627,
      "learning_rate": 3.0798267326732676e-05,
      "loss": 8.1647,
      "step": 6210
    },
    {
      "epoch": 1.9240584641559044,
      "grad_norm": 0.6705347895622253,
      "learning_rate": 3.076732673267327e-05,
      "loss": 8.1553,
      "step": 6220
    },
    {
      "epoch": 1.9271518057381485,
      "grad_norm": 0.8091216087341309,
      "learning_rate": 3.0736386138613866e-05,
      "loss": 8.1697,
      "step": 6230
    },
    {
      "epoch": 1.930245147320393,
      "grad_norm": 0.8407959938049316,
      "learning_rate": 3.0705445544554455e-05,
      "loss": 8.1808,
      "step": 6240
    },
    {
      "epoch": 1.933338488902637,
      "grad_norm": 0.7498672604560852,
      "learning_rate": 3.067450495049505e-05,
      "loss": 8.1439,
      "step": 6250
    },
    {
      "epoch": 1.9364318304848813,
      "grad_norm": 1.1580160856246948,
      "learning_rate": 3.0643564356435646e-05,
      "loss": 8.1694,
      "step": 6260
    },
    {
      "epoch": 1.9395251720671256,
      "grad_norm": 0.732685923576355,
      "learning_rate": 3.061262376237624e-05,
      "loss": 8.166,
      "step": 6270
    },
    {
      "epoch": 1.9426185136493697,
      "grad_norm": 0.8561883568763733,
      "learning_rate": 3.058168316831683e-05,
      "loss": 8.182,
      "step": 6280
    },
    {
      "epoch": 1.945711855231614,
      "grad_norm": 0.5598315596580505,
      "learning_rate": 3.055074257425743e-05,
      "loss": 8.1616,
      "step": 6290
    },
    {
      "epoch": 1.9488051968138582,
      "grad_norm": 0.6322380304336548,
      "learning_rate": 3.051980198019802e-05,
      "loss": 8.1677,
      "step": 6300
    },
    {
      "epoch": 1.9518985383961023,
      "grad_norm": 0.722793459892273,
      "learning_rate": 3.0488861386138616e-05,
      "loss": 8.166,
      "step": 6310
    },
    {
      "epoch": 1.9549918799783466,
      "grad_norm": 0.9101493954658508,
      "learning_rate": 3.0457920792079208e-05,
      "loss": 8.1549,
      "step": 6320
    },
    {
      "epoch": 1.9580852215605908,
      "grad_norm": 1.1809587478637695,
      "learning_rate": 3.0426980198019807e-05,
      "loss": 8.1551,
      "step": 6330
    },
    {
      "epoch": 1.961178563142835,
      "grad_norm": 0.9287338852882385,
      "learning_rate": 3.0396039603960395e-05,
      "loss": 8.1741,
      "step": 6340
    },
    {
      "epoch": 1.9642719047250794,
      "grad_norm": 0.6922351121902466,
      "learning_rate": 3.0365099009900994e-05,
      "loss": 8.1644,
      "step": 6350
    },
    {
      "epoch": 1.9673652463073235,
      "grad_norm": 0.626541018486023,
      "learning_rate": 3.0334158415841586e-05,
      "loss": 8.1686,
      "step": 6360
    },
    {
      "epoch": 1.9704585878895677,
      "grad_norm": 0.8416675925254822,
      "learning_rate": 3.030321782178218e-05,
      "loss": 8.1739,
      "step": 6370
    },
    {
      "epoch": 1.973551929471812,
      "grad_norm": 0.8686550259590149,
      "learning_rate": 3.0272277227722774e-05,
      "loss": 8.176,
      "step": 6380
    },
    {
      "epoch": 1.976645271054056,
      "grad_norm": 0.6437850594520569,
      "learning_rate": 3.024133663366337e-05,
      "loss": 8.1645,
      "step": 6390
    },
    {
      "epoch": 1.9797386126363004,
      "grad_norm": 0.7487887144088745,
      "learning_rate": 3.021039603960396e-05,
      "loss": 8.158,
      "step": 6400
    },
    {
      "epoch": 1.9828319542185446,
      "grad_norm": 0.7612968683242798,
      "learning_rate": 3.0179455445544556e-05,
      "loss": 8.1778,
      "step": 6410
    },
    {
      "epoch": 1.9859252958007887,
      "grad_norm": 1.4509085416793823,
      "learning_rate": 3.014851485148515e-05,
      "loss": 8.1678,
      "step": 6420
    },
    {
      "epoch": 1.989018637383033,
      "grad_norm": 0.44345802068710327,
      "learning_rate": 3.0117574257425747e-05,
      "loss": 8.1723,
      "step": 6430
    },
    {
      "epoch": 1.9921119789652773,
      "grad_norm": 0.5903739333152771,
      "learning_rate": 3.0086633663366336e-05,
      "loss": 8.1681,
      "step": 6440
    },
    {
      "epoch": 1.9952053205475213,
      "grad_norm": 0.8666324615478516,
      "learning_rate": 3.0055693069306934e-05,
      "loss": 8.1741,
      "step": 6450
    },
    {
      "epoch": 1.9982986621297658,
      "grad_norm": 0.654120683670044,
      "learning_rate": 3.0024752475247526e-05,
      "loss": 8.1606,
      "step": 6460
    },
    {
      "epoch": 2.00139200371201,
      "grad_norm": 0.7698984742164612,
      "learning_rate": 2.9993811881188122e-05,
      "loss": 8.1591,
      "step": 6470
    },
    {
      "epoch": 2.004485345294254,
      "grad_norm": 0.7580122351646423,
      "learning_rate": 2.9962871287128714e-05,
      "loss": 8.1712,
      "step": 6480
    },
    {
      "epoch": 2.0075786868764984,
      "grad_norm": 0.7067974209785461,
      "learning_rate": 2.993193069306931e-05,
      "loss": 8.1612,
      "step": 6490
    },
    {
      "epoch": 2.0106720284587425,
      "grad_norm": 0.6846701502799988,
      "learning_rate": 2.99009900990099e-05,
      "loss": 8.1682,
      "step": 6500
    },
    {
      "epoch": 2.013765370040987,
      "grad_norm": 0.8338038325309753,
      "learning_rate": 2.9870049504950497e-05,
      "loss": 8.1539,
      "step": 6510
    },
    {
      "epoch": 2.016858711623231,
      "grad_norm": 0.9943915605545044,
      "learning_rate": 2.983910891089109e-05,
      "loss": 8.171,
      "step": 6520
    },
    {
      "epoch": 2.019952053205475,
      "grad_norm": 0.6240391135215759,
      "learning_rate": 2.9808168316831687e-05,
      "loss": 8.1589,
      "step": 6530
    },
    {
      "epoch": 2.0230453947877196,
      "grad_norm": 0.8082389831542969,
      "learning_rate": 2.9777227722772276e-05,
      "loss": 8.1596,
      "step": 6540
    },
    {
      "epoch": 2.0261387363699637,
      "grad_norm": 0.6899868845939636,
      "learning_rate": 2.9746287128712875e-05,
      "loss": 8.1677,
      "step": 6550
    },
    {
      "epoch": 2.0292320779522077,
      "grad_norm": 0.6070777177810669,
      "learning_rate": 2.9715346534653467e-05,
      "loss": 8.1629,
      "step": 6560
    },
    {
      "epoch": 2.0323254195344522,
      "grad_norm": 1.0307278633117676,
      "learning_rate": 2.9684405940594062e-05,
      "loss": 8.1605,
      "step": 6570
    },
    {
      "epoch": 2.0354187611166963,
      "grad_norm": 0.6303502917289734,
      "learning_rate": 2.9653465346534654e-05,
      "loss": 8.1664,
      "step": 6580
    },
    {
      "epoch": 2.0385121026989403,
      "grad_norm": 1.0963951349258423,
      "learning_rate": 2.962252475247525e-05,
      "loss": 8.1652,
      "step": 6590
    },
    {
      "epoch": 2.041605444281185,
      "grad_norm": 1.1331210136413574,
      "learning_rate": 2.959158415841584e-05,
      "loss": 8.177,
      "step": 6600
    },
    {
      "epoch": 2.044698785863429,
      "grad_norm": 1.116052269935608,
      "learning_rate": 2.9560643564356437e-05,
      "loss": 8.1561,
      "step": 6610
    },
    {
      "epoch": 2.0477921274456734,
      "grad_norm": 0.6471065878868103,
      "learning_rate": 2.952970297029703e-05,
      "loss": 8.1582,
      "step": 6620
    },
    {
      "epoch": 2.0508854690279175,
      "grad_norm": 0.5943728089332581,
      "learning_rate": 2.9498762376237628e-05,
      "loss": 8.1648,
      "step": 6630
    },
    {
      "epoch": 2.0539788106101615,
      "grad_norm": 0.7635798454284668,
      "learning_rate": 2.9467821782178216e-05,
      "loss": 8.154,
      "step": 6640
    },
    {
      "epoch": 2.057072152192406,
      "grad_norm": 0.7453480958938599,
      "learning_rate": 2.9436881188118815e-05,
      "loss": 8.1725,
      "step": 6650
    },
    {
      "epoch": 2.06016549377465,
      "grad_norm": 0.7330225110054016,
      "learning_rate": 2.9405940594059407e-05,
      "loss": 8.1778,
      "step": 6660
    },
    {
      "epoch": 2.063258835356894,
      "grad_norm": 0.7854148745536804,
      "learning_rate": 2.9375000000000003e-05,
      "loss": 8.1498,
      "step": 6670
    },
    {
      "epoch": 2.0663521769391386,
      "grad_norm": 0.7260506749153137,
      "learning_rate": 2.9344059405940595e-05,
      "loss": 8.1476,
      "step": 6680
    },
    {
      "epoch": 2.0694455185213827,
      "grad_norm": 0.71758633852005,
      "learning_rate": 2.931311881188119e-05,
      "loss": 8.1657,
      "step": 6690
    },
    {
      "epoch": 2.0725388601036268,
      "grad_norm": 0.8062655925750732,
      "learning_rate": 2.9282178217821782e-05,
      "loss": 8.162,
      "step": 6700
    },
    {
      "epoch": 2.0756322016858713,
      "grad_norm": 0.8517839908599854,
      "learning_rate": 2.9251237623762377e-05,
      "loss": 8.1614,
      "step": 6710
    },
    {
      "epoch": 2.0787255432681153,
      "grad_norm": 0.9589406251907349,
      "learning_rate": 2.922029702970297e-05,
      "loss": 8.151,
      "step": 6720
    },
    {
      "epoch": 2.0818188848503594,
      "grad_norm": 1.050733208656311,
      "learning_rate": 2.9189356435643568e-05,
      "loss": 8.1591,
      "step": 6730
    },
    {
      "epoch": 2.084912226432604,
      "grad_norm": 0.7210530638694763,
      "learning_rate": 2.9158415841584157e-05,
      "loss": 8.1838,
      "step": 6740
    },
    {
      "epoch": 2.088005568014848,
      "grad_norm": 0.6137667894363403,
      "learning_rate": 2.9127475247524755e-05,
      "loss": 8.15,
      "step": 6750
    },
    {
      "epoch": 2.0910989095970924,
      "grad_norm": 1.1073970794677734,
      "learning_rate": 2.9096534653465347e-05,
      "loss": 8.1707,
      "step": 6760
    },
    {
      "epoch": 2.0941922511793365,
      "grad_norm": 0.9996305108070374,
      "learning_rate": 2.9065594059405943e-05,
      "loss": 8.1719,
      "step": 6770
    },
    {
      "epoch": 2.0972855927615806,
      "grad_norm": 0.7041974663734436,
      "learning_rate": 2.9034653465346535e-05,
      "loss": 8.1649,
      "step": 6780
    },
    {
      "epoch": 2.100378934343825,
      "grad_norm": 0.6175642609596252,
      "learning_rate": 2.900371287128713e-05,
      "loss": 8.172,
      "step": 6790
    },
    {
      "epoch": 2.103472275926069,
      "grad_norm": 0.6949612498283386,
      "learning_rate": 2.8972772277227722e-05,
      "loss": 8.1613,
      "step": 6800
    },
    {
      "epoch": 2.106565617508313,
      "grad_norm": 0.6679258346557617,
      "learning_rate": 2.8941831683168318e-05,
      "loss": 8.1651,
      "step": 6810
    },
    {
      "epoch": 2.1096589590905577,
      "grad_norm": 0.677106499671936,
      "learning_rate": 2.891089108910891e-05,
      "loss": 8.1718,
      "step": 6820
    },
    {
      "epoch": 2.1127523006728017,
      "grad_norm": 0.9725459218025208,
      "learning_rate": 2.887995049504951e-05,
      "loss": 8.1648,
      "step": 6830
    },
    {
      "epoch": 2.115845642255046,
      "grad_norm": 0.7129029035568237,
      "learning_rate": 2.8849009900990097e-05,
      "loss": 8.1662,
      "step": 6840
    },
    {
      "epoch": 2.1189389838372903,
      "grad_norm": 0.9916512966156006,
      "learning_rate": 2.8818069306930696e-05,
      "loss": 8.1634,
      "step": 6850
    },
    {
      "epoch": 2.1220323254195343,
      "grad_norm": 0.8249142169952393,
      "learning_rate": 2.8787128712871288e-05,
      "loss": 8.1624,
      "step": 6860
    },
    {
      "epoch": 2.125125667001779,
      "grad_norm": 0.8144975900650024,
      "learning_rate": 2.8756188118811883e-05,
      "loss": 8.1647,
      "step": 6870
    },
    {
      "epoch": 2.128219008584023,
      "grad_norm": 0.725691556930542,
      "learning_rate": 2.8725247524752475e-05,
      "loss": 8.1679,
      "step": 6880
    },
    {
      "epoch": 2.131312350166267,
      "grad_norm": 0.7131584286689758,
      "learning_rate": 2.869430693069307e-05,
      "loss": 8.1585,
      "step": 6890
    },
    {
      "epoch": 2.1344056917485115,
      "grad_norm": 0.5320817232131958,
      "learning_rate": 2.8663366336633663e-05,
      "loss": 8.1715,
      "step": 6900
    },
    {
      "epoch": 2.1374990333307555,
      "grad_norm": 0.6670334339141846,
      "learning_rate": 2.8632425742574258e-05,
      "loss": 8.1717,
      "step": 6910
    },
    {
      "epoch": 2.1405923749129996,
      "grad_norm": 0.929577648639679,
      "learning_rate": 2.860148514851485e-05,
      "loss": 8.1556,
      "step": 6920
    },
    {
      "epoch": 2.143685716495244,
      "grad_norm": 0.765360951423645,
      "learning_rate": 2.857054455445545e-05,
      "loss": 8.1693,
      "step": 6930
    },
    {
      "epoch": 2.146779058077488,
      "grad_norm": 1.0731624364852905,
      "learning_rate": 2.8539603960396037e-05,
      "loss": 8.1605,
      "step": 6940
    },
    {
      "epoch": 2.149872399659732,
      "grad_norm": 1.5375107526779175,
      "learning_rate": 2.8508663366336636e-05,
      "loss": 8.158,
      "step": 6950
    },
    {
      "epoch": 2.1529657412419767,
      "grad_norm": 1.0261706113815308,
      "learning_rate": 2.8477722772277228e-05,
      "loss": 8.1617,
      "step": 6960
    },
    {
      "epoch": 2.1560590828242208,
      "grad_norm": 0.7507632374763489,
      "learning_rate": 2.8446782178217823e-05,
      "loss": 8.1516,
      "step": 6970
    },
    {
      "epoch": 2.1591524244064653,
      "grad_norm": 0.9439650177955627,
      "learning_rate": 2.8415841584158415e-05,
      "loss": 8.1597,
      "step": 6980
    },
    {
      "epoch": 2.1622457659887093,
      "grad_norm": 0.700110673904419,
      "learning_rate": 2.838490099009901e-05,
      "loss": 8.17,
      "step": 6990
    },
    {
      "epoch": 2.1653391075709534,
      "grad_norm": 0.7831120491027832,
      "learning_rate": 2.8353960396039603e-05,
      "loss": 8.1678,
      "step": 7000
    },
    {
      "epoch": 2.168432449153198,
      "grad_norm": 0.6248547434806824,
      "learning_rate": 2.8323019801980198e-05,
      "loss": 8.1718,
      "step": 7010
    },
    {
      "epoch": 2.171525790735442,
      "grad_norm": 0.646549642086029,
      "learning_rate": 2.829207920792079e-05,
      "loss": 8.1733,
      "step": 7020
    },
    {
      "epoch": 2.174619132317686,
      "grad_norm": 0.582464873790741,
      "learning_rate": 2.826113861386139e-05,
      "loss": 8.1532,
      "step": 7030
    },
    {
      "epoch": 2.1777124738999305,
      "grad_norm": 1.1374576091766357,
      "learning_rate": 2.8230198019801978e-05,
      "loss": 8.1764,
      "step": 7040
    },
    {
      "epoch": 2.1808058154821746,
      "grad_norm": 0.9252243638038635,
      "learning_rate": 2.8199257425742576e-05,
      "loss": 8.1714,
      "step": 7050
    },
    {
      "epoch": 2.1838991570644186,
      "grad_norm": 0.6676756739616394,
      "learning_rate": 2.816831683168317e-05,
      "loss": 8.1674,
      "step": 7060
    },
    {
      "epoch": 2.186992498646663,
      "grad_norm": 0.6426259279251099,
      "learning_rate": 2.8137376237623764e-05,
      "loss": 8.1534,
      "step": 7070
    },
    {
      "epoch": 2.190085840228907,
      "grad_norm": 1.0050709247589111,
      "learning_rate": 2.810643564356436e-05,
      "loss": 8.149,
      "step": 7080
    },
    {
      "epoch": 2.1931791818111517,
      "grad_norm": 0.9734458923339844,
      "learning_rate": 2.807549504950495e-05,
      "loss": 8.1676,
      "step": 7090
    },
    {
      "epoch": 2.1962725233933957,
      "grad_norm": 0.5569804906845093,
      "learning_rate": 2.804455445544555e-05,
      "loss": 8.1546,
      "step": 7100
    },
    {
      "epoch": 2.19936586497564,
      "grad_norm": 1.1818209886550903,
      "learning_rate": 2.801361386138614e-05,
      "loss": 8.1664,
      "step": 7110
    },
    {
      "epoch": 2.2024592065578843,
      "grad_norm": 0.6266962885856628,
      "learning_rate": 2.7982673267326737e-05,
      "loss": 8.1494,
      "step": 7120
    },
    {
      "epoch": 2.2055525481401284,
      "grad_norm": 0.6685594320297241,
      "learning_rate": 2.795173267326733e-05,
      "loss": 8.171,
      "step": 7130
    },
    {
      "epoch": 2.2086458897223724,
      "grad_norm": 1.642941951751709,
      "learning_rate": 2.7920792079207925e-05,
      "loss": 8.1584,
      "step": 7140
    },
    {
      "epoch": 2.211739231304617,
      "grad_norm": 0.5903218984603882,
      "learning_rate": 2.7889851485148517e-05,
      "loss": 8.1649,
      "step": 7150
    },
    {
      "epoch": 2.214832572886861,
      "grad_norm": 0.9463276863098145,
      "learning_rate": 2.7858910891089112e-05,
      "loss": 8.1532,
      "step": 7160
    },
    {
      "epoch": 2.217925914469105,
      "grad_norm": 0.7609720230102539,
      "learning_rate": 2.7827970297029704e-05,
      "loss": 8.1643,
      "step": 7170
    },
    {
      "epoch": 2.2210192560513495,
      "grad_norm": 0.7662705183029175,
      "learning_rate": 2.77970297029703e-05,
      "loss": 8.1642,
      "step": 7180
    },
    {
      "epoch": 2.2241125976335936,
      "grad_norm": 0.7410362362861633,
      "learning_rate": 2.776608910891089e-05,
      "loss": 8.1528,
      "step": 7190
    },
    {
      "epoch": 2.227205939215838,
      "grad_norm": 0.8748669028282166,
      "learning_rate": 2.773514851485149e-05,
      "loss": 8.1669,
      "step": 7200
    },
    {
      "epoch": 2.230299280798082,
      "grad_norm": 0.9633622169494629,
      "learning_rate": 2.770420792079208e-05,
      "loss": 8.1513,
      "step": 7210
    },
    {
      "epoch": 2.233392622380326,
      "grad_norm": 0.6454031467437744,
      "learning_rate": 2.7673267326732678e-05,
      "loss": 8.1579,
      "step": 7220
    },
    {
      "epoch": 2.2364859639625707,
      "grad_norm": 0.9252223372459412,
      "learning_rate": 2.764232673267327e-05,
      "loss": 8.1795,
      "step": 7230
    },
    {
      "epoch": 2.2395793055448148,
      "grad_norm": 1.2264606952667236,
      "learning_rate": 2.7611386138613865e-05,
      "loss": 8.1577,
      "step": 7240
    },
    {
      "epoch": 2.242672647127059,
      "grad_norm": 1.2678428888320923,
      "learning_rate": 2.7580445544554457e-05,
      "loss": 8.1538,
      "step": 7250
    },
    {
      "epoch": 2.2457659887093033,
      "grad_norm": 0.9013413190841675,
      "learning_rate": 2.7549504950495052e-05,
      "loss": 8.1625,
      "step": 7260
    },
    {
      "epoch": 2.2488593302915474,
      "grad_norm": 0.9653320908546448,
      "learning_rate": 2.7518564356435644e-05,
      "loss": 8.1715,
      "step": 7270
    },
    {
      "epoch": 2.2519526718737914,
      "grad_norm": 1.1379119157791138,
      "learning_rate": 2.748762376237624e-05,
      "loss": 8.1631,
      "step": 7280
    },
    {
      "epoch": 2.255046013456036,
      "grad_norm": 0.7507748007774353,
      "learning_rate": 2.7456683168316832e-05,
      "loss": 8.1641,
      "step": 7290
    },
    {
      "epoch": 2.25813935503828,
      "grad_norm": 1.3790440559387207,
      "learning_rate": 2.742574257425743e-05,
      "loss": 8.1784,
      "step": 7300
    },
    {
      "epoch": 2.2612326966205245,
      "grad_norm": 1.3544920682907104,
      "learning_rate": 2.739480198019802e-05,
      "loss": 8.1528,
      "step": 7310
    },
    {
      "epoch": 2.2643260382027686,
      "grad_norm": 1.8672831058502197,
      "learning_rate": 2.7363861386138618e-05,
      "loss": 8.1623,
      "step": 7320
    },
    {
      "epoch": 2.2674193797850126,
      "grad_norm": 1.1109334230422974,
      "learning_rate": 2.733292079207921e-05,
      "loss": 8.1817,
      "step": 7330
    },
    {
      "epoch": 2.270512721367257,
      "grad_norm": 0.8270970582962036,
      "learning_rate": 2.7301980198019805e-05,
      "loss": 8.154,
      "step": 7340
    },
    {
      "epoch": 2.273606062949501,
      "grad_norm": 0.8774046301841736,
      "learning_rate": 2.7271039603960397e-05,
      "loss": 8.1746,
      "step": 7350
    },
    {
      "epoch": 2.2766994045317452,
      "grad_norm": 0.9053404331207275,
      "learning_rate": 2.7240099009900993e-05,
      "loss": 8.1548,
      "step": 7360
    },
    {
      "epoch": 2.2797927461139897,
      "grad_norm": 0.42046988010406494,
      "learning_rate": 2.7209158415841585e-05,
      "loss": 8.1573,
      "step": 7370
    },
    {
      "epoch": 2.282886087696234,
      "grad_norm": 0.6767187714576721,
      "learning_rate": 2.717821782178218e-05,
      "loss": 8.1733,
      "step": 7380
    },
    {
      "epoch": 2.285979429278478,
      "grad_norm": 1.2540231943130493,
      "learning_rate": 2.7147277227722772e-05,
      "loss": 8.1651,
      "step": 7390
    },
    {
      "epoch": 2.2890727708607224,
      "grad_norm": 0.49287891387939453,
      "learning_rate": 2.711633663366337e-05,
      "loss": 8.1672,
      "step": 7400
    },
    {
      "epoch": 2.2921661124429664,
      "grad_norm": 0.6965528130531311,
      "learning_rate": 2.708539603960396e-05,
      "loss": 8.1748,
      "step": 7410
    },
    {
      "epoch": 2.295259454025211,
      "grad_norm": 0.5771989822387695,
      "learning_rate": 2.7054455445544558e-05,
      "loss": 8.1636,
      "step": 7420
    },
    {
      "epoch": 2.298352795607455,
      "grad_norm": 0.6396113038063049,
      "learning_rate": 2.702351485148515e-05,
      "loss": 8.1439,
      "step": 7430
    },
    {
      "epoch": 2.301446137189699,
      "grad_norm": 0.7495498061180115,
      "learning_rate": 2.6992574257425746e-05,
      "loss": 8.1735,
      "step": 7440
    },
    {
      "epoch": 2.3045394787719435,
      "grad_norm": 0.9238883852958679,
      "learning_rate": 2.6961633663366338e-05,
      "loss": 8.1661,
      "step": 7450
    },
    {
      "epoch": 2.3076328203541876,
      "grad_norm": 0.632891058921814,
      "learning_rate": 2.6930693069306933e-05,
      "loss": 8.1525,
      "step": 7460
    },
    {
      "epoch": 2.3107261619364317,
      "grad_norm": 0.7909314632415771,
      "learning_rate": 2.6899752475247525e-05,
      "loss": 8.164,
      "step": 7470
    },
    {
      "epoch": 2.313819503518676,
      "grad_norm": 0.685536801815033,
      "learning_rate": 2.686881188118812e-05,
      "loss": 8.1486,
      "step": 7480
    },
    {
      "epoch": 2.31691284510092,
      "grad_norm": 0.6618837118148804,
      "learning_rate": 2.6837871287128712e-05,
      "loss": 8.1616,
      "step": 7490
    },
    {
      "epoch": 2.3200061866831643,
      "grad_norm": 0.6939462423324585,
      "learning_rate": 2.680693069306931e-05,
      "loss": 8.1747,
      "step": 7500
    },
    {
      "epoch": 2.3230995282654088,
      "grad_norm": 0.6607227325439453,
      "learning_rate": 2.67759900990099e-05,
      "loss": 8.156,
      "step": 7510
    },
    {
      "epoch": 2.326192869847653,
      "grad_norm": 0.8078633546829224,
      "learning_rate": 2.67450495049505e-05,
      "loss": 8.1581,
      "step": 7520
    },
    {
      "epoch": 2.3292862114298973,
      "grad_norm": 0.720177173614502,
      "learning_rate": 2.671410891089109e-05,
      "loss": 8.1671,
      "step": 7530
    },
    {
      "epoch": 2.3323795530121414,
      "grad_norm": 0.854478120803833,
      "learning_rate": 2.6683168316831686e-05,
      "loss": 8.1522,
      "step": 7540
    },
    {
      "epoch": 2.3354728945943855,
      "grad_norm": 0.7257182002067566,
      "learning_rate": 2.6652227722772278e-05,
      "loss": 8.1656,
      "step": 7550
    },
    {
      "epoch": 2.33856623617663,
      "grad_norm": 0.6538126468658447,
      "learning_rate": 2.6621287128712873e-05,
      "loss": 8.1596,
      "step": 7560
    },
    {
      "epoch": 2.341659577758874,
      "grad_norm": 0.6985321640968323,
      "learning_rate": 2.6590346534653465e-05,
      "loss": 8.1661,
      "step": 7570
    },
    {
      "epoch": 2.344752919341118,
      "grad_norm": 0.8281847834587097,
      "learning_rate": 2.655940594059406e-05,
      "loss": 8.1805,
      "step": 7580
    },
    {
      "epoch": 2.3478462609233626,
      "grad_norm": 0.7349568009376526,
      "learning_rate": 2.6528465346534653e-05,
      "loss": 8.151,
      "step": 7590
    },
    {
      "epoch": 2.3509396025056066,
      "grad_norm": 0.6287476420402527,
      "learning_rate": 2.649752475247525e-05,
      "loss": 8.1663,
      "step": 7600
    },
    {
      "epoch": 2.3540329440878507,
      "grad_norm": 0.6188287138938904,
      "learning_rate": 2.646658415841584e-05,
      "loss": 8.1659,
      "step": 7610
    },
    {
      "epoch": 2.357126285670095,
      "grad_norm": 0.744907796382904,
      "learning_rate": 2.643564356435644e-05,
      "loss": 8.1491,
      "step": 7620
    },
    {
      "epoch": 2.3602196272523392,
      "grad_norm": 0.8335258960723877,
      "learning_rate": 2.640470297029703e-05,
      "loss": 8.1738,
      "step": 7630
    },
    {
      "epoch": 2.3633129688345837,
      "grad_norm": 1.172412633895874,
      "learning_rate": 2.6373762376237626e-05,
      "loss": 8.1674,
      "step": 7640
    },
    {
      "epoch": 2.366406310416828,
      "grad_norm": 0.5382717251777649,
      "learning_rate": 2.634282178217822e-05,
      "loss": 8.1621,
      "step": 7650
    },
    {
      "epoch": 2.369499651999072,
      "grad_norm": 0.8791709542274475,
      "learning_rate": 2.6311881188118814e-05,
      "loss": 8.1588,
      "step": 7660
    },
    {
      "epoch": 2.3725929935813164,
      "grad_norm": 0.7024769186973572,
      "learning_rate": 2.6280940594059406e-05,
      "loss": 8.1769,
      "step": 7670
    },
    {
      "epoch": 2.3756863351635604,
      "grad_norm": 1.5313935279846191,
      "learning_rate": 2.625e-05,
      "loss": 8.204,
      "step": 7680
    },
    {
      "epoch": 2.3787796767458045,
      "grad_norm": 0.8079920411109924,
      "learning_rate": 2.6219059405940593e-05,
      "loss": 8.1602,
      "step": 7690
    },
    {
      "epoch": 2.381873018328049,
      "grad_norm": 0.6568776369094849,
      "learning_rate": 2.6188118811881192e-05,
      "loss": 8.1548,
      "step": 7700
    },
    {
      "epoch": 2.384966359910293,
      "grad_norm": 0.9198424816131592,
      "learning_rate": 2.615717821782178e-05,
      "loss": 8.1666,
      "step": 7710
    },
    {
      "epoch": 2.388059701492537,
      "grad_norm": 1.0026365518569946,
      "learning_rate": 2.612623762376238e-05,
      "loss": 8.1746,
      "step": 7720
    },
    {
      "epoch": 2.3911530430747816,
      "grad_norm": 1.2519922256469727,
      "learning_rate": 2.609529702970297e-05,
      "loss": 8.1602,
      "step": 7730
    },
    {
      "epoch": 2.3942463846570257,
      "grad_norm": 0.6697324514389038,
      "learning_rate": 2.6064356435643567e-05,
      "loss": 8.1594,
      "step": 7740
    },
    {
      "epoch": 2.39733972623927,
      "grad_norm": 0.7110733389854431,
      "learning_rate": 2.603341584158416e-05,
      "loss": 8.1685,
      "step": 7750
    },
    {
      "epoch": 2.400433067821514,
      "grad_norm": 0.8241046667098999,
      "learning_rate": 2.6002475247524754e-05,
      "loss": 8.1653,
      "step": 7760
    },
    {
      "epoch": 2.4035264094037583,
      "grad_norm": 0.6552348136901855,
      "learning_rate": 2.5971534653465346e-05,
      "loss": 8.1711,
      "step": 7770
    },
    {
      "epoch": 2.406619750986003,
      "grad_norm": 0.8765715956687927,
      "learning_rate": 2.594059405940594e-05,
      "loss": 8.1563,
      "step": 7780
    },
    {
      "epoch": 2.409713092568247,
      "grad_norm": 0.7670398950576782,
      "learning_rate": 2.5909653465346533e-05,
      "loss": 8.1683,
      "step": 7790
    },
    {
      "epoch": 2.412806434150491,
      "grad_norm": 0.999138593673706,
      "learning_rate": 2.5878712871287132e-05,
      "loss": 8.1684,
      "step": 7800
    },
    {
      "epoch": 2.4158997757327354,
      "grad_norm": 0.9029545187950134,
      "learning_rate": 2.584777227722772e-05,
      "loss": 8.1706,
      "step": 7810
    },
    {
      "epoch": 2.4189931173149795,
      "grad_norm": 0.8635143041610718,
      "learning_rate": 2.581683168316832e-05,
      "loss": 8.1629,
      "step": 7820
    },
    {
      "epoch": 2.4220864588972235,
      "grad_norm": 0.7534936666488647,
      "learning_rate": 2.578589108910891e-05,
      "loss": 8.1536,
      "step": 7830
    },
    {
      "epoch": 2.425179800479468,
      "grad_norm": 0.7551003694534302,
      "learning_rate": 2.5754950495049507e-05,
      "loss": 8.1615,
      "step": 7840
    },
    {
      "epoch": 2.428273142061712,
      "grad_norm": 0.8790870308876038,
      "learning_rate": 2.57240099009901e-05,
      "loss": 8.1728,
      "step": 7850
    },
    {
      "epoch": 2.4313664836439566,
      "grad_norm": 0.7133420705795288,
      "learning_rate": 2.5693069306930694e-05,
      "loss": 8.1659,
      "step": 7860
    },
    {
      "epoch": 2.4344598252262006,
      "grad_norm": 1.0700602531433105,
      "learning_rate": 2.5662128712871286e-05,
      "loss": 8.1663,
      "step": 7870
    },
    {
      "epoch": 2.4375531668084447,
      "grad_norm": 1.1287144422531128,
      "learning_rate": 2.5631188118811882e-05,
      "loss": 8.1486,
      "step": 7880
    },
    {
      "epoch": 2.440646508390689,
      "grad_norm": 0.8080631494522095,
      "learning_rate": 2.5600247524752474e-05,
      "loss": 8.1666,
      "step": 7890
    },
    {
      "epoch": 2.4437398499729333,
      "grad_norm": 0.6713945269584656,
      "learning_rate": 2.5569306930693073e-05,
      "loss": 8.1639,
      "step": 7900
    },
    {
      "epoch": 2.4468331915551773,
      "grad_norm": 0.6905048489570618,
      "learning_rate": 2.553836633663366e-05,
      "loss": 8.1839,
      "step": 7910
    },
    {
      "epoch": 2.449926533137422,
      "grad_norm": 0.6658698916435242,
      "learning_rate": 2.550742574257426e-05,
      "loss": 8.1607,
      "step": 7920
    },
    {
      "epoch": 2.453019874719666,
      "grad_norm": 0.7645483613014221,
      "learning_rate": 2.5476485148514852e-05,
      "loss": 8.1751,
      "step": 7930
    },
    {
      "epoch": 2.45611321630191,
      "grad_norm": 0.5381191968917847,
      "learning_rate": 2.5445544554455447e-05,
      "loss": 8.156,
      "step": 7940
    },
    {
      "epoch": 2.4592065578841544,
      "grad_norm": 0.580566942691803,
      "learning_rate": 2.541460396039604e-05,
      "loss": 8.1679,
      "step": 7950
    },
    {
      "epoch": 2.4622998994663985,
      "grad_norm": 1.0833381414413452,
      "learning_rate": 2.5383663366336635e-05,
      "loss": 8.1624,
      "step": 7960
    },
    {
      "epoch": 2.465393241048643,
      "grad_norm": 0.6436618566513062,
      "learning_rate": 2.5352722772277227e-05,
      "loss": 8.1855,
      "step": 7970
    },
    {
      "epoch": 2.468486582630887,
      "grad_norm": 0.6640287041664124,
      "learning_rate": 2.5321782178217822e-05,
      "loss": 8.1678,
      "step": 7980
    },
    {
      "epoch": 2.471579924213131,
      "grad_norm": 0.5078837275505066,
      "learning_rate": 2.5290841584158414e-05,
      "loss": 8.1607,
      "step": 7990
    },
    {
      "epoch": 2.4746732657953756,
      "grad_norm": 0.7582833170890808,
      "learning_rate": 2.5259900990099013e-05,
      "loss": 8.1583,
      "step": 8000
    },
    {
      "epoch": 2.4777666073776197,
      "grad_norm": 1.0093492269515991,
      "learning_rate": 2.52289603960396e-05,
      "loss": 8.16,
      "step": 8010
    },
    {
      "epoch": 2.4808599489598637,
      "grad_norm": 0.8601843118667603,
      "learning_rate": 2.51980198019802e-05,
      "loss": 8.1623,
      "step": 8020
    },
    {
      "epoch": 2.4839532905421082,
      "grad_norm": 0.8220055103302002,
      "learning_rate": 2.5167079207920792e-05,
      "loss": 8.1684,
      "step": 8030
    },
    {
      "epoch": 2.4870466321243523,
      "grad_norm": 0.7904735207557678,
      "learning_rate": 2.5136138613861388e-05,
      "loss": 8.1682,
      "step": 8040
    },
    {
      "epoch": 2.4901399737065963,
      "grad_norm": 1.0930078029632568,
      "learning_rate": 2.510519801980198e-05,
      "loss": 8.1745,
      "step": 8050
    },
    {
      "epoch": 2.493233315288841,
      "grad_norm": 0.8562052249908447,
      "learning_rate": 2.5074257425742575e-05,
      "loss": 8.1719,
      "step": 8060
    },
    {
      "epoch": 2.496326656871085,
      "grad_norm": 1.034310221672058,
      "learning_rate": 2.5043316831683167e-05,
      "loss": 8.163,
      "step": 8070
    },
    {
      "epoch": 2.4994199984533294,
      "grad_norm": 0.42895758152008057,
      "learning_rate": 2.5012376237623762e-05,
      "loss": 8.1699,
      "step": 8080
    },
    {
      "epoch": 2.5025133400355735,
      "grad_norm": 0.5394324660301208,
      "learning_rate": 2.4981435643564358e-05,
      "loss": 8.1616,
      "step": 8090
    },
    {
      "epoch": 2.5056066816178175,
      "grad_norm": 0.598774790763855,
      "learning_rate": 2.4950495049504953e-05,
      "loss": 8.1676,
      "step": 8100
    },
    {
      "epoch": 2.508700023200062,
      "grad_norm": 0.6807217001914978,
      "learning_rate": 2.4919554455445545e-05,
      "loss": 8.1733,
      "step": 8110
    },
    {
      "epoch": 2.511793364782306,
      "grad_norm": 0.856927752494812,
      "learning_rate": 2.488861386138614e-05,
      "loss": 8.1774,
      "step": 8120
    },
    {
      "epoch": 2.51488670636455,
      "grad_norm": 0.5002461075782776,
      "learning_rate": 2.4857673267326733e-05,
      "loss": 8.153,
      "step": 8130
    },
    {
      "epoch": 2.5179800479467946,
      "grad_norm": 0.6032215356826782,
      "learning_rate": 2.4826732673267328e-05,
      "loss": 8.1618,
      "step": 8140
    },
    {
      "epoch": 2.5210733895290387,
      "grad_norm": 0.8363120555877686,
      "learning_rate": 2.4795792079207923e-05,
      "loss": 8.1515,
      "step": 8150
    },
    {
      "epoch": 2.5241667311112828,
      "grad_norm": 0.729881227016449,
      "learning_rate": 2.4764851485148515e-05,
      "loss": 8.1568,
      "step": 8160
    },
    {
      "epoch": 2.5272600726935273,
      "grad_norm": 0.901315450668335,
      "learning_rate": 2.473391089108911e-05,
      "loss": 8.1647,
      "step": 8170
    },
    {
      "epoch": 2.5303534142757713,
      "grad_norm": 1.1737146377563477,
      "learning_rate": 2.4702970297029703e-05,
      "loss": 8.1768,
      "step": 8180
    },
    {
      "epoch": 2.533446755858016,
      "grad_norm": 0.5630459785461426,
      "learning_rate": 2.4672029702970298e-05,
      "loss": 8.1505,
      "step": 8190
    },
    {
      "epoch": 2.53654009744026,
      "grad_norm": 0.9626954793930054,
      "learning_rate": 2.4641089108910893e-05,
      "loss": 8.1471,
      "step": 8200
    },
    {
      "epoch": 2.539633439022504,
      "grad_norm": 1.0149751901626587,
      "learning_rate": 2.4610148514851485e-05,
      "loss": 8.1611,
      "step": 8210
    },
    {
      "epoch": 2.5427267806047484,
      "grad_norm": 0.9106602668762207,
      "learning_rate": 2.457920792079208e-05,
      "loss": 8.1625,
      "step": 8220
    },
    {
      "epoch": 2.5458201221869925,
      "grad_norm": 0.6391934156417847,
      "learning_rate": 2.4548267326732673e-05,
      "loss": 8.1739,
      "step": 8230
    },
    {
      "epoch": 2.5489134637692366,
      "grad_norm": 0.5842658281326294,
      "learning_rate": 2.4517326732673268e-05,
      "loss": 8.1531,
      "step": 8240
    },
    {
      "epoch": 2.552006805351481,
      "grad_norm": 0.7733488082885742,
      "learning_rate": 2.4486386138613864e-05,
      "loss": 8.1508,
      "step": 8250
    },
    {
      "epoch": 2.555100146933725,
      "grad_norm": 0.402291864156723,
      "learning_rate": 2.4455445544554456e-05,
      "loss": 8.1464,
      "step": 8260
    },
    {
      "epoch": 2.558193488515969,
      "grad_norm": 0.7660793662071228,
      "learning_rate": 2.442450495049505e-05,
      "loss": 8.156,
      "step": 8270
    },
    {
      "epoch": 2.5612868300982137,
      "grad_norm": 1.2346888780593872,
      "learning_rate": 2.4393564356435643e-05,
      "loss": 8.1433,
      "step": 8280
    },
    {
      "epoch": 2.5643801716804577,
      "grad_norm": 1.4248000383377075,
      "learning_rate": 2.436262376237624e-05,
      "loss": 8.1569,
      "step": 8290
    },
    {
      "epoch": 2.5674735132627022,
      "grad_norm": 1.3419773578643799,
      "learning_rate": 2.4331683168316834e-05,
      "loss": 8.1703,
      "step": 8300
    },
    {
      "epoch": 2.5705668548449463,
      "grad_norm": 0.6713047027587891,
      "learning_rate": 2.4300742574257426e-05,
      "loss": 8.1649,
      "step": 8310
    },
    {
      "epoch": 2.5736601964271903,
      "grad_norm": 0.5905224680900574,
      "learning_rate": 2.426980198019802e-05,
      "loss": 8.1513,
      "step": 8320
    },
    {
      "epoch": 2.576753538009435,
      "grad_norm": 0.5600820779800415,
      "learning_rate": 2.4238861386138613e-05,
      "loss": 8.1612,
      "step": 8330
    },
    {
      "epoch": 2.579846879591679,
      "grad_norm": 0.7674791812896729,
      "learning_rate": 2.420792079207921e-05,
      "loss": 8.1591,
      "step": 8340
    },
    {
      "epoch": 2.582940221173923,
      "grad_norm": 0.8495990037918091,
      "learning_rate": 2.4176980198019804e-05,
      "loss": 8.1713,
      "step": 8350
    },
    {
      "epoch": 2.5860335627561675,
      "grad_norm": 0.6062689423561096,
      "learning_rate": 2.4146039603960396e-05,
      "loss": 8.1624,
      "step": 8360
    },
    {
      "epoch": 2.5891269043384115,
      "grad_norm": 0.4999196231365204,
      "learning_rate": 2.411509900990099e-05,
      "loss": 8.1688,
      "step": 8370
    },
    {
      "epoch": 2.5922202459206556,
      "grad_norm": 0.5591761469841003,
      "learning_rate": 2.4084158415841583e-05,
      "loss": 8.1703,
      "step": 8380
    },
    {
      "epoch": 2.5953135875029,
      "grad_norm": 0.7950795292854309,
      "learning_rate": 2.405321782178218e-05,
      "loss": 8.1622,
      "step": 8390
    },
    {
      "epoch": 2.598406929085144,
      "grad_norm": 0.8166947960853577,
      "learning_rate": 2.4022277227722774e-05,
      "loss": 8.1586,
      "step": 8400
    },
    {
      "epoch": 2.6015002706673886,
      "grad_norm": 0.531068742275238,
      "learning_rate": 2.3991336633663366e-05,
      "loss": 8.1553,
      "step": 8410
    },
    {
      "epoch": 2.6045936122496327,
      "grad_norm": 0.9548463225364685,
      "learning_rate": 2.396039603960396e-05,
      "loss": 8.1409,
      "step": 8420
    },
    {
      "epoch": 2.6076869538318768,
      "grad_norm": 0.6498140692710876,
      "learning_rate": 2.3929455445544553e-05,
      "loss": 8.1736,
      "step": 8430
    },
    {
      "epoch": 2.6107802954141213,
      "grad_norm": 0.6012818217277527,
      "learning_rate": 2.389851485148515e-05,
      "loss": 8.1621,
      "step": 8440
    },
    {
      "epoch": 2.6138736369963653,
      "grad_norm": 0.6312527060508728,
      "learning_rate": 2.3867574257425744e-05,
      "loss": 8.1566,
      "step": 8450
    },
    {
      "epoch": 2.6169669785786094,
      "grad_norm": 0.6812205910682678,
      "learning_rate": 2.3836633663366336e-05,
      "loss": 8.1676,
      "step": 8460
    },
    {
      "epoch": 2.620060320160854,
      "grad_norm": 0.8099148273468018,
      "learning_rate": 2.380569306930693e-05,
      "loss": 8.177,
      "step": 8470
    },
    {
      "epoch": 2.623153661743098,
      "grad_norm": 0.5474889278411865,
      "learning_rate": 2.3774752475247524e-05,
      "loss": 8.1503,
      "step": 8480
    },
    {
      "epoch": 2.626247003325342,
      "grad_norm": 0.8319555521011353,
      "learning_rate": 2.374381188118812e-05,
      "loss": 8.1572,
      "step": 8490
    },
    {
      "epoch": 2.6293403449075865,
      "grad_norm": 1.3008440732955933,
      "learning_rate": 2.3712871287128714e-05,
      "loss": 8.1681,
      "step": 8500
    },
    {
      "epoch": 2.6324336864898306,
      "grad_norm": 1.316590666770935,
      "learning_rate": 2.3681930693069306e-05,
      "loss": 8.1492,
      "step": 8510
    },
    {
      "epoch": 2.635527028072075,
      "grad_norm": 0.7210091948509216,
      "learning_rate": 2.3650990099009902e-05,
      "loss": 8.1709,
      "step": 8520
    },
    {
      "epoch": 2.638620369654319,
      "grad_norm": 0.6746721267700195,
      "learning_rate": 2.3620049504950494e-05,
      "loss": 8.1602,
      "step": 8530
    },
    {
      "epoch": 2.641713711236563,
      "grad_norm": 0.43000873923301697,
      "learning_rate": 2.358910891089109e-05,
      "loss": 8.1453,
      "step": 8540
    },
    {
      "epoch": 2.6448070528188077,
      "grad_norm": 0.8498892784118652,
      "learning_rate": 2.3558168316831685e-05,
      "loss": 8.1552,
      "step": 8550
    },
    {
      "epoch": 2.6479003944010517,
      "grad_norm": 0.7756481766700745,
      "learning_rate": 2.3527227722772277e-05,
      "loss": 8.1769,
      "step": 8560
    },
    {
      "epoch": 2.650993735983296,
      "grad_norm": 0.5486235022544861,
      "learning_rate": 2.3496287128712872e-05,
      "loss": 8.1638,
      "step": 8570
    },
    {
      "epoch": 2.6540870775655403,
      "grad_norm": 0.4493403732776642,
      "learning_rate": 2.3465346534653464e-05,
      "loss": 8.1652,
      "step": 8580
    },
    {
      "epoch": 2.6571804191477844,
      "grad_norm": 0.7993671298027039,
      "learning_rate": 2.3434405940594063e-05,
      "loss": 8.17,
      "step": 8590
    },
    {
      "epoch": 2.6602737607300284,
      "grad_norm": 0.6535208821296692,
      "learning_rate": 2.3403465346534655e-05,
      "loss": 8.1403,
      "step": 8600
    },
    {
      "epoch": 2.663367102312273,
      "grad_norm": 0.7985489964485168,
      "learning_rate": 2.337252475247525e-05,
      "loss": 8.1589,
      "step": 8610
    },
    {
      "epoch": 2.666460443894517,
      "grad_norm": 0.7968044877052307,
      "learning_rate": 2.3341584158415846e-05,
      "loss": 8.1538,
      "step": 8620
    },
    {
      "epoch": 2.6695537854767615,
      "grad_norm": 0.6484664082527161,
      "learning_rate": 2.3310643564356438e-05,
      "loss": 8.1542,
      "step": 8630
    },
    {
      "epoch": 2.6726471270590055,
      "grad_norm": 0.45569244027137756,
      "learning_rate": 2.3279702970297033e-05,
      "loss": 8.1448,
      "step": 8640
    },
    {
      "epoch": 2.6757404686412496,
      "grad_norm": 0.7270462512969971,
      "learning_rate": 2.3248762376237625e-05,
      "loss": 8.1713,
      "step": 8650
    },
    {
      "epoch": 2.678833810223494,
      "grad_norm": 0.8093847036361694,
      "learning_rate": 2.321782178217822e-05,
      "loss": 8.1775,
      "step": 8660
    },
    {
      "epoch": 2.681927151805738,
      "grad_norm": 0.9767118692398071,
      "learning_rate": 2.3186881188118816e-05,
      "loss": 8.1672,
      "step": 8670
    },
    {
      "epoch": 2.685020493387982,
      "grad_norm": 0.6451356410980225,
      "learning_rate": 2.3155940594059408e-05,
      "loss": 8.1488,
      "step": 8680
    },
    {
      "epoch": 2.6881138349702267,
      "grad_norm": 0.9326886534690857,
      "learning_rate": 2.3125000000000003e-05,
      "loss": 8.1622,
      "step": 8690
    },
    {
      "epoch": 2.6912071765524708,
      "grad_norm": 0.5774797797203064,
      "learning_rate": 2.3094059405940595e-05,
      "loss": 8.1473,
      "step": 8700
    },
    {
      "epoch": 2.694300518134715,
      "grad_norm": 0.6374669671058655,
      "learning_rate": 2.306311881188119e-05,
      "loss": 8.1666,
      "step": 8710
    },
    {
      "epoch": 2.6973938597169593,
      "grad_norm": 1.0584473609924316,
      "learning_rate": 2.3032178217821786e-05,
      "loss": 8.1806,
      "step": 8720
    },
    {
      "epoch": 2.7004872012992034,
      "grad_norm": 1.2220503091812134,
      "learning_rate": 2.3001237623762378e-05,
      "loss": 8.1773,
      "step": 8730
    },
    {
      "epoch": 2.703580542881448,
      "grad_norm": 0.8357137441635132,
      "learning_rate": 2.2970297029702973e-05,
      "loss": 8.1532,
      "step": 8740
    },
    {
      "epoch": 2.706673884463692,
      "grad_norm": 0.7819618582725525,
      "learning_rate": 2.2939356435643565e-05,
      "loss": 8.1389,
      "step": 8750
    },
    {
      "epoch": 2.709767226045936,
      "grad_norm": 0.6992998719215393,
      "learning_rate": 2.290841584158416e-05,
      "loss": 8.1717,
      "step": 8760
    },
    {
      "epoch": 2.7128605676281805,
      "grad_norm": 0.7349309921264648,
      "learning_rate": 2.2877475247524756e-05,
      "loss": 8.1664,
      "step": 8770
    },
    {
      "epoch": 2.7159539092104246,
      "grad_norm": 0.7938475012779236,
      "learning_rate": 2.2846534653465348e-05,
      "loss": 8.1722,
      "step": 8780
    },
    {
      "epoch": 2.7190472507926686,
      "grad_norm": 1.0003968477249146,
      "learning_rate": 2.2815594059405943e-05,
      "loss": 8.1824,
      "step": 8790
    },
    {
      "epoch": 2.722140592374913,
      "grad_norm": 1.0018706321716309,
      "learning_rate": 2.2784653465346535e-05,
      "loss": 8.1713,
      "step": 8800
    },
    {
      "epoch": 2.725233933957157,
      "grad_norm": 1.0310972929000854,
      "learning_rate": 2.275371287128713e-05,
      "loss": 8.1465,
      "step": 8810
    },
    {
      "epoch": 2.7283272755394012,
      "grad_norm": 1.193655252456665,
      "learning_rate": 2.2722772277227726e-05,
      "loss": 8.1604,
      "step": 8820
    },
    {
      "epoch": 2.7314206171216457,
      "grad_norm": 0.8337824940681458,
      "learning_rate": 2.2691831683168318e-05,
      "loss": 8.1783,
      "step": 8830
    },
    {
      "epoch": 2.73451395870389,
      "grad_norm": 0.6390815377235413,
      "learning_rate": 2.2660891089108914e-05,
      "loss": 8.1496,
      "step": 8840
    },
    {
      "epoch": 2.7376073002861343,
      "grad_norm": 0.6089914441108704,
      "learning_rate": 2.2629950495049506e-05,
      "loss": 8.1715,
      "step": 8850
    },
    {
      "epoch": 2.7407006418683784,
      "grad_norm": 0.7705383896827698,
      "learning_rate": 2.25990099009901e-05,
      "loss": 8.1531,
      "step": 8860
    },
    {
      "epoch": 2.7437939834506224,
      "grad_norm": 0.7629464268684387,
      "learning_rate": 2.2568069306930696e-05,
      "loss": 8.1675,
      "step": 8870
    },
    {
      "epoch": 2.7468873250328665,
      "grad_norm": 0.964788019657135,
      "learning_rate": 2.253712871287129e-05,
      "loss": 8.1678,
      "step": 8880
    },
    {
      "epoch": 2.749980666615111,
      "grad_norm": 0.9144389629364014,
      "learning_rate": 2.2506188118811884e-05,
      "loss": 8.147,
      "step": 8890
    },
    {
      "epoch": 2.753074008197355,
      "grad_norm": 0.5942326188087463,
      "learning_rate": 2.2475247524752476e-05,
      "loss": 8.1526,
      "step": 8900
    },
    {
      "epoch": 2.7561673497795995,
      "grad_norm": 0.8601642847061157,
      "learning_rate": 2.244430693069307e-05,
      "loss": 8.1604,
      "step": 8910
    },
    {
      "epoch": 2.7592606913618436,
      "grad_norm": 1.0043092966079712,
      "learning_rate": 2.2413366336633666e-05,
      "loss": 8.1683,
      "step": 8920
    },
    {
      "epoch": 2.7623540329440877,
      "grad_norm": 0.5821577906608582,
      "learning_rate": 2.238242574257426e-05,
      "loss": 8.1657,
      "step": 8930
    },
    {
      "epoch": 2.765447374526332,
      "grad_norm": 0.8371016383171082,
      "learning_rate": 2.2351485148514854e-05,
      "loss": 8.1571,
      "step": 8940
    },
    {
      "epoch": 2.768540716108576,
      "grad_norm": 1.0573420524597168,
      "learning_rate": 2.2320544554455446e-05,
      "loss": 8.1528,
      "step": 8950
    },
    {
      "epoch": 2.7716340576908207,
      "grad_norm": 0.783964216709137,
      "learning_rate": 2.228960396039604e-05,
      "loss": 8.1458,
      "step": 8960
    },
    {
      "epoch": 2.7747273992730648,
      "grad_norm": 0.703933596611023,
      "learning_rate": 2.2258663366336637e-05,
      "loss": 8.1811,
      "step": 8970
    },
    {
      "epoch": 2.777820740855309,
      "grad_norm": 0.9339376091957092,
      "learning_rate": 2.222772277227723e-05,
      "loss": 8.1735,
      "step": 8980
    },
    {
      "epoch": 2.780914082437553,
      "grad_norm": 0.672668993473053,
      "learning_rate": 2.2196782178217824e-05,
      "loss": 8.1708,
      "step": 8990
    },
    {
      "epoch": 2.7840074240197974,
      "grad_norm": 0.6629233956336975,
      "learning_rate": 2.2165841584158416e-05,
      "loss": 8.1722,
      "step": 9000
    },
    {
      "epoch": 2.7871007656020415,
      "grad_norm": 0.7750963568687439,
      "learning_rate": 2.213490099009901e-05,
      "loss": 8.162,
      "step": 9010
    },
    {
      "epoch": 2.790194107184286,
      "grad_norm": 0.737922728061676,
      "learning_rate": 2.2103960396039607e-05,
      "loss": 8.1604,
      "step": 9020
    },
    {
      "epoch": 2.79328744876653,
      "grad_norm": 0.9375251531600952,
      "learning_rate": 2.20730198019802e-05,
      "loss": 8.1617,
      "step": 9030
    },
    {
      "epoch": 2.796380790348774,
      "grad_norm": 0.758112907409668,
      "learning_rate": 2.2042079207920794e-05,
      "loss": 8.1663,
      "step": 9040
    },
    {
      "epoch": 2.7994741319310186,
      "grad_norm": 0.6534364223480225,
      "learning_rate": 2.2011138613861386e-05,
      "loss": 8.1661,
      "step": 9050
    },
    {
      "epoch": 2.8025674735132626,
      "grad_norm": 0.8108487129211426,
      "learning_rate": 2.198019801980198e-05,
      "loss": 8.1707,
      "step": 9060
    },
    {
      "epoch": 2.805660815095507,
      "grad_norm": 0.7406586408615112,
      "learning_rate": 2.1949257425742577e-05,
      "loss": 8.1665,
      "step": 9070
    },
    {
      "epoch": 2.808754156677751,
      "grad_norm": 0.505652129650116,
      "learning_rate": 2.191831683168317e-05,
      "loss": 8.167,
      "step": 9080
    },
    {
      "epoch": 2.8118474982599952,
      "grad_norm": 0.5404419898986816,
      "learning_rate": 2.1887376237623764e-05,
      "loss": 8.1607,
      "step": 9090
    },
    {
      "epoch": 2.8149408398422393,
      "grad_norm": 0.7562737464904785,
      "learning_rate": 2.1856435643564356e-05,
      "loss": 8.1587,
      "step": 9100
    },
    {
      "epoch": 2.818034181424484,
      "grad_norm": 1.4662718772888184,
      "learning_rate": 2.1825495049504952e-05,
      "loss": 8.1492,
      "step": 9110
    },
    {
      "epoch": 2.821127523006728,
      "grad_norm": 0.9297873973846436,
      "learning_rate": 2.1794554455445547e-05,
      "loss": 8.1734,
      "step": 9120
    },
    {
      "epoch": 2.8242208645889724,
      "grad_norm": 0.7422440648078918,
      "learning_rate": 2.176361386138614e-05,
      "loss": 8.1586,
      "step": 9130
    },
    {
      "epoch": 2.8273142061712164,
      "grad_norm": 0.7038840055465698,
      "learning_rate": 2.1732673267326734e-05,
      "loss": 8.1584,
      "step": 9140
    },
    {
      "epoch": 2.8304075477534605,
      "grad_norm": 0.760150671005249,
      "learning_rate": 2.1701732673267326e-05,
      "loss": 8.1442,
      "step": 9150
    },
    {
      "epoch": 2.833500889335705,
      "grad_norm": 0.7814859747886658,
      "learning_rate": 2.1670792079207922e-05,
      "loss": 8.152,
      "step": 9160
    },
    {
      "epoch": 2.836594230917949,
      "grad_norm": 0.7698177695274353,
      "learning_rate": 2.1639851485148517e-05,
      "loss": 8.1537,
      "step": 9170
    },
    {
      "epoch": 2.8396875725001935,
      "grad_norm": 1.38546621799469,
      "learning_rate": 2.160891089108911e-05,
      "loss": 8.1778,
      "step": 9180
    },
    {
      "epoch": 2.8427809140824376,
      "grad_norm": 1.0584040880203247,
      "learning_rate": 2.1577970297029705e-05,
      "loss": 8.1601,
      "step": 9190
    },
    {
      "epoch": 2.8458742556646817,
      "grad_norm": 0.4982001781463623,
      "learning_rate": 2.1547029702970297e-05,
      "loss": 8.164,
      "step": 9200
    },
    {
      "epoch": 2.8489675972469257,
      "grad_norm": 0.6727417707443237,
      "learning_rate": 2.1516089108910892e-05,
      "loss": 8.1622,
      "step": 9210
    },
    {
      "epoch": 2.8520609388291702,
      "grad_norm": 0.7641196846961975,
      "learning_rate": 2.1485148514851487e-05,
      "loss": 8.1727,
      "step": 9220
    },
    {
      "epoch": 2.8551542804114143,
      "grad_norm": 0.7569591999053955,
      "learning_rate": 2.145420792079208e-05,
      "loss": 8.1854,
      "step": 9230
    },
    {
      "epoch": 2.858247621993659,
      "grad_norm": 0.8037167191505432,
      "learning_rate": 2.1423267326732675e-05,
      "loss": 8.1705,
      "step": 9240
    },
    {
      "epoch": 2.861340963575903,
      "grad_norm": 0.7027794122695923,
      "learning_rate": 2.1392326732673267e-05,
      "loss": 8.1623,
      "step": 9250
    },
    {
      "epoch": 2.864434305158147,
      "grad_norm": 0.7689963579177856,
      "learning_rate": 2.1361386138613862e-05,
      "loss": 8.1481,
      "step": 9260
    },
    {
      "epoch": 2.8675276467403914,
      "grad_norm": 0.6874437928199768,
      "learning_rate": 2.1330445544554458e-05,
      "loss": 8.1561,
      "step": 9270
    },
    {
      "epoch": 2.8706209883226355,
      "grad_norm": 0.40278592705726624,
      "learning_rate": 2.129950495049505e-05,
      "loss": 8.172,
      "step": 9280
    },
    {
      "epoch": 2.87371432990488,
      "grad_norm": 0.794599175453186,
      "learning_rate": 2.1268564356435645e-05,
      "loss": 8.172,
      "step": 9290
    },
    {
      "epoch": 2.876807671487124,
      "grad_norm": 0.8359684944152832,
      "learning_rate": 2.1237623762376237e-05,
      "loss": 8.1716,
      "step": 9300
    },
    {
      "epoch": 2.879901013069368,
      "grad_norm": 0.7892533540725708,
      "learning_rate": 2.1206683168316832e-05,
      "loss": 8.1628,
      "step": 9310
    },
    {
      "epoch": 2.882994354651612,
      "grad_norm": 0.6688382029533386,
      "learning_rate": 2.1175742574257428e-05,
      "loss": 8.1578,
      "step": 9320
    },
    {
      "epoch": 2.8860876962338566,
      "grad_norm": 0.9846413731575012,
      "learning_rate": 2.114480198019802e-05,
      "loss": 8.176,
      "step": 9330
    },
    {
      "epoch": 2.8891810378161007,
      "grad_norm": 0.8967652916908264,
      "learning_rate": 2.1113861386138615e-05,
      "loss": 8.1715,
      "step": 9340
    },
    {
      "epoch": 2.892274379398345,
      "grad_norm": 0.8563565611839294,
      "learning_rate": 2.1082920792079207e-05,
      "loss": 8.1638,
      "step": 9350
    },
    {
      "epoch": 2.8953677209805893,
      "grad_norm": 0.5658190846443176,
      "learning_rate": 2.1051980198019803e-05,
      "loss": 8.1418,
      "step": 9360
    },
    {
      "epoch": 2.8984610625628333,
      "grad_norm": 0.8699821829795837,
      "learning_rate": 2.1021039603960398e-05,
      "loss": 8.157,
      "step": 9370
    },
    {
      "epoch": 2.901554404145078,
      "grad_norm": 0.815542995929718,
      "learning_rate": 2.099009900990099e-05,
      "loss": 8.1485,
      "step": 9380
    },
    {
      "epoch": 2.904647745727322,
      "grad_norm": 0.644237756729126,
      "learning_rate": 2.0959158415841585e-05,
      "loss": 8.1544,
      "step": 9390
    },
    {
      "epoch": 2.9077410873095664,
      "grad_norm": 0.747269868850708,
      "learning_rate": 2.0928217821782177e-05,
      "loss": 8.1671,
      "step": 9400
    },
    {
      "epoch": 2.9108344288918104,
      "grad_norm": 0.6769375205039978,
      "learning_rate": 2.0897277227722773e-05,
      "loss": 8.1507,
      "step": 9410
    },
    {
      "epoch": 2.9139277704740545,
      "grad_norm": 0.5300764441490173,
      "learning_rate": 2.0866336633663368e-05,
      "loss": 8.1648,
      "step": 9420
    },
    {
      "epoch": 2.9170211120562985,
      "grad_norm": 0.48915091156959534,
      "learning_rate": 2.083539603960396e-05,
      "loss": 8.1711,
      "step": 9430
    },
    {
      "epoch": 2.920114453638543,
      "grad_norm": 0.8020381927490234,
      "learning_rate": 2.0804455445544555e-05,
      "loss": 8.169,
      "step": 9440
    },
    {
      "epoch": 2.923207795220787,
      "grad_norm": 0.6952884793281555,
      "learning_rate": 2.0773514851485147e-05,
      "loss": 8.1655,
      "step": 9450
    },
    {
      "epoch": 2.9263011368030316,
      "grad_norm": 0.7596636414527893,
      "learning_rate": 2.0742574257425743e-05,
      "loss": 8.166,
      "step": 9460
    },
    {
      "epoch": 2.9293944783852757,
      "grad_norm": 0.5710711479187012,
      "learning_rate": 2.0711633663366338e-05,
      "loss": 8.162,
      "step": 9470
    },
    {
      "epoch": 2.9324878199675197,
      "grad_norm": 0.9549721479415894,
      "learning_rate": 2.068069306930693e-05,
      "loss": 8.185,
      "step": 9480
    },
    {
      "epoch": 2.9355811615497642,
      "grad_norm": 0.772719144821167,
      "learning_rate": 2.0649752475247526e-05,
      "loss": 8.158,
      "step": 9490
    },
    {
      "epoch": 2.9386745031320083,
      "grad_norm": 0.7774770259857178,
      "learning_rate": 2.0618811881188118e-05,
      "loss": 8.1594,
      "step": 9500
    },
    {
      "epoch": 2.941767844714253,
      "grad_norm": 0.8801842331886292,
      "learning_rate": 2.0587871287128713e-05,
      "loss": 8.158,
      "step": 9510
    },
    {
      "epoch": 2.944861186296497,
      "grad_norm": 0.5895189642906189,
      "learning_rate": 2.055693069306931e-05,
      "loss": 8.1646,
      "step": 9520
    },
    {
      "epoch": 2.947954527878741,
      "grad_norm": 0.5793755650520325,
      "learning_rate": 2.05259900990099e-05,
      "loss": 8.1525,
      "step": 9530
    },
    {
      "epoch": 2.951047869460985,
      "grad_norm": 0.9398245215415955,
      "learning_rate": 2.0495049504950496e-05,
      "loss": 8.1563,
      "step": 9540
    },
    {
      "epoch": 2.9541412110432295,
      "grad_norm": 0.9480753540992737,
      "learning_rate": 2.0464108910891088e-05,
      "loss": 8.1702,
      "step": 9550
    },
    {
      "epoch": 2.9572345526254735,
      "grad_norm": 1.1020456552505493,
      "learning_rate": 2.0433168316831683e-05,
      "loss": 8.1798,
      "step": 9560
    },
    {
      "epoch": 2.960327894207718,
      "grad_norm": 0.7483857870101929,
      "learning_rate": 2.040222772277228e-05,
      "loss": 8.1578,
      "step": 9570
    },
    {
      "epoch": 2.963421235789962,
      "grad_norm": 0.8119867444038391,
      "learning_rate": 2.037128712871287e-05,
      "loss": 8.1516,
      "step": 9580
    },
    {
      "epoch": 2.966514577372206,
      "grad_norm": 0.8297714591026306,
      "learning_rate": 2.0340346534653466e-05,
      "loss": 8.1805,
      "step": 9590
    },
    {
      "epoch": 2.9696079189544506,
      "grad_norm": 0.9739369750022888,
      "learning_rate": 2.030940594059406e-05,
      "loss": 8.1674,
      "step": 9600
    },
    {
      "epoch": 2.9727012605366947,
      "grad_norm": 0.9883198738098145,
      "learning_rate": 2.0278465346534657e-05,
      "loss": 8.167,
      "step": 9610
    },
    {
      "epoch": 2.975794602118939,
      "grad_norm": 0.9259018898010254,
      "learning_rate": 2.024752475247525e-05,
      "loss": 8.1637,
      "step": 9620
    },
    {
      "epoch": 2.9788879437011833,
      "grad_norm": 0.8617345690727234,
      "learning_rate": 2.0216584158415844e-05,
      "loss": 8.1681,
      "step": 9630
    },
    {
      "epoch": 2.9819812852834273,
      "grad_norm": 1.1483654975891113,
      "learning_rate": 2.018564356435644e-05,
      "loss": 8.1503,
      "step": 9640
    },
    {
      "epoch": 2.9850746268656714,
      "grad_norm": 0.7533524036407471,
      "learning_rate": 2.015470297029703e-05,
      "loss": 8.1659,
      "step": 9650
    },
    {
      "epoch": 2.988167968447916,
      "grad_norm": 0.9276480078697205,
      "learning_rate": 2.0123762376237627e-05,
      "loss": 8.1758,
      "step": 9660
    },
    {
      "epoch": 2.99126131003016,
      "grad_norm": 0.8154336810112,
      "learning_rate": 2.009282178217822e-05,
      "loss": 8.1659,
      "step": 9670
    },
    {
      "epoch": 2.9943546516124044,
      "grad_norm": 1.1155047416687012,
      "learning_rate": 2.0061881188118814e-05,
      "loss": 8.1505,
      "step": 9680
    },
    {
      "epoch": 2.9974479931946485,
      "grad_norm": 0.5964629054069519,
      "learning_rate": 2.003094059405941e-05,
      "loss": 8.1746,
      "step": 9690
    }
  ],
  "logging_steps": 10,
  "max_steps": 16160,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 4.024901161853338e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
