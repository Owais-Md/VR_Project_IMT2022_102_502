{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9993039981439953,
  "eval_steps": 500,
  "global_step": 9696,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 1.0565555095672607,
      "learning_rate": 4.995874587458746e-05,
      "loss": 10.317,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.0399458408355713,
      "learning_rate": 4.990717821782179e-05,
      "loss": 10.0917,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 0.8372367024421692,
      "learning_rate": 4.985561056105611e-05,
      "loss": 9.8997,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 0.7324730753898621,
      "learning_rate": 4.980404290429043e-05,
      "loss": 9.7408,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 0.5262312293052673,
      "learning_rate": 4.975247524752475e-05,
      "loss": 9.5724,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 0.5877265930175781,
      "learning_rate": 4.970090759075908e-05,
      "loss": 9.4637,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 1.1703845262527466,
      "learning_rate": 4.96493399339934e-05,
      "loss": 9.3584,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.4456365406513214,
      "learning_rate": 4.959777227722773e-05,
      "loss": 9.239,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.3888629078865051,
      "learning_rate": 4.954620462046205e-05,
      "loss": 9.1232,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.34968331456184387,
      "learning_rate": 4.949463696369637e-05,
      "loss": 9.0151,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 0.34789127111434937,
      "learning_rate": 4.944306930693069e-05,
      "loss": 8.9644,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.33336520195007324,
      "learning_rate": 4.939150165016502e-05,
      "loss": 8.8865,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.32195472717285156,
      "learning_rate": 4.933993399339934e-05,
      "loss": 8.829,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.3589722514152527,
      "learning_rate": 4.928836633663367e-05,
      "loss": 8.761,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.2880726754665375,
      "learning_rate": 4.923679867986799e-05,
      "loss": 8.6978,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.3471258878707886,
      "learning_rate": 4.918523102310231e-05,
      "loss": 8.6479,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.3235485255718231,
      "learning_rate": 4.913366336633663e-05,
      "loss": 8.6432,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.3881101608276367,
      "learning_rate": 4.908209570957096e-05,
      "loss": 8.5666,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.2700231075286865,
      "learning_rate": 4.903052805280528e-05,
      "loss": 8.5608,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 0.5078184008598328,
      "learning_rate": 4.897896039603961e-05,
      "loss": 8.5281,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 0.441998690366745,
      "learning_rate": 4.892739273927393e-05,
      "loss": 8.5024,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.28030455112457275,
      "learning_rate": 4.887582508250825e-05,
      "loss": 8.4773,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 0.27497541904449463,
      "learning_rate": 4.882425742574257e-05,
      "loss": 8.4451,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 0.49432000517845154,
      "learning_rate": 4.87726897689769e-05,
      "loss": 8.45,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 0.582364559173584,
      "learning_rate": 4.872112211221123e-05,
      "loss": 8.419,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 0.41934067010879517,
      "learning_rate": 4.866955445544555e-05,
      "loss": 8.4041,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.35000211000442505,
      "learning_rate": 4.861798679867987e-05,
      "loss": 8.4011,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 0.3687727153301239,
      "learning_rate": 4.856641914191419e-05,
      "loss": 8.3837,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 0.3106289505958557,
      "learning_rate": 4.851485148514851e-05,
      "loss": 8.3476,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 0.3289656341075897,
      "learning_rate": 4.846328382838284e-05,
      "loss": 8.3549,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 0.7753923535346985,
      "learning_rate": 4.841171617161717e-05,
      "loss": 8.3392,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 0.6723453998565674,
      "learning_rate": 4.836014851485149e-05,
      "loss": 8.3168,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.3239496946334839,
      "learning_rate": 4.830858085808581e-05,
      "loss": 8.3169,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.5454972386360168,
      "learning_rate": 4.825701320132013e-05,
      "loss": 8.3205,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 0.5544485449790955,
      "learning_rate": 4.8205445544554454e-05,
      "loss": 8.2991,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 0.589152991771698,
      "learning_rate": 4.815387788778878e-05,
      "loss": 8.2962,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 0.38646960258483887,
      "learning_rate": 4.810231023102311e-05,
      "loss": 8.2903,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 0.27092379331588745,
      "learning_rate": 4.805074257425743e-05,
      "loss": 8.2906,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 0.26978766918182373,
      "learning_rate": 4.799917491749175e-05,
      "loss": 8.2735,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 0.34780269861221313,
      "learning_rate": 4.794760726072607e-05,
      "loss": 8.2779,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 0.5075212121009827,
      "learning_rate": 4.7896039603960394e-05,
      "loss": 8.2558,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 0.9800662398338318,
      "learning_rate": 4.784447194719472e-05,
      "loss": 8.2718,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 1.2811180353164673,
      "learning_rate": 4.779290429042905e-05,
      "loss": 8.2596,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 0.352987676858902,
      "learning_rate": 4.774133663366337e-05,
      "loss": 8.2639,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 0.7094700336456299,
      "learning_rate": 4.768976897689769e-05,
      "loss": 8.2569,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 0.2987173795700073,
      "learning_rate": 4.763820132013201e-05,
      "loss": 8.2677,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.5161851644515991,
      "learning_rate": 4.7586633663366334e-05,
      "loss": 8.2417,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 0.3100646138191223,
      "learning_rate": 4.753506600660066e-05,
      "loss": 8.2375,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 0.725845217704773,
      "learning_rate": 4.748349834983499e-05,
      "loss": 8.2403,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.6864117383956909,
      "learning_rate": 4.743193069306931e-05,
      "loss": 8.2279,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 0.3854181170463562,
      "learning_rate": 4.738036303630363e-05,
      "loss": 8.2288,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 0.47533729672431946,
      "learning_rate": 4.732879537953795e-05,
      "loss": 8.2515,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 0.5718740820884705,
      "learning_rate": 4.7277227722772274e-05,
      "loss": 8.2451,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.290677934885025,
      "learning_rate": 4.72256600660066e-05,
      "loss": 8.2489,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 0.2483932375907898,
      "learning_rate": 4.717409240924093e-05,
      "loss": 8.2423,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 0.3760654032230377,
      "learning_rate": 4.712252475247525e-05,
      "loss": 8.2423,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.27780771255493164,
      "learning_rate": 4.707095709570957e-05,
      "loss": 8.2412,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 0.22569721937179565,
      "learning_rate": 4.7019389438943894e-05,
      "loss": 8.2265,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 0.3034077286720276,
      "learning_rate": 4.6967821782178215e-05,
      "loss": 8.2315,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 0.3238346576690674,
      "learning_rate": 4.691625412541254e-05,
      "loss": 8.2238,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 0.46669769287109375,
      "learning_rate": 4.686468646864687e-05,
      "loss": 8.2221,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 0.27213725447654724,
      "learning_rate": 4.681311881188119e-05,
      "loss": 8.2295,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.4789974093437195,
      "learning_rate": 4.676155115511551e-05,
      "loss": 8.2345,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 0.3715915381908417,
      "learning_rate": 4.6709983498349834e-05,
      "loss": 8.2209,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.7415069937705994,
      "learning_rate": 4.665841584158416e-05,
      "loss": 8.2158,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.5272858142852783,
      "learning_rate": 4.660684818481848e-05,
      "loss": 8.2304,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 0.7707914710044861,
      "learning_rate": 4.655528052805281e-05,
      "loss": 8.2263,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 0.6944137215614319,
      "learning_rate": 4.650371287128713e-05,
      "loss": 8.2252,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 0.37001848220825195,
      "learning_rate": 4.645214521452145e-05,
      "loss": 8.2318,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 0.5079053044319153,
      "learning_rate": 4.6400577557755774e-05,
      "loss": 8.2085,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 0.308148592710495,
      "learning_rate": 4.63490099009901e-05,
      "loss": 8.2183,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 0.4860559105873108,
      "learning_rate": 4.629744224422442e-05,
      "loss": 8.2167,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 0.4019593298435211,
      "learning_rate": 4.624587458745875e-05,
      "loss": 8.2334,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 0.4290893077850342,
      "learning_rate": 4.619430693069307e-05,
      "loss": 8.2321,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 0.5633763670921326,
      "learning_rate": 4.6142739273927394e-05,
      "loss": 8.2126,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 0.46133989095687866,
      "learning_rate": 4.6091171617161715e-05,
      "loss": 8.2015,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 0.2623341381549835,
      "learning_rate": 4.603960396039604e-05,
      "loss": 8.203,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 0.9119935035705566,
      "learning_rate": 4.5988036303630364e-05,
      "loss": 8.223,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 0.8564001321792603,
      "learning_rate": 4.593646864686469e-05,
      "loss": 8.2312,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 0.2978377640247345,
      "learning_rate": 4.588490099009901e-05,
      "loss": 8.2289,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 0.3063042461872101,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 8.207,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.43957996368408203,
      "learning_rate": 4.578176567656766e-05,
      "loss": 8.2174,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.4443880021572113,
      "learning_rate": 4.573019801980198e-05,
      "loss": 8.2067,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 0.272327721118927,
      "learning_rate": 4.5678630363036304e-05,
      "loss": 8.2167,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 0.3797045946121216,
      "learning_rate": 4.562706270627063e-05,
      "loss": 8.2217,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.504453718662262,
      "learning_rate": 4.557549504950495e-05,
      "loss": 8.2035,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.4906189739704132,
      "learning_rate": 4.5523927392739274e-05,
      "loss": 8.2181,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 0.6800859570503235,
      "learning_rate": 4.54723597359736e-05,
      "loss": 8.1936,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 0.3829074203968048,
      "learning_rate": 4.542079207920792e-05,
      "loss": 8.1953,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 0.556553304195404,
      "learning_rate": 4.5369224422442244e-05,
      "loss": 8.2145,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.3774890899658203,
      "learning_rate": 4.531765676567657e-05,
      "loss": 8.2199,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 0.4384826719760895,
      "learning_rate": 4.526608910891089e-05,
      "loss": 8.2221,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 0.49910715222358704,
      "learning_rate": 4.5214521452145214e-05,
      "loss": 8.2061,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 0.4327700138092041,
      "learning_rate": 4.516295379537954e-05,
      "loss": 8.2127,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 0.6294150352478027,
      "learning_rate": 4.5111386138613864e-05,
      "loss": 8.2003,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 0.6277406811714172,
      "learning_rate": 4.5059818481848185e-05,
      "loss": 8.1994,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 0.9094993472099304,
      "learning_rate": 4.500825082508251e-05,
      "loss": 8.218,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 0.28750014305114746,
      "learning_rate": 4.4956683168316834e-05,
      "loss": 8.206,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.35400182008743286,
      "learning_rate": 4.4905115511551155e-05,
      "loss": 8.2148,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 0.3658556342124939,
      "learning_rate": 4.485354785478548e-05,
      "loss": 8.206,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 0.3834856450557709,
      "learning_rate": 4.4801980198019804e-05,
      "loss": 8.2079,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 0.3486127555370331,
      "learning_rate": 4.4750412541254125e-05,
      "loss": 8.1986,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 0.8183367252349854,
      "learning_rate": 4.469884488448845e-05,
      "loss": 8.2125,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 0.6145647168159485,
      "learning_rate": 4.4647277227722774e-05,
      "loss": 8.2079,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 0.2893054783344269,
      "learning_rate": 4.45957095709571e-05,
      "loss": 8.1967,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 0.9608153104782104,
      "learning_rate": 4.454414191419142e-05,
      "loss": 8.2127,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 1.0759103298187256,
      "learning_rate": 4.4492574257425744e-05,
      "loss": 8.2083,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 0.9710957407951355,
      "learning_rate": 4.4441006600660065e-05,
      "loss": 8.1976,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.56833815574646,
      "learning_rate": 4.438943894389439e-05,
      "loss": 8.2033,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.055946946144104,
      "learning_rate": 4.4337871287128714e-05,
      "loss": 8.1939,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 0.5963489413261414,
      "learning_rate": 4.428630363036304e-05,
      "loss": 8.2132,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 0.41320452094078064,
      "learning_rate": 4.423473597359736e-05,
      "loss": 8.203,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 0.36218947172164917,
      "learning_rate": 4.4183168316831684e-05,
      "loss": 8.1901,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 0.3527924418449402,
      "learning_rate": 4.4131600660066006e-05,
      "loss": 8.1998,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 0.3640836775302887,
      "learning_rate": 4.4080033003300333e-05,
      "loss": 8.1778,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 0.3603845536708832,
      "learning_rate": 4.4028465346534655e-05,
      "loss": 8.1906,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 0.48659634590148926,
      "learning_rate": 4.397689768976898e-05,
      "loss": 8.1998,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 1.1557831764221191,
      "learning_rate": 4.3925330033003304e-05,
      "loss": 8.1942,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 0.3266906440258026,
      "learning_rate": 4.3873762376237625e-05,
      "loss": 8.1978,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 0.3403061032295227,
      "learning_rate": 4.3822194719471946e-05,
      "loss": 8.2091,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 0.8684284687042236,
      "learning_rate": 4.3770627062706274e-05,
      "loss": 8.1871,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 0.586765706539154,
      "learning_rate": 4.37190594059406e-05,
      "loss": 8.2009,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 0.3757753372192383,
      "learning_rate": 4.366749174917492e-05,
      "loss": 8.1973,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 0.3641539216041565,
      "learning_rate": 4.3615924092409244e-05,
      "loss": 8.1934,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 0.3916618227958679,
      "learning_rate": 4.3564356435643565e-05,
      "loss": 8.1885,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 1.1773916482925415,
      "learning_rate": 4.3512788778877886e-05,
      "loss": 8.1716,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 1.1602495908737183,
      "learning_rate": 4.3461221122112214e-05,
      "loss": 8.141,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 0.48604947328567505,
      "learning_rate": 4.340965346534654e-05,
      "loss": 8.146,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 0.47183433175086975,
      "learning_rate": 4.335808580858086e-05,
      "loss": 8.1476,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 0.4256132245063782,
      "learning_rate": 4.3306518151815184e-05,
      "loss": 8.1511,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 0.3924328088760376,
      "learning_rate": 4.3254950495049505e-05,
      "loss": 8.1382,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 0.3695896863937378,
      "learning_rate": 4.3203382838283827e-05,
      "loss": 8.1539,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 1.0166791677474976,
      "learning_rate": 4.3151815181518154e-05,
      "loss": 8.1419,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 0.34996649622917175,
      "learning_rate": 4.310024752475248e-05,
      "loss": 8.1217,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 0.2747478187084198,
      "learning_rate": 4.3048679867986803e-05,
      "loss": 8.1316,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 0.41667020320892334,
      "learning_rate": 4.2997112211221125e-05,
      "loss": 8.1295,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 0.3027843236923218,
      "learning_rate": 4.2945544554455446e-05,
      "loss": 8.1229,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.43693044781684875,
      "learning_rate": 4.289397689768977e-05,
      "loss": 8.1334,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.5062724351882935,
      "learning_rate": 4.2842409240924095e-05,
      "loss": 8.1474,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 0.3002518117427826,
      "learning_rate": 4.279084158415842e-05,
      "loss": 8.1191,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 0.8714909553527832,
      "learning_rate": 4.2739273927392744e-05,
      "loss": 8.1466,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 1.1028579473495483,
      "learning_rate": 4.2687706270627065e-05,
      "loss": 8.1439,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 0.3875997066497803,
      "learning_rate": 4.2636138613861386e-05,
      "loss": 8.143,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 0.6221628189086914,
      "learning_rate": 4.258457095709571e-05,
      "loss": 8.1414,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 0.8331373929977417,
      "learning_rate": 4.2533003300330035e-05,
      "loss": 8.1404,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 0.5329532027244568,
      "learning_rate": 4.248143564356436e-05,
      "loss": 8.1312,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 0.4392077922821045,
      "learning_rate": 4.2429867986798684e-05,
      "loss": 8.1345,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 0.4811130166053772,
      "learning_rate": 4.2378300330033005e-05,
      "loss": 8.1354,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 0.3947867751121521,
      "learning_rate": 4.2326732673267326e-05,
      "loss": 8.1558,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 0.466060996055603,
      "learning_rate": 4.227516501650165e-05,
      "loss": 8.1224,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 1.3809500932693481,
      "learning_rate": 4.2223597359735975e-05,
      "loss": 8.1392,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 0.2857910692691803,
      "learning_rate": 4.21720297029703e-05,
      "loss": 8.1449,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 0.9009313583374023,
      "learning_rate": 4.2120462046204624e-05,
      "loss": 8.1361,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 0.3106480538845062,
      "learning_rate": 4.2068894389438946e-05,
      "loss": 8.1483,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 0.4126911461353302,
      "learning_rate": 4.201732673267327e-05,
      "loss": 8.1224,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.33747777342796326,
      "learning_rate": 4.196575907590759e-05,
      "loss": 8.1104,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 0.4033467769622803,
      "learning_rate": 4.1914191419141916e-05,
      "loss": 8.1489,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 1.1209030151367188,
      "learning_rate": 4.1862623762376244e-05,
      "loss": 8.1302,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 0.40263622999191284,
      "learning_rate": 4.1811056105610565e-05,
      "loss": 8.1246,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 1.0671911239624023,
      "learning_rate": 4.1759488448844886e-05,
      "loss": 8.1366,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 0.5966104865074158,
      "learning_rate": 4.170792079207921e-05,
      "loss": 8.1322,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 0.7771298289299011,
      "learning_rate": 4.1656353135313535e-05,
      "loss": 8.1317,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 1.1947914361953735,
      "learning_rate": 4.1604785478547856e-05,
      "loss": 8.111,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 0.3502797484397888,
      "learning_rate": 4.1553217821782184e-05,
      "loss": 8.1375,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 0.5238884091377258,
      "learning_rate": 4.1501650165016505e-05,
      "loss": 8.1351,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 0.7352068424224854,
      "learning_rate": 4.1450082508250826e-05,
      "loss": 8.1264,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 0.35258346796035767,
      "learning_rate": 4.1403671617161716e-05,
      "loss": 8.1375,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 0.37848493456840515,
      "learning_rate": 4.1352103960396044e-05,
      "loss": 8.1153,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 0.7814749479293823,
      "learning_rate": 4.1300536303630365e-05,
      "loss": 8.1232,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 0.6817078590393066,
      "learning_rate": 4.1248968646864686e-05,
      "loss": 8.1284,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 0.24339263141155243,
      "learning_rate": 4.1197400990099014e-05,
      "loss": 8.1397,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 0.31780698895454407,
      "learning_rate": 4.1145833333333335e-05,
      "loss": 8.1299,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 0.34242910146713257,
      "learning_rate": 4.109426567656766e-05,
      "loss": 8.123,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 0.49349915981292725,
      "learning_rate": 4.1042698019801984e-05,
      "loss": 8.1315,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 0.5724009275436401,
      "learning_rate": 4.0991130363036305e-05,
      "loss": 8.1367,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 0.31627073884010315,
      "learning_rate": 4.0939562706270626e-05,
      "loss": 8.142,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 1.0704926252365112,
      "learning_rate": 4.0887995049504954e-05,
      "loss": 8.1173,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 0.6183575987815857,
      "learning_rate": 4.0836427392739275e-05,
      "loss": 8.1379,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 0.8377038240432739,
      "learning_rate": 4.07848597359736e-05,
      "loss": 8.1336,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 0.4174825847148895,
      "learning_rate": 4.0733292079207924e-05,
      "loss": 8.1264,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 0.9179137349128723,
      "learning_rate": 4.0681724422442246e-05,
      "loss": 8.1173,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 0.7545443177223206,
      "learning_rate": 4.063015676567657e-05,
      "loss": 8.1418,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 0.41321244835853577,
      "learning_rate": 4.0578589108910895e-05,
      "loss": 8.1325,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 0.390316367149353,
      "learning_rate": 4.0527021452145216e-05,
      "loss": 8.1266,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 0.5091736912727356,
      "learning_rate": 4.0475453795379544e-05,
      "loss": 8.1227,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 0.5188171863555908,
      "learning_rate": 4.0423886138613865e-05,
      "loss": 8.1405,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 0.38445693254470825,
      "learning_rate": 4.0372318481848186e-05,
      "loss": 8.1516,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 0.7498568892478943,
      "learning_rate": 4.032075082508251e-05,
      "loss": 8.1285,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.5680442452430725,
      "learning_rate": 4.0269183168316835e-05,
      "loss": 8.1272,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 1.1229215860366821,
      "learning_rate": 4.0217615511551156e-05,
      "loss": 8.1374,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 0.6570364832878113,
      "learning_rate": 4.0166047854785484e-05,
      "loss": 8.131,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 0.7372750043869019,
      "learning_rate": 4.0114480198019805e-05,
      "loss": 8.1344,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.34970682859420776,
      "learning_rate": 4.0062912541254126e-05,
      "loss": 8.1327,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 0.547315776348114,
      "learning_rate": 4.001134488448845e-05,
      "loss": 8.1373,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 0.7643601894378662,
      "learning_rate": 3.9959777227722775e-05,
      "loss": 8.123,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 0.6446273922920227,
      "learning_rate": 3.99082095709571e-05,
      "loss": 8.12,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 0.40875551104545593,
      "learning_rate": 3.9856641914191424e-05,
      "loss": 8.1135,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 0.4373060166835785,
      "learning_rate": 3.9805074257425745e-05,
      "loss": 8.1233,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 0.25874030590057373,
      "learning_rate": 3.9753506600660067e-05,
      "loss": 8.1315,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 0.4552953243255615,
      "learning_rate": 3.970193894389439e-05,
      "loss": 8.1346,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.2765510678291321,
      "learning_rate": 3.9650371287128716e-05,
      "loss": 8.128,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 0.5007600784301758,
      "learning_rate": 3.9598803630363043e-05,
      "loss": 8.1461,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 0.574668288230896,
      "learning_rate": 3.9547235973597365e-05,
      "loss": 8.1267,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 0.6520045399665833,
      "learning_rate": 3.9495668316831686e-05,
      "loss": 8.1404,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 1.0773191452026367,
      "learning_rate": 3.944410066006601e-05,
      "loss": 8.1334,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 0.773710310459137,
      "learning_rate": 3.939253300330033e-05,
      "loss": 8.128,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 0.678584098815918,
      "learning_rate": 3.9340965346534656e-05,
      "loss": 8.1312,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 0.4250548183917999,
      "learning_rate": 3.9289397689768984e-05,
      "loss": 8.1457,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 0.6002156138420105,
      "learning_rate": 3.9237830033003305e-05,
      "loss": 8.1368,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 0.6485240459442139,
      "learning_rate": 3.9186262376237626e-05,
      "loss": 8.1449,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 0.39589083194732666,
      "learning_rate": 3.913469471947195e-05,
      "loss": 8.1301,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 0.41564857959747314,
      "learning_rate": 3.908312706270627e-05,
      "loss": 8.1249,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 0.6284973621368408,
      "learning_rate": 3.9031559405940596e-05,
      "loss": 8.1366,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 0.5497670769691467,
      "learning_rate": 3.8979991749174924e-05,
      "loss": 8.1377,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 0.7107080221176147,
      "learning_rate": 3.8928424092409245e-05,
      "loss": 8.1459,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 0.967587947845459,
      "learning_rate": 3.8876856435643566e-05,
      "loss": 8.1208,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 0.5329194068908691,
      "learning_rate": 3.882528877887789e-05,
      "loss": 8.1268,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 0.48438340425491333,
      "learning_rate": 3.877372112211221e-05,
      "loss": 8.1264,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 0.3653985857963562,
      "learning_rate": 3.8722153465346537e-05,
      "loss": 8.1208,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 0.44317343831062317,
      "learning_rate": 3.8670585808580864e-05,
      "loss": 8.1377,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 0.9510945677757263,
      "learning_rate": 3.8619018151815186e-05,
      "loss": 8.1267,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 0.8991702198982239,
      "learning_rate": 3.856745049504951e-05,
      "loss": 8.1362,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 0.4080759286880493,
      "learning_rate": 3.851588283828383e-05,
      "loss": 8.1313,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 0.8143441677093506,
      "learning_rate": 3.846431518151815e-05,
      "loss": 8.1291,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.7032778263092041,
      "learning_rate": 3.841274752475248e-05,
      "loss": 8.1496,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 0.9029533863067627,
      "learning_rate": 3.8361179867986805e-05,
      "loss": 8.1223,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 0.5039496421813965,
      "learning_rate": 3.8309612211221126e-05,
      "loss": 8.1094,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 0.8314266204833984,
      "learning_rate": 3.825804455445545e-05,
      "loss": 8.1182,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 0.29918500781059265,
      "learning_rate": 3.820647689768977e-05,
      "loss": 8.1264,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 0.40951746702194214,
      "learning_rate": 3.815490924092409e-05,
      "loss": 8.128,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 0.2980818748474121,
      "learning_rate": 3.810334158415842e-05,
      "loss": 8.1162,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.4017033278942108,
      "learning_rate": 3.8051773927392745e-05,
      "loss": 8.1268,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 0.5063818693161011,
      "learning_rate": 3.8000206270627066e-05,
      "loss": 8.1231,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 0.35118499398231506,
      "learning_rate": 3.794863861386139e-05,
      "loss": 8.1364,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 0.550618052482605,
      "learning_rate": 3.789707095709571e-05,
      "loss": 8.1115,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 0.44091475009918213,
      "learning_rate": 3.7845503300330036e-05,
      "loss": 8.1416,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 0.3570210933685303,
      "learning_rate": 3.779393564356436e-05,
      "loss": 8.1264,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 0.337750107049942,
      "learning_rate": 3.7742367986798685e-05,
      "loss": 8.1255,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 0.43572911620140076,
      "learning_rate": 3.7690800330033007e-05,
      "loss": 8.1158,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 0.30651938915252686,
      "learning_rate": 3.763923267326733e-05,
      "loss": 8.1176,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 0.4226876497268677,
      "learning_rate": 3.758766501650165e-05,
      "loss": 8.123,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 1.0687974691390991,
      "learning_rate": 3.753609735973598e-05,
      "loss": 8.1347,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 0.437526136636734,
      "learning_rate": 3.74845297029703e-05,
      "loss": 8.1287,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 0.7829475998878479,
      "learning_rate": 3.7432962046204626e-05,
      "loss": 8.1389,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 1.2735480070114136,
      "learning_rate": 3.738139438943895e-05,
      "loss": 8.1319,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.6305181384086609,
      "learning_rate": 3.732982673267327e-05,
      "loss": 8.1331,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.4683740437030792,
      "learning_rate": 3.727825907590759e-05,
      "loss": 8.1057,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 0.28665226697921753,
      "learning_rate": 3.722669141914192e-05,
      "loss": 8.1227,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 0.348328560590744,
      "learning_rate": 3.717512376237624e-05,
      "loss": 8.1402,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 0.4140527546405792,
      "learning_rate": 3.7123556105610566e-05,
      "loss": 8.1215,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.3625510036945343,
      "learning_rate": 3.707198844884489e-05,
      "loss": 8.1387,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 0.303415447473526,
      "learning_rate": 3.702042079207921e-05,
      "loss": 8.1227,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 0.350281298160553,
      "learning_rate": 3.6968853135313536e-05,
      "loss": 8.1006,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 0.331625759601593,
      "learning_rate": 3.691728547854786e-05,
      "loss": 8.1281,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 0.6135042905807495,
      "learning_rate": 3.686571782178218e-05,
      "loss": 8.1184,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 0.3820091485977173,
      "learning_rate": 3.6814150165016506e-05,
      "loss": 8.1296,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 0.4865269958972931,
      "learning_rate": 3.676258250825083e-05,
      "loss": 8.1289,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 0.34087127447128296,
      "learning_rate": 3.671101485148515e-05,
      "loss": 8.1253,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 0.31458431482315063,
      "learning_rate": 3.6659447194719476e-05,
      "loss": 8.1312,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 0.46448275446891785,
      "learning_rate": 3.66078795379538e-05,
      "loss": 8.133,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 0.42894142866134644,
      "learning_rate": 3.655631188118812e-05,
      "loss": 8.1204,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 0.5991010069847107,
      "learning_rate": 3.650474422442245e-05,
      "loss": 8.105,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 0.3278878927230835,
      "learning_rate": 3.645317656765677e-05,
      "loss": 8.1081,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 0.31309399008750916,
      "learning_rate": 3.640160891089109e-05,
      "loss": 8.1268,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 0.3627416789531708,
      "learning_rate": 3.635004125412542e-05,
      "loss": 8.1354,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 0.5185185074806213,
      "learning_rate": 3.629847359735974e-05,
      "loss": 8.1217,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 0.7601810097694397,
      "learning_rate": 3.624690594059406e-05,
      "loss": 8.1273,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 0.8919748067855835,
      "learning_rate": 3.619533828382839e-05,
      "loss": 8.1407,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 0.4341886639595032,
      "learning_rate": 3.614377062706271e-05,
      "loss": 8.1239,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 0.38412824273109436,
      "learning_rate": 3.609220297029703e-05,
      "loss": 8.1186,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 0.5315512418746948,
      "learning_rate": 3.604063531353136e-05,
      "loss": 8.1256,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 0.7404510974884033,
      "learning_rate": 3.598906765676568e-05,
      "loss": 8.1437,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 0.9590985178947449,
      "learning_rate": 3.59375e-05,
      "loss": 8.1334,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 0.39767009019851685,
      "learning_rate": 3.588593234323433e-05,
      "loss": 8.1171,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.3042120635509491,
      "learning_rate": 3.583436468646865e-05,
      "loss": 8.1328,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 0.389634370803833,
      "learning_rate": 3.5782797029702976e-05,
      "loss": 8.1386,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 0.33430975675582886,
      "learning_rate": 3.57312293729373e-05,
      "loss": 8.1313,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 0.38440993428230286,
      "learning_rate": 3.567966171617162e-05,
      "loss": 8.1248,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.3980347812175751,
      "learning_rate": 3.562809405940594e-05,
      "loss": 8.1149,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.3470490276813507,
      "learning_rate": 3.557652640264027e-05,
      "loss": 8.1389,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.28660619258880615,
      "learning_rate": 3.552495874587459e-05,
      "loss": 8.1123,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 0.46574026346206665,
      "learning_rate": 3.547339108910892e-05,
      "loss": 8.111,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 0.2823634147644043,
      "learning_rate": 3.542182343234324e-05,
      "loss": 8.1521,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.3352123498916626,
      "learning_rate": 3.537025577557756e-05,
      "loss": 8.1244,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.8018085360527039,
      "learning_rate": 3.531868811881188e-05,
      "loss": 8.1305,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 0.7485525608062744,
      "learning_rate": 3.526712046204621e-05,
      "loss": 8.123,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.38008561730384827,
      "learning_rate": 3.521555280528053e-05,
      "loss": 8.1242,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 0.3900972902774811,
      "learning_rate": 3.516398514851486e-05,
      "loss": 8.1222,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 0.30585187673568726,
      "learning_rate": 3.511241749174918e-05,
      "loss": 8.1204,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 0.24571025371551514,
      "learning_rate": 3.50608498349835e-05,
      "loss": 8.1308,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 0.6372361779212952,
      "learning_rate": 3.500928217821782e-05,
      "loss": 8.1378,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 0.4712018072605133,
      "learning_rate": 3.495771452145215e-05,
      "loss": 8.131,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 0.6792766451835632,
      "learning_rate": 3.490614686468647e-05,
      "loss": 8.1315,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 0.5273013114929199,
      "learning_rate": 3.48545792079208e-05,
      "loss": 8.1168,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 0.32319390773773193,
      "learning_rate": 3.480301155115512e-05,
      "loss": 8.1153,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 0.5794802308082581,
      "learning_rate": 3.475144389438944e-05,
      "loss": 8.1149,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 0.44578197598457336,
      "learning_rate": 3.469987623762376e-05,
      "loss": 8.1215,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 0.33984482288360596,
      "learning_rate": 3.464830858085809e-05,
      "loss": 8.1203,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 0.5207871198654175,
      "learning_rate": 3.4596740924092416e-05,
      "loss": 8.1337,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 0.3359738290309906,
      "learning_rate": 3.454517326732674e-05,
      "loss": 8.1315,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.29692766070365906,
      "learning_rate": 3.449360561056106e-05,
      "loss": 8.1027,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.4548894166946411,
      "learning_rate": 3.444203795379538e-05,
      "loss": 8.1251,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.5032789707183838,
      "learning_rate": 3.43904702970297e-05,
      "loss": 8.1214,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 0.25614356994628906,
      "learning_rate": 3.433890264026403e-05,
      "loss": 8.1167,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 0.46262654662132263,
      "learning_rate": 3.428733498349836e-05,
      "loss": 8.1334,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 0.27074962854385376,
      "learning_rate": 3.423576732673268e-05,
      "loss": 8.1324,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 0.3832576274871826,
      "learning_rate": 3.4184199669967e-05,
      "loss": 8.1346,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 0.41327574849128723,
      "learning_rate": 3.413263201320132e-05,
      "loss": 8.1344,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 0.44750240445137024,
      "learning_rate": 3.408106435643564e-05,
      "loss": 8.1109,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 0.3942849934101105,
      "learning_rate": 3.402949669966997e-05,
      "loss": 8.1234,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 0.3912838101387024,
      "learning_rate": 3.39779290429043e-05,
      "loss": 8.1297,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 0.44661974906921387,
      "learning_rate": 3.392636138613862e-05,
      "loss": 8.1134,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 0.6265203356742859,
      "learning_rate": 3.387479372937294e-05,
      "loss": 8.1243,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 0.32146382331848145,
      "learning_rate": 3.382322607260726e-05,
      "loss": 8.1184,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 0.4319128096103668,
      "learning_rate": 3.377165841584158e-05,
      "loss": 8.1328,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.5804173946380615,
      "learning_rate": 3.372009075907591e-05,
      "loss": 8.1276,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 0.6959319114685059,
      "learning_rate": 3.366852310231024e-05,
      "loss": 8.1175,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 0.9252429008483887,
      "learning_rate": 3.361695544554456e-05,
      "loss": 8.1212,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.5588838458061218,
      "learning_rate": 3.356538778877888e-05,
      "loss": 8.1157,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 0.4152226448059082,
      "learning_rate": 3.35138201320132e-05,
      "loss": 8.1173,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 0.43063798546791077,
      "learning_rate": 3.346225247524752e-05,
      "loss": 8.1214,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.29873234033584595,
      "learning_rate": 3.341068481848185e-05,
      "loss": 8.1185,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 0.45878633856773376,
      "learning_rate": 3.335911716171618e-05,
      "loss": 8.1124,
      "step": 3230
    },
    {
      "epoch": 1.002242672647127,
      "grad_norm": 0.545609712600708,
      "learning_rate": 3.33075495049505e-05,
      "loss": 8.1133,
      "step": 3240
    },
    {
      "epoch": 1.0053360142293712,
      "grad_norm": 0.2801823019981384,
      "learning_rate": 3.325598184818482e-05,
      "loss": 8.1306,
      "step": 3250
    },
    {
      "epoch": 1.0084293558116155,
      "grad_norm": 0.46023809909820557,
      "learning_rate": 3.320441419141914e-05,
      "loss": 8.1106,
      "step": 3260
    },
    {
      "epoch": 1.0115226973938598,
      "grad_norm": 0.600797176361084,
      "learning_rate": 3.315284653465346e-05,
      "loss": 8.1056,
      "step": 3270
    },
    {
      "epoch": 1.0146160389761039,
      "grad_norm": 0.6101308465003967,
      "learning_rate": 3.310127887788779e-05,
      "loss": 8.1136,
      "step": 3280
    },
    {
      "epoch": 1.0177093805583481,
      "grad_norm": 0.8300682306289673,
      "learning_rate": 3.304971122112212e-05,
      "loss": 8.114,
      "step": 3290
    },
    {
      "epoch": 1.0208027221405924,
      "grad_norm": 0.6056985259056091,
      "learning_rate": 3.299814356435644e-05,
      "loss": 8.1307,
      "step": 3300
    },
    {
      "epoch": 1.0238960637228367,
      "grad_norm": 0.4353797733783722,
      "learning_rate": 3.294657590759076e-05,
      "loss": 8.1303,
      "step": 3310
    },
    {
      "epoch": 1.0269894053050808,
      "grad_norm": 0.5106632113456726,
      "learning_rate": 3.289500825082508e-05,
      "loss": 8.1288,
      "step": 3320
    },
    {
      "epoch": 1.030082746887325,
      "grad_norm": 0.4481848180294037,
      "learning_rate": 3.28434405940594e-05,
      "loss": 8.1136,
      "step": 3330
    },
    {
      "epoch": 1.0331760884695693,
      "grad_norm": 0.4268108010292053,
      "learning_rate": 3.279187293729373e-05,
      "loss": 8.1268,
      "step": 3340
    },
    {
      "epoch": 1.0362694300518134,
      "grad_norm": 0.3164180517196655,
      "learning_rate": 3.274030528052806e-05,
      "loss": 8.1091,
      "step": 3350
    },
    {
      "epoch": 1.0393627716340577,
      "grad_norm": 0.36220887303352356,
      "learning_rate": 3.268873762376238e-05,
      "loss": 8.1263,
      "step": 3360
    },
    {
      "epoch": 1.042456113216302,
      "grad_norm": 0.3075656294822693,
      "learning_rate": 3.26371699669967e-05,
      "loss": 8.1275,
      "step": 3370
    },
    {
      "epoch": 1.0455494547985462,
      "grad_norm": 0.3411880135536194,
      "learning_rate": 3.258560231023102e-05,
      "loss": 8.1093,
      "step": 3380
    },
    {
      "epoch": 1.0486427963807903,
      "grad_norm": 0.36366158723831177,
      "learning_rate": 3.253403465346535e-05,
      "loss": 8.1121,
      "step": 3390
    },
    {
      "epoch": 1.0517361379630346,
      "grad_norm": 0.5017037391662598,
      "learning_rate": 3.248246699669967e-05,
      "loss": 8.1212,
      "step": 3400
    },
    {
      "epoch": 1.0548294795452788,
      "grad_norm": 0.8089635968208313,
      "learning_rate": 3.2430899339934e-05,
      "loss": 8.1183,
      "step": 3410
    },
    {
      "epoch": 1.057922821127523,
      "grad_norm": 0.24707962572574615,
      "learning_rate": 3.237933168316832e-05,
      "loss": 8.106,
      "step": 3420
    },
    {
      "epoch": 1.0610161627097672,
      "grad_norm": 0.35806116461753845,
      "learning_rate": 3.232776402640264e-05,
      "loss": 8.1226,
      "step": 3430
    },
    {
      "epoch": 1.0641095042920115,
      "grad_norm": 0.34156671166419983,
      "learning_rate": 3.227619636963696e-05,
      "loss": 8.1174,
      "step": 3440
    },
    {
      "epoch": 1.0672028458742557,
      "grad_norm": 0.32917043566703796,
      "learning_rate": 3.222462871287129e-05,
      "loss": 8.1296,
      "step": 3450
    },
    {
      "epoch": 1.0702961874564998,
      "grad_norm": 0.4427207112312317,
      "learning_rate": 3.217306105610561e-05,
      "loss": 8.1322,
      "step": 3460
    },
    {
      "epoch": 1.073389529038744,
      "grad_norm": 0.41673484444618225,
      "learning_rate": 3.212149339933994e-05,
      "loss": 8.1157,
      "step": 3470
    },
    {
      "epoch": 1.0764828706209884,
      "grad_norm": 0.33704590797424316,
      "learning_rate": 3.206992574257426e-05,
      "loss": 8.1295,
      "step": 3480
    },
    {
      "epoch": 1.0795762122032326,
      "grad_norm": 0.43005138635635376,
      "learning_rate": 3.201835808580858e-05,
      "loss": 8.1422,
      "step": 3490
    },
    {
      "epoch": 1.0826695537854767,
      "grad_norm": 0.3690285384654999,
      "learning_rate": 3.19667904290429e-05,
      "loss": 8.1112,
      "step": 3500
    },
    {
      "epoch": 1.085762895367721,
      "grad_norm": 0.22731666266918182,
      "learning_rate": 3.191522277227723e-05,
      "loss": 8.1105,
      "step": 3510
    },
    {
      "epoch": 1.0888562369499653,
      "grad_norm": 0.4495694935321808,
      "learning_rate": 3.186365511551155e-05,
      "loss": 8.1196,
      "step": 3520
    },
    {
      "epoch": 1.0919495785322093,
      "grad_norm": 0.37618497014045715,
      "learning_rate": 3.181208745874588e-05,
      "loss": 8.1339,
      "step": 3530
    },
    {
      "epoch": 1.0950429201144536,
      "grad_norm": 0.37077558040618896,
      "learning_rate": 3.17605198019802e-05,
      "loss": 8.1237,
      "step": 3540
    },
    {
      "epoch": 1.0981362616966979,
      "grad_norm": 0.34764328598976135,
      "learning_rate": 3.170895214521452e-05,
      "loss": 8.1288,
      "step": 3550
    },
    {
      "epoch": 1.1012296032789421,
      "grad_norm": 0.34802743792533875,
      "learning_rate": 3.165738448844885e-05,
      "loss": 8.1126,
      "step": 3560
    },
    {
      "epoch": 1.1043229448611862,
      "grad_norm": 0.2546907365322113,
      "learning_rate": 3.160581683168317e-05,
      "loss": 8.1275,
      "step": 3570
    },
    {
      "epoch": 1.1074162864434305,
      "grad_norm": 0.3745822310447693,
      "learning_rate": 3.155424917491749e-05,
      "loss": 8.1134,
      "step": 3580
    },
    {
      "epoch": 1.1105096280256748,
      "grad_norm": 0.3907036781311035,
      "learning_rate": 3.150268151815182e-05,
      "loss": 8.1114,
      "step": 3590
    },
    {
      "epoch": 1.113602969607919,
      "grad_norm": 0.3914617896080017,
      "learning_rate": 3.145111386138614e-05,
      "loss": 8.1262,
      "step": 3600
    },
    {
      "epoch": 1.116696311190163,
      "grad_norm": 0.30437007546424866,
      "learning_rate": 3.139954620462046e-05,
      "loss": 8.124,
      "step": 3610
    },
    {
      "epoch": 1.1197896527724074,
      "grad_norm": 0.30547043681144714,
      "learning_rate": 3.134797854785479e-05,
      "loss": 8.1154,
      "step": 3620
    },
    {
      "epoch": 1.1228829943546517,
      "grad_norm": 0.5757488012313843,
      "learning_rate": 3.129641089108911e-05,
      "loss": 8.1391,
      "step": 3630
    },
    {
      "epoch": 1.1259763359368957,
      "grad_norm": 0.5146521329879761,
      "learning_rate": 3.124484323432343e-05,
      "loss": 8.1189,
      "step": 3640
    },
    {
      "epoch": 1.12906967751914,
      "grad_norm": 0.4515989124774933,
      "learning_rate": 3.119327557755776e-05,
      "loss": 8.1261,
      "step": 3650
    },
    {
      "epoch": 1.1321630191013843,
      "grad_norm": 0.4993474781513214,
      "learning_rate": 3.114170792079208e-05,
      "loss": 8.129,
      "step": 3660
    },
    {
      "epoch": 1.1352563606836286,
      "grad_norm": 0.5771251320838928,
      "learning_rate": 3.10901402640264e-05,
      "loss": 8.1116,
      "step": 3670
    },
    {
      "epoch": 1.1383497022658726,
      "grad_norm": 0.29773980379104614,
      "learning_rate": 3.103857260726073e-05,
      "loss": 8.1128,
      "step": 3680
    },
    {
      "epoch": 1.141443043848117,
      "grad_norm": 0.33953896164894104,
      "learning_rate": 3.098700495049505e-05,
      "loss": 8.108,
      "step": 3690
    },
    {
      "epoch": 1.1445363854303612,
      "grad_norm": 0.41498368978500366,
      "learning_rate": 3.093543729372937e-05,
      "loss": 8.1176,
      "step": 3700
    },
    {
      "epoch": 1.1476297270126055,
      "grad_norm": 0.34426039457321167,
      "learning_rate": 3.08838696369637e-05,
      "loss": 8.1058,
      "step": 3710
    },
    {
      "epoch": 1.1507230685948495,
      "grad_norm": 0.3060349225997925,
      "learning_rate": 3.083230198019802e-05,
      "loss": 8.1171,
      "step": 3720
    },
    {
      "epoch": 1.1538164101770938,
      "grad_norm": 0.5850794315338135,
      "learning_rate": 3.078073432343234e-05,
      "loss": 8.1127,
      "step": 3730
    },
    {
      "epoch": 1.156909751759338,
      "grad_norm": 0.3654874563217163,
      "learning_rate": 3.072916666666667e-05,
      "loss": 8.1262,
      "step": 3740
    },
    {
      "epoch": 1.1600030933415821,
      "grad_norm": 0.567943274974823,
      "learning_rate": 3.067759900990099e-05,
      "loss": 8.1103,
      "step": 3750
    },
    {
      "epoch": 1.1630964349238264,
      "grad_norm": 0.2911977767944336,
      "learning_rate": 3.062603135313531e-05,
      "loss": 8.1232,
      "step": 3760
    },
    {
      "epoch": 1.1661897765060707,
      "grad_norm": 0.3311375379562378,
      "learning_rate": 3.057446369636964e-05,
      "loss": 8.1153,
      "step": 3770
    },
    {
      "epoch": 1.169283118088315,
      "grad_norm": 0.4166555106639862,
      "learning_rate": 3.052289603960396e-05,
      "loss": 8.1142,
      "step": 3780
    },
    {
      "epoch": 1.172376459670559,
      "grad_norm": 0.38923534750938416,
      "learning_rate": 3.0471328382838286e-05,
      "loss": 8.125,
      "step": 3790
    },
    {
      "epoch": 1.1754698012528033,
      "grad_norm": 0.29464074969291687,
      "learning_rate": 3.041976072607261e-05,
      "loss": 8.1289,
      "step": 3800
    },
    {
      "epoch": 1.1785631428350476,
      "grad_norm": 0.31073540449142456,
      "learning_rate": 3.0368193069306932e-05,
      "loss": 8.1221,
      "step": 3810
    },
    {
      "epoch": 1.1816564844172919,
      "grad_norm": 0.7192683815956116,
      "learning_rate": 3.0316625412541256e-05,
      "loss": 8.1241,
      "step": 3820
    },
    {
      "epoch": 1.184749825999536,
      "grad_norm": 0.37380465865135193,
      "learning_rate": 3.0265057755775578e-05,
      "loss": 8.103,
      "step": 3830
    },
    {
      "epoch": 1.1878431675817802,
      "grad_norm": 0.31254735589027405,
      "learning_rate": 3.02134900990099e-05,
      "loss": 8.1234,
      "step": 3840
    },
    {
      "epoch": 1.1909365091640245,
      "grad_norm": 0.4189766049385071,
      "learning_rate": 3.0161922442244227e-05,
      "loss": 8.116,
      "step": 3850
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 0.6033019423484802,
      "learning_rate": 3.011035478547855e-05,
      "loss": 8.1241,
      "step": 3860
    },
    {
      "epoch": 1.1971231923285128,
      "grad_norm": 0.7565121650695801,
      "learning_rate": 3.0058787128712872e-05,
      "loss": 8.1112,
      "step": 3870
    },
    {
      "epoch": 1.200216533910757,
      "grad_norm": 0.772359311580658,
      "learning_rate": 3.0007219471947197e-05,
      "loss": 8.1289,
      "step": 3880
    },
    {
      "epoch": 1.2033098754930014,
      "grad_norm": 0.608413577079773,
      "learning_rate": 2.9955651815181518e-05,
      "loss": 8.1147,
      "step": 3890
    },
    {
      "epoch": 1.2064032170752454,
      "grad_norm": 0.42433735728263855,
      "learning_rate": 2.990408415841584e-05,
      "loss": 8.1163,
      "step": 3900
    },
    {
      "epoch": 1.2094965586574897,
      "grad_norm": 0.3343408703804016,
      "learning_rate": 2.9852516501650167e-05,
      "loss": 8.1171,
      "step": 3910
    },
    {
      "epoch": 1.212589900239734,
      "grad_norm": 0.36650922894477844,
      "learning_rate": 2.980094884488449e-05,
      "loss": 8.116,
      "step": 3920
    },
    {
      "epoch": 1.2156832418219783,
      "grad_norm": 0.4840732514858246,
      "learning_rate": 2.9749381188118813e-05,
      "loss": 8.1039,
      "step": 3930
    },
    {
      "epoch": 1.2187765834042223,
      "grad_norm": 0.27618497610092163,
      "learning_rate": 2.9697813531353137e-05,
      "loss": 8.1287,
      "step": 3940
    },
    {
      "epoch": 1.2218699249864666,
      "grad_norm": 0.3611321747303009,
      "learning_rate": 2.9646245874587458e-05,
      "loss": 8.1127,
      "step": 3950
    },
    {
      "epoch": 1.224963266568711,
      "grad_norm": 0.35339638590812683,
      "learning_rate": 2.9594678217821786e-05,
      "loss": 8.1186,
      "step": 3960
    },
    {
      "epoch": 1.228056608150955,
      "grad_norm": 0.39735960960388184,
      "learning_rate": 2.9543110561056107e-05,
      "loss": 8.1184,
      "step": 3970
    },
    {
      "epoch": 1.2311499497331992,
      "grad_norm": 0.55670565366745,
      "learning_rate": 2.9491542904290432e-05,
      "loss": 8.1079,
      "step": 3980
    },
    {
      "epoch": 1.2342432913154435,
      "grad_norm": 0.33344078063964844,
      "learning_rate": 2.9439975247524753e-05,
      "loss": 8.1201,
      "step": 3990
    },
    {
      "epoch": 1.2373366328976878,
      "grad_norm": 0.41633161902427673,
      "learning_rate": 2.9388407590759077e-05,
      "loss": 8.1225,
      "step": 4000
    },
    {
      "epoch": 1.2404299744799319,
      "grad_norm": 0.501749575138092,
      "learning_rate": 2.93368399339934e-05,
      "loss": 8.1213,
      "step": 4010
    },
    {
      "epoch": 1.2435233160621761,
      "grad_norm": 0.3026311993598938,
      "learning_rate": 2.9285272277227726e-05,
      "loss": 8.1176,
      "step": 4020
    },
    {
      "epoch": 1.2466166576444204,
      "grad_norm": 0.2881377339363098,
      "learning_rate": 2.9233704620462048e-05,
      "loss": 8.1196,
      "step": 4030
    },
    {
      "epoch": 1.2497099992266647,
      "grad_norm": 0.514464795589447,
      "learning_rate": 2.9182136963696372e-05,
      "loss": 8.1188,
      "step": 4040
    },
    {
      "epoch": 1.2528033408089088,
      "grad_norm": 0.712583601474762,
      "learning_rate": 2.9130569306930693e-05,
      "loss": 8.1071,
      "step": 4050
    },
    {
      "epoch": 1.255896682391153,
      "grad_norm": 0.43396151065826416,
      "learning_rate": 2.9079001650165018e-05,
      "loss": 8.1116,
      "step": 4060
    },
    {
      "epoch": 1.2589900239733973,
      "grad_norm": 0.4087185561656952,
      "learning_rate": 2.902743399339934e-05,
      "loss": 8.108,
      "step": 4070
    },
    {
      "epoch": 1.2620833655556414,
      "grad_norm": 0.2883619964122772,
      "learning_rate": 2.8975866336633667e-05,
      "loss": 8.1394,
      "step": 4080
    },
    {
      "epoch": 1.2651767071378857,
      "grad_norm": 0.3366921544075012,
      "learning_rate": 2.8924298679867988e-05,
      "loss": 8.1338,
      "step": 4090
    },
    {
      "epoch": 1.26827004872013,
      "grad_norm": 0.5183612704277039,
      "learning_rate": 2.8872731023102312e-05,
      "loss": 8.1083,
      "step": 4100
    },
    {
      "epoch": 1.2713633903023742,
      "grad_norm": 0.2704402804374695,
      "learning_rate": 2.8821163366336634e-05,
      "loss": 8.1054,
      "step": 4110
    },
    {
      "epoch": 1.2744567318846183,
      "grad_norm": 0.41018396615982056,
      "learning_rate": 2.8769595709570958e-05,
      "loss": 8.1006,
      "step": 4120
    },
    {
      "epoch": 1.2775500734668626,
      "grad_norm": 0.3978213965892792,
      "learning_rate": 2.871802805280528e-05,
      "loss": 8.1181,
      "step": 4130
    },
    {
      "epoch": 1.2806434150491068,
      "grad_norm": 0.4056070148944855,
      "learning_rate": 2.8666460396039607e-05,
      "loss": 8.1203,
      "step": 4140
    },
    {
      "epoch": 1.2837367566313511,
      "grad_norm": 0.3829956650733948,
      "learning_rate": 2.8614892739273928e-05,
      "loss": 8.1206,
      "step": 4150
    },
    {
      "epoch": 1.2868300982135952,
      "grad_norm": 0.38594838976860046,
      "learning_rate": 2.8563325082508253e-05,
      "loss": 8.1253,
      "step": 4160
    },
    {
      "epoch": 1.2899234397958395,
      "grad_norm": 0.37698736786842346,
      "learning_rate": 2.8511757425742574e-05,
      "loss": 8.1261,
      "step": 4170
    },
    {
      "epoch": 1.2930167813780837,
      "grad_norm": 0.5026506781578064,
      "learning_rate": 2.84601897689769e-05,
      "loss": 8.1211,
      "step": 4180
    },
    {
      "epoch": 1.2961101229603278,
      "grad_norm": 0.6055474281311035,
      "learning_rate": 2.8408622112211226e-05,
      "loss": 8.1094,
      "step": 4190
    },
    {
      "epoch": 1.299203464542572,
      "grad_norm": 0.6824824213981628,
      "learning_rate": 2.8357054455445547e-05,
      "loss": 8.1114,
      "step": 4200
    },
    {
      "epoch": 1.3022968061248164,
      "grad_norm": 0.5478950142860413,
      "learning_rate": 2.830548679867987e-05,
      "loss": 8.1217,
      "step": 4210
    },
    {
      "epoch": 1.3053901477070606,
      "grad_norm": 0.3826694190502167,
      "learning_rate": 2.8253919141914193e-05,
      "loss": 8.1253,
      "step": 4220
    },
    {
      "epoch": 1.3084834892893047,
      "grad_norm": 0.48064130544662476,
      "learning_rate": 2.8202351485148514e-05,
      "loss": 8.1138,
      "step": 4230
    },
    {
      "epoch": 1.311576830871549,
      "grad_norm": 0.39160454273223877,
      "learning_rate": 2.815078382838284e-05,
      "loss": 8.1188,
      "step": 4240
    },
    {
      "epoch": 1.3146701724537933,
      "grad_norm": 0.2657986581325531,
      "learning_rate": 2.8099216171617167e-05,
      "loss": 8.1138,
      "step": 4250
    },
    {
      "epoch": 1.3177635140360375,
      "grad_norm": 0.532305896282196,
      "learning_rate": 2.8047648514851488e-05,
      "loss": 8.1053,
      "step": 4260
    },
    {
      "epoch": 1.3208568556182816,
      "grad_norm": 0.6080952286720276,
      "learning_rate": 2.799608085808581e-05,
      "loss": 8.1192,
      "step": 4270
    },
    {
      "epoch": 1.3239501972005259,
      "grad_norm": 0.48363572359085083,
      "learning_rate": 2.7944513201320133e-05,
      "loss": 8.1123,
      "step": 4280
    },
    {
      "epoch": 1.3270435387827701,
      "grad_norm": 0.3829319179058075,
      "learning_rate": 2.7892945544554454e-05,
      "loss": 8.1207,
      "step": 4290
    },
    {
      "epoch": 1.3301368803650142,
      "grad_norm": 0.3387044370174408,
      "learning_rate": 2.784137788778878e-05,
      "loss": 8.112,
      "step": 4300
    },
    {
      "epoch": 1.3332302219472585,
      "grad_norm": 0.39727500081062317,
      "learning_rate": 2.7789810231023107e-05,
      "loss": 8.1033,
      "step": 4310
    },
    {
      "epoch": 1.3363235635295028,
      "grad_norm": 0.3162422180175781,
      "learning_rate": 2.7738242574257428e-05,
      "loss": 8.1165,
      "step": 4320
    },
    {
      "epoch": 1.339416905111747,
      "grad_norm": 0.32134270668029785,
      "learning_rate": 2.768667491749175e-05,
      "loss": 8.1217,
      "step": 4330
    },
    {
      "epoch": 1.342510246693991,
      "grad_norm": 0.3691631257534027,
      "learning_rate": 2.7635107260726074e-05,
      "loss": 8.1115,
      "step": 4340
    },
    {
      "epoch": 1.3456035882762354,
      "grad_norm": 0.2945464253425598,
      "learning_rate": 2.7583539603960395e-05,
      "loss": 8.1148,
      "step": 4350
    },
    {
      "epoch": 1.3486969298584797,
      "grad_norm": 0.35692381858825684,
      "learning_rate": 2.7531971947194723e-05,
      "loss": 8.1247,
      "step": 4360
    },
    {
      "epoch": 1.351790271440724,
      "grad_norm": 0.4028315544128418,
      "learning_rate": 2.7480404290429047e-05,
      "loss": 8.1223,
      "step": 4370
    },
    {
      "epoch": 1.354883613022968,
      "grad_norm": 0.3515092730522156,
      "learning_rate": 2.742883663366337e-05,
      "loss": 8.129,
      "step": 4380
    },
    {
      "epoch": 1.3579769546052123,
      "grad_norm": 0.3504179120063782,
      "learning_rate": 2.737726897689769e-05,
      "loss": 8.1166,
      "step": 4390
    },
    {
      "epoch": 1.3610702961874566,
      "grad_norm": 0.39078179001808167,
      "learning_rate": 2.7325701320132014e-05,
      "loss": 8.1082,
      "step": 4400
    },
    {
      "epoch": 1.3641636377697006,
      "grad_norm": 0.3065117597579956,
      "learning_rate": 2.7274133663366335e-05,
      "loss": 8.1135,
      "step": 4410
    },
    {
      "epoch": 1.367256979351945,
      "grad_norm": 0.42437365651130676,
      "learning_rate": 2.7222566006600663e-05,
      "loss": 8.1251,
      "step": 4420
    },
    {
      "epoch": 1.3703503209341892,
      "grad_norm": 0.4533443748950958,
      "learning_rate": 2.7170998349834988e-05,
      "loss": 8.134,
      "step": 4430
    },
    {
      "epoch": 1.3734436625164332,
      "grad_norm": 0.4104002118110657,
      "learning_rate": 2.711943069306931e-05,
      "loss": 8.117,
      "step": 4440
    },
    {
      "epoch": 1.3765370040986775,
      "grad_norm": 0.45117637515068054,
      "learning_rate": 2.706786303630363e-05,
      "loss": 8.1103,
      "step": 4450
    },
    {
      "epoch": 1.3796303456809218,
      "grad_norm": 0.4322780966758728,
      "learning_rate": 2.7016295379537954e-05,
      "loss": 8.1204,
      "step": 4460
    },
    {
      "epoch": 1.382723687263166,
      "grad_norm": 0.4125745892524719,
      "learning_rate": 2.6964727722772275e-05,
      "loss": 8.1169,
      "step": 4470
    },
    {
      "epoch": 1.3858170288454104,
      "grad_norm": 0.30549347400665283,
      "learning_rate": 2.6913160066006603e-05,
      "loss": 8.1115,
      "step": 4480
    },
    {
      "epoch": 1.3889103704276544,
      "grad_norm": 0.586147129535675,
      "learning_rate": 2.6861592409240928e-05,
      "loss": 8.1169,
      "step": 4490
    },
    {
      "epoch": 1.3920037120098987,
      "grad_norm": 0.5700451135635376,
      "learning_rate": 2.681002475247525e-05,
      "loss": 8.1042,
      "step": 4500
    },
    {
      "epoch": 1.395097053592143,
      "grad_norm": 0.6171928644180298,
      "learning_rate": 2.675845709570957e-05,
      "loss": 8.1124,
      "step": 4510
    },
    {
      "epoch": 1.398190395174387,
      "grad_norm": 0.32964393496513367,
      "learning_rate": 2.6706889438943895e-05,
      "loss": 8.1078,
      "step": 4520
    },
    {
      "epoch": 1.4012837367566313,
      "grad_norm": 0.5399203896522522,
      "learning_rate": 2.6655321782178216e-05,
      "loss": 8.1289,
      "step": 4530
    },
    {
      "epoch": 1.4043770783388756,
      "grad_norm": 0.5968202948570251,
      "learning_rate": 2.6603754125412544e-05,
      "loss": 8.1289,
      "step": 4540
    },
    {
      "epoch": 1.4074704199211197,
      "grad_norm": 0.3861410915851593,
      "learning_rate": 2.6552186468646868e-05,
      "loss": 8.1067,
      "step": 4550
    },
    {
      "epoch": 1.410563761503364,
      "grad_norm": 0.4893192648887634,
      "learning_rate": 2.650061881188119e-05,
      "loss": 8.1267,
      "step": 4560
    },
    {
      "epoch": 1.4136571030856082,
      "grad_norm": 0.5214089155197144,
      "learning_rate": 2.644905115511551e-05,
      "loss": 8.1288,
      "step": 4570
    },
    {
      "epoch": 1.4167504446678525,
      "grad_norm": 0.3378344774246216,
      "learning_rate": 2.6397483498349835e-05,
      "loss": 8.1134,
      "step": 4580
    },
    {
      "epoch": 1.4198437862500968,
      "grad_norm": 0.44469159841537476,
      "learning_rate": 2.6345915841584163e-05,
      "loss": 8.1031,
      "step": 4590
    },
    {
      "epoch": 1.4229371278323408,
      "grad_norm": 0.22259089350700378,
      "learning_rate": 2.6294348184818484e-05,
      "loss": 8.0982,
      "step": 4600
    },
    {
      "epoch": 1.4260304694145851,
      "grad_norm": 0.36517998576164246,
      "learning_rate": 2.624278052805281e-05,
      "loss": 8.1066,
      "step": 4610
    },
    {
      "epoch": 1.4291238109968294,
      "grad_norm": 0.7719037532806396,
      "learning_rate": 2.619121287128713e-05,
      "loss": 8.1144,
      "step": 4620
    },
    {
      "epoch": 1.4322171525790734,
      "grad_norm": 0.6055254936218262,
      "learning_rate": 2.613964521452145e-05,
      "loss": 8.1215,
      "step": 4630
    },
    {
      "epoch": 1.4353104941613177,
      "grad_norm": 0.4271112382411957,
      "learning_rate": 2.6088077557755775e-05,
      "loss": 8.1198,
      "step": 4640
    },
    {
      "epoch": 1.438403835743562,
      "grad_norm": 0.379040390253067,
      "learning_rate": 2.6036509900990103e-05,
      "loss": 8.1218,
      "step": 4650
    },
    {
      "epoch": 1.441497177325806,
      "grad_norm": 0.5326071977615356,
      "learning_rate": 2.5984942244224424e-05,
      "loss": 8.1125,
      "step": 4660
    },
    {
      "epoch": 1.4445905189080503,
      "grad_norm": 0.32501220703125,
      "learning_rate": 2.593337458745875e-05,
      "loss": 8.1223,
      "step": 4670
    },
    {
      "epoch": 1.4476838604902946,
      "grad_norm": 0.3981221616268158,
      "learning_rate": 2.588180693069307e-05,
      "loss": 8.1189,
      "step": 4680
    },
    {
      "epoch": 1.450777202072539,
      "grad_norm": 0.3031429052352905,
      "learning_rate": 2.583023927392739e-05,
      "loss": 8.1081,
      "step": 4690
    },
    {
      "epoch": 1.4538705436547832,
      "grad_norm": 0.37634286284446716,
      "learning_rate": 2.5778671617161716e-05,
      "loss": 8.1187,
      "step": 4700
    },
    {
      "epoch": 1.4569638852370272,
      "grad_norm": 0.315674364566803,
      "learning_rate": 2.5727103960396043e-05,
      "loss": 8.118,
      "step": 4710
    },
    {
      "epoch": 1.4600572268192715,
      "grad_norm": 0.3284686505794525,
      "learning_rate": 2.5675536303630365e-05,
      "loss": 8.122,
      "step": 4720
    },
    {
      "epoch": 1.4631505684015158,
      "grad_norm": 0.4847637712955475,
      "learning_rate": 2.562396864686469e-05,
      "loss": 8.1165,
      "step": 4730
    },
    {
      "epoch": 1.4662439099837599,
      "grad_norm": 0.39466243982315063,
      "learning_rate": 2.557240099009901e-05,
      "loss": 8.1149,
      "step": 4740
    },
    {
      "epoch": 1.4693372515660041,
      "grad_norm": 0.5453352332115173,
      "learning_rate": 2.552083333333333e-05,
      "loss": 8.1136,
      "step": 4750
    },
    {
      "epoch": 1.4724305931482484,
      "grad_norm": 0.4886261820793152,
      "learning_rate": 2.546926567656766e-05,
      "loss": 8.1199,
      "step": 4760
    },
    {
      "epoch": 1.4755239347304925,
      "grad_norm": 0.42821526527404785,
      "learning_rate": 2.5417698019801984e-05,
      "loss": 8.1178,
      "step": 4770
    },
    {
      "epoch": 1.4786172763127368,
      "grad_norm": 0.7074984908103943,
      "learning_rate": 2.5366130363036305e-05,
      "loss": 8.1241,
      "step": 4780
    },
    {
      "epoch": 1.481710617894981,
      "grad_norm": 0.5807104706764221,
      "learning_rate": 2.531456270627063e-05,
      "loss": 8.1186,
      "step": 4790
    },
    {
      "epoch": 1.4848039594772253,
      "grad_norm": 0.4224538803100586,
      "learning_rate": 2.526299504950495e-05,
      "loss": 8.1122,
      "step": 4800
    },
    {
      "epoch": 1.4878973010594696,
      "grad_norm": 0.38677918910980225,
      "learning_rate": 2.5211427392739272e-05,
      "loss": 8.1171,
      "step": 4810
    },
    {
      "epoch": 1.4909906426417137,
      "grad_norm": 0.38343796133995056,
      "learning_rate": 2.51598597359736e-05,
      "loss": 8.1138,
      "step": 4820
    },
    {
      "epoch": 1.494083984223958,
      "grad_norm": 0.40355929732322693,
      "learning_rate": 2.5108292079207924e-05,
      "loss": 8.1262,
      "step": 4830
    },
    {
      "epoch": 1.4971773258062022,
      "grad_norm": 0.35928165912628174,
      "learning_rate": 2.5056724422442245e-05,
      "loss": 8.1086,
      "step": 4840
    },
    {
      "epoch": 1.5002706673884463,
      "grad_norm": 0.3801371455192566,
      "learning_rate": 2.500515676567657e-05,
      "loss": 8.103,
      "step": 4850
    },
    {
      "epoch": 1.5033640089706906,
      "grad_norm": 0.30460894107818604,
      "learning_rate": 2.4953589108910894e-05,
      "loss": 8.1031,
      "step": 4860
    },
    {
      "epoch": 1.5064573505529348,
      "grad_norm": 0.282023161649704,
      "learning_rate": 2.4902021452145215e-05,
      "loss": 8.1141,
      "step": 4870
    },
    {
      "epoch": 1.509550692135179,
      "grad_norm": 0.3324950933456421,
      "learning_rate": 2.485045379537954e-05,
      "loss": 8.1191,
      "step": 4880
    },
    {
      "epoch": 1.5126440337174234,
      "grad_norm": 0.2611704468727112,
      "learning_rate": 2.4798886138613864e-05,
      "loss": 8.1161,
      "step": 4890
    },
    {
      "epoch": 1.5157373752996675,
      "grad_norm": 0.2603551745414734,
      "learning_rate": 2.4747318481848186e-05,
      "loss": 8.1197,
      "step": 4900
    },
    {
      "epoch": 1.5188307168819117,
      "grad_norm": 0.2768305242061615,
      "learning_rate": 2.469575082508251e-05,
      "loss": 8.1101,
      "step": 4910
    },
    {
      "epoch": 1.521924058464156,
      "grad_norm": 0.43332505226135254,
      "learning_rate": 2.4644183168316835e-05,
      "loss": 8.1193,
      "step": 4920
    },
    {
      "epoch": 1.5250174000464,
      "grad_norm": 0.3475053906440735,
      "learning_rate": 2.4592615511551156e-05,
      "loss": 8.0995,
      "step": 4930
    },
    {
      "epoch": 1.5281107416286444,
      "grad_norm": 0.3111686408519745,
      "learning_rate": 2.454104785478548e-05,
      "loss": 8.123,
      "step": 4940
    },
    {
      "epoch": 1.5312040832108886,
      "grad_norm": 0.2939739525318146,
      "learning_rate": 2.4489480198019805e-05,
      "loss": 8.1141,
      "step": 4950
    },
    {
      "epoch": 1.5342974247931327,
      "grad_norm": 0.31812024116516113,
      "learning_rate": 2.4437912541254126e-05,
      "loss": 8.1272,
      "step": 4960
    },
    {
      "epoch": 1.537390766375377,
      "grad_norm": 0.43832147121429443,
      "learning_rate": 2.438634488448845e-05,
      "loss": 8.1171,
      "step": 4970
    },
    {
      "epoch": 1.5404841079576213,
      "grad_norm": 0.27086737751960754,
      "learning_rate": 2.4334777227722775e-05,
      "loss": 8.1233,
      "step": 4980
    },
    {
      "epoch": 1.5435774495398653,
      "grad_norm": 0.43045997619628906,
      "learning_rate": 2.4283209570957096e-05,
      "loss": 8.1182,
      "step": 4990
    },
    {
      "epoch": 1.5466707911221098,
      "grad_norm": 0.2715110778808594,
      "learning_rate": 2.423164191419142e-05,
      "loss": 8.1046,
      "step": 5000
    },
    {
      "epoch": 1.5497641327043539,
      "grad_norm": 0.49153652787208557,
      "learning_rate": 2.4180074257425745e-05,
      "loss": 8.1262,
      "step": 5010
    },
    {
      "epoch": 1.5528574742865981,
      "grad_norm": 0.35572725534439087,
      "learning_rate": 2.4128506600660066e-05,
      "loss": 8.0977,
      "step": 5020
    },
    {
      "epoch": 1.5559508158688424,
      "grad_norm": 0.43426141142845154,
      "learning_rate": 2.407693894389439e-05,
      "loss": 8.1164,
      "step": 5030
    },
    {
      "epoch": 1.5590441574510865,
      "grad_norm": 0.4378886818885803,
      "learning_rate": 2.4025371287128715e-05,
      "loss": 8.1089,
      "step": 5040
    },
    {
      "epoch": 1.5621374990333308,
      "grad_norm": 0.3830816447734833,
      "learning_rate": 2.3973803630363036e-05,
      "loss": 8.104,
      "step": 5050
    },
    {
      "epoch": 1.565230840615575,
      "grad_norm": 0.2644784450531006,
      "learning_rate": 2.392223597359736e-05,
      "loss": 8.1279,
      "step": 5060
    },
    {
      "epoch": 1.568324182197819,
      "grad_norm": 0.4401475191116333,
      "learning_rate": 2.3870668316831685e-05,
      "loss": 8.0953,
      "step": 5070
    },
    {
      "epoch": 1.5714175237800634,
      "grad_norm": 0.5617383718490601,
      "learning_rate": 2.3819100660066007e-05,
      "loss": 8.1143,
      "step": 5080
    },
    {
      "epoch": 1.5745108653623077,
      "grad_norm": 0.31983160972595215,
      "learning_rate": 2.376753300330033e-05,
      "loss": 8.1228,
      "step": 5090
    },
    {
      "epoch": 1.5776042069445517,
      "grad_norm": 0.3521036207675934,
      "learning_rate": 2.3715965346534656e-05,
      "loss": 8.1217,
      "step": 5100
    },
    {
      "epoch": 1.5806975485267962,
      "grad_norm": 0.39423707127571106,
      "learning_rate": 2.3664397689768977e-05,
      "loss": 8.1238,
      "step": 5110
    },
    {
      "epoch": 1.5837908901090403,
      "grad_norm": 0.2931222915649414,
      "learning_rate": 2.36128300330033e-05,
      "loss": 8.1081,
      "step": 5120
    },
    {
      "epoch": 1.5868842316912846,
      "grad_norm": 0.2821921408176422,
      "learning_rate": 2.3561262376237626e-05,
      "loss": 8.1068,
      "step": 5130
    },
    {
      "epoch": 1.5899775732735288,
      "grad_norm": 0.2658059597015381,
      "learning_rate": 2.3509694719471947e-05,
      "loss": 8.1145,
      "step": 5140
    },
    {
      "epoch": 1.593070914855773,
      "grad_norm": 0.4207652509212494,
      "learning_rate": 2.345812706270627e-05,
      "loss": 8.1298,
      "step": 5150
    },
    {
      "epoch": 1.5961642564380172,
      "grad_norm": 0.42718178033828735,
      "learning_rate": 2.3406559405940596e-05,
      "loss": 8.1127,
      "step": 5160
    },
    {
      "epoch": 1.5992575980202615,
      "grad_norm": 0.2940678000450134,
      "learning_rate": 2.3354991749174917e-05,
      "loss": 8.1026,
      "step": 5170
    },
    {
      "epoch": 1.6023509396025055,
      "grad_norm": 0.3087778091430664,
      "learning_rate": 2.330342409240924e-05,
      "loss": 8.1216,
      "step": 5180
    },
    {
      "epoch": 1.6054442811847498,
      "grad_norm": 0.5367524027824402,
      "learning_rate": 2.3251856435643566e-05,
      "loss": 8.1147,
      "step": 5190
    },
    {
      "epoch": 1.608537622766994,
      "grad_norm": 0.36596015095710754,
      "learning_rate": 2.3200288778877887e-05,
      "loss": 8.1109,
      "step": 5200
    },
    {
      "epoch": 1.6116309643492381,
      "grad_norm": 0.2646893858909607,
      "learning_rate": 2.314872112211221e-05,
      "loss": 8.1211,
      "step": 5210
    },
    {
      "epoch": 1.6147243059314826,
      "grad_norm": 0.3614557087421417,
      "learning_rate": 2.3097153465346536e-05,
      "loss": 8.0991,
      "step": 5220
    },
    {
      "epoch": 1.6178176475137267,
      "grad_norm": 0.3392017185688019,
      "learning_rate": 2.3045585808580857e-05,
      "loss": 8.1221,
      "step": 5230
    },
    {
      "epoch": 1.620910989095971,
      "grad_norm": 0.3323889970779419,
      "learning_rate": 2.2994018151815182e-05,
      "loss": 8.1287,
      "step": 5240
    },
    {
      "epoch": 1.6240043306782153,
      "grad_norm": 0.492569237947464,
      "learning_rate": 2.2942450495049506e-05,
      "loss": 8.1192,
      "step": 5250
    },
    {
      "epoch": 1.6270976722604593,
      "grad_norm": 0.3047148883342743,
      "learning_rate": 2.289088283828383e-05,
      "loss": 8.1199,
      "step": 5260
    },
    {
      "epoch": 1.6301910138427036,
      "grad_norm": 0.29260000586509705,
      "learning_rate": 2.2839315181518152e-05,
      "loss": 8.1173,
      "step": 5270
    },
    {
      "epoch": 1.6332843554249479,
      "grad_norm": 0.4867652356624603,
      "learning_rate": 2.2787747524752477e-05,
      "loss": 8.1438,
      "step": 5280
    },
    {
      "epoch": 1.636377697007192,
      "grad_norm": 0.3653852045536041,
      "learning_rate": 2.27361798679868e-05,
      "loss": 8.1076,
      "step": 5290
    },
    {
      "epoch": 1.6394710385894362,
      "grad_norm": 0.4558671712875366,
      "learning_rate": 2.2684612211221122e-05,
      "loss": 8.1142,
      "step": 5300
    },
    {
      "epoch": 1.6425643801716805,
      "grad_norm": 0.4491367042064667,
      "learning_rate": 2.2633044554455447e-05,
      "loss": 8.1132,
      "step": 5310
    },
    {
      "epoch": 1.6456577217539246,
      "grad_norm": 0.25708335638046265,
      "learning_rate": 2.258147689768977e-05,
      "loss": 8.1103,
      "step": 5320
    },
    {
      "epoch": 1.648751063336169,
      "grad_norm": 0.2708413898944855,
      "learning_rate": 2.2529909240924092e-05,
      "loss": 8.1009,
      "step": 5330
    },
    {
      "epoch": 1.6518444049184131,
      "grad_norm": 0.5272645354270935,
      "learning_rate": 2.2478341584158417e-05,
      "loss": 8.1164,
      "step": 5340
    },
    {
      "epoch": 1.6549377465006572,
      "grad_norm": 0.3993907868862152,
      "learning_rate": 2.242677392739274e-05,
      "loss": 8.1135,
      "step": 5350
    },
    {
      "epoch": 1.6580310880829017,
      "grad_norm": 0.3561858832836151,
      "learning_rate": 2.2375206270627062e-05,
      "loss": 8.1075,
      "step": 5360
    },
    {
      "epoch": 1.6611244296651457,
      "grad_norm": 0.37055259943008423,
      "learning_rate": 2.2323638613861387e-05,
      "loss": 8.1202,
      "step": 5370
    },
    {
      "epoch": 1.66421777124739,
      "grad_norm": 0.48123592138290405,
      "learning_rate": 2.227207095709571e-05,
      "loss": 8.1229,
      "step": 5380
    },
    {
      "epoch": 1.6673111128296343,
      "grad_norm": 0.35667717456817627,
      "learning_rate": 2.2220503300330033e-05,
      "loss": 8.1108,
      "step": 5390
    },
    {
      "epoch": 1.6704044544118783,
      "grad_norm": 0.36848005652427673,
      "learning_rate": 2.2168935643564357e-05,
      "loss": 8.1235,
      "step": 5400
    },
    {
      "epoch": 1.6734977959941226,
      "grad_norm": 0.34149351716041565,
      "learning_rate": 2.211736798679868e-05,
      "loss": 8.1018,
      "step": 5410
    },
    {
      "epoch": 1.676591137576367,
      "grad_norm": 0.4111079275608063,
      "learning_rate": 2.2065800330033003e-05,
      "loss": 8.0896,
      "step": 5420
    },
    {
      "epoch": 1.679684479158611,
      "grad_norm": 0.36586764454841614,
      "learning_rate": 2.2014232673267327e-05,
      "loss": 8.0966,
      "step": 5430
    },
    {
      "epoch": 1.6827778207408555,
      "grad_norm": 0.3386894464492798,
      "learning_rate": 2.1962665016501652e-05,
      "loss": 8.1228,
      "step": 5440
    },
    {
      "epoch": 1.6858711623230995,
      "grad_norm": 0.5258281826972961,
      "learning_rate": 2.1911097359735973e-05,
      "loss": 8.1333,
      "step": 5450
    },
    {
      "epoch": 1.6889645039053436,
      "grad_norm": 0.3382556140422821,
      "learning_rate": 2.18595297029703e-05,
      "loss": 8.1198,
      "step": 5460
    },
    {
      "epoch": 1.692057845487588,
      "grad_norm": 0.4059039354324341,
      "learning_rate": 2.1807962046204622e-05,
      "loss": 8.1241,
      "step": 5470
    },
    {
      "epoch": 1.6951511870698321,
      "grad_norm": 0.269603431224823,
      "learning_rate": 2.1756394389438943e-05,
      "loss": 8.1301,
      "step": 5480
    },
    {
      "epoch": 1.6982445286520764,
      "grad_norm": 0.5168254971504211,
      "learning_rate": 2.170482673267327e-05,
      "loss": 8.1223,
      "step": 5490
    },
    {
      "epoch": 1.7013378702343207,
      "grad_norm": 0.30710679292678833,
      "learning_rate": 2.1653259075907592e-05,
      "loss": 8.1058,
      "step": 5500
    },
    {
      "epoch": 1.7044312118165648,
      "grad_norm": 0.3913874626159668,
      "learning_rate": 2.1601691419141913e-05,
      "loss": 8.108,
      "step": 5510
    },
    {
      "epoch": 1.707524553398809,
      "grad_norm": 0.2808830738067627,
      "learning_rate": 2.155012376237624e-05,
      "loss": 8.1287,
      "step": 5520
    },
    {
      "epoch": 1.7106178949810533,
      "grad_norm": 0.40196681022644043,
      "learning_rate": 2.1498556105610562e-05,
      "loss": 8.1053,
      "step": 5530
    },
    {
      "epoch": 1.7137112365632974,
      "grad_norm": 0.3925170302391052,
      "learning_rate": 2.1446988448844883e-05,
      "loss": 8.1182,
      "step": 5540
    },
    {
      "epoch": 1.7168045781455419,
      "grad_norm": 0.547409176826477,
      "learning_rate": 2.139542079207921e-05,
      "loss": 8.1187,
      "step": 5550
    },
    {
      "epoch": 1.719897919727786,
      "grad_norm": 0.39337581396102905,
      "learning_rate": 2.1343853135313532e-05,
      "loss": 8.1085,
      "step": 5560
    },
    {
      "epoch": 1.72299126131003,
      "grad_norm": 0.4212924540042877,
      "learning_rate": 2.1292285478547854e-05,
      "loss": 8.1179,
      "step": 5570
    },
    {
      "epoch": 1.7260846028922745,
      "grad_norm": 0.4779733121395111,
      "learning_rate": 2.124071782178218e-05,
      "loss": 8.1171,
      "step": 5580
    },
    {
      "epoch": 1.7291779444745186,
      "grad_norm": 0.4392246901988983,
      "learning_rate": 2.1189150165016503e-05,
      "loss": 8.1076,
      "step": 5590
    },
    {
      "epoch": 1.7322712860567628,
      "grad_norm": 0.320768266916275,
      "learning_rate": 2.1137582508250824e-05,
      "loss": 8.1314,
      "step": 5600
    },
    {
      "epoch": 1.7353646276390071,
      "grad_norm": 0.26916560530662537,
      "learning_rate": 2.108601485148515e-05,
      "loss": 8.1142,
      "step": 5610
    },
    {
      "epoch": 1.7384579692212512,
      "grad_norm": 0.30974334478378296,
      "learning_rate": 2.1034447194719473e-05,
      "loss": 8.1203,
      "step": 5620
    },
    {
      "epoch": 1.7415513108034955,
      "grad_norm": 0.3137980103492737,
      "learning_rate": 2.0988036303630366e-05,
      "loss": 8.1215,
      "step": 5630
    },
    {
      "epoch": 1.7446446523857397,
      "grad_norm": 0.3749192953109741,
      "learning_rate": 2.0936468646864687e-05,
      "loss": 8.1246,
      "step": 5640
    },
    {
      "epoch": 1.7477379939679838,
      "grad_norm": 0.4229094088077545,
      "learning_rate": 2.088490099009901e-05,
      "loss": 8.1132,
      "step": 5650
    },
    {
      "epoch": 1.7508313355502283,
      "grad_norm": 0.42650091648101807,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 8.1182,
      "step": 5660
    },
    {
      "epoch": 1.7539246771324724,
      "grad_norm": 0.3548744320869446,
      "learning_rate": 2.0781765676567657e-05,
      "loss": 8.1259,
      "step": 5670
    },
    {
      "epoch": 1.7570180187147164,
      "grad_norm": 0.396202027797699,
      "learning_rate": 2.073019801980198e-05,
      "loss": 8.1283,
      "step": 5680
    },
    {
      "epoch": 1.760111360296961,
      "grad_norm": 0.3664032220840454,
      "learning_rate": 2.0678630363036306e-05,
      "loss": 8.1191,
      "step": 5690
    },
    {
      "epoch": 1.763204701879205,
      "grad_norm": 0.44298452138900757,
      "learning_rate": 2.0627062706270627e-05,
      "loss": 8.1129,
      "step": 5700
    },
    {
      "epoch": 1.7662980434614493,
      "grad_norm": 0.4628540277481079,
      "learning_rate": 2.0575495049504952e-05,
      "loss": 8.1319,
      "step": 5710
    },
    {
      "epoch": 1.7693913850436935,
      "grad_norm": 0.4609465003013611,
      "learning_rate": 2.0523927392739276e-05,
      "loss": 8.1154,
      "step": 5720
    },
    {
      "epoch": 1.7724847266259376,
      "grad_norm": 0.35635679960250854,
      "learning_rate": 2.0472359735973597e-05,
      "loss": 8.1076,
      "step": 5730
    },
    {
      "epoch": 1.7755780682081819,
      "grad_norm": 0.19998767971992493,
      "learning_rate": 2.0420792079207922e-05,
      "loss": 8.1182,
      "step": 5740
    },
    {
      "epoch": 1.7786714097904262,
      "grad_norm": 0.32161298394203186,
      "learning_rate": 2.0369224422442247e-05,
      "loss": 8.1023,
      "step": 5750
    },
    {
      "epoch": 1.7817647513726702,
      "grad_norm": 0.46438688039779663,
      "learning_rate": 2.0317656765676568e-05,
      "loss": 8.1072,
      "step": 5760
    },
    {
      "epoch": 1.7848580929549147,
      "grad_norm": 0.2995588481426239,
      "learning_rate": 2.0266089108910892e-05,
      "loss": 8.1169,
      "step": 5770
    },
    {
      "epoch": 1.7879514345371588,
      "grad_norm": 0.3121281862258911,
      "learning_rate": 2.0214521452145217e-05,
      "loss": 8.1117,
      "step": 5780
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 0.3287404477596283,
      "learning_rate": 2.0162953795379538e-05,
      "loss": 8.1138,
      "step": 5790
    },
    {
      "epoch": 1.7941381177016473,
      "grad_norm": 0.36478081345558167,
      "learning_rate": 2.0111386138613862e-05,
      "loss": 8.1118,
      "step": 5800
    },
    {
      "epoch": 1.7972314592838914,
      "grad_norm": 0.3136803209781647,
      "learning_rate": 2.0059818481848187e-05,
      "loss": 8.1145,
      "step": 5810
    },
    {
      "epoch": 1.8003248008661357,
      "grad_norm": 0.3161429166793823,
      "learning_rate": 2.0008250825082508e-05,
      "loss": 8.114,
      "step": 5820
    },
    {
      "epoch": 1.80341814244838,
      "grad_norm": 0.2943817377090454,
      "learning_rate": 1.9956683168316832e-05,
      "loss": 8.1073,
      "step": 5830
    },
    {
      "epoch": 1.806511484030624,
      "grad_norm": 0.2662045955657959,
      "learning_rate": 1.9905115511551157e-05,
      "loss": 8.1031,
      "step": 5840
    },
    {
      "epoch": 1.8096048256128683,
      "grad_norm": 0.29222506284713745,
      "learning_rate": 1.9853547854785478e-05,
      "loss": 8.1127,
      "step": 5850
    },
    {
      "epoch": 1.8126981671951126,
      "grad_norm": 0.37422361969947815,
      "learning_rate": 1.9801980198019803e-05,
      "loss": 8.1232,
      "step": 5860
    },
    {
      "epoch": 1.8157915087773566,
      "grad_norm": 0.765714168548584,
      "learning_rate": 1.9750412541254127e-05,
      "loss": 8.1064,
      "step": 5870
    },
    {
      "epoch": 1.8188848503596011,
      "grad_norm": 0.7310808897018433,
      "learning_rate": 1.9698844884488448e-05,
      "loss": 8.1248,
      "step": 5880
    },
    {
      "epoch": 1.8219781919418452,
      "grad_norm": 0.6556851863861084,
      "learning_rate": 1.9647277227722773e-05,
      "loss": 8.1217,
      "step": 5890
    },
    {
      "epoch": 1.8250715335240892,
      "grad_norm": 0.31249043345451355,
      "learning_rate": 1.9595709570957097e-05,
      "loss": 8.1036,
      "step": 5900
    },
    {
      "epoch": 1.8281648751063337,
      "grad_norm": 0.30837708711624146,
      "learning_rate": 1.954414191419142e-05,
      "loss": 8.1067,
      "step": 5910
    },
    {
      "epoch": 1.8312582166885778,
      "grad_norm": 0.42265650629997253,
      "learning_rate": 1.9492574257425743e-05,
      "loss": 8.1149,
      "step": 5920
    },
    {
      "epoch": 1.834351558270822,
      "grad_norm": 0.2732248902320862,
      "learning_rate": 1.9441006600660067e-05,
      "loss": 8.1131,
      "step": 5930
    },
    {
      "epoch": 1.8374448998530664,
      "grad_norm": 0.3759763538837433,
      "learning_rate": 1.938943894389439e-05,
      "loss": 8.1215,
      "step": 5940
    },
    {
      "epoch": 1.8405382414353104,
      "grad_norm": 0.27948513627052307,
      "learning_rate": 1.9337871287128713e-05,
      "loss": 8.124,
      "step": 5950
    },
    {
      "epoch": 1.8436315830175547,
      "grad_norm": 0.4848470091819763,
      "learning_rate": 1.9286303630363038e-05,
      "loss": 8.1167,
      "step": 5960
    },
    {
      "epoch": 1.846724924599799,
      "grad_norm": 0.3232216536998749,
      "learning_rate": 1.9234735973597362e-05,
      "loss": 8.1155,
      "step": 5970
    },
    {
      "epoch": 1.849818266182043,
      "grad_norm": 0.5008972883224487,
      "learning_rate": 1.9183168316831683e-05,
      "loss": 8.1032,
      "step": 5980
    },
    {
      "epoch": 1.8529116077642873,
      "grad_norm": 0.24281291663646698,
      "learning_rate": 1.9131600660066008e-05,
      "loss": 8.1095,
      "step": 5990
    },
    {
      "epoch": 1.8560049493465316,
      "grad_norm": 0.3586095869541168,
      "learning_rate": 1.9080033003300332e-05,
      "loss": 8.1009,
      "step": 6000
    },
    {
      "epoch": 1.8590982909287757,
      "grad_norm": 0.267071932554245,
      "learning_rate": 1.9028465346534653e-05,
      "loss": 8.1246,
      "step": 6010
    },
    {
      "epoch": 1.8621916325110202,
      "grad_norm": 0.29441991448402405,
      "learning_rate": 1.8976897689768978e-05,
      "loss": 8.1123,
      "step": 6020
    },
    {
      "epoch": 1.8652849740932642,
      "grad_norm": 0.21051467955112457,
      "learning_rate": 1.8925330033003302e-05,
      "loss": 8.115,
      "step": 6030
    },
    {
      "epoch": 1.8683783156755085,
      "grad_norm": 0.3026285469532013,
      "learning_rate": 1.8873762376237624e-05,
      "loss": 8.1248,
      "step": 6040
    },
    {
      "epoch": 1.8714716572577528,
      "grad_norm": 0.38800904154777527,
      "learning_rate": 1.8822194719471948e-05,
      "loss": 8.1034,
      "step": 6050
    },
    {
      "epoch": 1.8745649988399968,
      "grad_norm": 0.36174675822257996,
      "learning_rate": 1.8770627062706273e-05,
      "loss": 8.1208,
      "step": 6060
    },
    {
      "epoch": 1.8776583404222411,
      "grad_norm": 0.4842912554740906,
      "learning_rate": 1.8719059405940594e-05,
      "loss": 8.1208,
      "step": 6070
    },
    {
      "epoch": 1.8807516820044854,
      "grad_norm": 0.5075324177742004,
      "learning_rate": 1.8667491749174918e-05,
      "loss": 8.1214,
      "step": 6080
    },
    {
      "epoch": 1.8838450235867295,
      "grad_norm": 0.22948698699474335,
      "learning_rate": 1.8615924092409243e-05,
      "loss": 8.1083,
      "step": 6090
    },
    {
      "epoch": 1.8869383651689737,
      "grad_norm": 0.6177296042442322,
      "learning_rate": 1.8564356435643564e-05,
      "loss": 8.0954,
      "step": 6100
    },
    {
      "epoch": 1.890031706751218,
      "grad_norm": 0.2756254971027374,
      "learning_rate": 1.851278877887789e-05,
      "loss": 8.1083,
      "step": 6110
    },
    {
      "epoch": 1.893125048333462,
      "grad_norm": 0.3222353160381317,
      "learning_rate": 1.8461221122112213e-05,
      "loss": 8.1203,
      "step": 6120
    },
    {
      "epoch": 1.8962183899157066,
      "grad_norm": 0.27206942439079285,
      "learning_rate": 1.8409653465346534e-05,
      "loss": 8.0999,
      "step": 6130
    },
    {
      "epoch": 1.8993117314979506,
      "grad_norm": 0.45384761691093445,
      "learning_rate": 1.835808580858086e-05,
      "loss": 8.1166,
      "step": 6140
    },
    {
      "epoch": 1.902405073080195,
      "grad_norm": 0.28409743309020996,
      "learning_rate": 1.8306518151815183e-05,
      "loss": 8.1181,
      "step": 6150
    },
    {
      "epoch": 1.9054984146624392,
      "grad_norm": 0.38042542338371277,
      "learning_rate": 1.8254950495049504e-05,
      "loss": 8.1005,
      "step": 6160
    },
    {
      "epoch": 1.9085917562446832,
      "grad_norm": 0.3177722990512848,
      "learning_rate": 1.8203382838283832e-05,
      "loss": 8.1063,
      "step": 6170
    },
    {
      "epoch": 1.9116850978269275,
      "grad_norm": 0.34601715207099915,
      "learning_rate": 1.8151815181518153e-05,
      "loss": 8.1198,
      "step": 6180
    },
    {
      "epoch": 1.9147784394091718,
      "grad_norm": 0.3328571617603302,
      "learning_rate": 1.8100247524752474e-05,
      "loss": 8.1173,
      "step": 6190
    },
    {
      "epoch": 1.9178717809914159,
      "grad_norm": 0.2715587615966797,
      "learning_rate": 1.8048679867986802e-05,
      "loss": 8.1251,
      "step": 6200
    },
    {
      "epoch": 1.9209651225736601,
      "grad_norm": 0.4588717520236969,
      "learning_rate": 1.7997112211221123e-05,
      "loss": 8.1114,
      "step": 6210
    },
    {
      "epoch": 1.9240584641559044,
      "grad_norm": 0.3753533363342285,
      "learning_rate": 1.7945544554455445e-05,
      "loss": 8.1033,
      "step": 6220
    },
    {
      "epoch": 1.9271518057381485,
      "grad_norm": 0.5988497734069824,
      "learning_rate": 1.7893976897689772e-05,
      "loss": 8.1163,
      "step": 6230
    },
    {
      "epoch": 1.930245147320393,
      "grad_norm": 0.28665950894355774,
      "learning_rate": 1.7842409240924094e-05,
      "loss": 8.1325,
      "step": 6240
    },
    {
      "epoch": 1.933338488902637,
      "grad_norm": 0.3457224667072296,
      "learning_rate": 1.7790841584158415e-05,
      "loss": 8.092,
      "step": 6250
    },
    {
      "epoch": 1.9364318304848813,
      "grad_norm": 0.26928579807281494,
      "learning_rate": 1.7739273927392743e-05,
      "loss": 8.1164,
      "step": 6260
    },
    {
      "epoch": 1.9395251720671256,
      "grad_norm": 0.27532437443733215,
      "learning_rate": 1.7687706270627064e-05,
      "loss": 8.1124,
      "step": 6270
    },
    {
      "epoch": 1.9426185136493697,
      "grad_norm": 0.3984276354312897,
      "learning_rate": 1.7636138613861385e-05,
      "loss": 8.1275,
      "step": 6280
    },
    {
      "epoch": 1.945711855231614,
      "grad_norm": 0.2620258629322052,
      "learning_rate": 1.7584570957095713e-05,
      "loss": 8.1087,
      "step": 6290
    },
    {
      "epoch": 1.9488051968138582,
      "grad_norm": 0.3988026976585388,
      "learning_rate": 1.7533003300330034e-05,
      "loss": 8.1124,
      "step": 6300
    },
    {
      "epoch": 1.9518985383961023,
      "grad_norm": 0.40145573019981384,
      "learning_rate": 1.7481435643564355e-05,
      "loss": 8.1145,
      "step": 6310
    },
    {
      "epoch": 1.9549918799783466,
      "grad_norm": 0.2604743242263794,
      "learning_rate": 1.7429867986798683e-05,
      "loss": 8.1008,
      "step": 6320
    },
    {
      "epoch": 1.9580852215605908,
      "grad_norm": 0.4447406232357025,
      "learning_rate": 1.7378300330033004e-05,
      "loss": 8.1022,
      "step": 6330
    },
    {
      "epoch": 1.961178563142835,
      "grad_norm": 0.26674920320510864,
      "learning_rate": 1.7326732673267325e-05,
      "loss": 8.1206,
      "step": 6340
    },
    {
      "epoch": 1.9642719047250794,
      "grad_norm": 0.4713584780693054,
      "learning_rate": 1.7275165016501653e-05,
      "loss": 8.1177,
      "step": 6350
    },
    {
      "epoch": 1.9673652463073235,
      "grad_norm": 0.2687281370162964,
      "learning_rate": 1.7223597359735974e-05,
      "loss": 8.1134,
      "step": 6360
    },
    {
      "epoch": 1.9704585878895677,
      "grad_norm": 0.2998368740081787,
      "learning_rate": 1.71720297029703e-05,
      "loss": 8.1209,
      "step": 6370
    },
    {
      "epoch": 1.973551929471812,
      "grad_norm": 0.4223797917366028,
      "learning_rate": 1.7120462046204623e-05,
      "loss": 8.1234,
      "step": 6380
    },
    {
      "epoch": 1.976645271054056,
      "grad_norm": 0.30387479066848755,
      "learning_rate": 1.7068894389438944e-05,
      "loss": 8.1116,
      "step": 6390
    },
    {
      "epoch": 1.9797386126363004,
      "grad_norm": 0.41366714239120483,
      "learning_rate": 1.701732673267327e-05,
      "loss": 8.104,
      "step": 6400
    },
    {
      "epoch": 1.9828319542185446,
      "grad_norm": 0.33373579382896423,
      "learning_rate": 1.6965759075907593e-05,
      "loss": 8.1258,
      "step": 6410
    },
    {
      "epoch": 1.9859252958007887,
      "grad_norm": 0.3197133243083954,
      "learning_rate": 1.6914191419141915e-05,
      "loss": 8.1133,
      "step": 6420
    },
    {
      "epoch": 1.989018637383033,
      "grad_norm": 0.2570471465587616,
      "learning_rate": 1.686262376237624e-05,
      "loss": 8.1176,
      "step": 6430
    },
    {
      "epoch": 1.9921119789652773,
      "grad_norm": 0.2578210234642029,
      "learning_rate": 1.6811056105610564e-05,
      "loss": 8.114,
      "step": 6440
    },
    {
      "epoch": 1.9952053205475213,
      "grad_norm": 0.3415299654006958,
      "learning_rate": 1.6759488448844885e-05,
      "loss": 8.1212,
      "step": 6450
    },
    {
      "epoch": 1.9982986621297658,
      "grad_norm": 0.32928600907325745,
      "learning_rate": 1.670792079207921e-05,
      "loss": 8.1084,
      "step": 6460
    },
    {
      "epoch": 2.00139200371201,
      "grad_norm": 0.3473144769668579,
      "learning_rate": 1.6656353135313534e-05,
      "loss": 8.1069,
      "step": 6470
    },
    {
      "epoch": 2.004485345294254,
      "grad_norm": 0.37175726890563965,
      "learning_rate": 1.6604785478547855e-05,
      "loss": 8.1177,
      "step": 6480
    },
    {
      "epoch": 2.0075786868764984,
      "grad_norm": 0.4128469228744507,
      "learning_rate": 1.655321782178218e-05,
      "loss": 8.1073,
      "step": 6490
    },
    {
      "epoch": 2.0106720284587425,
      "grad_norm": 0.32730695605278015,
      "learning_rate": 1.6501650165016504e-05,
      "loss": 8.1137,
      "step": 6500
    },
    {
      "epoch": 2.013765370040987,
      "grad_norm": 0.3261179029941559,
      "learning_rate": 1.6450082508250825e-05,
      "loss": 8.1027,
      "step": 6510
    },
    {
      "epoch": 2.016858711623231,
      "grad_norm": 0.3720901608467102,
      "learning_rate": 1.639851485148515e-05,
      "loss": 8.1175,
      "step": 6520
    },
    {
      "epoch": 2.019952053205475,
      "grad_norm": 0.28922131657600403,
      "learning_rate": 1.6346947194719474e-05,
      "loss": 8.1065,
      "step": 6530
    },
    {
      "epoch": 2.0230453947877196,
      "grad_norm": 0.27868109941482544,
      "learning_rate": 1.6295379537953795e-05,
      "loss": 8.1077,
      "step": 6540
    },
    {
      "epoch": 2.0261387363699637,
      "grad_norm": 0.4218499958515167,
      "learning_rate": 1.624381188118812e-05,
      "loss": 8.1155,
      "step": 6550
    },
    {
      "epoch": 2.0292320779522077,
      "grad_norm": 0.26156070828437805,
      "learning_rate": 1.6192244224422444e-05,
      "loss": 8.1096,
      "step": 6560
    },
    {
      "epoch": 2.0323254195344522,
      "grad_norm": 0.25813406705856323,
      "learning_rate": 1.614067656765677e-05,
      "loss": 8.108,
      "step": 6570
    },
    {
      "epoch": 2.0354187611166963,
      "grad_norm": 0.3111276924610138,
      "learning_rate": 1.608910891089109e-05,
      "loss": 8.1131,
      "step": 6580
    },
    {
      "epoch": 2.0385121026989403,
      "grad_norm": 0.4862297773361206,
      "learning_rate": 1.6037541254125414e-05,
      "loss": 8.1185,
      "step": 6590
    },
    {
      "epoch": 2.041605444281185,
      "grad_norm": 0.5999602675437927,
      "learning_rate": 1.598597359735974e-05,
      "loss": 8.1228,
      "step": 6600
    },
    {
      "epoch": 2.044698785863429,
      "grad_norm": 0.4186936020851135,
      "learning_rate": 1.593440594059406e-05,
      "loss": 8.1044,
      "step": 6610
    },
    {
      "epoch": 2.0477921274456734,
      "grad_norm": 0.28737542033195496,
      "learning_rate": 1.5882838283828385e-05,
      "loss": 8.1052,
      "step": 6620
    },
    {
      "epoch": 2.0508854690279175,
      "grad_norm": 0.4794773459434509,
      "learning_rate": 1.583127062706271e-05,
      "loss": 8.1119,
      "step": 6630
    },
    {
      "epoch": 2.0539788106101615,
      "grad_norm": 0.3499177098274231,
      "learning_rate": 1.577970297029703e-05,
      "loss": 8.1022,
      "step": 6640
    },
    {
      "epoch": 2.057072152192406,
      "grad_norm": 0.37198951840400696,
      "learning_rate": 1.5728135313531355e-05,
      "loss": 8.1179,
      "step": 6650
    },
    {
      "epoch": 2.06016549377465,
      "grad_norm": 0.34858450293540955,
      "learning_rate": 1.567656765676568e-05,
      "loss": 8.1225,
      "step": 6660
    },
    {
      "epoch": 2.063258835356894,
      "grad_norm": 0.3488282561302185,
      "learning_rate": 1.5625e-05,
      "loss": 8.0965,
      "step": 6670
    },
    {
      "epoch": 2.0663521769391386,
      "grad_norm": 0.3803441524505615,
      "learning_rate": 1.5573432343234325e-05,
      "loss": 8.0966,
      "step": 6680
    },
    {
      "epoch": 2.0694455185213827,
      "grad_norm": 0.34569627046585083,
      "learning_rate": 1.552186468646865e-05,
      "loss": 8.1127,
      "step": 6690
    },
    {
      "epoch": 2.0725388601036268,
      "grad_norm": 0.3481329083442688,
      "learning_rate": 1.547029702970297e-05,
      "loss": 8.1106,
      "step": 6700
    },
    {
      "epoch": 2.0756322016858713,
      "grad_norm": 0.2873219847679138,
      "learning_rate": 1.5418729372937295e-05,
      "loss": 8.1097,
      "step": 6710
    },
    {
      "epoch": 2.0787255432681153,
      "grad_norm": 0.32094720005989075,
      "learning_rate": 1.536716171617162e-05,
      "loss": 8.1001,
      "step": 6720
    },
    {
      "epoch": 2.0818188848503594,
      "grad_norm": 0.3336035907268524,
      "learning_rate": 1.531559405940594e-05,
      "loss": 8.1049,
      "step": 6730
    },
    {
      "epoch": 2.084912226432604,
      "grad_norm": 0.41885706782341003,
      "learning_rate": 1.5264026402640265e-05,
      "loss": 8.1293,
      "step": 6740
    },
    {
      "epoch": 2.088005568014848,
      "grad_norm": 0.31827980279922485,
      "learning_rate": 1.5212458745874588e-05,
      "loss": 8.0954,
      "step": 6750
    },
    {
      "epoch": 2.0910989095970924,
      "grad_norm": 0.39858725666999817,
      "learning_rate": 1.516089108910891e-05,
      "loss": 8.1191,
      "step": 6760
    },
    {
      "epoch": 2.0941922511793365,
      "grad_norm": 0.31525784730911255,
      "learning_rate": 1.5109323432343234e-05,
      "loss": 8.1191,
      "step": 6770
    },
    {
      "epoch": 2.0972855927615806,
      "grad_norm": 0.4423603415489197,
      "learning_rate": 1.5057755775577558e-05,
      "loss": 8.1107,
      "step": 6780
    },
    {
      "epoch": 2.100378934343825,
      "grad_norm": 0.3455723226070404,
      "learning_rate": 1.5006188118811881e-05,
      "loss": 8.1186,
      "step": 6790
    },
    {
      "epoch": 2.103472275926069,
      "grad_norm": 0.3108551800251007,
      "learning_rate": 1.4954620462046207e-05,
      "loss": 8.1086,
      "step": 6800
    },
    {
      "epoch": 2.106565617508313,
      "grad_norm": 0.37281981110572815,
      "learning_rate": 1.4903052805280528e-05,
      "loss": 8.1135,
      "step": 6810
    },
    {
      "epoch": 2.1096589590905577,
      "grad_norm": 0.22430337965488434,
      "learning_rate": 1.4851485148514851e-05,
      "loss": 8.1182,
      "step": 6820
    },
    {
      "epoch": 2.1127523006728017,
      "grad_norm": 0.4419286549091339,
      "learning_rate": 1.4799917491749177e-05,
      "loss": 8.1112,
      "step": 6830
    },
    {
      "epoch": 2.115845642255046,
      "grad_norm": 0.333791047334671,
      "learning_rate": 1.4748349834983498e-05,
      "loss": 8.113,
      "step": 6840
    },
    {
      "epoch": 2.1189389838372903,
      "grad_norm": 0.3952817916870117,
      "learning_rate": 1.4696782178217821e-05,
      "loss": 8.1102,
      "step": 6850
    },
    {
      "epoch": 2.1220323254195343,
      "grad_norm": 0.3149864077568054,
      "learning_rate": 1.4645214521452147e-05,
      "loss": 8.1109,
      "step": 6860
    },
    {
      "epoch": 2.125125667001779,
      "grad_norm": 0.3381395637989044,
      "learning_rate": 1.4593646864686469e-05,
      "loss": 8.1094,
      "step": 6870
    },
    {
      "epoch": 2.128219008584023,
      "grad_norm": 0.5811271667480469,
      "learning_rate": 1.4542079207920791e-05,
      "loss": 8.114,
      "step": 6880
    },
    {
      "epoch": 2.131312350166267,
      "grad_norm": 0.305721253156662,
      "learning_rate": 1.4490511551155118e-05,
      "loss": 8.1059,
      "step": 6890
    },
    {
      "epoch": 2.1344056917485115,
      "grad_norm": 0.3305288255214691,
      "learning_rate": 1.4438943894389439e-05,
      "loss": 8.1192,
      "step": 6900
    },
    {
      "epoch": 2.1374990333307555,
      "grad_norm": 0.39431706070899963,
      "learning_rate": 1.4387376237623762e-05,
      "loss": 8.1187,
      "step": 6910
    },
    {
      "epoch": 2.1405923749129996,
      "grad_norm": 0.34418433904647827,
      "learning_rate": 1.4335808580858088e-05,
      "loss": 8.1095,
      "step": 6920
    },
    {
      "epoch": 2.143685716495244,
      "grad_norm": 0.3364504873752594,
      "learning_rate": 1.4284240924092409e-05,
      "loss": 8.1227,
      "step": 6930
    },
    {
      "epoch": 2.146779058077488,
      "grad_norm": 0.2513867914676666,
      "learning_rate": 1.4232673267326732e-05,
      "loss": 8.1081,
      "step": 6940
    },
    {
      "epoch": 2.149872399659732,
      "grad_norm": 0.4405776262283325,
      "learning_rate": 1.4181105610561058e-05,
      "loss": 8.1115,
      "step": 6950
    },
    {
      "epoch": 2.1529657412419767,
      "grad_norm": 0.33518165349960327,
      "learning_rate": 1.4129537953795379e-05,
      "loss": 8.1165,
      "step": 6960
    },
    {
      "epoch": 2.1560590828242208,
      "grad_norm": 0.5020695328712463,
      "learning_rate": 1.4077970297029702e-05,
      "loss": 8.1104,
      "step": 6970
    },
    {
      "epoch": 2.1591524244064653,
      "grad_norm": 0.38686853647232056,
      "learning_rate": 1.4026402640264028e-05,
      "loss": 8.1114,
      "step": 6980
    },
    {
      "epoch": 2.1622457659887093,
      "grad_norm": 0.29850274324417114,
      "learning_rate": 1.397483498349835e-05,
      "loss": 8.1163,
      "step": 6990
    },
    {
      "epoch": 2.1653391075709534,
      "grad_norm": 0.3843124806880951,
      "learning_rate": 1.3923267326732675e-05,
      "loss": 8.1131,
      "step": 7000
    },
    {
      "epoch": 2.168432449153198,
      "grad_norm": 0.2536541223526001,
      "learning_rate": 1.3871699669966998e-05,
      "loss": 8.1191,
      "step": 7010
    },
    {
      "epoch": 2.171525790735442,
      "grad_norm": 0.37446925044059753,
      "learning_rate": 1.382013201320132e-05,
      "loss": 8.1203,
      "step": 7020
    },
    {
      "epoch": 2.174619132317686,
      "grad_norm": 0.23496659100055695,
      "learning_rate": 1.3768564356435646e-05,
      "loss": 8.1,
      "step": 7030
    },
    {
      "epoch": 2.1777124738999305,
      "grad_norm": 0.5001416802406311,
      "learning_rate": 1.3716996699669968e-05,
      "loss": 8.1237,
      "step": 7040
    },
    {
      "epoch": 2.1808058154821746,
      "grad_norm": 0.3223192095756531,
      "learning_rate": 1.366542904290429e-05,
      "loss": 8.1183,
      "step": 7050
    },
    {
      "epoch": 2.1838991570644186,
      "grad_norm": 0.31799086928367615,
      "learning_rate": 1.3613861386138616e-05,
      "loss": 8.114,
      "step": 7060
    },
    {
      "epoch": 2.186992498646663,
      "grad_norm": 0.24849160015583038,
      "learning_rate": 1.3562293729372939e-05,
      "loss": 8.1005,
      "step": 7070
    },
    {
      "epoch": 2.190085840228907,
      "grad_norm": 0.32890620827674866,
      "learning_rate": 1.351072607260726e-05,
      "loss": 8.0968,
      "step": 7080
    },
    {
      "epoch": 2.1931791818111517,
      "grad_norm": 0.43208062648773193,
      "learning_rate": 1.3459158415841586e-05,
      "loss": 8.121,
      "step": 7090
    },
    {
      "epoch": 2.1962725233933957,
      "grad_norm": 0.34522971510887146,
      "learning_rate": 1.3407590759075909e-05,
      "loss": 8.1012,
      "step": 7100
    },
    {
      "epoch": 2.19936586497564,
      "grad_norm": 0.2224474549293518,
      "learning_rate": 1.335602310231023e-05,
      "loss": 8.1126,
      "step": 7110
    },
    {
      "epoch": 2.2024592065578843,
      "grad_norm": 0.5191652774810791,
      "learning_rate": 1.3304455445544556e-05,
      "loss": 8.0982,
      "step": 7120
    },
    {
      "epoch": 2.2055525481401284,
      "grad_norm": 0.3318387567996979,
      "learning_rate": 1.3252887788778879e-05,
      "loss": 8.1171,
      "step": 7130
    },
    {
      "epoch": 2.2086458897223724,
      "grad_norm": 0.622927725315094,
      "learning_rate": 1.32013201320132e-05,
      "loss": 8.1053,
      "step": 7140
    },
    {
      "epoch": 2.211739231304617,
      "grad_norm": 0.4508971869945526,
      "learning_rate": 1.3149752475247526e-05,
      "loss": 8.1113,
      "step": 7150
    },
    {
      "epoch": 2.214832572886861,
      "grad_norm": 0.3500223159790039,
      "learning_rate": 1.3098184818481849e-05,
      "loss": 8.1,
      "step": 7160
    },
    {
      "epoch": 2.217925914469105,
      "grad_norm": 0.26169466972351074,
      "learning_rate": 1.304661716171617e-05,
      "loss": 8.111,
      "step": 7170
    },
    {
      "epoch": 2.2210192560513495,
      "grad_norm": 0.3949160575866699,
      "learning_rate": 1.2995049504950496e-05,
      "loss": 8.1095,
      "step": 7180
    },
    {
      "epoch": 2.2241125976335936,
      "grad_norm": 0.3084624409675598,
      "learning_rate": 1.294348184818482e-05,
      "loss": 8.0995,
      "step": 7190
    },
    {
      "epoch": 2.227205939215838,
      "grad_norm": 0.5754108428955078,
      "learning_rate": 1.2891914191419144e-05,
      "loss": 8.1145,
      "step": 7200
    },
    {
      "epoch": 2.230299280798082,
      "grad_norm": 0.485371857881546,
      "learning_rate": 1.2840346534653467e-05,
      "loss": 8.0993,
      "step": 7210
    },
    {
      "epoch": 2.233392622380326,
      "grad_norm": 0.28555023670196533,
      "learning_rate": 1.278877887788779e-05,
      "loss": 8.1073,
      "step": 7220
    },
    {
      "epoch": 2.2364859639625707,
      "grad_norm": 0.28207850456237793,
      "learning_rate": 1.2737211221122114e-05,
      "loss": 8.1265,
      "step": 7230
    },
    {
      "epoch": 2.2395793055448148,
      "grad_norm": 0.32800203561782837,
      "learning_rate": 1.2685643564356437e-05,
      "loss": 8.1054,
      "step": 7240
    },
    {
      "epoch": 2.242672647127059,
      "grad_norm": 0.5099066495895386,
      "learning_rate": 1.263407590759076e-05,
      "loss": 8.1019,
      "step": 7250
    },
    {
      "epoch": 2.2457659887093033,
      "grad_norm": 0.3544037938117981,
      "learning_rate": 1.2582508250825084e-05,
      "loss": 8.1092,
      "step": 7260
    },
    {
      "epoch": 2.2488593302915474,
      "grad_norm": 0.3332846462726593,
      "learning_rate": 1.2530940594059407e-05,
      "loss": 8.1186,
      "step": 7270
    },
    {
      "epoch": 2.2519526718737914,
      "grad_norm": 0.38313186168670654,
      "learning_rate": 1.247937293729373e-05,
      "loss": 8.1106,
      "step": 7280
    },
    {
      "epoch": 2.255046013456036,
      "grad_norm": 0.29555103182792664,
      "learning_rate": 1.2427805280528053e-05,
      "loss": 8.1123,
      "step": 7290
    },
    {
      "epoch": 2.25813935503828,
      "grad_norm": 0.5405069589614868,
      "learning_rate": 1.2376237623762377e-05,
      "loss": 8.1262,
      "step": 7300
    },
    {
      "epoch": 2.2612326966205245,
      "grad_norm": 0.19613923132419586,
      "learning_rate": 1.23246699669967e-05,
      "loss": 8.1006,
      "step": 7310
    },
    {
      "epoch": 2.2643260382027686,
      "grad_norm": 0.5267183184623718,
      "learning_rate": 1.2273102310231023e-05,
      "loss": 8.109,
      "step": 7320
    },
    {
      "epoch": 2.2674193797850126,
      "grad_norm": 0.3067196309566498,
      "learning_rate": 1.2221534653465347e-05,
      "loss": 8.1287,
      "step": 7330
    },
    {
      "epoch": 2.270512721367257,
      "grad_norm": 0.3392188251018524,
      "learning_rate": 1.216996699669967e-05,
      "loss": 8.1003,
      "step": 7340
    },
    {
      "epoch": 2.273606062949501,
      "grad_norm": 0.3185349106788635,
      "learning_rate": 1.2118399339933995e-05,
      "loss": 8.1215,
      "step": 7350
    },
    {
      "epoch": 2.2766994045317452,
      "grad_norm": 0.25745296478271484,
      "learning_rate": 1.2066831683168317e-05,
      "loss": 8.1022,
      "step": 7360
    },
    {
      "epoch": 2.2797927461139897,
      "grad_norm": 0.19551445543766022,
      "learning_rate": 1.201526402640264e-05,
      "loss": 8.1037,
      "step": 7370
    },
    {
      "epoch": 2.282886087696234,
      "grad_norm": 0.2238142490386963,
      "learning_rate": 1.1963696369636965e-05,
      "loss": 8.1197,
      "step": 7380
    },
    {
      "epoch": 2.285979429278478,
      "grad_norm": 0.4369620382785797,
      "learning_rate": 1.1912128712871288e-05,
      "loss": 8.1124,
      "step": 7390
    },
    {
      "epoch": 2.2890727708607224,
      "grad_norm": 0.29455870389938354,
      "learning_rate": 1.186056105610561e-05,
      "loss": 8.1137,
      "step": 7400
    },
    {
      "epoch": 2.2921661124429664,
      "grad_norm": 0.3621985912322998,
      "learning_rate": 1.1808993399339935e-05,
      "loss": 8.1215,
      "step": 7410
    },
    {
      "epoch": 2.295259454025211,
      "grad_norm": 0.2541598081588745,
      "learning_rate": 1.1757425742574258e-05,
      "loss": 8.1098,
      "step": 7420
    },
    {
      "epoch": 2.298352795607455,
      "grad_norm": 0.3141932487487793,
      "learning_rate": 1.170585808580858e-05,
      "loss": 8.0915,
      "step": 7430
    },
    {
      "epoch": 2.301446137189699,
      "grad_norm": 0.3515627980232239,
      "learning_rate": 1.1654290429042905e-05,
      "loss": 8.1201,
      "step": 7440
    },
    {
      "epoch": 2.3045394787719435,
      "grad_norm": 0.3666357100009918,
      "learning_rate": 1.160272277227723e-05,
      "loss": 8.1139,
      "step": 7450
    },
    {
      "epoch": 2.3076328203541876,
      "grad_norm": 0.4203639328479767,
      "learning_rate": 1.155115511551155e-05,
      "loss": 8.0998,
      "step": 7460
    },
    {
      "epoch": 2.3107261619364317,
      "grad_norm": 0.32783398032188416,
      "learning_rate": 1.1499587458745875e-05,
      "loss": 8.1114,
      "step": 7470
    },
    {
      "epoch": 2.313819503518676,
      "grad_norm": 0.2934970557689667,
      "learning_rate": 1.14480198019802e-05,
      "loss": 8.0958,
      "step": 7480
    },
    {
      "epoch": 2.31691284510092,
      "grad_norm": 0.4009140729904175,
      "learning_rate": 1.139645214521452e-05,
      "loss": 8.1085,
      "step": 7490
    },
    {
      "epoch": 2.3200061866831643,
      "grad_norm": 0.46097657084465027,
      "learning_rate": 1.1344884488448845e-05,
      "loss": 8.1206,
      "step": 7500
    },
    {
      "epoch": 2.3230995282654088,
      "grad_norm": 0.3952081501483917,
      "learning_rate": 1.129331683168317e-05,
      "loss": 8.1036,
      "step": 7510
    },
    {
      "epoch": 2.326192869847653,
      "grad_norm": 0.3756515681743622,
      "learning_rate": 1.1241749174917491e-05,
      "loss": 8.1059,
      "step": 7520
    },
    {
      "epoch": 2.3292862114298973,
      "grad_norm": 0.2977612018585205,
      "learning_rate": 1.1190181518151816e-05,
      "loss": 8.1137,
      "step": 7530
    },
    {
      "epoch": 2.3323795530121414,
      "grad_norm": 0.3486195206642151,
      "learning_rate": 1.113861386138614e-05,
      "loss": 8.0993,
      "step": 7540
    },
    {
      "epoch": 2.3354728945943855,
      "grad_norm": 0.3217616379261017,
      "learning_rate": 1.1087046204620463e-05,
      "loss": 8.1197,
      "step": 7550
    },
    {
      "epoch": 2.33856623617663,
      "grad_norm": 0.3226429522037506,
      "learning_rate": 1.1035478547854786e-05,
      "loss": 8.1072,
      "step": 7560
    },
    {
      "epoch": 2.341659577758874,
      "grad_norm": 0.3320188820362091,
      "learning_rate": 1.098391089108911e-05,
      "loss": 8.1126,
      "step": 7570
    },
    {
      "epoch": 2.344752919341118,
      "grad_norm": 0.32037827372550964,
      "learning_rate": 1.0932343234323433e-05,
      "loss": 8.1279,
      "step": 7580
    },
    {
      "epoch": 2.3478462609233626,
      "grad_norm": 0.43165159225463867,
      "learning_rate": 1.0880775577557756e-05,
      "loss": 8.1001,
      "step": 7590
    },
    {
      "epoch": 2.3509396025056066,
      "grad_norm": 0.3089400827884674,
      "learning_rate": 1.082920792079208e-05,
      "loss": 8.1129,
      "step": 7600
    },
    {
      "epoch": 2.3540329440878507,
      "grad_norm": 0.32670632004737854,
      "learning_rate": 1.0777640264026403e-05,
      "loss": 8.1118,
      "step": 7610
    },
    {
      "epoch": 2.357126285670095,
      "grad_norm": 0.36056235432624817,
      "learning_rate": 1.0726072607260726e-05,
      "loss": 8.0979,
      "step": 7620
    },
    {
      "epoch": 2.3602196272523392,
      "grad_norm": 0.2947986125946045,
      "learning_rate": 1.067450495049505e-05,
      "loss": 8.12,
      "step": 7630
    },
    {
      "epoch": 2.3633129688345837,
      "grad_norm": 0.3334864377975464,
      "learning_rate": 1.0622937293729373e-05,
      "loss": 8.1132,
      "step": 7640
    },
    {
      "epoch": 2.366406310416828,
      "grad_norm": 0.31012433767318726,
      "learning_rate": 1.0571369636963698e-05,
      "loss": 8.1101,
      "step": 7650
    },
    {
      "epoch": 2.369499651999072,
      "grad_norm": 0.3646813631057739,
      "learning_rate": 1.051980198019802e-05,
      "loss": 8.1062,
      "step": 7660
    },
    {
      "epoch": 2.3725929935813164,
      "grad_norm": 0.3756188750267029,
      "learning_rate": 1.0468234323432343e-05,
      "loss": 8.122,
      "step": 7670
    },
    {
      "epoch": 2.3756863351635604,
      "grad_norm": 0.36069580912590027,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 8.1488,
      "step": 7680
    },
    {
      "epoch": 2.3787796767458045,
      "grad_norm": 0.32919472455978394,
      "learning_rate": 1.036509900990099e-05,
      "loss": 8.1075,
      "step": 7690
    },
    {
      "epoch": 2.381873018328049,
      "grad_norm": 0.24454806745052338,
      "learning_rate": 1.0313531353135314e-05,
      "loss": 8.1024,
      "step": 7700
    },
    {
      "epoch": 2.384966359910293,
      "grad_norm": 0.48895126581192017,
      "learning_rate": 1.0261963696369638e-05,
      "loss": 8.1139,
      "step": 7710
    },
    {
      "epoch": 2.388059701492537,
      "grad_norm": 0.4190235137939453,
      "learning_rate": 1.0210396039603961e-05,
      "loss": 8.1203,
      "step": 7720
    },
    {
      "epoch": 2.3911530430747816,
      "grad_norm": 0.45832785964012146,
      "learning_rate": 1.0158828382838284e-05,
      "loss": 8.1145,
      "step": 7730
    },
    {
      "epoch": 2.3942463846570257,
      "grad_norm": 0.22377556562423706,
      "learning_rate": 1.0107260726072608e-05,
      "loss": 8.1091,
      "step": 7740
    },
    {
      "epoch": 2.39733972623927,
      "grad_norm": 0.32344114780426025,
      "learning_rate": 1.0055693069306931e-05,
      "loss": 8.1159,
      "step": 7750
    },
    {
      "epoch": 2.400433067821514,
      "grad_norm": 0.3553217351436615,
      "learning_rate": 1.0004125412541254e-05,
      "loss": 8.1122,
      "step": 7760
    },
    {
      "epoch": 2.4035264094037583,
      "grad_norm": 0.28356507420539856,
      "learning_rate": 9.952557755775578e-06,
      "loss": 8.1177,
      "step": 7770
    },
    {
      "epoch": 2.406619750986003,
      "grad_norm": 0.39406734704971313,
      "learning_rate": 9.900990099009901e-06,
      "loss": 8.1038,
      "step": 7780
    },
    {
      "epoch": 2.409713092568247,
      "grad_norm": 0.3574425280094147,
      "learning_rate": 9.849422442244224e-06,
      "loss": 8.116,
      "step": 7790
    },
    {
      "epoch": 2.412806434150491,
      "grad_norm": 0.31916147470474243,
      "learning_rate": 9.797854785478549e-06,
      "loss": 8.115,
      "step": 7800
    },
    {
      "epoch": 2.4158997757327354,
      "grad_norm": 0.371366947889328,
      "learning_rate": 9.746287128712871e-06,
      "loss": 8.1173,
      "step": 7810
    },
    {
      "epoch": 2.4189931173149795,
      "grad_norm": 0.3109040856361389,
      "learning_rate": 9.694719471947194e-06,
      "loss": 8.1099,
      "step": 7820
    },
    {
      "epoch": 2.4220864588972235,
      "grad_norm": 0.2839575409889221,
      "learning_rate": 9.643151815181519e-06,
      "loss": 8.1035,
      "step": 7830
    },
    {
      "epoch": 2.425179800479468,
      "grad_norm": 0.34703314304351807,
      "learning_rate": 9.591584158415842e-06,
      "loss": 8.1075,
      "step": 7840
    },
    {
      "epoch": 2.428273142061712,
      "grad_norm": 0.21459703147411346,
      "learning_rate": 9.540016501650166e-06,
      "loss": 8.119,
      "step": 7850
    },
    {
      "epoch": 2.4313664836439566,
      "grad_norm": 0.3384955823421478,
      "learning_rate": 9.488448844884489e-06,
      "loss": 8.1142,
      "step": 7860
    },
    {
      "epoch": 2.4344598252262006,
      "grad_norm": 0.31447163224220276,
      "learning_rate": 9.436881188118812e-06,
      "loss": 8.1138,
      "step": 7870
    },
    {
      "epoch": 2.4375531668084447,
      "grad_norm": 0.3955182433128357,
      "learning_rate": 9.385313531353136e-06,
      "loss": 8.0969,
      "step": 7880
    },
    {
      "epoch": 2.440646508390689,
      "grad_norm": 0.5326186418533325,
      "learning_rate": 9.333745874587459e-06,
      "loss": 8.1204,
      "step": 7890
    },
    {
      "epoch": 2.4437398499729333,
      "grad_norm": 0.4690989851951599,
      "learning_rate": 9.282178217821782e-06,
      "loss": 8.1125,
      "step": 7900
    },
    {
      "epoch": 2.4468331915551773,
      "grad_norm": 0.38301119208335876,
      "learning_rate": 9.230610561056106e-06,
      "loss": 8.1289,
      "step": 7910
    },
    {
      "epoch": 2.449926533137422,
      "grad_norm": 0.3380622863769531,
      "learning_rate": 9.17904290429043e-06,
      "loss": 8.107,
      "step": 7920
    },
    {
      "epoch": 2.453019874719666,
      "grad_norm": 0.33498677611351013,
      "learning_rate": 9.127475247524752e-06,
      "loss": 8.1219,
      "step": 7930
    },
    {
      "epoch": 2.45611321630191,
      "grad_norm": 0.3604329228401184,
      "learning_rate": 9.075907590759077e-06,
      "loss": 8.104,
      "step": 7940
    },
    {
      "epoch": 2.4592065578841544,
      "grad_norm": 0.27695393562316895,
      "learning_rate": 9.024339933993401e-06,
      "loss": 8.1201,
      "step": 7950
    },
    {
      "epoch": 2.4622998994663985,
      "grad_norm": 0.43804433941841125,
      "learning_rate": 8.972772277227722e-06,
      "loss": 8.1086,
      "step": 7960
    },
    {
      "epoch": 2.465393241048643,
      "grad_norm": 0.3860844075679779,
      "learning_rate": 8.921204620462047e-06,
      "loss": 8.1314,
      "step": 7970
    },
    {
      "epoch": 2.468486582630887,
      "grad_norm": 0.26091134548187256,
      "learning_rate": 8.869636963696371e-06,
      "loss": 8.1139,
      "step": 7980
    },
    {
      "epoch": 2.471579924213131,
      "grad_norm": 0.23065893352031708,
      "learning_rate": 8.818069306930692e-06,
      "loss": 8.106,
      "step": 7990
    },
    {
      "epoch": 2.4746732657953756,
      "grad_norm": 0.33941078186035156,
      "learning_rate": 8.766501650165017e-06,
      "loss": 8.1048,
      "step": 8000
    },
    {
      "epoch": 2.4777666073776197,
      "grad_norm": 0.4138546586036682,
      "learning_rate": 8.714933993399341e-06,
      "loss": 8.1083,
      "step": 8010
    },
    {
      "epoch": 2.4808599489598637,
      "grad_norm": 0.38670963048934937,
      "learning_rate": 8.663366336633663e-06,
      "loss": 8.1102,
      "step": 8020
    },
    {
      "epoch": 2.4839532905421082,
      "grad_norm": 0.509506344795227,
      "learning_rate": 8.611798679867987e-06,
      "loss": 8.115,
      "step": 8030
    },
    {
      "epoch": 2.4870466321243523,
      "grad_norm": 0.30876922607421875,
      "learning_rate": 8.560231023102312e-06,
      "loss": 8.1149,
      "step": 8040
    },
    {
      "epoch": 2.4901399737065963,
      "grad_norm": 0.3423945903778076,
      "learning_rate": 8.508663366336634e-06,
      "loss": 8.1202,
      "step": 8050
    },
    {
      "epoch": 2.493233315288841,
      "grad_norm": 0.36048421263694763,
      "learning_rate": 8.457095709570957e-06,
      "loss": 8.1173,
      "step": 8060
    },
    {
      "epoch": 2.496326656871085,
      "grad_norm": 0.49476224184036255,
      "learning_rate": 8.405528052805282e-06,
      "loss": 8.1098,
      "step": 8070
    },
    {
      "epoch": 2.4994199984533294,
      "grad_norm": 0.20458224415779114,
      "learning_rate": 8.353960396039605e-06,
      "loss": 8.1168,
      "step": 8080
    },
    {
      "epoch": 2.5025133400355735,
      "grad_norm": 0.24706433713436127,
      "learning_rate": 8.302392739273927e-06,
      "loss": 8.1094,
      "step": 8090
    },
    {
      "epoch": 2.5056066816178175,
      "grad_norm": 0.36105644702911377,
      "learning_rate": 8.250825082508252e-06,
      "loss": 8.1155,
      "step": 8100
    },
    {
      "epoch": 2.508700023200062,
      "grad_norm": 0.3075394034385681,
      "learning_rate": 8.199257425742575e-06,
      "loss": 8.1202,
      "step": 8110
    },
    {
      "epoch": 2.511793364782306,
      "grad_norm": 0.3712874948978424,
      "learning_rate": 8.147689768976898e-06,
      "loss": 8.1286,
      "step": 8120
    },
    {
      "epoch": 2.51488670636455,
      "grad_norm": 0.3592918813228607,
      "learning_rate": 8.096122112211222e-06,
      "loss": 8.1,
      "step": 8130
    },
    {
      "epoch": 2.5179800479467946,
      "grad_norm": 0.27837076783180237,
      "learning_rate": 8.044554455445545e-06,
      "loss": 8.1084,
      "step": 8140
    },
    {
      "epoch": 2.5210733895290387,
      "grad_norm": 0.32924914360046387,
      "learning_rate": 7.99298679867987e-06,
      "loss": 8.1001,
      "step": 8150
    },
    {
      "epoch": 2.5241667311112828,
      "grad_norm": 0.3149007558822632,
      "learning_rate": 7.941419141914192e-06,
      "loss": 8.1037,
      "step": 8160
    },
    {
      "epoch": 2.5272600726935273,
      "grad_norm": 0.2745351195335388,
      "learning_rate": 7.889851485148515e-06,
      "loss": 8.1112,
      "step": 8170
    },
    {
      "epoch": 2.5303534142757713,
      "grad_norm": 0.336830735206604,
      "learning_rate": 7.83828382838284e-06,
      "loss": 8.1247,
      "step": 8180
    },
    {
      "epoch": 2.533446755858016,
      "grad_norm": 0.3412970006465912,
      "learning_rate": 7.786716171617162e-06,
      "loss": 8.0977,
      "step": 8190
    },
    {
      "epoch": 2.53654009744026,
      "grad_norm": 0.35293543338775635,
      "learning_rate": 7.735148514851485e-06,
      "loss": 8.0952,
      "step": 8200
    },
    {
      "epoch": 2.539633439022504,
      "grad_norm": 0.4360285997390747,
      "learning_rate": 7.68358085808581e-06,
      "loss": 8.109,
      "step": 8210
    },
    {
      "epoch": 2.5427267806047484,
      "grad_norm": 0.4016687273979187,
      "learning_rate": 7.632013201320133e-06,
      "loss": 8.109,
      "step": 8220
    },
    {
      "epoch": 2.5458201221869925,
      "grad_norm": 0.3121470808982849,
      "learning_rate": 7.580445544554455e-06,
      "loss": 8.1207,
      "step": 8230
    },
    {
      "epoch": 2.5489134637692366,
      "grad_norm": 0.25662755966186523,
      "learning_rate": 7.528877887788779e-06,
      "loss": 8.101,
      "step": 8240
    },
    {
      "epoch": 2.552006805351481,
      "grad_norm": 0.20741833746433258,
      "learning_rate": 7.477310231023104e-06,
      "loss": 8.0987,
      "step": 8250
    },
    {
      "epoch": 2.555100146933725,
      "grad_norm": 0.18172262609004974,
      "learning_rate": 7.4257425742574256e-06,
      "loss": 8.0943,
      "step": 8260
    },
    {
      "epoch": 2.558193488515969,
      "grad_norm": 0.2581201195716858,
      "learning_rate": 7.374174917491749e-06,
      "loss": 8.1041,
      "step": 8270
    },
    {
      "epoch": 2.5612868300982137,
      "grad_norm": 0.27727240324020386,
      "learning_rate": 7.322607260726074e-06,
      "loss": 8.0928,
      "step": 8280
    },
    {
      "epoch": 2.5643801716804577,
      "grad_norm": 0.3512379825115204,
      "learning_rate": 7.271039603960396e-06,
      "loss": 8.1055,
      "step": 8290
    },
    {
      "epoch": 2.5674735132627022,
      "grad_norm": 0.35485899448394775,
      "learning_rate": 7.219471947194719e-06,
      "loss": 8.1177,
      "step": 8300
    },
    {
      "epoch": 2.5705668548449463,
      "grad_norm": 0.47493776679039,
      "learning_rate": 7.167904290429044e-06,
      "loss": 8.1142,
      "step": 8310
    },
    {
      "epoch": 2.5736601964271903,
      "grad_norm": 0.2312805950641632,
      "learning_rate": 7.116336633663366e-06,
      "loss": 8.1001,
      "step": 8320
    },
    {
      "epoch": 2.576753538009435,
      "grad_norm": 0.20915602147579193,
      "learning_rate": 7.0647689768976895e-06,
      "loss": 8.108,
      "step": 8330
    },
    {
      "epoch": 2.579846879591679,
      "grad_norm": 0.3597864508628845,
      "learning_rate": 7.013201320132014e-06,
      "loss": 8.1069,
      "step": 8340
    },
    {
      "epoch": 2.582940221173923,
      "grad_norm": 0.40077680349349976,
      "learning_rate": 6.961633663366338e-06,
      "loss": 8.1179,
      "step": 8350
    },
    {
      "epoch": 2.5860335627561675,
      "grad_norm": 0.2985949218273163,
      "learning_rate": 6.91006600660066e-06,
      "loss": 8.1103,
      "step": 8360
    },
    {
      "epoch": 2.5891269043384115,
      "grad_norm": 0.2867656648159027,
      "learning_rate": 6.858498349834984e-06,
      "loss": 8.1151,
      "step": 8370
    },
    {
      "epoch": 2.5922202459206556,
      "grad_norm": 0.2472161203622818,
      "learning_rate": 6.806930693069308e-06,
      "loss": 8.1167,
      "step": 8380
    },
    {
      "epoch": 2.5953135875029,
      "grad_norm": 0.3251870274543762,
      "learning_rate": 6.75536303630363e-06,
      "loss": 8.1107,
      "step": 8390
    },
    {
      "epoch": 2.598406929085144,
      "grad_norm": 0.40318867564201355,
      "learning_rate": 6.703795379537954e-06,
      "loss": 8.107,
      "step": 8400
    },
    {
      "epoch": 2.6015002706673886,
      "grad_norm": 0.3131076395511627,
      "learning_rate": 6.652227722772278e-06,
      "loss": 8.1015,
      "step": 8410
    },
    {
      "epoch": 2.6045936122496327,
      "grad_norm": 0.313877135515213,
      "learning_rate": 6.6006600660066e-06,
      "loss": 8.089,
      "step": 8420
    },
    {
      "epoch": 2.6076869538318768,
      "grad_norm": 0.2962549924850464,
      "learning_rate": 6.5490924092409245e-06,
      "loss": 8.1215,
      "step": 8430
    },
    {
      "epoch": 2.6107802954141213,
      "grad_norm": 0.3060670495033264,
      "learning_rate": 6.497524752475248e-06,
      "loss": 8.1095,
      "step": 8440
    },
    {
      "epoch": 2.6138736369963653,
      "grad_norm": 0.3444605767726898,
      "learning_rate": 6.445957095709572e-06,
      "loss": 8.1048,
      "step": 8450
    },
    {
      "epoch": 2.6169669785786094,
      "grad_norm": 0.3506050407886505,
      "learning_rate": 6.394389438943895e-06,
      "loss": 8.1202,
      "step": 8460
    },
    {
      "epoch": 2.620060320160854,
      "grad_norm": 0.424407958984375,
      "learning_rate": 6.342821782178218e-06,
      "loss": 8.122,
      "step": 8470
    },
    {
      "epoch": 2.623153661743098,
      "grad_norm": 0.4952224791049957,
      "learning_rate": 6.291254125412542e-06,
      "loss": 8.0989,
      "step": 8480
    },
    {
      "epoch": 2.626247003325342,
      "grad_norm": 0.37712493538856506,
      "learning_rate": 6.239686468646865e-06,
      "loss": 8.1043,
      "step": 8490
    },
    {
      "epoch": 2.6293403449075865,
      "grad_norm": 0.2291664332151413,
      "learning_rate": 6.1881188118811885e-06,
      "loss": 8.116,
      "step": 8500
    },
    {
      "epoch": 2.6324336864898306,
      "grad_norm": 0.2249414324760437,
      "learning_rate": 6.136551155115511e-06,
      "loss": 8.0985,
      "step": 8510
    },
    {
      "epoch": 2.635527028072075,
      "grad_norm": 0.2797929644584656,
      "learning_rate": 6.084983498349835e-06,
      "loss": 8.1187,
      "step": 8520
    },
    {
      "epoch": 2.638620369654319,
      "grad_norm": 0.28362709283828735,
      "learning_rate": 6.033415841584159e-06,
      "loss": 8.1086,
      "step": 8530
    },
    {
      "epoch": 2.641713711236563,
      "grad_norm": 0.2307949662208557,
      "learning_rate": 5.981848184818482e-06,
      "loss": 8.095,
      "step": 8540
    },
    {
      "epoch": 2.6448070528188077,
      "grad_norm": 0.43647366762161255,
      "learning_rate": 5.930280528052805e-06,
      "loss": 8.1042,
      "step": 8550
    },
    {
      "epoch": 2.6479003944010517,
      "grad_norm": 0.342934787273407,
      "learning_rate": 5.878712871287129e-06,
      "loss": 8.1228,
      "step": 8560
    },
    {
      "epoch": 2.650993735983296,
      "grad_norm": 0.2343638390302658,
      "learning_rate": 5.8271452145214525e-06,
      "loss": 8.1125,
      "step": 8570
    },
    {
      "epoch": 2.6540870775655403,
      "grad_norm": 0.20064377784729004,
      "learning_rate": 5.775577557755775e-06,
      "loss": 8.1125,
      "step": 8580
    },
    {
      "epoch": 2.6571804191477844,
      "grad_norm": 0.35494402050971985,
      "learning_rate": 5.7240099009901e-06,
      "loss": 8.1176,
      "step": 8590
    },
    {
      "epoch": 2.6602737607300284,
      "grad_norm": 0.31250646710395813,
      "learning_rate": 5.672442244224423e-06,
      "loss": 8.0869,
      "step": 8600
    },
    {
      "epoch": 2.663367102312273,
      "grad_norm": 0.430189847946167,
      "learning_rate": 5.6208745874587455e-06,
      "loss": 8.1056,
      "step": 8610
    },
    {
      "epoch": 2.666460443894517,
      "grad_norm": 0.5200949311256409,
      "learning_rate": 5.56930693069307e-06,
      "loss": 8.1068,
      "step": 8620
    },
    {
      "epoch": 2.6695537854767615,
      "grad_norm": 0.5665346384048462,
      "learning_rate": 5.517739273927393e-06,
      "loss": 8.102,
      "step": 8630
    },
    {
      "epoch": 2.6726471270590055,
      "grad_norm": 0.23341412842273712,
      "learning_rate": 5.4661716171617165e-06,
      "loss": 8.0948,
      "step": 8640
    },
    {
      "epoch": 2.6757404686412496,
      "grad_norm": 0.30485665798187256,
      "learning_rate": 5.41460396039604e-06,
      "loss": 8.1173,
      "step": 8650
    },
    {
      "epoch": 2.678833810223494,
      "grad_norm": 0.38122284412384033,
      "learning_rate": 5.363036303630363e-06,
      "loss": 8.124,
      "step": 8660
    },
    {
      "epoch": 2.681927151805738,
      "grad_norm": 0.3190942406654358,
      "learning_rate": 5.311468646864687e-06,
      "loss": 8.1156,
      "step": 8670
    },
    {
      "epoch": 2.685020493387982,
      "grad_norm": 0.2597811818122864,
      "learning_rate": 5.25990099009901e-06,
      "loss": 8.0961,
      "step": 8680
    },
    {
      "epoch": 2.6881138349702267,
      "grad_norm": 0.30081674456596375,
      "learning_rate": 5.208333333333334e-06,
      "loss": 8.1099,
      "step": 8690
    },
    {
      "epoch": 2.6912071765524708,
      "grad_norm": 0.2859728932380676,
      "learning_rate": 5.156765676567657e-06,
      "loss": 8.0997,
      "step": 8700
    },
    {
      "epoch": 2.694300518134715,
      "grad_norm": 0.2899883985519409,
      "learning_rate": 5.1051980198019805e-06,
      "loss": 8.1134,
      "step": 8710
    },
    {
      "epoch": 2.6973938597169593,
      "grad_norm": 0.2634900212287903,
      "learning_rate": 5.053630363036304e-06,
      "loss": 8.1272,
      "step": 8720
    },
    {
      "epoch": 2.7004872012992034,
      "grad_norm": 0.3200931251049042,
      "learning_rate": 5.002062706270627e-06,
      "loss": 8.1228,
      "step": 8730
    },
    {
      "epoch": 2.703580542881448,
      "grad_norm": 0.3664238154888153,
      "learning_rate": 4.950495049504951e-06,
      "loss": 8.1021,
      "step": 8740
    },
    {
      "epoch": 2.706673884463692,
      "grad_norm": 0.34336379170417786,
      "learning_rate": 4.898927392739274e-06,
      "loss": 8.0873,
      "step": 8750
    },
    {
      "epoch": 2.709767226045936,
      "grad_norm": 0.2800767719745636,
      "learning_rate": 4.847359735973597e-06,
      "loss": 8.1182,
      "step": 8760
    },
    {
      "epoch": 2.7128605676281805,
      "grad_norm": 0.30428391695022583,
      "learning_rate": 4.795792079207921e-06,
      "loss": 8.1135,
      "step": 8770
    },
    {
      "epoch": 2.7159539092104246,
      "grad_norm": 0.27436956763267517,
      "learning_rate": 4.7442244224422445e-06,
      "loss": 8.1187,
      "step": 8780
    },
    {
      "epoch": 2.7190472507926686,
      "grad_norm": 0.4409172534942627,
      "learning_rate": 4.692656765676568e-06,
      "loss": 8.1294,
      "step": 8790
    },
    {
      "epoch": 2.722140592374913,
      "grad_norm": 0.43060022592544556,
      "learning_rate": 4.641089108910891e-06,
      "loss": 8.118,
      "step": 8800
    },
    {
      "epoch": 2.725233933957157,
      "grad_norm": 0.5109432935714722,
      "learning_rate": 4.589521452145215e-06,
      "loss": 8.0942,
      "step": 8810
    },
    {
      "epoch": 2.7283272755394012,
      "grad_norm": 0.3382355570793152,
      "learning_rate": 4.537953795379538e-06,
      "loss": 8.1078,
      "step": 8820
    },
    {
      "epoch": 2.7314206171216457,
      "grad_norm": 0.42178285121917725,
      "learning_rate": 4.486386138613861e-06,
      "loss": 8.1247,
      "step": 8830
    },
    {
      "epoch": 2.73451395870389,
      "grad_norm": 0.23834222555160522,
      "learning_rate": 4.434818481848186e-06,
      "loss": 8.0978,
      "step": 8840
    },
    {
      "epoch": 2.7376073002861343,
      "grad_norm": 0.3150412440299988,
      "learning_rate": 4.3832508250825085e-06,
      "loss": 8.1194,
      "step": 8850
    },
    {
      "epoch": 2.7407006418683784,
      "grad_norm": 0.3877890408039093,
      "learning_rate": 4.331683168316831e-06,
      "loss": 8.1008,
      "step": 8860
    },
    {
      "epoch": 2.7437939834506224,
      "grad_norm": 0.27845731377601624,
      "learning_rate": 4.280115511551156e-06,
      "loss": 8.1133,
      "step": 8870
    },
    {
      "epoch": 2.7468873250328665,
      "grad_norm": 0.30234500765800476,
      "learning_rate": 4.228547854785479e-06,
      "loss": 8.1142,
      "step": 8880
    },
    {
      "epoch": 2.749980666615111,
      "grad_norm": 0.2965157926082611,
      "learning_rate": 4.176980198019802e-06,
      "loss": 8.0961,
      "step": 8890
    },
    {
      "epoch": 2.753074008197355,
      "grad_norm": 0.26165884733200073,
      "learning_rate": 4.125412541254126e-06,
      "loss": 8.1022,
      "step": 8900
    },
    {
      "epoch": 2.7561673497795995,
      "grad_norm": 0.26087361574172974,
      "learning_rate": 4.073844884488449e-06,
      "loss": 8.1075,
      "step": 8910
    },
    {
      "epoch": 2.7592606913618436,
      "grad_norm": 0.3832179605960846,
      "learning_rate": 4.0222772277227725e-06,
      "loss": 8.1158,
      "step": 8920
    },
    {
      "epoch": 2.7623540329440877,
      "grad_norm": 0.2974841296672821,
      "learning_rate": 3.970709570957096e-06,
      "loss": 8.1166,
      "step": 8930
    },
    {
      "epoch": 2.765447374526332,
      "grad_norm": 0.32569313049316406,
      "learning_rate": 3.91914191419142e-06,
      "loss": 8.108,
      "step": 8940
    },
    {
      "epoch": 2.768540716108576,
      "grad_norm": 0.3614080250263214,
      "learning_rate": 3.867574257425743e-06,
      "loss": 8.1019,
      "step": 8950
    },
    {
      "epoch": 2.7716340576908207,
      "grad_norm": 0.23892995715141296,
      "learning_rate": 3.816006600660066e-06,
      "loss": 8.0944,
      "step": 8960
    },
    {
      "epoch": 2.7747273992730648,
      "grad_norm": 0.3027453124523163,
      "learning_rate": 3.7644389438943895e-06,
      "loss": 8.1273,
      "step": 8970
    },
    {
      "epoch": 2.777820740855309,
      "grad_norm": 0.372183620929718,
      "learning_rate": 3.7128712871287128e-06,
      "loss": 8.1191,
      "step": 8980
    },
    {
      "epoch": 2.780914082437553,
      "grad_norm": 0.2771608531475067,
      "learning_rate": 3.661303630363037e-06,
      "loss": 8.117,
      "step": 8990
    },
    {
      "epoch": 2.7840074240197974,
      "grad_norm": 0.3088119626045227,
      "learning_rate": 3.6097359735973597e-06,
      "loss": 8.1187,
      "step": 9000
    },
    {
      "epoch": 2.7871007656020415,
      "grad_norm": 0.36012154817581177,
      "learning_rate": 3.558168316831683e-06,
      "loss": 8.1092,
      "step": 9010
    },
    {
      "epoch": 2.790194107184286,
      "grad_norm": 0.3369991183280945,
      "learning_rate": 3.506600660066007e-06,
      "loss": 8.1075,
      "step": 9020
    },
    {
      "epoch": 2.79328744876653,
      "grad_norm": 0.4518505334854126,
      "learning_rate": 3.45503300330033e-06,
      "loss": 8.1099,
      "step": 9030
    },
    {
      "epoch": 2.796380790348774,
      "grad_norm": 0.32915735244750977,
      "learning_rate": 3.403465346534654e-06,
      "loss": 8.1135,
      "step": 9040
    },
    {
      "epoch": 2.7994741319310186,
      "grad_norm": 0.42461395263671875,
      "learning_rate": 3.351897689768977e-06,
      "loss": 8.115,
      "step": 9050
    },
    {
      "epoch": 2.8025674735132626,
      "grad_norm": 0.39091911911964417,
      "learning_rate": 3.3003300330033e-06,
      "loss": 8.1245,
      "step": 9060
    },
    {
      "epoch": 2.805660815095507,
      "grad_norm": 0.34796541929244995,
      "learning_rate": 3.248762376237624e-06,
      "loss": 8.1149,
      "step": 9070
    },
    {
      "epoch": 2.808754156677751,
      "grad_norm": 0.23516814410686493,
      "learning_rate": 3.1971947194719474e-06,
      "loss": 8.1184,
      "step": 9080
    },
    {
      "epoch": 2.8118474982599952,
      "grad_norm": 0.2641599178314209,
      "learning_rate": 3.145627062706271e-06,
      "loss": 8.1156,
      "step": 9090
    },
    {
      "epoch": 2.8149408398422393,
      "grad_norm": 0.2162105292081833,
      "learning_rate": 3.0940594059405943e-06,
      "loss": 8.1057,
      "step": 9100
    },
    {
      "epoch": 2.818034181424484,
      "grad_norm": 0.25012657046318054,
      "learning_rate": 3.0424917491749175e-06,
      "loss": 8.097,
      "step": 9110
    },
    {
      "epoch": 2.821127523006728,
      "grad_norm": 0.25062477588653564,
      "learning_rate": 2.990924092409241e-06,
      "loss": 8.1212,
      "step": 9120
    },
    {
      "epoch": 2.8242208645889724,
      "grad_norm": 0.331827849149704,
      "learning_rate": 2.9393564356435644e-06,
      "loss": 8.1068,
      "step": 9130
    },
    {
      "epoch": 2.8273142061712164,
      "grad_norm": 0.32120126485824585,
      "learning_rate": 2.8877887788778877e-06,
      "loss": 8.1056,
      "step": 9140
    },
    {
      "epoch": 2.8304075477534605,
      "grad_norm": 0.25231507420539856,
      "learning_rate": 2.8362211221122113e-06,
      "loss": 8.0939,
      "step": 9150
    },
    {
      "epoch": 2.833500889335705,
      "grad_norm": 0.3976095914840698,
      "learning_rate": 2.784653465346535e-06,
      "loss": 8.0997,
      "step": 9160
    },
    {
      "epoch": 2.836594230917949,
      "grad_norm": 0.38044917583465576,
      "learning_rate": 2.7330858085808583e-06,
      "loss": 8.1024,
      "step": 9170
    },
    {
      "epoch": 2.8396875725001935,
      "grad_norm": 0.4023313820362091,
      "learning_rate": 2.6815181518151815e-06,
      "loss": 8.124,
      "step": 9180
    },
    {
      "epoch": 2.8427809140824376,
      "grad_norm": 0.30548837780952454,
      "learning_rate": 2.629950495049505e-06,
      "loss": 8.1089,
      "step": 9190
    },
    {
      "epoch": 2.8458742556646817,
      "grad_norm": 0.20359891653060913,
      "learning_rate": 2.5783828382838284e-06,
      "loss": 8.1104,
      "step": 9200
    },
    {
      "epoch": 2.8489675972469257,
      "grad_norm": 0.35071778297424316,
      "learning_rate": 2.526815181518152e-06,
      "loss": 8.1089,
      "step": 9210
    },
    {
      "epoch": 2.8520609388291702,
      "grad_norm": 0.3813796639442444,
      "learning_rate": 2.4752475247524753e-06,
      "loss": 8.1185,
      "step": 9220
    },
    {
      "epoch": 2.8551542804114143,
      "grad_norm": 0.32254958152770996,
      "learning_rate": 2.4236798679867986e-06,
      "loss": 8.131,
      "step": 9230
    },
    {
      "epoch": 2.858247621993659,
      "grad_norm": 0.35404670238494873,
      "learning_rate": 2.3721122112211222e-06,
      "loss": 8.1166,
      "step": 9240
    },
    {
      "epoch": 2.861340963575903,
      "grad_norm": 0.3107675611972809,
      "learning_rate": 2.3205445544554455e-06,
      "loss": 8.11,
      "step": 9250
    },
    {
      "epoch": 2.864434305158147,
      "grad_norm": 0.26229211688041687,
      "learning_rate": 2.268976897689769e-06,
      "loss": 8.0956,
      "step": 9260
    },
    {
      "epoch": 2.8675276467403914,
      "grad_norm": 0.279665470123291,
      "learning_rate": 2.217409240924093e-06,
      "loss": 8.1036,
      "step": 9270
    },
    {
      "epoch": 2.8706209883226355,
      "grad_norm": 0.22749735414981842,
      "learning_rate": 2.1658415841584156e-06,
      "loss": 8.119,
      "step": 9280
    },
    {
      "epoch": 2.87371432990488,
      "grad_norm": 0.3059122562408447,
      "learning_rate": 2.1142739273927393e-06,
      "loss": 8.1229,
      "step": 9290
    },
    {
      "epoch": 2.876807671487124,
      "grad_norm": 0.33357566595077515,
      "learning_rate": 2.062706270627063e-06,
      "loss": 8.12,
      "step": 9300
    },
    {
      "epoch": 2.879901013069368,
      "grad_norm": 0.3453192710876465,
      "learning_rate": 2.0111386138613862e-06,
      "loss": 8.1091,
      "step": 9310
    },
    {
      "epoch": 2.882994354651612,
      "grad_norm": 0.242585688829422,
      "learning_rate": 1.95957095709571e-06,
      "loss": 8.1069,
      "step": 9320
    },
    {
      "epoch": 2.8860876962338566,
      "grad_norm": 0.24223539233207703,
      "learning_rate": 1.908003300330033e-06,
      "loss": 8.1238,
      "step": 9330
    },
    {
      "epoch": 2.8891810378161007,
      "grad_norm": 0.3859902024269104,
      "learning_rate": 1.8564356435643564e-06,
      "loss": 8.1224,
      "step": 9340
    },
    {
      "epoch": 2.892274379398345,
      "grad_norm": 0.36520951986312866,
      "learning_rate": 1.8048679867986798e-06,
      "loss": 8.1121,
      "step": 9350
    },
    {
      "epoch": 2.8953677209805893,
      "grad_norm": 0.2281496375799179,
      "learning_rate": 1.7533003300330035e-06,
      "loss": 8.091,
      "step": 9360
    },
    {
      "epoch": 2.8984610625628333,
      "grad_norm": 0.4133196175098419,
      "learning_rate": 1.701732673267327e-06,
      "loss": 8.1052,
      "step": 9370
    },
    {
      "epoch": 2.901554404145078,
      "grad_norm": 0.31377455592155457,
      "learning_rate": 1.65016501650165e-06,
      "loss": 8.0993,
      "step": 9380
    },
    {
      "epoch": 2.904647745727322,
      "grad_norm": 0.32027360796928406,
      "learning_rate": 1.5985973597359737e-06,
      "loss": 8.1025,
      "step": 9390
    },
    {
      "epoch": 2.9077410873095664,
      "grad_norm": 0.29153507947921753,
      "learning_rate": 1.5470297029702971e-06,
      "loss": 8.1143,
      "step": 9400
    },
    {
      "epoch": 2.9108344288918104,
      "grad_norm": 0.2588947117328644,
      "learning_rate": 1.4954620462046206e-06,
      "loss": 8.098,
      "step": 9410
    },
    {
      "epoch": 2.9139277704740545,
      "grad_norm": 0.23128235340118408,
      "learning_rate": 1.4438943894389438e-06,
      "loss": 8.1107,
      "step": 9420
    },
    {
      "epoch": 2.9170211120562985,
      "grad_norm": 0.3340567350387573,
      "learning_rate": 1.3923267326732675e-06,
      "loss": 8.1183,
      "step": 9430
    },
    {
      "epoch": 2.920114453638543,
      "grad_norm": 0.5506354570388794,
      "learning_rate": 1.3407590759075907e-06,
      "loss": 8.1216,
      "step": 9440
    },
    {
      "epoch": 2.923207795220787,
      "grad_norm": 0.35901540517807007,
      "learning_rate": 1.2891914191419142e-06,
      "loss": 8.1141,
      "step": 9450
    },
    {
      "epoch": 2.9263011368030316,
      "grad_norm": 0.3185494840145111,
      "learning_rate": 1.2376237623762377e-06,
      "loss": 8.112,
      "step": 9460
    },
    {
      "epoch": 2.9293944783852757,
      "grad_norm": 0.25856900215148926,
      "learning_rate": 1.1860561056105611e-06,
      "loss": 8.1142,
      "step": 9470
    },
    {
      "epoch": 2.9324878199675197,
      "grad_norm": 0.35876598954200745,
      "learning_rate": 1.1344884488448846e-06,
      "loss": 8.1315,
      "step": 9480
    },
    {
      "epoch": 2.9355811615497642,
      "grad_norm": 0.37995395064353943,
      "learning_rate": 1.0829207920792078e-06,
      "loss": 8.1055,
      "step": 9490
    },
    {
      "epoch": 2.9386745031320083,
      "grad_norm": 0.22720038890838623,
      "learning_rate": 1.0313531353135315e-06,
      "loss": 8.1073,
      "step": 9500
    },
    {
      "epoch": 2.941767844714253,
      "grad_norm": 0.48876166343688965,
      "learning_rate": 9.79785478547855e-07,
      "loss": 8.1052,
      "step": 9510
    },
    {
      "epoch": 2.944861186296497,
      "grad_norm": 0.30713990330696106,
      "learning_rate": 9.282178217821782e-07,
      "loss": 8.1125,
      "step": 9520
    },
    {
      "epoch": 2.947954527878741,
      "grad_norm": 0.2715272009372711,
      "learning_rate": 8.766501650165018e-07,
      "loss": 8.1008,
      "step": 9530
    },
    {
      "epoch": 2.951047869460985,
      "grad_norm": 0.27448415756225586,
      "learning_rate": 8.25082508250825e-07,
      "loss": 8.1033,
      "step": 9540
    },
    {
      "epoch": 2.9541412110432295,
      "grad_norm": 0.40731021761894226,
      "learning_rate": 7.735148514851486e-07,
      "loss": 8.1174,
      "step": 9550
    },
    {
      "epoch": 2.9572345526254735,
      "grad_norm": 0.36749133467674255,
      "learning_rate": 7.219471947194719e-07,
      "loss": 8.1275,
      "step": 9560
    },
    {
      "epoch": 2.960327894207718,
      "grad_norm": 0.2470099776983261,
      "learning_rate": 6.703795379537954e-07,
      "loss": 8.1075,
      "step": 9570
    },
    {
      "epoch": 2.963421235789962,
      "grad_norm": 0.39421892166137695,
      "learning_rate": 6.188118811881188e-07,
      "loss": 8.1004,
      "step": 9580
    },
    {
      "epoch": 2.966514577372206,
      "grad_norm": 0.30774790048599243,
      "learning_rate": 5.672442244224423e-07,
      "loss": 8.1274,
      "step": 9590
    },
    {
      "epoch": 2.9696079189544506,
      "grad_norm": 0.29356440901756287,
      "learning_rate": 5.156765676567657e-07,
      "loss": 8.114,
      "step": 9600
    },
    {
      "epoch": 2.9727012605366947,
      "grad_norm": 0.3029252290725708,
      "learning_rate": 4.641089108910891e-07,
      "loss": 8.114,
      "step": 9610
    },
    {
      "epoch": 2.975794602118939,
      "grad_norm": 0.34613046050071716,
      "learning_rate": 4.125412541254125e-07,
      "loss": 8.1133,
      "step": 9620
    },
    {
      "epoch": 2.9788879437011833,
      "grad_norm": 0.2711690366268158,
      "learning_rate": 3.6097359735973596e-07,
      "loss": 8.1148,
      "step": 9630
    },
    {
      "epoch": 2.9819812852834273,
      "grad_norm": 0.30113500356674194,
      "learning_rate": 3.094059405940594e-07,
      "loss": 8.1003,
      "step": 9640
    },
    {
      "epoch": 2.9850746268656714,
      "grad_norm": 0.2832053601741791,
      "learning_rate": 2.5783828382838287e-07,
      "loss": 8.1135,
      "step": 9650
    },
    {
      "epoch": 2.988167968447916,
      "grad_norm": 0.39125677943229675,
      "learning_rate": 2.0627062706270625e-07,
      "loss": 8.1236,
      "step": 9660
    },
    {
      "epoch": 2.99126131003016,
      "grad_norm": 0.3432117998600006,
      "learning_rate": 1.547029702970297e-07,
      "loss": 8.1149,
      "step": 9670
    },
    {
      "epoch": 2.9943546516124044,
      "grad_norm": 0.3021639585494995,
      "learning_rate": 1.0313531353135313e-07,
      "loss": 8.0985,
      "step": 9680
    },
    {
      "epoch": 2.9974479931946485,
      "grad_norm": 0.28546687960624695,
      "learning_rate": 5.156765676567656e-08,
      "loss": 8.1211,
      "step": 9690
    }
  ],
  "logging_steps": 10,
  "max_steps": 9696,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "total_flos": 4.057347653369856e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
