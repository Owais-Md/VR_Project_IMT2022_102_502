{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9997679993813317,
  "eval_steps": 500,
  "global_step": 3232,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 1.0565555095672607,
      "learning_rate": 4.995874587458746e-05,
      "loss": 10.317,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.0399458408355713,
      "learning_rate": 4.990717821782179e-05,
      "loss": 10.0917,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 0.8372367024421692,
      "learning_rate": 4.985561056105611e-05,
      "loss": 9.8997,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 0.7324730753898621,
      "learning_rate": 4.980404290429043e-05,
      "loss": 9.7408,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 0.5262312293052673,
      "learning_rate": 4.975247524752475e-05,
      "loss": 9.5724,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 0.5877265930175781,
      "learning_rate": 4.970090759075908e-05,
      "loss": 9.4637,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 1.1703845262527466,
      "learning_rate": 4.96493399339934e-05,
      "loss": 9.3584,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.4456365406513214,
      "learning_rate": 4.959777227722773e-05,
      "loss": 9.239,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.3888629078865051,
      "learning_rate": 4.954620462046205e-05,
      "loss": 9.1232,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.34968331456184387,
      "learning_rate": 4.949463696369637e-05,
      "loss": 9.0151,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 0.34789127111434937,
      "learning_rate": 4.944306930693069e-05,
      "loss": 8.9644,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.33336520195007324,
      "learning_rate": 4.939150165016502e-05,
      "loss": 8.8865,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.32195472717285156,
      "learning_rate": 4.933993399339934e-05,
      "loss": 8.829,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.3589722514152527,
      "learning_rate": 4.928836633663367e-05,
      "loss": 8.761,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.2880726754665375,
      "learning_rate": 4.923679867986799e-05,
      "loss": 8.6978,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.3471258878707886,
      "learning_rate": 4.918523102310231e-05,
      "loss": 8.6479,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.3235485255718231,
      "learning_rate": 4.913366336633663e-05,
      "loss": 8.6432,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.3881101608276367,
      "learning_rate": 4.908209570957096e-05,
      "loss": 8.5666,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.2700231075286865,
      "learning_rate": 4.903052805280528e-05,
      "loss": 8.5608,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 0.5078184008598328,
      "learning_rate": 4.897896039603961e-05,
      "loss": 8.5281,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 0.441998690366745,
      "learning_rate": 4.892739273927393e-05,
      "loss": 8.5024,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.28030455112457275,
      "learning_rate": 4.887582508250825e-05,
      "loss": 8.4773,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 0.27497541904449463,
      "learning_rate": 4.882425742574257e-05,
      "loss": 8.4451,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 0.49432000517845154,
      "learning_rate": 4.87726897689769e-05,
      "loss": 8.45,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 0.582364559173584,
      "learning_rate": 4.872112211221123e-05,
      "loss": 8.419,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 0.41934067010879517,
      "learning_rate": 4.866955445544555e-05,
      "loss": 8.4041,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.35000211000442505,
      "learning_rate": 4.861798679867987e-05,
      "loss": 8.4011,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 0.3687727153301239,
      "learning_rate": 4.856641914191419e-05,
      "loss": 8.3837,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 0.3106289505958557,
      "learning_rate": 4.851485148514851e-05,
      "loss": 8.3476,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 0.3289656341075897,
      "learning_rate": 4.846328382838284e-05,
      "loss": 8.3549,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 0.7753923535346985,
      "learning_rate": 4.841171617161717e-05,
      "loss": 8.3392,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 0.6723453998565674,
      "learning_rate": 4.836014851485149e-05,
      "loss": 8.3168,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.3239496946334839,
      "learning_rate": 4.830858085808581e-05,
      "loss": 8.3169,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.5454972386360168,
      "learning_rate": 4.825701320132013e-05,
      "loss": 8.3205,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 0.5544485449790955,
      "learning_rate": 4.8205445544554454e-05,
      "loss": 8.2991,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 0.589152991771698,
      "learning_rate": 4.815387788778878e-05,
      "loss": 8.2962,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 0.38646960258483887,
      "learning_rate": 4.810231023102311e-05,
      "loss": 8.2903,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 0.27092379331588745,
      "learning_rate": 4.805074257425743e-05,
      "loss": 8.2906,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 0.26978766918182373,
      "learning_rate": 4.799917491749175e-05,
      "loss": 8.2735,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 0.34780269861221313,
      "learning_rate": 4.794760726072607e-05,
      "loss": 8.2779,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 0.5075212121009827,
      "learning_rate": 4.7896039603960394e-05,
      "loss": 8.2558,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 0.9800662398338318,
      "learning_rate": 4.784447194719472e-05,
      "loss": 8.2718,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 1.2811180353164673,
      "learning_rate": 4.779290429042905e-05,
      "loss": 8.2596,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 0.352987676858902,
      "learning_rate": 4.774133663366337e-05,
      "loss": 8.2639,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 0.7094700336456299,
      "learning_rate": 4.768976897689769e-05,
      "loss": 8.2569,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 0.2987173795700073,
      "learning_rate": 4.763820132013201e-05,
      "loss": 8.2677,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.5161851644515991,
      "learning_rate": 4.7586633663366334e-05,
      "loss": 8.2417,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 0.3100646138191223,
      "learning_rate": 4.753506600660066e-05,
      "loss": 8.2375,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 0.725845217704773,
      "learning_rate": 4.748349834983499e-05,
      "loss": 8.2403,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.6864117383956909,
      "learning_rate": 4.743193069306931e-05,
      "loss": 8.2279,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 0.3854181170463562,
      "learning_rate": 4.738036303630363e-05,
      "loss": 8.2288,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 0.47533729672431946,
      "learning_rate": 4.732879537953795e-05,
      "loss": 8.2515,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 0.5718740820884705,
      "learning_rate": 4.7277227722772274e-05,
      "loss": 8.2451,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.290677934885025,
      "learning_rate": 4.72256600660066e-05,
      "loss": 8.2489,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 0.2483932375907898,
      "learning_rate": 4.717409240924093e-05,
      "loss": 8.2423,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 0.3760654032230377,
      "learning_rate": 4.712252475247525e-05,
      "loss": 8.2423,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.27780771255493164,
      "learning_rate": 4.707095709570957e-05,
      "loss": 8.2412,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 0.22569721937179565,
      "learning_rate": 4.7019389438943894e-05,
      "loss": 8.2265,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 0.3034077286720276,
      "learning_rate": 4.6967821782178215e-05,
      "loss": 8.2315,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 0.3238346576690674,
      "learning_rate": 4.691625412541254e-05,
      "loss": 8.2238,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 0.46669769287109375,
      "learning_rate": 4.686468646864687e-05,
      "loss": 8.2221,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 0.27213725447654724,
      "learning_rate": 4.681311881188119e-05,
      "loss": 8.2295,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.4789974093437195,
      "learning_rate": 4.676155115511551e-05,
      "loss": 8.2345,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 0.3715915381908417,
      "learning_rate": 4.6709983498349834e-05,
      "loss": 8.2209,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.7415069937705994,
      "learning_rate": 4.665841584158416e-05,
      "loss": 8.2158,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.5272858142852783,
      "learning_rate": 4.660684818481848e-05,
      "loss": 8.2304,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 0.7707914710044861,
      "learning_rate": 4.655528052805281e-05,
      "loss": 8.2263,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 0.6944137215614319,
      "learning_rate": 4.650371287128713e-05,
      "loss": 8.2252,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 0.37001848220825195,
      "learning_rate": 4.645214521452145e-05,
      "loss": 8.2318,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 0.5079053044319153,
      "learning_rate": 4.6400577557755774e-05,
      "loss": 8.2085,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 0.308148592710495,
      "learning_rate": 4.63490099009901e-05,
      "loss": 8.2183,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 0.4860559105873108,
      "learning_rate": 4.629744224422442e-05,
      "loss": 8.2167,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 0.4019593298435211,
      "learning_rate": 4.624587458745875e-05,
      "loss": 8.2334,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 0.4290893077850342,
      "learning_rate": 4.619430693069307e-05,
      "loss": 8.2321,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 0.5633763670921326,
      "learning_rate": 4.6142739273927394e-05,
      "loss": 8.2126,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 0.46133989095687866,
      "learning_rate": 4.6091171617161715e-05,
      "loss": 8.2015,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 0.2623341381549835,
      "learning_rate": 4.603960396039604e-05,
      "loss": 8.203,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 0.9119935035705566,
      "learning_rate": 4.5988036303630364e-05,
      "loss": 8.223,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 0.8564001321792603,
      "learning_rate": 4.593646864686469e-05,
      "loss": 8.2312,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 0.2978377640247345,
      "learning_rate": 4.588490099009901e-05,
      "loss": 8.2289,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 0.3063042461872101,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 8.207,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.43957996368408203,
      "learning_rate": 4.578176567656766e-05,
      "loss": 8.2174,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.4443880021572113,
      "learning_rate": 4.573019801980198e-05,
      "loss": 8.2067,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 0.272327721118927,
      "learning_rate": 4.5678630363036304e-05,
      "loss": 8.2167,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 0.3797045946121216,
      "learning_rate": 4.562706270627063e-05,
      "loss": 8.2217,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.504453718662262,
      "learning_rate": 4.557549504950495e-05,
      "loss": 8.2035,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.4906189739704132,
      "learning_rate": 4.5523927392739274e-05,
      "loss": 8.2181,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 0.6800859570503235,
      "learning_rate": 4.54723597359736e-05,
      "loss": 8.1936,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 0.3829074203968048,
      "learning_rate": 4.542079207920792e-05,
      "loss": 8.1953,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 0.556553304195404,
      "learning_rate": 4.5369224422442244e-05,
      "loss": 8.2145,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.3774890899658203,
      "learning_rate": 4.531765676567657e-05,
      "loss": 8.2199,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 0.4384826719760895,
      "learning_rate": 4.526608910891089e-05,
      "loss": 8.2221,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 0.49910715222358704,
      "learning_rate": 4.5214521452145214e-05,
      "loss": 8.2061,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 0.4327700138092041,
      "learning_rate": 4.516295379537954e-05,
      "loss": 8.2127,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 0.6294150352478027,
      "learning_rate": 4.5111386138613864e-05,
      "loss": 8.2003,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 0.6277406811714172,
      "learning_rate": 4.5059818481848185e-05,
      "loss": 8.1994,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 0.9094993472099304,
      "learning_rate": 4.500825082508251e-05,
      "loss": 8.218,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 0.28750014305114746,
      "learning_rate": 4.4956683168316834e-05,
      "loss": 8.206,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.35400182008743286,
      "learning_rate": 4.4905115511551155e-05,
      "loss": 8.2148,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 0.3658556342124939,
      "learning_rate": 4.485354785478548e-05,
      "loss": 8.206,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 0.3834856450557709,
      "learning_rate": 4.4801980198019804e-05,
      "loss": 8.2079,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 0.3486127555370331,
      "learning_rate": 4.4750412541254125e-05,
      "loss": 8.1986,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 0.8183367252349854,
      "learning_rate": 4.469884488448845e-05,
      "loss": 8.2125,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 0.6145647168159485,
      "learning_rate": 4.4647277227722774e-05,
      "loss": 8.2079,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 0.2893054783344269,
      "learning_rate": 4.45957095709571e-05,
      "loss": 8.1967,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 0.9608153104782104,
      "learning_rate": 4.454414191419142e-05,
      "loss": 8.2127,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 1.0759103298187256,
      "learning_rate": 4.4492574257425744e-05,
      "loss": 8.2083,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 0.9710957407951355,
      "learning_rate": 4.4441006600660065e-05,
      "loss": 8.1976,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.56833815574646,
      "learning_rate": 4.438943894389439e-05,
      "loss": 8.2033,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.055946946144104,
      "learning_rate": 4.4337871287128714e-05,
      "loss": 8.1939,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 0.5963489413261414,
      "learning_rate": 4.428630363036304e-05,
      "loss": 8.2132,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 0.41320452094078064,
      "learning_rate": 4.423473597359736e-05,
      "loss": 8.203,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 0.36218947172164917,
      "learning_rate": 4.4183168316831684e-05,
      "loss": 8.1901,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 0.3527924418449402,
      "learning_rate": 4.4131600660066006e-05,
      "loss": 8.1998,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 0.3640836775302887,
      "learning_rate": 4.4080033003300333e-05,
      "loss": 8.1778,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 0.3603845536708832,
      "learning_rate": 4.4028465346534655e-05,
      "loss": 8.1906,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 0.48659634590148926,
      "learning_rate": 4.397689768976898e-05,
      "loss": 8.1998,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 1.1557831764221191,
      "learning_rate": 4.3925330033003304e-05,
      "loss": 8.1942,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 0.3266906440258026,
      "learning_rate": 4.3873762376237625e-05,
      "loss": 8.1978,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 0.3403061032295227,
      "learning_rate": 4.3822194719471946e-05,
      "loss": 8.2091,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 0.8684284687042236,
      "learning_rate": 4.3770627062706274e-05,
      "loss": 8.1871,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 0.586765706539154,
      "learning_rate": 4.37190594059406e-05,
      "loss": 8.2009,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 0.3757753372192383,
      "learning_rate": 4.366749174917492e-05,
      "loss": 8.1973,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 0.3641539216041565,
      "learning_rate": 4.3615924092409244e-05,
      "loss": 8.1934,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 0.3916618227958679,
      "learning_rate": 4.3564356435643565e-05,
      "loss": 8.1885,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 1.1773916482925415,
      "learning_rate": 4.3512788778877886e-05,
      "loss": 8.1716,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 1.1602495908737183,
      "learning_rate": 4.3461221122112214e-05,
      "loss": 8.141,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 0.48604947328567505,
      "learning_rate": 4.340965346534654e-05,
      "loss": 8.146,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 0.47183433175086975,
      "learning_rate": 4.335808580858086e-05,
      "loss": 8.1476,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 0.4256132245063782,
      "learning_rate": 4.3306518151815184e-05,
      "loss": 8.1511,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 0.3924328088760376,
      "learning_rate": 4.3254950495049505e-05,
      "loss": 8.1382,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 0.3695896863937378,
      "learning_rate": 4.3203382838283827e-05,
      "loss": 8.1539,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 1.0166791677474976,
      "learning_rate": 4.3151815181518154e-05,
      "loss": 8.1419,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 0.34996649622917175,
      "learning_rate": 4.310024752475248e-05,
      "loss": 8.1217,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 0.2747478187084198,
      "learning_rate": 4.3048679867986803e-05,
      "loss": 8.1316,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 0.41667020320892334,
      "learning_rate": 4.2997112211221125e-05,
      "loss": 8.1295,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 0.3027843236923218,
      "learning_rate": 4.2945544554455446e-05,
      "loss": 8.1229,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.43693044781684875,
      "learning_rate": 4.289397689768977e-05,
      "loss": 8.1334,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.5062724351882935,
      "learning_rate": 4.2842409240924095e-05,
      "loss": 8.1474,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 0.3002518117427826,
      "learning_rate": 4.279084158415842e-05,
      "loss": 8.1191,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 0.8714909553527832,
      "learning_rate": 4.2739273927392744e-05,
      "loss": 8.1466,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 1.1028579473495483,
      "learning_rate": 4.2687706270627065e-05,
      "loss": 8.1439,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 0.3875997066497803,
      "learning_rate": 4.2636138613861386e-05,
      "loss": 8.143,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 0.6221628189086914,
      "learning_rate": 4.258457095709571e-05,
      "loss": 8.1414,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 0.8331373929977417,
      "learning_rate": 4.2533003300330035e-05,
      "loss": 8.1404,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 0.5329532027244568,
      "learning_rate": 4.248143564356436e-05,
      "loss": 8.1312,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 0.4392077922821045,
      "learning_rate": 4.2429867986798684e-05,
      "loss": 8.1345,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 0.4811130166053772,
      "learning_rate": 4.2378300330033005e-05,
      "loss": 8.1354,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 0.3947867751121521,
      "learning_rate": 4.2326732673267326e-05,
      "loss": 8.1558,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 0.466060996055603,
      "learning_rate": 4.227516501650165e-05,
      "loss": 8.1224,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 1.3809500932693481,
      "learning_rate": 4.2223597359735975e-05,
      "loss": 8.1392,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 0.2857910692691803,
      "learning_rate": 4.21720297029703e-05,
      "loss": 8.1449,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 0.9009313583374023,
      "learning_rate": 4.2120462046204624e-05,
      "loss": 8.1361,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 0.3106480538845062,
      "learning_rate": 4.2068894389438946e-05,
      "loss": 8.1483,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 0.4126911461353302,
      "learning_rate": 4.201732673267327e-05,
      "loss": 8.1224,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.33747777342796326,
      "learning_rate": 4.196575907590759e-05,
      "loss": 8.1104,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 0.4033467769622803,
      "learning_rate": 4.1914191419141916e-05,
      "loss": 8.1489,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 1.1209030151367188,
      "learning_rate": 4.1862623762376244e-05,
      "loss": 8.1302,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 0.40263622999191284,
      "learning_rate": 4.1811056105610565e-05,
      "loss": 8.1246,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 1.0671911239624023,
      "learning_rate": 4.1759488448844886e-05,
      "loss": 8.1366,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 0.5966104865074158,
      "learning_rate": 4.170792079207921e-05,
      "loss": 8.1322,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 0.7771298289299011,
      "learning_rate": 4.1656353135313535e-05,
      "loss": 8.1317,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 1.1947914361953735,
      "learning_rate": 4.1604785478547856e-05,
      "loss": 8.111,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 0.3502797484397888,
      "learning_rate": 4.1553217821782184e-05,
      "loss": 8.1375,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 0.5238884091377258,
      "learning_rate": 4.1501650165016505e-05,
      "loss": 8.1351,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 0.7352068424224854,
      "learning_rate": 4.1450082508250826e-05,
      "loss": 8.1264,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 0.35258346796035767,
      "learning_rate": 4.1403671617161716e-05,
      "loss": 8.1375,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 0.37848493456840515,
      "learning_rate": 4.1352103960396044e-05,
      "loss": 8.1153,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 0.7814749479293823,
      "learning_rate": 4.1300536303630365e-05,
      "loss": 8.1232,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 0.6817078590393066,
      "learning_rate": 4.1248968646864686e-05,
      "loss": 8.1284,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 0.24339263141155243,
      "learning_rate": 4.1197400990099014e-05,
      "loss": 8.1397,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 0.31780698895454407,
      "learning_rate": 4.1145833333333335e-05,
      "loss": 8.1299,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 0.34242910146713257,
      "learning_rate": 4.109426567656766e-05,
      "loss": 8.123,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 0.49349915981292725,
      "learning_rate": 4.1042698019801984e-05,
      "loss": 8.1315,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 0.5724009275436401,
      "learning_rate": 4.0991130363036305e-05,
      "loss": 8.1367,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 0.31627073884010315,
      "learning_rate": 4.0939562706270626e-05,
      "loss": 8.142,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 1.0704926252365112,
      "learning_rate": 4.0887995049504954e-05,
      "loss": 8.1173,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 0.6183575987815857,
      "learning_rate": 4.0836427392739275e-05,
      "loss": 8.1379,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 0.8377038240432739,
      "learning_rate": 4.07848597359736e-05,
      "loss": 8.1336,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 0.4174825847148895,
      "learning_rate": 4.0733292079207924e-05,
      "loss": 8.1264,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 0.9179137349128723,
      "learning_rate": 4.0681724422442246e-05,
      "loss": 8.1173,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 0.7545443177223206,
      "learning_rate": 4.063015676567657e-05,
      "loss": 8.1418,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 0.41321244835853577,
      "learning_rate": 4.0578589108910895e-05,
      "loss": 8.1325,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 0.390316367149353,
      "learning_rate": 4.0527021452145216e-05,
      "loss": 8.1266,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 0.5091736912727356,
      "learning_rate": 4.0475453795379544e-05,
      "loss": 8.1227,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 0.5188171863555908,
      "learning_rate": 4.0423886138613865e-05,
      "loss": 8.1405,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 0.38445693254470825,
      "learning_rate": 4.0372318481848186e-05,
      "loss": 8.1516,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 0.7498568892478943,
      "learning_rate": 4.032075082508251e-05,
      "loss": 8.1285,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.5680442452430725,
      "learning_rate": 4.0269183168316835e-05,
      "loss": 8.1272,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 1.1229215860366821,
      "learning_rate": 4.0217615511551156e-05,
      "loss": 8.1374,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 0.6570364832878113,
      "learning_rate": 4.0166047854785484e-05,
      "loss": 8.131,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 0.7372750043869019,
      "learning_rate": 4.0114480198019805e-05,
      "loss": 8.1344,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.34970682859420776,
      "learning_rate": 4.0062912541254126e-05,
      "loss": 8.1327,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 0.547315776348114,
      "learning_rate": 4.001134488448845e-05,
      "loss": 8.1373,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 0.7643601894378662,
      "learning_rate": 3.9959777227722775e-05,
      "loss": 8.123,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 0.6446273922920227,
      "learning_rate": 3.99082095709571e-05,
      "loss": 8.12,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 0.40875551104545593,
      "learning_rate": 3.9856641914191424e-05,
      "loss": 8.1135,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 0.4373060166835785,
      "learning_rate": 3.9805074257425745e-05,
      "loss": 8.1233,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 0.25874030590057373,
      "learning_rate": 3.9753506600660067e-05,
      "loss": 8.1315,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 0.4552953243255615,
      "learning_rate": 3.970193894389439e-05,
      "loss": 8.1346,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.2765510678291321,
      "learning_rate": 3.9650371287128716e-05,
      "loss": 8.128,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 0.5007600784301758,
      "learning_rate": 3.9598803630363043e-05,
      "loss": 8.1461,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 0.574668288230896,
      "learning_rate": 3.9547235973597365e-05,
      "loss": 8.1267,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 0.6520045399665833,
      "learning_rate": 3.9495668316831686e-05,
      "loss": 8.1404,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 1.0773191452026367,
      "learning_rate": 3.944410066006601e-05,
      "loss": 8.1334,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 0.773710310459137,
      "learning_rate": 3.939253300330033e-05,
      "loss": 8.128,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 0.678584098815918,
      "learning_rate": 3.9340965346534656e-05,
      "loss": 8.1312,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 0.4250548183917999,
      "learning_rate": 3.9289397689768984e-05,
      "loss": 8.1457,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 0.6002156138420105,
      "learning_rate": 3.9237830033003305e-05,
      "loss": 8.1368,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 0.6485240459442139,
      "learning_rate": 3.9186262376237626e-05,
      "loss": 8.1449,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 0.39589083194732666,
      "learning_rate": 3.913469471947195e-05,
      "loss": 8.1301,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 0.41564857959747314,
      "learning_rate": 3.908312706270627e-05,
      "loss": 8.1249,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 0.6284973621368408,
      "learning_rate": 3.9031559405940596e-05,
      "loss": 8.1366,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 0.5497670769691467,
      "learning_rate": 3.8979991749174924e-05,
      "loss": 8.1377,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 0.7107080221176147,
      "learning_rate": 3.8928424092409245e-05,
      "loss": 8.1459,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 0.967587947845459,
      "learning_rate": 3.8876856435643566e-05,
      "loss": 8.1208,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 0.5329194068908691,
      "learning_rate": 3.882528877887789e-05,
      "loss": 8.1268,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 0.48438340425491333,
      "learning_rate": 3.877372112211221e-05,
      "loss": 8.1264,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 0.3653985857963562,
      "learning_rate": 3.8722153465346537e-05,
      "loss": 8.1208,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 0.44317343831062317,
      "learning_rate": 3.8670585808580864e-05,
      "loss": 8.1377,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 0.9510945677757263,
      "learning_rate": 3.8619018151815186e-05,
      "loss": 8.1267,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 0.8991702198982239,
      "learning_rate": 3.856745049504951e-05,
      "loss": 8.1362,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 0.4080759286880493,
      "learning_rate": 3.851588283828383e-05,
      "loss": 8.1313,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 0.8143441677093506,
      "learning_rate": 3.846431518151815e-05,
      "loss": 8.1291,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.7032778263092041,
      "learning_rate": 3.841274752475248e-05,
      "loss": 8.1496,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 0.9029533863067627,
      "learning_rate": 3.8361179867986805e-05,
      "loss": 8.1223,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 0.5039496421813965,
      "learning_rate": 3.8309612211221126e-05,
      "loss": 8.1094,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 0.8314266204833984,
      "learning_rate": 3.825804455445545e-05,
      "loss": 8.1182,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 0.29918500781059265,
      "learning_rate": 3.820647689768977e-05,
      "loss": 8.1264,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 0.40951746702194214,
      "learning_rate": 3.815490924092409e-05,
      "loss": 8.128,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 0.2980818748474121,
      "learning_rate": 3.810334158415842e-05,
      "loss": 8.1162,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.4017033278942108,
      "learning_rate": 3.8051773927392745e-05,
      "loss": 8.1268,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 0.5063818693161011,
      "learning_rate": 3.8000206270627066e-05,
      "loss": 8.1231,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 0.35118499398231506,
      "learning_rate": 3.794863861386139e-05,
      "loss": 8.1364,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 0.550618052482605,
      "learning_rate": 3.789707095709571e-05,
      "loss": 8.1115,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 0.44091475009918213,
      "learning_rate": 3.7845503300330036e-05,
      "loss": 8.1416,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 0.3570210933685303,
      "learning_rate": 3.779393564356436e-05,
      "loss": 8.1264,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 0.337750107049942,
      "learning_rate": 3.7742367986798685e-05,
      "loss": 8.1255,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 0.43572911620140076,
      "learning_rate": 3.7690800330033007e-05,
      "loss": 8.1158,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 0.30651938915252686,
      "learning_rate": 3.763923267326733e-05,
      "loss": 8.1176,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 0.4226876497268677,
      "learning_rate": 3.758766501650165e-05,
      "loss": 8.123,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 1.0687974691390991,
      "learning_rate": 3.753609735973598e-05,
      "loss": 8.1347,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 0.437526136636734,
      "learning_rate": 3.74845297029703e-05,
      "loss": 8.1287,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 0.7829475998878479,
      "learning_rate": 3.7432962046204626e-05,
      "loss": 8.1389,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 1.2735480070114136,
      "learning_rate": 3.738139438943895e-05,
      "loss": 8.1319,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.6305181384086609,
      "learning_rate": 3.732982673267327e-05,
      "loss": 8.1331,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.4683740437030792,
      "learning_rate": 3.727825907590759e-05,
      "loss": 8.1057,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 0.28665226697921753,
      "learning_rate": 3.722669141914192e-05,
      "loss": 8.1227,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 0.348328560590744,
      "learning_rate": 3.717512376237624e-05,
      "loss": 8.1402,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 0.4140527546405792,
      "learning_rate": 3.7123556105610566e-05,
      "loss": 8.1215,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.3625510036945343,
      "learning_rate": 3.707198844884489e-05,
      "loss": 8.1387,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 0.303415447473526,
      "learning_rate": 3.702042079207921e-05,
      "loss": 8.1227,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 0.350281298160553,
      "learning_rate": 3.6968853135313536e-05,
      "loss": 8.1006,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 0.331625759601593,
      "learning_rate": 3.691728547854786e-05,
      "loss": 8.1281,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 0.6135042905807495,
      "learning_rate": 3.686571782178218e-05,
      "loss": 8.1184,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 0.3820091485977173,
      "learning_rate": 3.6814150165016506e-05,
      "loss": 8.1296,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 0.4865269958972931,
      "learning_rate": 3.676258250825083e-05,
      "loss": 8.1289,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 0.34087127447128296,
      "learning_rate": 3.671101485148515e-05,
      "loss": 8.1253,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 0.31458431482315063,
      "learning_rate": 3.6659447194719476e-05,
      "loss": 8.1312,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 0.46448275446891785,
      "learning_rate": 3.66078795379538e-05,
      "loss": 8.133,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 0.42894142866134644,
      "learning_rate": 3.655631188118812e-05,
      "loss": 8.1204,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 0.5991010069847107,
      "learning_rate": 3.650474422442245e-05,
      "loss": 8.105,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 0.3278878927230835,
      "learning_rate": 3.645317656765677e-05,
      "loss": 8.1081,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 0.31309399008750916,
      "learning_rate": 3.640160891089109e-05,
      "loss": 8.1268,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 0.3627416789531708,
      "learning_rate": 3.635004125412542e-05,
      "loss": 8.1354,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 0.5185185074806213,
      "learning_rate": 3.629847359735974e-05,
      "loss": 8.1217,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 0.7601810097694397,
      "learning_rate": 3.624690594059406e-05,
      "loss": 8.1273,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 0.8919748067855835,
      "learning_rate": 3.619533828382839e-05,
      "loss": 8.1407,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 0.4341886639595032,
      "learning_rate": 3.614377062706271e-05,
      "loss": 8.1239,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 0.38412824273109436,
      "learning_rate": 3.609220297029703e-05,
      "loss": 8.1186,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 0.5315512418746948,
      "learning_rate": 3.604063531353136e-05,
      "loss": 8.1256,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 0.7404510974884033,
      "learning_rate": 3.598906765676568e-05,
      "loss": 8.1437,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 0.9590985178947449,
      "learning_rate": 3.59375e-05,
      "loss": 8.1334,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 0.39767009019851685,
      "learning_rate": 3.588593234323433e-05,
      "loss": 8.1171,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.3042120635509491,
      "learning_rate": 3.583436468646865e-05,
      "loss": 8.1328,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 0.389634370803833,
      "learning_rate": 3.5782797029702976e-05,
      "loss": 8.1386,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 0.33430975675582886,
      "learning_rate": 3.57312293729373e-05,
      "loss": 8.1313,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 0.38440993428230286,
      "learning_rate": 3.567966171617162e-05,
      "loss": 8.1248,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.3980347812175751,
      "learning_rate": 3.562809405940594e-05,
      "loss": 8.1149,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.3470490276813507,
      "learning_rate": 3.557652640264027e-05,
      "loss": 8.1389,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.28660619258880615,
      "learning_rate": 3.552495874587459e-05,
      "loss": 8.1123,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 0.46574026346206665,
      "learning_rate": 3.547339108910892e-05,
      "loss": 8.111,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 0.2823634147644043,
      "learning_rate": 3.542182343234324e-05,
      "loss": 8.1521,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.3352123498916626,
      "learning_rate": 3.537025577557756e-05,
      "loss": 8.1244,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.8018085360527039,
      "learning_rate": 3.531868811881188e-05,
      "loss": 8.1305,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 0.7485525608062744,
      "learning_rate": 3.526712046204621e-05,
      "loss": 8.123,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.38008561730384827,
      "learning_rate": 3.521555280528053e-05,
      "loss": 8.1242,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 0.3900972902774811,
      "learning_rate": 3.516398514851486e-05,
      "loss": 8.1222,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 0.30585187673568726,
      "learning_rate": 3.511241749174918e-05,
      "loss": 8.1204,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 0.24571025371551514,
      "learning_rate": 3.50608498349835e-05,
      "loss": 8.1308,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 0.6372361779212952,
      "learning_rate": 3.500928217821782e-05,
      "loss": 8.1378,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 0.4712018072605133,
      "learning_rate": 3.495771452145215e-05,
      "loss": 8.131,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 0.6792766451835632,
      "learning_rate": 3.490614686468647e-05,
      "loss": 8.1315,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 0.5273013114929199,
      "learning_rate": 3.48545792079208e-05,
      "loss": 8.1168,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 0.32319390773773193,
      "learning_rate": 3.480301155115512e-05,
      "loss": 8.1153,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 0.5794802308082581,
      "learning_rate": 3.475144389438944e-05,
      "loss": 8.1149,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 0.44578197598457336,
      "learning_rate": 3.469987623762376e-05,
      "loss": 8.1215,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 0.33984482288360596,
      "learning_rate": 3.464830858085809e-05,
      "loss": 8.1203,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 0.5207871198654175,
      "learning_rate": 3.4596740924092416e-05,
      "loss": 8.1337,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 0.3359738290309906,
      "learning_rate": 3.454517326732674e-05,
      "loss": 8.1315,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.29692766070365906,
      "learning_rate": 3.449360561056106e-05,
      "loss": 8.1027,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.4548894166946411,
      "learning_rate": 3.444203795379538e-05,
      "loss": 8.1251,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.5032789707183838,
      "learning_rate": 3.43904702970297e-05,
      "loss": 8.1214,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 0.25614356994628906,
      "learning_rate": 3.433890264026403e-05,
      "loss": 8.1167,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 0.46262654662132263,
      "learning_rate": 3.428733498349836e-05,
      "loss": 8.1334,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 0.27074962854385376,
      "learning_rate": 3.423576732673268e-05,
      "loss": 8.1324,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 0.3832576274871826,
      "learning_rate": 3.4184199669967e-05,
      "loss": 8.1346,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 0.41327574849128723,
      "learning_rate": 3.413263201320132e-05,
      "loss": 8.1344,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 0.44750240445137024,
      "learning_rate": 3.408106435643564e-05,
      "loss": 8.1109,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 0.3942849934101105,
      "learning_rate": 3.402949669966997e-05,
      "loss": 8.1234,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 0.3912838101387024,
      "learning_rate": 3.39779290429043e-05,
      "loss": 8.1297,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 0.44661974906921387,
      "learning_rate": 3.392636138613862e-05,
      "loss": 8.1134,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 0.6265203356742859,
      "learning_rate": 3.387479372937294e-05,
      "loss": 8.1243,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 0.32146382331848145,
      "learning_rate": 3.382322607260726e-05,
      "loss": 8.1184,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 0.4319128096103668,
      "learning_rate": 3.377165841584158e-05,
      "loss": 8.1328,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.5804173946380615,
      "learning_rate": 3.372009075907591e-05,
      "loss": 8.1276,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 0.6959319114685059,
      "learning_rate": 3.366852310231024e-05,
      "loss": 8.1175,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 0.9252429008483887,
      "learning_rate": 3.361695544554456e-05,
      "loss": 8.1212,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.5588838458061218,
      "learning_rate": 3.356538778877888e-05,
      "loss": 8.1157,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 0.4152226448059082,
      "learning_rate": 3.35138201320132e-05,
      "loss": 8.1173,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 0.43063798546791077,
      "learning_rate": 3.346225247524752e-05,
      "loss": 8.1214,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.29873234033584595,
      "learning_rate": 3.341068481848185e-05,
      "loss": 8.1185,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 0.45878633856773376,
      "learning_rate": 3.335911716171618e-05,
      "loss": 8.1124,
      "step": 3230
    }
  ],
  "logging_steps": 10,
  "max_steps": 9696,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "total_flos": 1.352763059656704e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
