{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9998453329208878,
  "eval_steps": 500,
  "global_step": 6465,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 2.2978525161743164,
      "learning_rate": 4.9982319660537485e-05,
      "loss": 10.3112,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.697009563446045,
      "learning_rate": 4.9962429278642154e-05,
      "loss": 10.1044,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 1.9493228197097778,
      "learning_rate": 4.9940328854314005e-05,
      "loss": 9.9147,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 1.2537775039672852,
      "learning_rate": 4.9918228429985856e-05,
      "loss": 9.7383,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 0.9784691333770752,
      "learning_rate": 4.9896128005657714e-05,
      "loss": 9.5668,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 1.3054183721542358,
      "learning_rate": 4.9874027581329565e-05,
      "loss": 9.4386,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 1.3254427909851074,
      "learning_rate": 4.9851927157001416e-05,
      "loss": 9.3105,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.8787651062011719,
      "learning_rate": 4.982982673267327e-05,
      "loss": 9.1853,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.7771647572517395,
      "learning_rate": 4.980772630834512e-05,
      "loss": 9.0639,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.7927806377410889,
      "learning_rate": 4.9785625884016976e-05,
      "loss": 8.9485,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 19.9823055267334,
      "learning_rate": 4.976352545968883e-05,
      "loss": 8.8797,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.7020925879478455,
      "learning_rate": 4.9741425035360684e-05,
      "loss": 8.7873,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.6891294717788696,
      "learning_rate": 4.9719324611032535e-05,
      "loss": 8.7412,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.6724461317062378,
      "learning_rate": 4.9697224186704386e-05,
      "loss": 8.6824,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.6063492298126221,
      "learning_rate": 4.9675123762376244e-05,
      "loss": 8.6219,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.649882435798645,
      "learning_rate": 4.9653023338048095e-05,
      "loss": 8.576,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.8346493244171143,
      "learning_rate": 4.9630922913719946e-05,
      "loss": 8.5714,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.7668482661247253,
      "learning_rate": 4.96088224893918e-05,
      "loss": 8.4979,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.81424480676651,
      "learning_rate": 4.9586722065063654e-05,
      "loss": 8.4936,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 1.412393569946289,
      "learning_rate": 4.9564621640735505e-05,
      "loss": 8.4625,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 0.9642157554626465,
      "learning_rate": 4.9542521216407356e-05,
      "loss": 8.439,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.5858935713768005,
      "learning_rate": 4.952042079207921e-05,
      "loss": 8.4209,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 1.2992138862609863,
      "learning_rate": 4.949832036775106e-05,
      "loss": 8.3929,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 1.6960211992263794,
      "learning_rate": 4.9476219943422916e-05,
      "loss": 8.4018,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 1.0526018142700195,
      "learning_rate": 4.9454119519094774e-05,
      "loss": 8.374,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 0.9247509241104126,
      "learning_rate": 4.9432019094766625e-05,
      "loss": 8.3619,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.650263249874115,
      "learning_rate": 4.9409918670438475e-05,
      "loss": 8.3572,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 0.8367137908935547,
      "learning_rate": 4.9387818246110326e-05,
      "loss": 8.3374,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 0.5976635813713074,
      "learning_rate": 4.9365717821782184e-05,
      "loss": 8.3021,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 1.133556604385376,
      "learning_rate": 4.9343617397454035e-05,
      "loss": 8.3094,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 2.212977409362793,
      "learning_rate": 4.9321516973125886e-05,
      "loss": 8.2983,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 0.8591291308403015,
      "learning_rate": 4.929941654879774e-05,
      "loss": 8.254,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.7119815349578857,
      "learning_rate": 4.9277316124469595e-05,
      "loss": 8.2402,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.860245406627655,
      "learning_rate": 4.9255215700141446e-05,
      "loss": 8.245,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 1.2802064418792725,
      "learning_rate": 4.9233115275813297e-05,
      "loss": 8.2269,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 0.6481074690818787,
      "learning_rate": 4.921101485148515e-05,
      "loss": 8.2231,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 1.1971083879470825,
      "learning_rate": 4.9188914427157e-05,
      "loss": 8.2189,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 1.7580838203430176,
      "learning_rate": 4.9166814002828856e-05,
      "loss": 8.2223,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 1.2845189571380615,
      "learning_rate": 4.9144713578500714e-05,
      "loss": 8.2078,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 2.1474101543426514,
      "learning_rate": 4.9122613154172565e-05,
      "loss": 8.2149,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 0.6936149001121521,
      "learning_rate": 4.9100512729844416e-05,
      "loss": 8.1938,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 2.1909961700439453,
      "learning_rate": 4.907841230551627e-05,
      "loss": 8.2125,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 0.7333984971046448,
      "learning_rate": 4.9056311881188124e-05,
      "loss": 8.1978,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 1.0266189575195312,
      "learning_rate": 4.9034211456859975e-05,
      "loss": 8.2031,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 0.7094117403030396,
      "learning_rate": 4.9012111032531826e-05,
      "loss": 8.1962,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 1.4269354343414307,
      "learning_rate": 4.899001060820368e-05,
      "loss": 8.2066,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.6357672810554504,
      "learning_rate": 4.8967910183875535e-05,
      "loss": 8.1813,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 1.524726390838623,
      "learning_rate": 4.8945809759547386e-05,
      "loss": 8.1788,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 1.4942302703857422,
      "learning_rate": 4.892370933521924e-05,
      "loss": 8.1809,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.7273604273796082,
      "learning_rate": 4.890160891089109e-05,
      "loss": 8.1683,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 1.1210496425628662,
      "learning_rate": 4.8879508486562946e-05,
      "loss": 8.1691,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 1.029540777206421,
      "learning_rate": 4.8857408062234796e-05,
      "loss": 8.1916,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 1.556471347808838,
      "learning_rate": 4.8835307637906654e-05,
      "loss": 8.1873,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.7839841246604919,
      "learning_rate": 4.8813207213578505e-05,
      "loss": 8.1909,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 2.0269980430603027,
      "learning_rate": 4.8791106789250356e-05,
      "loss": 8.1842,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 1.3354445695877075,
      "learning_rate": 4.876900636492221e-05,
      "loss": 8.1838,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.6603084206581116,
      "learning_rate": 4.8746905940594065e-05,
      "loss": 8.1838,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 1.0205258131027222,
      "learning_rate": 4.8724805516265916e-05,
      "loss": 8.1684,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 0.6809381246566772,
      "learning_rate": 4.870270509193777e-05,
      "loss": 8.1736,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 0.7379045486450195,
      "learning_rate": 4.868060466760962e-05,
      "loss": 8.1663,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 1.0289238691329956,
      "learning_rate": 4.8658504243281475e-05,
      "loss": 8.1652,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 1.4722477197647095,
      "learning_rate": 4.8636403818953326e-05,
      "loss": 8.1719,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.985119640827179,
      "learning_rate": 4.861430339462518e-05,
      "loss": 8.1764,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 2.193415880203247,
      "learning_rate": 4.859220297029703e-05,
      "loss": 8.1621,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.7924662828445435,
      "learning_rate": 4.8570102545968886e-05,
      "loss": 8.1579,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.7754663228988647,
      "learning_rate": 4.854800212164074e-05,
      "loss": 8.1731,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 1.1253646612167358,
      "learning_rate": 4.8525901697312595e-05,
      "loss": 8.1679,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 1.7341458797454834,
      "learning_rate": 4.8503801272984445e-05,
      "loss": 8.1671,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 1.9493780136108398,
      "learning_rate": 4.8481700848656296e-05,
      "loss": 8.1735,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 3.367149829864502,
      "learning_rate": 4.845960042432815e-05,
      "loss": 8.1516,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 1.8500995635986328,
      "learning_rate": 4.8437500000000005e-05,
      "loss": 8.1614,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 0.8552808165550232,
      "learning_rate": 4.8415399575671856e-05,
      "loss": 8.1589,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 1.2967503070831299,
      "learning_rate": 4.839329915134371e-05,
      "loss": 8.1755,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 1.5053879022598267,
      "learning_rate": 4.837119872701556e-05,
      "loss": 8.1735,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 0.7907457947731018,
      "learning_rate": 4.8349098302687416e-05,
      "loss": 8.1565,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 0.8512012958526611,
      "learning_rate": 4.8326997878359267e-05,
      "loss": 8.1454,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 0.8684555292129517,
      "learning_rate": 4.830489745403112e-05,
      "loss": 8.1468,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 2.083754301071167,
      "learning_rate": 4.8282797029702975e-05,
      "loss": 8.1667,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 0.9785476922988892,
      "learning_rate": 4.8260696605374826e-05,
      "loss": 8.1745,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 0.8621866703033447,
      "learning_rate": 4.823859618104668e-05,
      "loss": 8.1734,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 1.065608024597168,
      "learning_rate": 4.8216495756718535e-05,
      "loss": 8.1508,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.7921701669692993,
      "learning_rate": 4.8194395332390386e-05,
      "loss": 8.1613,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.9583138823509216,
      "learning_rate": 4.817229490806224e-05,
      "loss": 8.1486,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 1.781218409538269,
      "learning_rate": 4.815019448373409e-05,
      "loss": 8.1601,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 1.411521315574646,
      "learning_rate": 4.8128094059405945e-05,
      "loss": 8.166,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.6862319707870483,
      "learning_rate": 4.8105993635077796e-05,
      "loss": 8.1452,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.910425066947937,
      "learning_rate": 4.808389321074965e-05,
      "loss": 8.162,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 1.6667001247406006,
      "learning_rate": 4.80617927864215e-05,
      "loss": 8.1382,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 1.0555663108825684,
      "learning_rate": 4.803969236209335e-05,
      "loss": 8.1397,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 1.4642082452774048,
      "learning_rate": 4.801759193776521e-05,
      "loss": 8.159,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.7943904399871826,
      "learning_rate": 4.799549151343706e-05,
      "loss": 8.1628,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 1.0083155632019043,
      "learning_rate": 4.7973391089108916e-05,
      "loss": 8.1653,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 0.8918737173080444,
      "learning_rate": 4.7951290664780766e-05,
      "loss": 8.1491,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 1.3468626737594604,
      "learning_rate": 4.792919024045262e-05,
      "loss": 8.1569,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 1.7926127910614014,
      "learning_rate": 4.7907089816124475e-05,
      "loss": 8.145,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 2.6235406398773193,
      "learning_rate": 4.7884989391796326e-05,
      "loss": 8.1438,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 1.613265037536621,
      "learning_rate": 4.786288896746818e-05,
      "loss": 8.1618,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 1.0502805709838867,
      "learning_rate": 4.784078854314003e-05,
      "loss": 8.1507,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.9201352596282959,
      "learning_rate": 4.7818688118811886e-05,
      "loss": 8.1586,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 0.9506177306175232,
      "learning_rate": 4.779658769448374e-05,
      "loss": 8.1499,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 0.8562129139900208,
      "learning_rate": 4.777448727015559e-05,
      "loss": 8.1545,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 1.137046456336975,
      "learning_rate": 4.775238684582744e-05,
      "loss": 8.1428,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 4.624128341674805,
      "learning_rate": 4.773028642149929e-05,
      "loss": 8.159,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 0.9169142246246338,
      "learning_rate": 4.770818599717115e-05,
      "loss": 8.1528,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 4.3146209716796875,
      "learning_rate": 4.7686085572843005e-05,
      "loss": 8.1421,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 1.1886427402496338,
      "learning_rate": 4.7663985148514856e-05,
      "loss": 8.1575,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 1.0226562023162842,
      "learning_rate": 4.764188472418671e-05,
      "loss": 8.1538,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 0.9351243376731873,
      "learning_rate": 4.761978429985856e-05,
      "loss": 8.1418,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.6706920862197876,
      "learning_rate": 4.7597683875530415e-05,
      "loss": 8.1471,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.1914315223693848,
      "learning_rate": 4.7575583451202266e-05,
      "loss": 8.1389,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 0.7992492914199829,
      "learning_rate": 4.755348302687412e-05,
      "loss": 8.1572,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 1.4357860088348389,
      "learning_rate": 4.753138260254597e-05,
      "loss": 8.147,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 0.9896734952926636,
      "learning_rate": 4.7509282178217826e-05,
      "loss": 8.1361,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 0.7115346193313599,
      "learning_rate": 4.748718175388968e-05,
      "loss": 8.1448,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 1.3110510110855103,
      "learning_rate": 4.746508132956153e-05,
      "loss": 8.1237,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 1.0065312385559082,
      "learning_rate": 4.744298090523338e-05,
      "loss": 8.135,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 1.3407121896743774,
      "learning_rate": 4.742088048090523e-05,
      "loss": 8.1458,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 1.2935097217559814,
      "learning_rate": 4.7398780056577094e-05,
      "loss": 8.1379,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 0.9102542996406555,
      "learning_rate": 4.7376679632248945e-05,
      "loss": 8.1432,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 2.4544589519500732,
      "learning_rate": 4.7354579207920796e-05,
      "loss": 8.1546,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 0.9030328989028931,
      "learning_rate": 4.733247878359265e-05,
      "loss": 8.1316,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 0.8985339403152466,
      "learning_rate": 4.73103783592645e-05,
      "loss": 8.1446,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 1.6634750366210938,
      "learning_rate": 4.7288277934936356e-05,
      "loss": 8.1418,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 3.069864511489868,
      "learning_rate": 4.726617751060821e-05,
      "loss": 8.1398,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 0.7385414838790894,
      "learning_rate": 4.724407708628006e-05,
      "loss": 8.1339,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 0.7566729187965393,
      "learning_rate": 4.722197666195191e-05,
      "loss": 8.1308,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 2.357363224029541,
      "learning_rate": 4.7199876237623766e-05,
      "loss": 8.1348,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 0.7281991839408875,
      "learning_rate": 4.717777581329562e-05,
      "loss": 8.1437,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 0.8826242685317993,
      "learning_rate": 4.715567538896747e-05,
      "loss": 8.1458,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 0.9293636083602905,
      "learning_rate": 4.713357496463932e-05,
      "loss": 8.1498,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 2.375598907470703,
      "learning_rate": 4.711147454031117e-05,
      "loss": 8.1371,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 1.2174696922302246,
      "learning_rate": 4.7089374115983035e-05,
      "loss": 8.1515,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 1.1991961002349854,
      "learning_rate": 4.7067273691654886e-05,
      "loss": 8.1401,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 0.7864306569099426,
      "learning_rate": 4.7045173267326737e-05,
      "loss": 8.1199,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 0.7330999374389648,
      "learning_rate": 4.702307284299859e-05,
      "loss": 8.1305,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 0.8905574083328247,
      "learning_rate": 4.700097241867044e-05,
      "loss": 8.1283,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 1.4793035984039307,
      "learning_rate": 4.6978871994342296e-05,
      "loss": 8.1219,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.665165901184082,
      "learning_rate": 4.695677157001415e-05,
      "loss": 8.131,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.6748032569885254,
      "learning_rate": 4.6934671145686e-05,
      "loss": 8.1446,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 0.8435137271881104,
      "learning_rate": 4.691257072135785e-05,
      "loss": 8.1182,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 0.8907887935638428,
      "learning_rate": 4.689047029702971e-05,
      "loss": 8.1445,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 0.9058837294578552,
      "learning_rate": 4.686836987270156e-05,
      "loss": 8.1434,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 2.731689214706421,
      "learning_rate": 4.684626944837341e-05,
      "loss": 8.143,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 1.3596062660217285,
      "learning_rate": 4.682416902404526e-05,
      "loss": 8.1403,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 1.5356475114822388,
      "learning_rate": 4.680206859971712e-05,
      "loss": 8.14,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 2.6440606117248535,
      "learning_rate": 4.6779968175388975e-05,
      "loss": 8.1299,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 1.2428972721099854,
      "learning_rate": 4.6757867751060826e-05,
      "loss": 8.134,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 2.447598695755005,
      "learning_rate": 4.673576732673268e-05,
      "loss": 8.1356,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 2.3701610565185547,
      "learning_rate": 4.671366690240453e-05,
      "loss": 8.1562,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 0.5955577492713928,
      "learning_rate": 4.669156647807638e-05,
      "loss": 8.1211,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 1.9649672508239746,
      "learning_rate": 4.6669466053748236e-05,
      "loss": 8.1387,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 1.5700713396072388,
      "learning_rate": 4.664736562942009e-05,
      "loss": 8.144,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 1.2700107097625732,
      "learning_rate": 4.662526520509194e-05,
      "loss": 8.1341,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 1.2104922533035278,
      "learning_rate": 4.660316478076379e-05,
      "loss": 8.1475,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 1.0661829710006714,
      "learning_rate": 4.658106435643565e-05,
      "loss": 8.1214,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.8409711122512817,
      "learning_rate": 4.65589639321075e-05,
      "loss": 8.1108,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 3.221015214920044,
      "learning_rate": 4.653686350777935e-05,
      "loss": 8.1492,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 1.6262084245681763,
      "learning_rate": 4.6514763083451207e-05,
      "loss": 8.1294,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 2.6076228618621826,
      "learning_rate": 4.649266265912306e-05,
      "loss": 8.1244,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 0.7324378490447998,
      "learning_rate": 4.6470562234794915e-05,
      "loss": 8.1377,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 0.8694437742233276,
      "learning_rate": 4.6448461810466766e-05,
      "loss": 8.1318,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 1.190567135810852,
      "learning_rate": 4.642636138613862e-05,
      "loss": 8.1313,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 1.044485092163086,
      "learning_rate": 4.640426096181047e-05,
      "loss": 8.1099,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 1.5426740646362305,
      "learning_rate": 4.638216053748232e-05,
      "loss": 8.1393,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 1.1623259782791138,
      "learning_rate": 4.636006011315418e-05,
      "loss": 8.1356,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 1.1647775173187256,
      "learning_rate": 4.633795968882603e-05,
      "loss": 8.1274,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 1.2641135454177856,
      "learning_rate": 4.631585926449788e-05,
      "loss": 8.1376,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 1.3211780786514282,
      "learning_rate": 4.629375884016973e-05,
      "loss": 8.116,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 1.3174850940704346,
      "learning_rate": 4.627165841584159e-05,
      "loss": 8.1222,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 1.4946551322937012,
      "learning_rate": 4.624955799151344e-05,
      "loss": 8.1274,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 1.0001411437988281,
      "learning_rate": 4.622745756718529e-05,
      "loss": 8.1387,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 1.0746123790740967,
      "learning_rate": 4.620535714285715e-05,
      "loss": 8.1302,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 1.3485393524169922,
      "learning_rate": 4.6183256718529e-05,
      "loss": 8.1223,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 1.9648051261901855,
      "learning_rate": 4.6161156294200856e-05,
      "loss": 8.1313,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 1.2787649631500244,
      "learning_rate": 4.6139055869872707e-05,
      "loss": 8.1354,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 0.7873222231864929,
      "learning_rate": 4.611695544554456e-05,
      "loss": 8.1405,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 2.2081856727600098,
      "learning_rate": 4.609485502121641e-05,
      "loss": 8.1167,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 0.6028438806533813,
      "learning_rate": 4.607275459688826e-05,
      "loss": 8.1369,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 0.9986103773117065,
      "learning_rate": 4.605065417256012e-05,
      "loss": 8.1335,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 0.6742298603057861,
      "learning_rate": 4.602855374823197e-05,
      "loss": 8.1261,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 1.2286006212234497,
      "learning_rate": 4.600645332390382e-05,
      "loss": 8.1172,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 0.8139893412590027,
      "learning_rate": 4.598435289957567e-05,
      "loss": 8.1406,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 1.3433475494384766,
      "learning_rate": 4.596225247524753e-05,
      "loss": 8.1309,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 1.5777674913406372,
      "learning_rate": 4.594015205091938e-05,
      "loss": 8.1259,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 0.9981272220611572,
      "learning_rate": 4.5918051626591236e-05,
      "loss": 8.1216,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 0.880254864692688,
      "learning_rate": 4.589595120226309e-05,
      "loss": 8.1399,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 1.2760629653930664,
      "learning_rate": 4.587385077793494e-05,
      "loss": 8.15,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 1.6959831714630127,
      "learning_rate": 4.5851750353606796e-05,
      "loss": 8.1277,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.7466061115264893,
      "learning_rate": 4.582964992927865e-05,
      "loss": 8.1268,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 1.1226886510849,
      "learning_rate": 4.58075495049505e-05,
      "loss": 8.1372,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 1.5224499702453613,
      "learning_rate": 4.578544908062235e-05,
      "loss": 8.13,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 1.4019792079925537,
      "learning_rate": 4.57633486562942e-05,
      "loss": 8.1344,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.8918095827102661,
      "learning_rate": 4.574124823196606e-05,
      "loss": 8.1311,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 0.9084849953651428,
      "learning_rate": 4.571914780763791e-05,
      "loss": 8.1366,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 0.7314757704734802,
      "learning_rate": 4.569704738330976e-05,
      "loss": 8.1226,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 0.8405730128288269,
      "learning_rate": 4.567494695898161e-05,
      "loss": 8.1186,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 0.7289599776268005,
      "learning_rate": 4.565284653465347e-05,
      "loss": 8.1128,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 2.0056517124176025,
      "learning_rate": 4.5630746110325326e-05,
      "loss": 8.1228,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 1.4175626039505005,
      "learning_rate": 4.5608645685997177e-05,
      "loss": 8.1313,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 1.7403383255004883,
      "learning_rate": 4.558654526166903e-05,
      "loss": 8.1341,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.9728478789329529,
      "learning_rate": 4.556444483734088e-05,
      "loss": 8.1278,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 0.9657310843467712,
      "learning_rate": 4.5542344413012736e-05,
      "loss": 8.1455,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 0.7016928791999817,
      "learning_rate": 4.552024398868459e-05,
      "loss": 8.1257,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 1.750995397567749,
      "learning_rate": 4.549814356435644e-05,
      "loss": 8.1391,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 3.0268523693084717,
      "learning_rate": 4.547604314002829e-05,
      "loss": 8.1331,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 2.257249116897583,
      "learning_rate": 4.545394271570014e-05,
      "loss": 8.1268,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 0.7347825169563293,
      "learning_rate": 4.5431842291372e-05,
      "loss": 8.1315,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 2.116239070892334,
      "learning_rate": 4.540974186704385e-05,
      "loss": 8.1446,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 2.6621463298797607,
      "learning_rate": 4.53876414427157e-05,
      "loss": 8.1376,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 0.8156794309616089,
      "learning_rate": 4.536554101838755e-05,
      "loss": 8.1435,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 1.779827356338501,
      "learning_rate": 4.534344059405941e-05,
      "loss": 8.1286,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 3.6931838989257812,
      "learning_rate": 4.5321340169731266e-05,
      "loss": 8.1238,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 1.4945954084396362,
      "learning_rate": 4.529923974540312e-05,
      "loss": 8.1367,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 1.838985800743103,
      "learning_rate": 4.527713932107497e-05,
      "loss": 8.1363,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 1.0356295108795166,
      "learning_rate": 4.525503889674682e-05,
      "loss": 8.1453,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 2.1280863285064697,
      "learning_rate": 4.5232938472418677e-05,
      "loss": 8.1204,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 2.179682731628418,
      "learning_rate": 4.521083804809053e-05,
      "loss": 8.1256,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 1.5666074752807617,
      "learning_rate": 4.518873762376238e-05,
      "loss": 8.1255,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 1.3898165225982666,
      "learning_rate": 4.516663719943423e-05,
      "loss": 8.1194,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 1.6152321100234985,
      "learning_rate": 4.514453677510608e-05,
      "loss": 8.1377,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 0.9872780442237854,
      "learning_rate": 4.512243635077794e-05,
      "loss": 8.1251,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 1.3864965438842773,
      "learning_rate": 4.510033592644979e-05,
      "loss": 8.1347,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 1.6805392503738403,
      "learning_rate": 4.507823550212164e-05,
      "loss": 8.1309,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 0.8944875597953796,
      "learning_rate": 4.505613507779349e-05,
      "loss": 8.1284,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.7924079895019531,
      "learning_rate": 4.503403465346535e-05,
      "loss": 8.1483,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 2.8641250133514404,
      "learning_rate": 4.5011934229137206e-05,
      "loss": 8.1206,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 2.6894214153289795,
      "learning_rate": 4.498983380480906e-05,
      "loss": 8.1099,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 1.2986316680908203,
      "learning_rate": 4.496773338048091e-05,
      "loss": 8.1174,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 0.7564579248428345,
      "learning_rate": 4.494563295615276e-05,
      "loss": 8.1263,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 2.1276535987854004,
      "learning_rate": 4.492353253182462e-05,
      "loss": 8.1272,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 1.3789678812026978,
      "learning_rate": 4.490143210749647e-05,
      "loss": 8.1159,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.9833824634552002,
      "learning_rate": 4.487933168316832e-05,
      "loss": 8.1261,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 1.6973011493682861,
      "learning_rate": 4.485723125884017e-05,
      "loss": 8.1218,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 1.2579853534698486,
      "learning_rate": 4.483513083451202e-05,
      "loss": 8.1356,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 1.0678609609603882,
      "learning_rate": 4.481303041018388e-05,
      "loss": 8.1115,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 1.0080513954162598,
      "learning_rate": 4.479092998585573e-05,
      "loss": 8.1416,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 1.728085994720459,
      "learning_rate": 4.476882956152758e-05,
      "loss": 8.1256,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 1.0013184547424316,
      "learning_rate": 4.474672913719944e-05,
      "loss": 8.1265,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 1.2561790943145752,
      "learning_rate": 4.472462871287129e-05,
      "loss": 8.1165,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 2.669976234436035,
      "learning_rate": 4.470252828854315e-05,
      "loss": 8.117,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 1.0632567405700684,
      "learning_rate": 4.4680427864215e-05,
      "loss": 8.1222,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 2.326310396194458,
      "learning_rate": 4.465832743988685e-05,
      "loss": 8.1339,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 1.9265451431274414,
      "learning_rate": 4.46362270155587e-05,
      "loss": 8.1283,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 1.9307504892349243,
      "learning_rate": 4.461412659123056e-05,
      "loss": 8.1387,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 1.3953375816345215,
      "learning_rate": 4.459202616690241e-05,
      "loss": 8.1301,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.7817894220352173,
      "learning_rate": 4.456992574257426e-05,
      "loss": 8.1312,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.9156811237335205,
      "learning_rate": 4.454782531824611e-05,
      "loss": 8.1052,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 1.400242567062378,
      "learning_rate": 4.452572489391796e-05,
      "loss": 8.1224,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 1.0398952960968018,
      "learning_rate": 4.450362446958982e-05,
      "loss": 8.1375,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 0.8242036700248718,
      "learning_rate": 4.448152404526167e-05,
      "loss": 8.1193,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.8784144520759583,
      "learning_rate": 4.445942362093352e-05,
      "loss": 8.1386,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 0.7130635976791382,
      "learning_rate": 4.443732319660538e-05,
      "loss": 8.122,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 0.9169226288795471,
      "learning_rate": 4.441522277227723e-05,
      "loss": 8.0999,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 0.8026123642921448,
      "learning_rate": 4.439312234794909e-05,
      "loss": 8.1268,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 3.0320956707000732,
      "learning_rate": 4.437102192362094e-05,
      "loss": 8.1179,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 1.7157331705093384,
      "learning_rate": 4.434892149929279e-05,
      "loss": 8.1289,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 1.0207972526550293,
      "learning_rate": 4.432682107496464e-05,
      "loss": 8.1293,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 1.2355436086654663,
      "learning_rate": 4.43047206506365e-05,
      "loss": 8.1244,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 0.8576319813728333,
      "learning_rate": 4.428262022630835e-05,
      "loss": 8.1309,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 1.5930135250091553,
      "learning_rate": 4.42605198019802e-05,
      "loss": 8.1319,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 1.2188942432403564,
      "learning_rate": 4.423841937765205e-05,
      "loss": 8.1204,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 1.5238261222839355,
      "learning_rate": 4.42163189533239e-05,
      "loss": 8.1038,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 1.3246231079101562,
      "learning_rate": 4.419421852899576e-05,
      "loss": 8.1085,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 2.379167318344116,
      "learning_rate": 4.417211810466761e-05,
      "loss": 8.1276,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 1.0612999200820923,
      "learning_rate": 4.415001768033947e-05,
      "loss": 8.1354,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 1.0679903030395508,
      "learning_rate": 4.412791725601132e-05,
      "loss": 8.1209,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 1.2253437042236328,
      "learning_rate": 4.410581683168317e-05,
      "loss": 8.1256,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 1.4744584560394287,
      "learning_rate": 4.408371640735503e-05,
      "loss": 8.141,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 0.7224366664886475,
      "learning_rate": 4.406161598302688e-05,
      "loss": 8.1234,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 1.8724217414855957,
      "learning_rate": 4.403951555869873e-05,
      "loss": 8.1192,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 0.7011707425117493,
      "learning_rate": 4.401741513437058e-05,
      "loss": 8.1254,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 0.8190743923187256,
      "learning_rate": 4.399531471004244e-05,
      "loss": 8.1438,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 1.3879348039627075,
      "learning_rate": 4.397321428571429e-05,
      "loss": 8.1342,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 1.4845677614212036,
      "learning_rate": 4.395111386138614e-05,
      "loss": 8.1168,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.7718964219093323,
      "learning_rate": 4.392901343705799e-05,
      "loss": 8.132,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 0.7378916144371033,
      "learning_rate": 4.390691301272984e-05,
      "loss": 8.1368,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 0.7976837754249573,
      "learning_rate": 4.38848125884017e-05,
      "loss": 8.1308,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 1.2536346912384033,
      "learning_rate": 4.386271216407356e-05,
      "loss": 8.1242,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.9537434577941895,
      "learning_rate": 4.384061173974541e-05,
      "loss": 8.1134,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.9349040389060974,
      "learning_rate": 4.381851131541726e-05,
      "loss": 8.1374,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.7789999842643738,
      "learning_rate": 4.379641089108911e-05,
      "loss": 8.1113,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 1.2549035549163818,
      "learning_rate": 4.377431046676097e-05,
      "loss": 8.1103,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 1.6388391256332397,
      "learning_rate": 4.375221004243282e-05,
      "loss": 8.1508,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.7326608300209045,
      "learning_rate": 4.373010961810467e-05,
      "loss": 8.1253,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.9195551872253418,
      "learning_rate": 4.370800919377652e-05,
      "loss": 8.1298,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 1.2265956401824951,
      "learning_rate": 4.368590876944838e-05,
      "loss": 8.1217,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.8913390040397644,
      "learning_rate": 4.366380834512023e-05,
      "loss": 8.1237,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 1.0213226079940796,
      "learning_rate": 4.364170792079208e-05,
      "loss": 8.1215,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 1.399215817451477,
      "learning_rate": 4.361960749646393e-05,
      "loss": 8.12,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 2.366387128829956,
      "learning_rate": 4.359750707213578e-05,
      "loss": 8.132,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 1.5653551816940308,
      "learning_rate": 4.357540664780764e-05,
      "loss": 8.1367,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 2.0236313343048096,
      "learning_rate": 4.35533062234795e-05,
      "loss": 8.1309,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 2.4536983966827393,
      "learning_rate": 4.353120579915135e-05,
      "loss": 8.1306,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 0.7204288840293884,
      "learning_rate": 4.35091053748232e-05,
      "loss": 8.1167,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 0.8028860688209534,
      "learning_rate": 4.348700495049505e-05,
      "loss": 8.1145,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 1.3860328197479248,
      "learning_rate": 4.346490452616691e-05,
      "loss": 8.1142,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 0.9409781694412231,
      "learning_rate": 4.344280410183876e-05,
      "loss": 8.1198,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 2.06516695022583,
      "learning_rate": 4.342070367751061e-05,
      "loss": 8.1193,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 1.0741915702819824,
      "learning_rate": 4.339860325318246e-05,
      "loss": 8.1338,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 1.0689867734909058,
      "learning_rate": 4.337650282885432e-05,
      "loss": 8.13,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.6213433146476746,
      "learning_rate": 4.335440240452617e-05,
      "loss": 8.1026,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.8502789735794067,
      "learning_rate": 4.333230198019802e-05,
      "loss": 8.1249,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.8461669087409973,
      "learning_rate": 4.331020155586987e-05,
      "loss": 8.1218,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 1.2638654708862305,
      "learning_rate": 4.328810113154172e-05,
      "loss": 8.1162,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 1.817523717880249,
      "learning_rate": 4.326600070721358e-05,
      "loss": 8.1324,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 1.7943769693374634,
      "learning_rate": 4.324390028288544e-05,
      "loss": 8.1317,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 1.5107828378677368,
      "learning_rate": 4.322179985855729e-05,
      "loss": 8.1342,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 1.071122407913208,
      "learning_rate": 4.319969943422914e-05,
      "loss": 8.1342,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 1.332771897315979,
      "learning_rate": 4.317759900990099e-05,
      "loss": 8.1103,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 1.1416230201721191,
      "learning_rate": 4.315549858557285e-05,
      "loss": 8.1236,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 0.8420162200927734,
      "learning_rate": 4.31333981612447e-05,
      "loss": 8.1285,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 2.129894256591797,
      "learning_rate": 4.311129773691655e-05,
      "loss": 8.113,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 0.8761067986488342,
      "learning_rate": 4.30891973125884e-05,
      "loss": 8.1245,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 1.02657949924469,
      "learning_rate": 4.306709688826026e-05,
      "loss": 8.1165,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 0.9194222092628479,
      "learning_rate": 4.304499646393211e-05,
      "loss": 8.1331,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.9798198938369751,
      "learning_rate": 4.302289603960396e-05,
      "loss": 8.127,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 0.6078124046325684,
      "learning_rate": 4.300079561527581e-05,
      "loss": 8.1163,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 0.9838354587554932,
      "learning_rate": 4.297869519094767e-05,
      "loss": 8.1217,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.9231829047203064,
      "learning_rate": 4.295659476661952e-05,
      "loss": 8.116,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 1.1258717775344849,
      "learning_rate": 4.293449434229138e-05,
      "loss": 8.1171,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 1.7347180843353271,
      "learning_rate": 4.291239391796323e-05,
      "loss": 8.1217,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.7764060497283936,
      "learning_rate": 4.289029349363508e-05,
      "loss": 8.1162,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 1.1703591346740723,
      "learning_rate": 4.286819306930693e-05,
      "loss": 8.1115,
      "step": 3230
    },
    {
      "epoch": 1.002242672647127,
      "grad_norm": 1.196240782737732,
      "learning_rate": 4.284609264497879e-05,
      "loss": 8.1123,
      "step": 3240
    },
    {
      "epoch": 1.0053360142293712,
      "grad_norm": 0.6723086833953857,
      "learning_rate": 4.282399222065064e-05,
      "loss": 8.129,
      "step": 3250
    },
    {
      "epoch": 1.0084293558116155,
      "grad_norm": 0.9803570508956909,
      "learning_rate": 4.280189179632249e-05,
      "loss": 8.1097,
      "step": 3260
    },
    {
      "epoch": 1.0115226973938598,
      "grad_norm": 0.9006723761558533,
      "learning_rate": 4.277979137199434e-05,
      "loss": 8.1046,
      "step": 3270
    },
    {
      "epoch": 1.0146160389761039,
      "grad_norm": 1.476979374885559,
      "learning_rate": 4.27576909476662e-05,
      "loss": 8.1122,
      "step": 3280
    },
    {
      "epoch": 1.0177093805583481,
      "grad_norm": 0.8841773271560669,
      "learning_rate": 4.273559052333805e-05,
      "loss": 8.1124,
      "step": 3290
    },
    {
      "epoch": 1.0208027221405924,
      "grad_norm": 0.9440211057662964,
      "learning_rate": 4.27134900990099e-05,
      "loss": 8.128,
      "step": 3300
    },
    {
      "epoch": 1.0238960637228367,
      "grad_norm": 0.9217091798782349,
      "learning_rate": 4.269138967468175e-05,
      "loss": 8.1291,
      "step": 3310
    },
    {
      "epoch": 1.0269894053050808,
      "grad_norm": 1.0334994792938232,
      "learning_rate": 4.266928925035361e-05,
      "loss": 8.1276,
      "step": 3320
    },
    {
      "epoch": 1.030082746887325,
      "grad_norm": 0.8725922107696533,
      "learning_rate": 4.264718882602546e-05,
      "loss": 8.1128,
      "step": 3330
    },
    {
      "epoch": 1.0331760884695693,
      "grad_norm": 0.8615626096725464,
      "learning_rate": 4.262508840169732e-05,
      "loss": 8.1265,
      "step": 3340
    },
    {
      "epoch": 1.0362694300518134,
      "grad_norm": 1.0167025327682495,
      "learning_rate": 4.260298797736917e-05,
      "loss": 8.109,
      "step": 3350
    },
    {
      "epoch": 1.0393627716340577,
      "grad_norm": 1.2046356201171875,
      "learning_rate": 4.258088755304102e-05,
      "loss": 8.1251,
      "step": 3360
    },
    {
      "epoch": 1.042456113216302,
      "grad_norm": 0.7053648829460144,
      "learning_rate": 4.255878712871287e-05,
      "loss": 8.1268,
      "step": 3370
    },
    {
      "epoch": 1.0455494547985462,
      "grad_norm": 0.7233771681785583,
      "learning_rate": 4.253668670438473e-05,
      "loss": 8.1078,
      "step": 3380
    },
    {
      "epoch": 1.0486427963807903,
      "grad_norm": 0.6681994795799255,
      "learning_rate": 4.251458628005658e-05,
      "loss": 8.1095,
      "step": 3390
    },
    {
      "epoch": 1.0517361379630346,
      "grad_norm": 0.8064314126968384,
      "learning_rate": 4.249248585572843e-05,
      "loss": 8.122,
      "step": 3400
    },
    {
      "epoch": 1.0548294795452788,
      "grad_norm": 1.7897396087646484,
      "learning_rate": 4.247038543140028e-05,
      "loss": 8.1182,
      "step": 3410
    },
    {
      "epoch": 1.057922821127523,
      "grad_norm": 0.9416254162788391,
      "learning_rate": 4.244828500707214e-05,
      "loss": 8.1054,
      "step": 3420
    },
    {
      "epoch": 1.0610161627097672,
      "grad_norm": 1.1845312118530273,
      "learning_rate": 4.242618458274399e-05,
      "loss": 8.1231,
      "step": 3430
    },
    {
      "epoch": 1.0641095042920115,
      "grad_norm": 1.0002448558807373,
      "learning_rate": 4.240408415841584e-05,
      "loss": 8.1166,
      "step": 3440
    },
    {
      "epoch": 1.0672028458742557,
      "grad_norm": 1.26829993724823,
      "learning_rate": 4.23819837340877e-05,
      "loss": 8.1306,
      "step": 3450
    },
    {
      "epoch": 1.0702961874564998,
      "grad_norm": 0.8291583061218262,
      "learning_rate": 4.235988330975955e-05,
      "loss": 8.1312,
      "step": 3460
    },
    {
      "epoch": 1.073389529038744,
      "grad_norm": 1.0712348222732544,
      "learning_rate": 4.23377828854314e-05,
      "loss": 8.1141,
      "step": 3470
    },
    {
      "epoch": 1.0764828706209884,
      "grad_norm": 0.7974317669868469,
      "learning_rate": 4.231568246110326e-05,
      "loss": 8.1273,
      "step": 3480
    },
    {
      "epoch": 1.0795762122032326,
      "grad_norm": 0.7962043285369873,
      "learning_rate": 4.229358203677511e-05,
      "loss": 8.142,
      "step": 3490
    },
    {
      "epoch": 1.0826695537854767,
      "grad_norm": 0.7399154305458069,
      "learning_rate": 4.227148161244696e-05,
      "loss": 8.1102,
      "step": 3500
    },
    {
      "epoch": 1.085762895367721,
      "grad_norm": 0.807029128074646,
      "learning_rate": 4.224938118811881e-05,
      "loss": 8.1096,
      "step": 3510
    },
    {
      "epoch": 1.0888562369499653,
      "grad_norm": 0.8599138259887695,
      "learning_rate": 4.222728076379067e-05,
      "loss": 8.119,
      "step": 3520
    },
    {
      "epoch": 1.0919495785322093,
      "grad_norm": 0.8368381857872009,
      "learning_rate": 4.220518033946252e-05,
      "loss": 8.1338,
      "step": 3530
    },
    {
      "epoch": 1.0950429201144536,
      "grad_norm": 1.4200211763381958,
      "learning_rate": 4.218307991513437e-05,
      "loss": 8.1225,
      "step": 3540
    },
    {
      "epoch": 1.0981362616966979,
      "grad_norm": 0.8795577883720398,
      "learning_rate": 4.216097949080622e-05,
      "loss": 8.1274,
      "step": 3550
    },
    {
      "epoch": 1.1012296032789421,
      "grad_norm": 0.6865503787994385,
      "learning_rate": 4.213887906647808e-05,
      "loss": 8.1112,
      "step": 3560
    },
    {
      "epoch": 1.1043229448611862,
      "grad_norm": 1.285470724105835,
      "learning_rate": 4.211677864214993e-05,
      "loss": 8.1272,
      "step": 3570
    },
    {
      "epoch": 1.1074162864434305,
      "grad_norm": 1.0266700983047485,
      "learning_rate": 4.209467821782179e-05,
      "loss": 8.1126,
      "step": 3580
    },
    {
      "epoch": 1.1105096280256748,
      "grad_norm": 1.6995761394500732,
      "learning_rate": 4.207257779349364e-05,
      "loss": 8.1124,
      "step": 3590
    },
    {
      "epoch": 1.113602969607919,
      "grad_norm": 0.841712236404419,
      "learning_rate": 4.205047736916549e-05,
      "loss": 8.125,
      "step": 3600
    },
    {
      "epoch": 1.116696311190163,
      "grad_norm": 0.7503789663314819,
      "learning_rate": 4.202837694483734e-05,
      "loss": 8.1223,
      "step": 3610
    },
    {
      "epoch": 1.1197896527724074,
      "grad_norm": 0.9557420611381531,
      "learning_rate": 4.20062765205092e-05,
      "loss": 8.1153,
      "step": 3620
    },
    {
      "epoch": 1.1228829943546517,
      "grad_norm": 0.7093141674995422,
      "learning_rate": 4.198417609618105e-05,
      "loss": 8.1388,
      "step": 3630
    },
    {
      "epoch": 1.1259763359368957,
      "grad_norm": 0.7262153625488281,
      "learning_rate": 4.19620756718529e-05,
      "loss": 8.1167,
      "step": 3640
    },
    {
      "epoch": 1.12906967751914,
      "grad_norm": 0.7020951509475708,
      "learning_rate": 4.193997524752475e-05,
      "loss": 8.1244,
      "step": 3650
    },
    {
      "epoch": 1.1321630191013843,
      "grad_norm": 1.3000520467758179,
      "learning_rate": 4.191787482319661e-05,
      "loss": 8.1277,
      "step": 3660
    },
    {
      "epoch": 1.1352563606836286,
      "grad_norm": 0.5666297674179077,
      "learning_rate": 4.189577439886846e-05,
      "loss": 8.1104,
      "step": 3670
    },
    {
      "epoch": 1.1383497022658726,
      "grad_norm": 1.1555589437484741,
      "learning_rate": 4.187367397454031e-05,
      "loss": 8.1115,
      "step": 3680
    },
    {
      "epoch": 1.141443043848117,
      "grad_norm": 0.8192570209503174,
      "learning_rate": 4.185157355021216e-05,
      "loss": 8.1068,
      "step": 3690
    },
    {
      "epoch": 1.1445363854303612,
      "grad_norm": 0.8242233395576477,
      "learning_rate": 4.182947312588402e-05,
      "loss": 8.1175,
      "step": 3700
    },
    {
      "epoch": 1.1476297270126055,
      "grad_norm": 1.2689039707183838,
      "learning_rate": 4.180737270155587e-05,
      "loss": 8.1058,
      "step": 3710
    },
    {
      "epoch": 1.1507230685948495,
      "grad_norm": 1.6071594953536987,
      "learning_rate": 4.178527227722773e-05,
      "loss": 8.1172,
      "step": 3720
    },
    {
      "epoch": 1.1538164101770938,
      "grad_norm": 1.1766440868377686,
      "learning_rate": 4.176317185289958e-05,
      "loss": 8.1127,
      "step": 3730
    },
    {
      "epoch": 1.156909751759338,
      "grad_norm": 1.276888370513916,
      "learning_rate": 4.174107142857143e-05,
      "loss": 8.1253,
      "step": 3740
    },
    {
      "epoch": 1.1600030933415821,
      "grad_norm": 0.7621655464172363,
      "learning_rate": 4.171897100424328e-05,
      "loss": 8.11,
      "step": 3750
    },
    {
      "epoch": 1.1630964349238264,
      "grad_norm": 0.8809208869934082,
      "learning_rate": 4.169687057991514e-05,
      "loss": 8.1232,
      "step": 3760
    },
    {
      "epoch": 1.1661897765060707,
      "grad_norm": 0.8856102824211121,
      "learning_rate": 4.167477015558699e-05,
      "loss": 8.1147,
      "step": 3770
    },
    {
      "epoch": 1.169283118088315,
      "grad_norm": 0.7884340882301331,
      "learning_rate": 4.165266973125884e-05,
      "loss": 8.1138,
      "step": 3780
    },
    {
      "epoch": 1.172376459670559,
      "grad_norm": 0.6883849501609802,
      "learning_rate": 4.163056930693069e-05,
      "loss": 8.1246,
      "step": 3790
    },
    {
      "epoch": 1.1754698012528033,
      "grad_norm": 0.6409220695495605,
      "learning_rate": 4.160846888260255e-05,
      "loss": 8.1282,
      "step": 3800
    },
    {
      "epoch": 1.1785631428350476,
      "grad_norm": 0.958209216594696,
      "learning_rate": 4.15863684582744e-05,
      "loss": 8.1206,
      "step": 3810
    },
    {
      "epoch": 1.1816564844172919,
      "grad_norm": 0.7926703095436096,
      "learning_rate": 4.156426803394625e-05,
      "loss": 8.1229,
      "step": 3820
    },
    {
      "epoch": 1.184749825999536,
      "grad_norm": 0.8342359662055969,
      "learning_rate": 4.15421676096181e-05,
      "loss": 8.101,
      "step": 3830
    },
    {
      "epoch": 1.1878431675817802,
      "grad_norm": 1.3700296878814697,
      "learning_rate": 4.152006718528996e-05,
      "loss": 8.1235,
      "step": 3840
    },
    {
      "epoch": 1.1909365091640245,
      "grad_norm": 1.2245920896530151,
      "learning_rate": 4.149796676096182e-05,
      "loss": 8.1154,
      "step": 3850
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 0.8439896106719971,
      "learning_rate": 4.147586633663367e-05,
      "loss": 8.1216,
      "step": 3860
    },
    {
      "epoch": 1.1971231923285128,
      "grad_norm": 1.4020590782165527,
      "learning_rate": 4.145376591230552e-05,
      "loss": 8.1103,
      "step": 3870
    },
    {
      "epoch": 1.200216533910757,
      "grad_norm": 0.6946353912353516,
      "learning_rate": 4.143166548797737e-05,
      "loss": 8.1273,
      "step": 3880
    },
    {
      "epoch": 1.2033098754930014,
      "grad_norm": 1.4112876653671265,
      "learning_rate": 4.140956506364922e-05,
      "loss": 8.1156,
      "step": 3890
    },
    {
      "epoch": 1.2064032170752454,
      "grad_norm": 1.538412094116211,
      "learning_rate": 4.138746463932108e-05,
      "loss": 8.1164,
      "step": 3900
    },
    {
      "epoch": 1.2094965586574897,
      "grad_norm": 0.8160387277603149,
      "learning_rate": 4.136536421499293e-05,
      "loss": 8.1171,
      "step": 3910
    },
    {
      "epoch": 1.212589900239734,
      "grad_norm": 0.8632400035858154,
      "learning_rate": 4.134326379066478e-05,
      "loss": 8.1156,
      "step": 3920
    },
    {
      "epoch": 1.2156832418219783,
      "grad_norm": 1.040489673614502,
      "learning_rate": 4.132116336633663e-05,
      "loss": 8.1033,
      "step": 3930
    },
    {
      "epoch": 1.2187765834042223,
      "grad_norm": 1.193398356437683,
      "learning_rate": 4.129906294200849e-05,
      "loss": 8.128,
      "step": 3940
    },
    {
      "epoch": 1.2218699249864666,
      "grad_norm": 0.8503872752189636,
      "learning_rate": 4.127696251768034e-05,
      "loss": 8.1126,
      "step": 3950
    },
    {
      "epoch": 1.224963266568711,
      "grad_norm": 2.0096020698547363,
      "learning_rate": 4.125486209335219e-05,
      "loss": 8.1267,
      "step": 3960
    },
    {
      "epoch": 1.228056608150955,
      "grad_norm": 2.299232244491577,
      "learning_rate": 4.123276166902404e-05,
      "loss": 8.1184,
      "step": 3970
    },
    {
      "epoch": 1.2311499497331992,
      "grad_norm": 0.7290074229240417,
      "learning_rate": 4.12106612446959e-05,
      "loss": 8.106,
      "step": 3980
    },
    {
      "epoch": 1.2342432913154435,
      "grad_norm": 1.5019081830978394,
      "learning_rate": 4.118856082036776e-05,
      "loss": 8.1194,
      "step": 3990
    },
    {
      "epoch": 1.2373366328976878,
      "grad_norm": 0.9979866743087769,
      "learning_rate": 4.116646039603961e-05,
      "loss": 8.1205,
      "step": 4000
    },
    {
      "epoch": 1.2404299744799319,
      "grad_norm": 0.748677670955658,
      "learning_rate": 4.114435997171146e-05,
      "loss": 8.1205,
      "step": 4010
    },
    {
      "epoch": 1.2435233160621761,
      "grad_norm": 0.5963840484619141,
      "learning_rate": 4.112225954738331e-05,
      "loss": 8.1159,
      "step": 4020
    },
    {
      "epoch": 1.2466166576444204,
      "grad_norm": 1.075423002243042,
      "learning_rate": 4.110015912305516e-05,
      "loss": 8.1272,
      "step": 4030
    },
    {
      "epoch": 1.2497099992266647,
      "grad_norm": 0.8496195673942566,
      "learning_rate": 4.107805869872702e-05,
      "loss": 8.1193,
      "step": 4040
    },
    {
      "epoch": 1.2528033408089088,
      "grad_norm": 0.7476891279220581,
      "learning_rate": 4.105595827439887e-05,
      "loss": 8.1056,
      "step": 4050
    },
    {
      "epoch": 1.255896682391153,
      "grad_norm": 1.019012212753296,
      "learning_rate": 4.103385785007072e-05,
      "loss": 8.1105,
      "step": 4060
    },
    {
      "epoch": 1.2589900239733973,
      "grad_norm": 0.9022393226623535,
      "learning_rate": 4.101175742574257e-05,
      "loss": 8.1065,
      "step": 4070
    },
    {
      "epoch": 1.2620833655556414,
      "grad_norm": 0.8938788771629333,
      "learning_rate": 4.098965700141443e-05,
      "loss": 8.1397,
      "step": 4080
    },
    {
      "epoch": 1.2651767071378857,
      "grad_norm": 0.9741756319999695,
      "learning_rate": 4.096755657708628e-05,
      "loss": 8.1337,
      "step": 4090
    },
    {
      "epoch": 1.26827004872013,
      "grad_norm": 1.0345206260681152,
      "learning_rate": 4.094545615275813e-05,
      "loss": 8.1068,
      "step": 4100
    },
    {
      "epoch": 1.2713633903023742,
      "grad_norm": 0.7918864488601685,
      "learning_rate": 4.092335572842998e-05,
      "loss": 8.104,
      "step": 4110
    },
    {
      "epoch": 1.2744567318846183,
      "grad_norm": 0.8275113701820374,
      "learning_rate": 4.090125530410184e-05,
      "loss": 8.1012,
      "step": 4120
    },
    {
      "epoch": 1.2775500734668626,
      "grad_norm": 0.8178954124450684,
      "learning_rate": 4.08791548797737e-05,
      "loss": 8.1174,
      "step": 4130
    },
    {
      "epoch": 1.2806434150491068,
      "grad_norm": 0.8959318399429321,
      "learning_rate": 4.085705445544555e-05,
      "loss": 8.1188,
      "step": 4140
    },
    {
      "epoch": 1.2837367566313511,
      "grad_norm": 1.0194681882858276,
      "learning_rate": 4.08349540311174e-05,
      "loss": 8.1197,
      "step": 4150
    },
    {
      "epoch": 1.2868300982135952,
      "grad_norm": 0.8763304948806763,
      "learning_rate": 4.081285360678925e-05,
      "loss": 8.1239,
      "step": 4160
    },
    {
      "epoch": 1.2899234397958395,
      "grad_norm": 0.9870306253433228,
      "learning_rate": 4.07907531824611e-05,
      "loss": 8.1268,
      "step": 4170
    },
    {
      "epoch": 1.2930167813780837,
      "grad_norm": 1.064742922782898,
      "learning_rate": 4.076865275813296e-05,
      "loss": 8.1194,
      "step": 4180
    },
    {
      "epoch": 1.2961101229603278,
      "grad_norm": 0.6587199568748474,
      "learning_rate": 4.074655233380481e-05,
      "loss": 8.1078,
      "step": 4190
    },
    {
      "epoch": 1.299203464542572,
      "grad_norm": 0.9830871224403381,
      "learning_rate": 4.072445190947666e-05,
      "loss": 8.1107,
      "step": 4200
    },
    {
      "epoch": 1.3022968061248164,
      "grad_norm": 1.1235051155090332,
      "learning_rate": 4.070235148514851e-05,
      "loss": 8.1202,
      "step": 4210
    },
    {
      "epoch": 1.3053901477070606,
      "grad_norm": 0.8844292759895325,
      "learning_rate": 4.068025106082037e-05,
      "loss": 8.125,
      "step": 4220
    },
    {
      "epoch": 1.3084834892893047,
      "grad_norm": 0.8180374503135681,
      "learning_rate": 4.0660360678925034e-05,
      "loss": 8.1132,
      "step": 4230
    },
    {
      "epoch": 1.311576830871549,
      "grad_norm": 0.7194439172744751,
      "learning_rate": 4.0638260254596885e-05,
      "loss": 8.1183,
      "step": 4240
    },
    {
      "epoch": 1.3146701724537933,
      "grad_norm": 0.6383553743362427,
      "learning_rate": 4.061615983026874e-05,
      "loss": 8.1124,
      "step": 4250
    },
    {
      "epoch": 1.3177635140360375,
      "grad_norm": 1.5439708232879639,
      "learning_rate": 4.05940594059406e-05,
      "loss": 8.1049,
      "step": 4260
    },
    {
      "epoch": 1.3208568556182816,
      "grad_norm": 0.8215919137001038,
      "learning_rate": 4.057195898161245e-05,
      "loss": 8.1187,
      "step": 4270
    },
    {
      "epoch": 1.3239501972005259,
      "grad_norm": 1.3486407995224,
      "learning_rate": 4.05498585572843e-05,
      "loss": 8.1113,
      "step": 4280
    },
    {
      "epoch": 1.3270435387827701,
      "grad_norm": 0.7948507070541382,
      "learning_rate": 4.052775813295615e-05,
      "loss": 8.1193,
      "step": 4290
    },
    {
      "epoch": 1.3301368803650142,
      "grad_norm": 0.8348109126091003,
      "learning_rate": 4.050565770862801e-05,
      "loss": 8.1097,
      "step": 4300
    },
    {
      "epoch": 1.3332302219472585,
      "grad_norm": 1.6734665632247925,
      "learning_rate": 4.048355728429986e-05,
      "loss": 8.1013,
      "step": 4310
    },
    {
      "epoch": 1.3363235635295028,
      "grad_norm": 1.6595321893692017,
      "learning_rate": 4.046145685997171e-05,
      "loss": 8.1157,
      "step": 4320
    },
    {
      "epoch": 1.339416905111747,
      "grad_norm": 1.155346155166626,
      "learning_rate": 4.0439356435643564e-05,
      "loss": 8.1204,
      "step": 4330
    },
    {
      "epoch": 1.342510246693991,
      "grad_norm": 1.04776930809021,
      "learning_rate": 4.041725601131542e-05,
      "loss": 8.1105,
      "step": 4340
    },
    {
      "epoch": 1.3456035882762354,
      "grad_norm": 0.9083101153373718,
      "learning_rate": 4.039515558698727e-05,
      "loss": 8.1142,
      "step": 4350
    },
    {
      "epoch": 1.3486969298584797,
      "grad_norm": 0.7897149920463562,
      "learning_rate": 4.037305516265912e-05,
      "loss": 8.1239,
      "step": 4360
    },
    {
      "epoch": 1.351790271440724,
      "grad_norm": 0.9427816867828369,
      "learning_rate": 4.0350954738330974e-05,
      "loss": 8.1215,
      "step": 4370
    },
    {
      "epoch": 1.354883613022968,
      "grad_norm": 0.8597368597984314,
      "learning_rate": 4.0328854314002825e-05,
      "loss": 8.129,
      "step": 4380
    },
    {
      "epoch": 1.3579769546052123,
      "grad_norm": 0.7657512426376343,
      "learning_rate": 4.030675388967468e-05,
      "loss": 8.1156,
      "step": 4390
    },
    {
      "epoch": 1.3610702961874566,
      "grad_norm": 1.1744600534439087,
      "learning_rate": 4.028465346534654e-05,
      "loss": 8.1072,
      "step": 4400
    },
    {
      "epoch": 1.3641636377697006,
      "grad_norm": 0.7139284014701843,
      "learning_rate": 4.026255304101839e-05,
      "loss": 8.1136,
      "step": 4410
    },
    {
      "epoch": 1.367256979351945,
      "grad_norm": 0.9807798266410828,
      "learning_rate": 4.024045261669024e-05,
      "loss": 8.124,
      "step": 4420
    },
    {
      "epoch": 1.3703503209341892,
      "grad_norm": 0.5589386224746704,
      "learning_rate": 4.0218352192362093e-05,
      "loss": 8.1331,
      "step": 4430
    },
    {
      "epoch": 1.3734436625164332,
      "grad_norm": 0.8512092232704163,
      "learning_rate": 4.019625176803395e-05,
      "loss": 8.1161,
      "step": 4440
    },
    {
      "epoch": 1.3765370040986775,
      "grad_norm": 0.8046604990959167,
      "learning_rate": 4.01741513437058e-05,
      "loss": 8.1099,
      "step": 4450
    },
    {
      "epoch": 1.3796303456809218,
      "grad_norm": 0.9923661947250366,
      "learning_rate": 4.015205091937765e-05,
      "loss": 8.1189,
      "step": 4460
    },
    {
      "epoch": 1.382723687263166,
      "grad_norm": 1.1229900121688843,
      "learning_rate": 4.0129950495049504e-05,
      "loss": 8.117,
      "step": 4470
    },
    {
      "epoch": 1.3858170288454104,
      "grad_norm": 1.0397707223892212,
      "learning_rate": 4.010785007072136e-05,
      "loss": 8.1115,
      "step": 4480
    },
    {
      "epoch": 1.3889103704276544,
      "grad_norm": 1.102938175201416,
      "learning_rate": 4.008574964639321e-05,
      "loss": 8.1159,
      "step": 4490
    },
    {
      "epoch": 1.3920037120098987,
      "grad_norm": 0.8214883208274841,
      "learning_rate": 4.0063649222065064e-05,
      "loss": 8.1035,
      "step": 4500
    },
    {
      "epoch": 1.395097053592143,
      "grad_norm": 0.5585210919380188,
      "learning_rate": 4.0041548797736915e-05,
      "loss": 8.1115,
      "step": 4510
    },
    {
      "epoch": 1.398190395174387,
      "grad_norm": 0.7796432971954346,
      "learning_rate": 4.0019448373408765e-05,
      "loss": 8.1078,
      "step": 4520
    },
    {
      "epoch": 1.4012837367566313,
      "grad_norm": 1.1832889318466187,
      "learning_rate": 3.999734794908063e-05,
      "loss": 8.1274,
      "step": 4530
    },
    {
      "epoch": 1.4043770783388756,
      "grad_norm": 0.934551477432251,
      "learning_rate": 3.997524752475248e-05,
      "loss": 8.1287,
      "step": 4540
    },
    {
      "epoch": 1.4074704199211197,
      "grad_norm": 0.9180008172988892,
      "learning_rate": 3.995314710042433e-05,
      "loss": 8.105,
      "step": 4550
    },
    {
      "epoch": 1.410563761503364,
      "grad_norm": 1.7560917139053345,
      "learning_rate": 3.993104667609618e-05,
      "loss": 8.1259,
      "step": 4560
    },
    {
      "epoch": 1.4136571030856082,
      "grad_norm": 0.662970244884491,
      "learning_rate": 3.9908946251768034e-05,
      "loss": 8.129,
      "step": 4570
    },
    {
      "epoch": 1.4167504446678525,
      "grad_norm": 0.7053908109664917,
      "learning_rate": 3.988684582743989e-05,
      "loss": 8.1133,
      "step": 4580
    },
    {
      "epoch": 1.4198437862500968,
      "grad_norm": 0.7108381986618042,
      "learning_rate": 3.986474540311174e-05,
      "loss": 8.1014,
      "step": 4590
    },
    {
      "epoch": 1.4229371278323408,
      "grad_norm": 0.5228309035301208,
      "learning_rate": 3.984264497878359e-05,
      "loss": 8.0972,
      "step": 4600
    },
    {
      "epoch": 1.4260304694145851,
      "grad_norm": 1.6444404125213623,
      "learning_rate": 3.9820544554455444e-05,
      "loss": 8.1057,
      "step": 4610
    },
    {
      "epoch": 1.4291238109968294,
      "grad_norm": 1.3708868026733398,
      "learning_rate": 3.97984441301273e-05,
      "loss": 8.1136,
      "step": 4620
    },
    {
      "epoch": 1.4322171525790734,
      "grad_norm": 0.8128147125244141,
      "learning_rate": 3.977634370579915e-05,
      "loss": 8.1201,
      "step": 4630
    },
    {
      "epoch": 1.4353104941613177,
      "grad_norm": 0.6589893102645874,
      "learning_rate": 3.9754243281471004e-05,
      "loss": 8.1182,
      "step": 4640
    },
    {
      "epoch": 1.438403835743562,
      "grad_norm": 0.6600621342658997,
      "learning_rate": 3.9732142857142855e-05,
      "loss": 8.121,
      "step": 4650
    },
    {
      "epoch": 1.441497177325806,
      "grad_norm": 0.7272864580154419,
      "learning_rate": 3.971004243281471e-05,
      "loss": 8.1113,
      "step": 4660
    },
    {
      "epoch": 1.4445905189080503,
      "grad_norm": 0.5461433529853821,
      "learning_rate": 3.968794200848657e-05,
      "loss": 8.1205,
      "step": 4670
    },
    {
      "epoch": 1.4476838604902946,
      "grad_norm": 1.0180320739746094,
      "learning_rate": 3.966584158415842e-05,
      "loss": 8.1157,
      "step": 4680
    },
    {
      "epoch": 1.450777202072539,
      "grad_norm": 0.6926457285881042,
      "learning_rate": 3.964374115983027e-05,
      "loss": 8.108,
      "step": 4690
    },
    {
      "epoch": 1.4538705436547832,
      "grad_norm": 0.7072666883468628,
      "learning_rate": 3.962164073550212e-05,
      "loss": 8.1183,
      "step": 4700
    },
    {
      "epoch": 1.4569638852370272,
      "grad_norm": 0.7059903740882874,
      "learning_rate": 3.9599540311173974e-05,
      "loss": 8.1171,
      "step": 4710
    },
    {
      "epoch": 1.4600572268192715,
      "grad_norm": 0.957748293876648,
      "learning_rate": 3.957743988684583e-05,
      "loss": 8.1209,
      "step": 4720
    },
    {
      "epoch": 1.4631505684015158,
      "grad_norm": 1.2396856546401978,
      "learning_rate": 3.955533946251768e-05,
      "loss": 8.1166,
      "step": 4730
    },
    {
      "epoch": 1.4662439099837599,
      "grad_norm": 0.7316197752952576,
      "learning_rate": 3.9533239038189534e-05,
      "loss": 8.1132,
      "step": 4740
    },
    {
      "epoch": 1.4693372515660041,
      "grad_norm": 1.0521752834320068,
      "learning_rate": 3.9511138613861385e-05,
      "loss": 8.1125,
      "step": 4750
    },
    {
      "epoch": 1.4724305931482484,
      "grad_norm": 0.9110093116760254,
      "learning_rate": 3.948903818953324e-05,
      "loss": 8.1194,
      "step": 4760
    },
    {
      "epoch": 1.4755239347304925,
      "grad_norm": 1.4379953145980835,
      "learning_rate": 3.946693776520509e-05,
      "loss": 8.1168,
      "step": 4770
    },
    {
      "epoch": 1.4786172763127368,
      "grad_norm": 2.6326723098754883,
      "learning_rate": 3.9444837340876944e-05,
      "loss": 8.1228,
      "step": 4780
    },
    {
      "epoch": 1.481710617894981,
      "grad_norm": 0.9093007445335388,
      "learning_rate": 3.9422736916548795e-05,
      "loss": 8.1172,
      "step": 4790
    },
    {
      "epoch": 1.4848039594772253,
      "grad_norm": 1.3808073997497559,
      "learning_rate": 3.940063649222065e-05,
      "loss": 8.1097,
      "step": 4800
    },
    {
      "epoch": 1.4878973010594696,
      "grad_norm": 1.156030297279358,
      "learning_rate": 3.937853606789251e-05,
      "loss": 8.1163,
      "step": 4810
    },
    {
      "epoch": 1.4909906426417137,
      "grad_norm": 0.951022207736969,
      "learning_rate": 3.935643564356436e-05,
      "loss": 8.1125,
      "step": 4820
    },
    {
      "epoch": 1.494083984223958,
      "grad_norm": 1.0353246927261353,
      "learning_rate": 3.933433521923621e-05,
      "loss": 8.1266,
      "step": 4830
    },
    {
      "epoch": 1.4971773258062022,
      "grad_norm": 1.5673549175262451,
      "learning_rate": 3.9312234794908063e-05,
      "loss": 8.1071,
      "step": 4840
    },
    {
      "epoch": 1.5002706673884463,
      "grad_norm": 0.8361724019050598,
      "learning_rate": 3.9290134370579914e-05,
      "loss": 8.1019,
      "step": 4850
    },
    {
      "epoch": 1.5033640089706906,
      "grad_norm": 1.9008229970932007,
      "learning_rate": 3.926803394625177e-05,
      "loss": 8.1018,
      "step": 4860
    },
    {
      "epoch": 1.5064573505529348,
      "grad_norm": 1.5485745668411255,
      "learning_rate": 3.924593352192362e-05,
      "loss": 8.1134,
      "step": 4870
    },
    {
      "epoch": 1.509550692135179,
      "grad_norm": 1.1073501110076904,
      "learning_rate": 3.9223833097595474e-05,
      "loss": 8.1189,
      "step": 4880
    },
    {
      "epoch": 1.5126440337174234,
      "grad_norm": 0.9330412149429321,
      "learning_rate": 3.9201732673267325e-05,
      "loss": 8.1145,
      "step": 4890
    },
    {
      "epoch": 1.5157373752996675,
      "grad_norm": 1.2551467418670654,
      "learning_rate": 3.917963224893918e-05,
      "loss": 8.1187,
      "step": 4900
    },
    {
      "epoch": 1.5188307168819117,
      "grad_norm": 0.8344804048538208,
      "learning_rate": 3.9157531824611034e-05,
      "loss": 8.1093,
      "step": 4910
    },
    {
      "epoch": 1.521924058464156,
      "grad_norm": 0.6430570483207703,
      "learning_rate": 3.9135431400282885e-05,
      "loss": 8.1202,
      "step": 4920
    },
    {
      "epoch": 1.5250174000464,
      "grad_norm": 0.8347640037536621,
      "learning_rate": 3.911333097595474e-05,
      "loss": 8.0982,
      "step": 4930
    },
    {
      "epoch": 1.5281107416286444,
      "grad_norm": 0.781800389289856,
      "learning_rate": 3.909123055162659e-05,
      "loss": 8.1216,
      "step": 4940
    },
    {
      "epoch": 1.5312040832108886,
      "grad_norm": 0.928343653678894,
      "learning_rate": 3.906913012729845e-05,
      "loss": 8.1129,
      "step": 4950
    },
    {
      "epoch": 1.5342974247931327,
      "grad_norm": 1.2262492179870605,
      "learning_rate": 3.90470297029703e-05,
      "loss": 8.1268,
      "step": 4960
    },
    {
      "epoch": 1.537390766375377,
      "grad_norm": 1.014028549194336,
      "learning_rate": 3.902492927864215e-05,
      "loss": 8.1158,
      "step": 4970
    },
    {
      "epoch": 1.5404841079576213,
      "grad_norm": 0.7858170866966248,
      "learning_rate": 3.9002828854314004e-05,
      "loss": 8.1221,
      "step": 4980
    },
    {
      "epoch": 1.5435774495398653,
      "grad_norm": 0.6599904894828796,
      "learning_rate": 3.8980728429985855e-05,
      "loss": 8.1171,
      "step": 4990
    },
    {
      "epoch": 1.5466707911221098,
      "grad_norm": 1.3178737163543701,
      "learning_rate": 3.895862800565771e-05,
      "loss": 8.1041,
      "step": 5000
    },
    {
      "epoch": 1.5497641327043539,
      "grad_norm": 1.6083639860153198,
      "learning_rate": 3.8936527581329563e-05,
      "loss": 8.1254,
      "step": 5010
    },
    {
      "epoch": 1.5528574742865981,
      "grad_norm": 1.5805290937423706,
      "learning_rate": 3.8914427157001414e-05,
      "loss": 8.0958,
      "step": 5020
    },
    {
      "epoch": 1.5559508158688424,
      "grad_norm": 1.2158139944076538,
      "learning_rate": 3.8892326732673265e-05,
      "loss": 8.1145,
      "step": 5030
    },
    {
      "epoch": 1.5590441574510865,
      "grad_norm": 0.7423880696296692,
      "learning_rate": 3.887022630834512e-05,
      "loss": 8.1075,
      "step": 5040
    },
    {
      "epoch": 1.5621374990333308,
      "grad_norm": 1.2053617238998413,
      "learning_rate": 3.8848125884016974e-05,
      "loss": 8.1019,
      "step": 5050
    },
    {
      "epoch": 1.565230840615575,
      "grad_norm": 0.5607873201370239,
      "learning_rate": 3.882602545968883e-05,
      "loss": 8.1265,
      "step": 5060
    },
    {
      "epoch": 1.568324182197819,
      "grad_norm": 0.7237718105316162,
      "learning_rate": 3.880392503536068e-05,
      "loss": 8.0937,
      "step": 5070
    },
    {
      "epoch": 1.5714175237800634,
      "grad_norm": 1.6400437355041504,
      "learning_rate": 3.8781824611032534e-05,
      "loss": 8.1132,
      "step": 5080
    },
    {
      "epoch": 1.5745108653623077,
      "grad_norm": 0.618736982345581,
      "learning_rate": 3.875972418670439e-05,
      "loss": 8.1222,
      "step": 5090
    },
    {
      "epoch": 1.5776042069445517,
      "grad_norm": 1.4394365549087524,
      "learning_rate": 3.873762376237624e-05,
      "loss": 8.1201,
      "step": 5100
    },
    {
      "epoch": 1.5806975485267962,
      "grad_norm": 1.4025369882583618,
      "learning_rate": 3.871552333804809e-05,
      "loss": 8.1226,
      "step": 5110
    },
    {
      "epoch": 1.5837908901090403,
      "grad_norm": 0.745320737361908,
      "learning_rate": 3.8693422913719944e-05,
      "loss": 8.1075,
      "step": 5120
    },
    {
      "epoch": 1.5868842316912846,
      "grad_norm": 0.6909487247467041,
      "learning_rate": 3.8671322489391795e-05,
      "loss": 8.1062,
      "step": 5130
    },
    {
      "epoch": 1.5899775732735288,
      "grad_norm": 1.0641508102416992,
      "learning_rate": 3.864922206506365e-05,
      "loss": 8.1132,
      "step": 5140
    },
    {
      "epoch": 1.593070914855773,
      "grad_norm": 1.2837423086166382,
      "learning_rate": 3.8627121640735504e-05,
      "loss": 8.1281,
      "step": 5150
    },
    {
      "epoch": 1.5961642564380172,
      "grad_norm": 0.9686927795410156,
      "learning_rate": 3.8605021216407355e-05,
      "loss": 8.111,
      "step": 5160
    },
    {
      "epoch": 1.5992575980202615,
      "grad_norm": 0.5163270235061646,
      "learning_rate": 3.8582920792079206e-05,
      "loss": 8.1001,
      "step": 5170
    },
    {
      "epoch": 1.6023509396025055,
      "grad_norm": 0.7700867652893066,
      "learning_rate": 3.856082036775106e-05,
      "loss": 8.1198,
      "step": 5180
    },
    {
      "epoch": 1.6054442811847498,
      "grad_norm": 1.0453859567642212,
      "learning_rate": 3.8538719943422914e-05,
      "loss": 8.114,
      "step": 5190
    },
    {
      "epoch": 1.608537622766994,
      "grad_norm": 1.1160624027252197,
      "learning_rate": 3.851661951909477e-05,
      "loss": 8.1098,
      "step": 5200
    },
    {
      "epoch": 1.6116309643492381,
      "grad_norm": 0.7706289887428284,
      "learning_rate": 3.849451909476662e-05,
      "loss": 8.1194,
      "step": 5210
    },
    {
      "epoch": 1.6147243059314826,
      "grad_norm": 1.5528191328048706,
      "learning_rate": 3.8472418670438474e-05,
      "loss": 8.0974,
      "step": 5220
    },
    {
      "epoch": 1.6178176475137267,
      "grad_norm": 1.085829257965088,
      "learning_rate": 3.845031824611033e-05,
      "loss": 8.1216,
      "step": 5230
    },
    {
      "epoch": 1.620910989095971,
      "grad_norm": 1.0217398405075073,
      "learning_rate": 3.842821782178218e-05,
      "loss": 8.1287,
      "step": 5240
    },
    {
      "epoch": 1.6240043306782153,
      "grad_norm": 1.269791603088379,
      "learning_rate": 3.8406117397454033e-05,
      "loss": 8.1177,
      "step": 5250
    },
    {
      "epoch": 1.6270976722604593,
      "grad_norm": 0.6634871363639832,
      "learning_rate": 3.8384016973125884e-05,
      "loss": 8.1193,
      "step": 5260
    },
    {
      "epoch": 1.6301910138427036,
      "grad_norm": 0.7631880044937134,
      "learning_rate": 3.8361916548797735e-05,
      "loss": 8.1177,
      "step": 5270
    },
    {
      "epoch": 1.6332843554249479,
      "grad_norm": 1.3291696310043335,
      "learning_rate": 3.833981612446959e-05,
      "loss": 8.1398,
      "step": 5280
    },
    {
      "epoch": 1.636377697007192,
      "grad_norm": 0.8691796660423279,
      "learning_rate": 3.8317715700141444e-05,
      "loss": 8.1067,
      "step": 5290
    },
    {
      "epoch": 1.6394710385894362,
      "grad_norm": 0.931352436542511,
      "learning_rate": 3.8295615275813295e-05,
      "loss": 8.1137,
      "step": 5300
    },
    {
      "epoch": 1.6425643801716805,
      "grad_norm": 0.9599955081939697,
      "learning_rate": 3.8273514851485146e-05,
      "loss": 8.1128,
      "step": 5310
    },
    {
      "epoch": 1.6456577217539246,
      "grad_norm": 0.7459596395492554,
      "learning_rate": 3.8251414427157004e-05,
      "loss": 8.109,
      "step": 5320
    },
    {
      "epoch": 1.648751063336169,
      "grad_norm": 0.8625908493995667,
      "learning_rate": 3.822931400282886e-05,
      "loss": 8.0995,
      "step": 5330
    },
    {
      "epoch": 1.6518444049184131,
      "grad_norm": 1.3661296367645264,
      "learning_rate": 3.820721357850071e-05,
      "loss": 8.1041,
      "step": 5340
    },
    {
      "epoch": 1.6549377465006572,
      "grad_norm": 1.1211706399917603,
      "learning_rate": 3.818511315417256e-05,
      "loss": 8.1119,
      "step": 5350
    },
    {
      "epoch": 1.6580310880829017,
      "grad_norm": 0.6514363288879395,
      "learning_rate": 3.8163012729844414e-05,
      "loss": 8.1057,
      "step": 5360
    },
    {
      "epoch": 1.6611244296651457,
      "grad_norm": 0.8911955952644348,
      "learning_rate": 3.814091230551627e-05,
      "loss": 8.1198,
      "step": 5370
    },
    {
      "epoch": 1.66421777124739,
      "grad_norm": 1.1355024576187134,
      "learning_rate": 3.811881188118812e-05,
      "loss": 8.1219,
      "step": 5380
    },
    {
      "epoch": 1.6673111128296343,
      "grad_norm": 0.9677493572235107,
      "learning_rate": 3.8096711456859974e-05,
      "loss": 8.1104,
      "step": 5390
    },
    {
      "epoch": 1.6704044544118783,
      "grad_norm": 0.7535940408706665,
      "learning_rate": 3.8074611032531825e-05,
      "loss": 8.1196,
      "step": 5400
    },
    {
      "epoch": 1.6734977959941226,
      "grad_norm": 0.5847275257110596,
      "learning_rate": 3.8052510608203676e-05,
      "loss": 8.1004,
      "step": 5410
    },
    {
      "epoch": 1.676591137576367,
      "grad_norm": 0.885806143283844,
      "learning_rate": 3.8030410183875533e-05,
      "loss": 8.0886,
      "step": 5420
    },
    {
      "epoch": 1.679684479158611,
      "grad_norm": 1.359310269355774,
      "learning_rate": 3.8008309759547384e-05,
      "loss": 8.0951,
      "step": 5430
    },
    {
      "epoch": 1.6827778207408555,
      "grad_norm": 2.5270769596099854,
      "learning_rate": 3.7986209335219235e-05,
      "loss": 8.1231,
      "step": 5440
    },
    {
      "epoch": 1.6858711623230995,
      "grad_norm": 1.284864068031311,
      "learning_rate": 3.7964108910891086e-05,
      "loss": 8.1244,
      "step": 5450
    },
    {
      "epoch": 1.6889645039053436,
      "grad_norm": 1.3578400611877441,
      "learning_rate": 3.7942008486562944e-05,
      "loss": 8.1189,
      "step": 5460
    },
    {
      "epoch": 1.692057845487588,
      "grad_norm": 2.665391683578491,
      "learning_rate": 3.79199080622348e-05,
      "loss": 8.1234,
      "step": 5470
    },
    {
      "epoch": 1.6951511870698321,
      "grad_norm": 0.6308792233467102,
      "learning_rate": 3.789780763790665e-05,
      "loss": 8.1219,
      "step": 5480
    },
    {
      "epoch": 1.6982445286520764,
      "grad_norm": 1.0213325023651123,
      "learning_rate": 3.7875707213578504e-05,
      "loss": 8.1212,
      "step": 5490
    },
    {
      "epoch": 1.7013378702343207,
      "grad_norm": 1.4457077980041504,
      "learning_rate": 3.7853606789250355e-05,
      "loss": 8.1045,
      "step": 5500
    },
    {
      "epoch": 1.7044312118165648,
      "grad_norm": 0.6500237584114075,
      "learning_rate": 3.783150636492221e-05,
      "loss": 8.1071,
      "step": 5510
    },
    {
      "epoch": 1.707524553398809,
      "grad_norm": 0.9573272466659546,
      "learning_rate": 3.780940594059406e-05,
      "loss": 8.1274,
      "step": 5520
    },
    {
      "epoch": 1.7106178949810533,
      "grad_norm": 0.8521197438240051,
      "learning_rate": 3.7787305516265914e-05,
      "loss": 8.1037,
      "step": 5530
    },
    {
      "epoch": 1.7137112365632974,
      "grad_norm": 0.679905891418457,
      "learning_rate": 3.7765205091937765e-05,
      "loss": 8.1103,
      "step": 5540
    },
    {
      "epoch": 1.7168045781455419,
      "grad_norm": 0.9739455580711365,
      "learning_rate": 3.7743104667609616e-05,
      "loss": 8.1163,
      "step": 5550
    },
    {
      "epoch": 1.719897919727786,
      "grad_norm": 1.4107309579849243,
      "learning_rate": 3.7721004243281474e-05,
      "loss": 8.1068,
      "step": 5560
    },
    {
      "epoch": 1.72299126131003,
      "grad_norm": 0.9260146617889404,
      "learning_rate": 3.7698903818953325e-05,
      "loss": 8.1167,
      "step": 5570
    },
    {
      "epoch": 1.7260846028922745,
      "grad_norm": 0.6201887130737305,
      "learning_rate": 3.7676803394625176e-05,
      "loss": 8.1159,
      "step": 5580
    },
    {
      "epoch": 1.7291779444745186,
      "grad_norm": 0.917417049407959,
      "learning_rate": 3.7654702970297027e-05,
      "loss": 8.106,
      "step": 5590
    },
    {
      "epoch": 1.7322712860567628,
      "grad_norm": 1.094301462173462,
      "learning_rate": 3.7632602545968884e-05,
      "loss": 8.1308,
      "step": 5600
    },
    {
      "epoch": 1.7353646276390071,
      "grad_norm": 0.8865593075752258,
      "learning_rate": 3.7612712164073554e-05,
      "loss": 8.1141,
      "step": 5610
    },
    {
      "epoch": 1.7384579692212512,
      "grad_norm": 2.1721909046173096,
      "learning_rate": 3.7590611739745405e-05,
      "loss": 8.1198,
      "step": 5620
    },
    {
      "epoch": 1.7415513108034955,
      "grad_norm": 2.3732712268829346,
      "learning_rate": 3.756851131541726e-05,
      "loss": 8.1197,
      "step": 5630
    },
    {
      "epoch": 1.7446446523857397,
      "grad_norm": 1.7584190368652344,
      "learning_rate": 3.7546410891089114e-05,
      "loss": 8.1233,
      "step": 5640
    },
    {
      "epoch": 1.7477379939679838,
      "grad_norm": 0.9037502408027649,
      "learning_rate": 3.7524310466760965e-05,
      "loss": 8.1121,
      "step": 5650
    },
    {
      "epoch": 1.7508313355502283,
      "grad_norm": 1.0689423084259033,
      "learning_rate": 3.7502210042432816e-05,
      "loss": 8.1178,
      "step": 5660
    },
    {
      "epoch": 1.7539246771324724,
      "grad_norm": 0.8617199063301086,
      "learning_rate": 3.748010961810467e-05,
      "loss": 8.1235,
      "step": 5670
    },
    {
      "epoch": 1.7570180187147164,
      "grad_norm": 0.687280535697937,
      "learning_rate": 3.7458009193776524e-05,
      "loss": 8.1259,
      "step": 5680
    },
    {
      "epoch": 1.760111360296961,
      "grad_norm": 0.6172989010810852,
      "learning_rate": 3.7435908769448375e-05,
      "loss": 8.1163,
      "step": 5690
    },
    {
      "epoch": 1.763204701879205,
      "grad_norm": 0.7951784133911133,
      "learning_rate": 3.7413808345120226e-05,
      "loss": 8.1104,
      "step": 5700
    },
    {
      "epoch": 1.7662980434614493,
      "grad_norm": 1.39143705368042,
      "learning_rate": 3.739170792079208e-05,
      "loss": 8.1237,
      "step": 5710
    },
    {
      "epoch": 1.7693913850436935,
      "grad_norm": 0.688704252243042,
      "learning_rate": 3.7369607496463935e-05,
      "loss": 8.1073,
      "step": 5720
    },
    {
      "epoch": 1.7724847266259376,
      "grad_norm": 0.890242338180542,
      "learning_rate": 3.7347507072135786e-05,
      "loss": 8.1066,
      "step": 5730
    },
    {
      "epoch": 1.7755780682081819,
      "grad_norm": 0.5613893866539001,
      "learning_rate": 3.732540664780764e-05,
      "loss": 8.1179,
      "step": 5740
    },
    {
      "epoch": 1.7786714097904262,
      "grad_norm": 1.1916487216949463,
      "learning_rate": 3.7303306223479495e-05,
      "loss": 8.101,
      "step": 5750
    },
    {
      "epoch": 1.7817647513726702,
      "grad_norm": 0.8538577556610107,
      "learning_rate": 3.7281205799151345e-05,
      "loss": 8.1059,
      "step": 5760
    },
    {
      "epoch": 1.7848580929549147,
      "grad_norm": 0.7381499409675598,
      "learning_rate": 3.72591053748232e-05,
      "loss": 8.1159,
      "step": 5770
    },
    {
      "epoch": 1.7879514345371588,
      "grad_norm": 0.6222342848777771,
      "learning_rate": 3.7237004950495054e-05,
      "loss": 8.1096,
      "step": 5780
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 0.6624096035957336,
      "learning_rate": 3.7214904526166905e-05,
      "loss": 8.1123,
      "step": 5790
    },
    {
      "epoch": 1.7941381177016473,
      "grad_norm": 1.1382503509521484,
      "learning_rate": 3.7192804101838756e-05,
      "loss": 8.111,
      "step": 5800
    },
    {
      "epoch": 1.7972314592838914,
      "grad_norm": 0.8287107348442078,
      "learning_rate": 3.717070367751061e-05,
      "loss": 8.1129,
      "step": 5810
    },
    {
      "epoch": 1.8003248008661357,
      "grad_norm": 0.6545336842536926,
      "learning_rate": 3.7148603253182465e-05,
      "loss": 8.1124,
      "step": 5820
    },
    {
      "epoch": 1.80341814244838,
      "grad_norm": 1.1369034051895142,
      "learning_rate": 3.7126502828854316e-05,
      "loss": 8.1055,
      "step": 5830
    },
    {
      "epoch": 1.806511484030624,
      "grad_norm": 1.0850688219070435,
      "learning_rate": 3.7104402404526167e-05,
      "loss": 8.1012,
      "step": 5840
    },
    {
      "epoch": 1.8096048256128683,
      "grad_norm": 0.6212559938430786,
      "learning_rate": 3.708230198019802e-05,
      "loss": 8.1106,
      "step": 5850
    },
    {
      "epoch": 1.8126981671951126,
      "grad_norm": 1.202318549156189,
      "learning_rate": 3.7060201555869875e-05,
      "loss": 8.1227,
      "step": 5860
    },
    {
      "epoch": 1.8157915087773566,
      "grad_norm": 1.0532689094543457,
      "learning_rate": 3.7038101131541726e-05,
      "loss": 8.0981,
      "step": 5870
    },
    {
      "epoch": 1.8188848503596011,
      "grad_norm": 1.1592659950256348,
      "learning_rate": 3.7016000707213584e-05,
      "loss": 8.1151,
      "step": 5880
    },
    {
      "epoch": 1.8219781919418452,
      "grad_norm": 0.48596423864364624,
      "learning_rate": 3.6993900282885435e-05,
      "loss": 8.1205,
      "step": 5890
    },
    {
      "epoch": 1.8250715335240892,
      "grad_norm": 0.6388185024261475,
      "learning_rate": 3.6971799858557286e-05,
      "loss": 8.1025,
      "step": 5900
    },
    {
      "epoch": 1.8281648751063337,
      "grad_norm": 0.7377865314483643,
      "learning_rate": 3.6949699434229143e-05,
      "loss": 8.0985,
      "step": 5910
    },
    {
      "epoch": 1.8312582166885778,
      "grad_norm": 0.7783490419387817,
      "learning_rate": 3.6927599009900994e-05,
      "loss": 8.1144,
      "step": 5920
    },
    {
      "epoch": 1.834351558270822,
      "grad_norm": 0.6981886029243469,
      "learning_rate": 3.6905498585572845e-05,
      "loss": 8.1129,
      "step": 5930
    },
    {
      "epoch": 1.8374448998530664,
      "grad_norm": 1.4069945812225342,
      "learning_rate": 3.6883398161244696e-05,
      "loss": 8.121,
      "step": 5940
    },
    {
      "epoch": 1.8405382414353104,
      "grad_norm": 0.6732660531997681,
      "learning_rate": 3.686129773691655e-05,
      "loss": 8.1234,
      "step": 5950
    },
    {
      "epoch": 1.8436315830175547,
      "grad_norm": 0.8745042681694031,
      "learning_rate": 3.6839197312588405e-05,
      "loss": 8.1142,
      "step": 5960
    },
    {
      "epoch": 1.846724924599799,
      "grad_norm": 0.6202366352081299,
      "learning_rate": 3.6817096888260256e-05,
      "loss": 8.1145,
      "step": 5970
    },
    {
      "epoch": 1.849818266182043,
      "grad_norm": 0.8213227391242981,
      "learning_rate": 3.679499646393211e-05,
      "loss": 8.1017,
      "step": 5980
    },
    {
      "epoch": 1.8529116077642873,
      "grad_norm": 0.5994757413864136,
      "learning_rate": 3.677289603960396e-05,
      "loss": 8.1097,
      "step": 5990
    },
    {
      "epoch": 1.8560049493465316,
      "grad_norm": 1.21660315990448,
      "learning_rate": 3.6750795615275816e-05,
      "loss": 8.0982,
      "step": 6000
    },
    {
      "epoch": 1.8590982909287757,
      "grad_norm": 0.48843103647232056,
      "learning_rate": 3.672869519094767e-05,
      "loss": 8.1246,
      "step": 6010
    },
    {
      "epoch": 1.8621916325110202,
      "grad_norm": 0.7663419246673584,
      "learning_rate": 3.6706594766619524e-05,
      "loss": 8.1105,
      "step": 6020
    },
    {
      "epoch": 1.8652849740932642,
      "grad_norm": 0.4496191740036011,
      "learning_rate": 3.6684494342291375e-05,
      "loss": 8.1142,
      "step": 6030
    },
    {
      "epoch": 1.8683783156755085,
      "grad_norm": 0.770578920841217,
      "learning_rate": 3.6662393917963226e-05,
      "loss": 8.1236,
      "step": 6040
    },
    {
      "epoch": 1.8714716572577528,
      "grad_norm": 1.0419385433197021,
      "learning_rate": 3.6640293493635084e-05,
      "loss": 8.1019,
      "step": 6050
    },
    {
      "epoch": 1.8745649988399968,
      "grad_norm": 0.9988660216331482,
      "learning_rate": 3.6618193069306935e-05,
      "loss": 8.119,
      "step": 6060
    },
    {
      "epoch": 1.8776583404222411,
      "grad_norm": 0.7319515347480774,
      "learning_rate": 3.6596092644978786e-05,
      "loss": 8.1195,
      "step": 6070
    },
    {
      "epoch": 1.8807516820044854,
      "grad_norm": 0.826828122138977,
      "learning_rate": 3.657399222065064e-05,
      "loss": 8.1205,
      "step": 6080
    },
    {
      "epoch": 1.8838450235867295,
      "grad_norm": 0.4903888702392578,
      "learning_rate": 3.655189179632249e-05,
      "loss": 8.107,
      "step": 6090
    },
    {
      "epoch": 1.8869383651689737,
      "grad_norm": 0.757673442363739,
      "learning_rate": 3.6529791371994345e-05,
      "loss": 8.0938,
      "step": 6100
    },
    {
      "epoch": 1.890031706751218,
      "grad_norm": 0.9070502519607544,
      "learning_rate": 3.6507690947666196e-05,
      "loss": 8.1064,
      "step": 6110
    },
    {
      "epoch": 1.893125048333462,
      "grad_norm": 0.5928594470024109,
      "learning_rate": 3.648559052333805e-05,
      "loss": 8.1188,
      "step": 6120
    },
    {
      "epoch": 1.8962183899157066,
      "grad_norm": 0.6357729434967041,
      "learning_rate": 3.64634900990099e-05,
      "loss": 8.0994,
      "step": 6130
    },
    {
      "epoch": 1.8993117314979506,
      "grad_norm": 0.7452264428138733,
      "learning_rate": 3.6441389674681756e-05,
      "loss": 8.1156,
      "step": 6140
    },
    {
      "epoch": 1.902405073080195,
      "grad_norm": 0.7934433817863464,
      "learning_rate": 3.6419289250353614e-05,
      "loss": 8.1179,
      "step": 6150
    },
    {
      "epoch": 1.9054984146624392,
      "grad_norm": 1.0771790742874146,
      "learning_rate": 3.6397188826025465e-05,
      "loss": 8.099,
      "step": 6160
    },
    {
      "epoch": 1.9085917562446832,
      "grad_norm": 0.8761550784111023,
      "learning_rate": 3.6375088401697315e-05,
      "loss": 8.1045,
      "step": 6170
    },
    {
      "epoch": 1.9116850978269275,
      "grad_norm": 0.6440709829330444,
      "learning_rate": 3.6352987977369166e-05,
      "loss": 8.1169,
      "step": 6180
    },
    {
      "epoch": 1.9147784394091718,
      "grad_norm": 0.9463909268379211,
      "learning_rate": 3.6330887553041024e-05,
      "loss": 8.1157,
      "step": 6190
    },
    {
      "epoch": 1.9178717809914159,
      "grad_norm": 0.8317248225212097,
      "learning_rate": 3.6308787128712875e-05,
      "loss": 8.1227,
      "step": 6200
    },
    {
      "epoch": 1.9209651225736601,
      "grad_norm": 0.7556623816490173,
      "learning_rate": 3.6286686704384726e-05,
      "loss": 8.1101,
      "step": 6210
    },
    {
      "epoch": 1.9240584641559044,
      "grad_norm": 1.0234439373016357,
      "learning_rate": 3.626458628005658e-05,
      "loss": 8.1006,
      "step": 6220
    },
    {
      "epoch": 1.9271518057381485,
      "grad_norm": 0.7974225878715515,
      "learning_rate": 3.624248585572843e-05,
      "loss": 8.1144,
      "step": 6230
    },
    {
      "epoch": 1.930245147320393,
      "grad_norm": 1.123907446861267,
      "learning_rate": 3.6220385431400286e-05,
      "loss": 8.1249,
      "step": 6240
    },
    {
      "epoch": 1.933338488902637,
      "grad_norm": 1.000357985496521,
      "learning_rate": 3.6198285007072137e-05,
      "loss": 8.0901,
      "step": 6250
    },
    {
      "epoch": 1.9364318304848813,
      "grad_norm": 1.2129359245300293,
      "learning_rate": 3.617618458274399e-05,
      "loss": 8.1148,
      "step": 6260
    },
    {
      "epoch": 1.9395251720671256,
      "grad_norm": 0.7437799572944641,
      "learning_rate": 3.615408415841584e-05,
      "loss": 8.1119,
      "step": 6270
    },
    {
      "epoch": 1.9426185136493697,
      "grad_norm": 1.0087193250656128,
      "learning_rate": 3.6131983734087696e-05,
      "loss": 8.1252,
      "step": 6280
    },
    {
      "epoch": 1.945711855231614,
      "grad_norm": 1.2354352474212646,
      "learning_rate": 3.6109883309759554e-05,
      "loss": 8.1075,
      "step": 6290
    },
    {
      "epoch": 1.9488051968138582,
      "grad_norm": 1.126196265220642,
      "learning_rate": 3.6087782885431405e-05,
      "loss": 8.1118,
      "step": 6300
    },
    {
      "epoch": 1.9518985383961023,
      "grad_norm": 0.8466517329216003,
      "learning_rate": 3.6065682461103256e-05,
      "loss": 8.1117,
      "step": 6310
    },
    {
      "epoch": 1.9549918799783466,
      "grad_norm": 0.7501287460327148,
      "learning_rate": 3.604358203677511e-05,
      "loss": 8.0991,
      "step": 6320
    },
    {
      "epoch": 1.9580852215605908,
      "grad_norm": 1.0328483581542969,
      "learning_rate": 3.6021481612446964e-05,
      "loss": 8.1006,
      "step": 6330
    },
    {
      "epoch": 1.961178563142835,
      "grad_norm": 1.1136817932128906,
      "learning_rate": 3.5999381188118815e-05,
      "loss": 8.1184,
      "step": 6340
    },
    {
      "epoch": 1.9642719047250794,
      "grad_norm": 1.0115580558776855,
      "learning_rate": 3.5977280763790666e-05,
      "loss": 8.1086,
      "step": 6350
    },
    {
      "epoch": 1.9673652463073235,
      "grad_norm": 0.8790600895881653,
      "learning_rate": 3.595518033946252e-05,
      "loss": 8.1123,
      "step": 6360
    },
    {
      "epoch": 1.9704585878895677,
      "grad_norm": 0.6049520969390869,
      "learning_rate": 3.593307991513437e-05,
      "loss": 8.1193,
      "step": 6370
    },
    {
      "epoch": 1.973551929471812,
      "grad_norm": 1.1578625440597534,
      "learning_rate": 3.5910979490806226e-05,
      "loss": 8.1196,
      "step": 6380
    },
    {
      "epoch": 1.976645271054056,
      "grad_norm": 0.6569232940673828,
      "learning_rate": 3.588887906647808e-05,
      "loss": 8.11,
      "step": 6390
    },
    {
      "epoch": 1.9797386126363004,
      "grad_norm": 0.8159201741218567,
      "learning_rate": 3.586677864214993e-05,
      "loss": 8.1017,
      "step": 6400
    },
    {
      "epoch": 1.9828319542185446,
      "grad_norm": 0.8416740298271179,
      "learning_rate": 3.5844678217821786e-05,
      "loss": 8.1226,
      "step": 6410
    },
    {
      "epoch": 1.9859252958007887,
      "grad_norm": 0.7170424461364746,
      "learning_rate": 3.5822577793493636e-05,
      "loss": 8.1122,
      "step": 6420
    },
    {
      "epoch": 1.989018637383033,
      "grad_norm": 0.48569872975349426,
      "learning_rate": 3.5800477369165494e-05,
      "loss": 8.1162,
      "step": 6430
    },
    {
      "epoch": 1.9921119789652773,
      "grad_norm": 0.9472678303718567,
      "learning_rate": 3.5778376944837345e-05,
      "loss": 8.1127,
      "step": 6440
    },
    {
      "epoch": 1.9952053205475213,
      "grad_norm": 0.8315709829330444,
      "learning_rate": 3.5756276520509196e-05,
      "loss": 8.1198,
      "step": 6450
    },
    {
      "epoch": 1.9982986621297658,
      "grad_norm": 0.8758796453475952,
      "learning_rate": 3.573417609618105e-05,
      "loss": 8.1063,
      "step": 6460
    }
  ],
  "logging_steps": 10,
  "max_steps": 22624,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 7,
  "save_steps": 500,
  "total_flos": 2.6832674412355584e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
