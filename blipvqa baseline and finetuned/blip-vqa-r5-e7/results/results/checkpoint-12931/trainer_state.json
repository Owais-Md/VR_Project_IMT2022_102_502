{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 12931,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0030933415822442193,
      "grad_norm": 2.2978525161743164,
      "learning_rate": 4.9982319660537485e-05,
      "loss": 10.3112,
      "step": 10
    },
    {
      "epoch": 0.006186683164488439,
      "grad_norm": 1.697009563446045,
      "learning_rate": 4.9962429278642154e-05,
      "loss": 10.1044,
      "step": 20
    },
    {
      "epoch": 0.009280024746732658,
      "grad_norm": 1.9493228197097778,
      "learning_rate": 4.9940328854314005e-05,
      "loss": 9.9147,
      "step": 30
    },
    {
      "epoch": 0.012373366328976877,
      "grad_norm": 1.2537775039672852,
      "learning_rate": 4.9918228429985856e-05,
      "loss": 9.7383,
      "step": 40
    },
    {
      "epoch": 0.015466707911221097,
      "grad_norm": 0.9784691333770752,
      "learning_rate": 4.9896128005657714e-05,
      "loss": 9.5668,
      "step": 50
    },
    {
      "epoch": 0.018560049493465316,
      "grad_norm": 1.3054183721542358,
      "learning_rate": 4.9874027581329565e-05,
      "loss": 9.4386,
      "step": 60
    },
    {
      "epoch": 0.021653391075709537,
      "grad_norm": 1.3254427909851074,
      "learning_rate": 4.9851927157001416e-05,
      "loss": 9.3105,
      "step": 70
    },
    {
      "epoch": 0.024746732657953754,
      "grad_norm": 0.8787651062011719,
      "learning_rate": 4.982982673267327e-05,
      "loss": 9.1853,
      "step": 80
    },
    {
      "epoch": 0.027840074240197975,
      "grad_norm": 0.7771647572517395,
      "learning_rate": 4.980772630834512e-05,
      "loss": 9.0639,
      "step": 90
    },
    {
      "epoch": 0.030933415822442193,
      "grad_norm": 0.7927806377410889,
      "learning_rate": 4.9785625884016976e-05,
      "loss": 8.9485,
      "step": 100
    },
    {
      "epoch": 0.034026757404686414,
      "grad_norm": 19.9823055267334,
      "learning_rate": 4.976352545968883e-05,
      "loss": 8.8797,
      "step": 110
    },
    {
      "epoch": 0.03712009898693063,
      "grad_norm": 0.7020925879478455,
      "learning_rate": 4.9741425035360684e-05,
      "loss": 8.7873,
      "step": 120
    },
    {
      "epoch": 0.04021344056917485,
      "grad_norm": 0.6891294717788696,
      "learning_rate": 4.9719324611032535e-05,
      "loss": 8.7412,
      "step": 130
    },
    {
      "epoch": 0.043306782151419074,
      "grad_norm": 0.6724461317062378,
      "learning_rate": 4.9697224186704386e-05,
      "loss": 8.6824,
      "step": 140
    },
    {
      "epoch": 0.04640012373366329,
      "grad_norm": 0.6063492298126221,
      "learning_rate": 4.9675123762376244e-05,
      "loss": 8.6219,
      "step": 150
    },
    {
      "epoch": 0.04949346531590751,
      "grad_norm": 0.649882435798645,
      "learning_rate": 4.9653023338048095e-05,
      "loss": 8.576,
      "step": 160
    },
    {
      "epoch": 0.052586806898151726,
      "grad_norm": 0.8346493244171143,
      "learning_rate": 4.9630922913719946e-05,
      "loss": 8.5714,
      "step": 170
    },
    {
      "epoch": 0.05568014848039595,
      "grad_norm": 0.7668482661247253,
      "learning_rate": 4.96088224893918e-05,
      "loss": 8.4979,
      "step": 180
    },
    {
      "epoch": 0.05877349006264017,
      "grad_norm": 0.81424480676651,
      "learning_rate": 4.9586722065063654e-05,
      "loss": 8.4936,
      "step": 190
    },
    {
      "epoch": 0.061866831644884386,
      "grad_norm": 1.412393569946289,
      "learning_rate": 4.9564621640735505e-05,
      "loss": 8.4625,
      "step": 200
    },
    {
      "epoch": 0.0649601732271286,
      "grad_norm": 0.9642157554626465,
      "learning_rate": 4.9542521216407356e-05,
      "loss": 8.439,
      "step": 210
    },
    {
      "epoch": 0.06805351480937283,
      "grad_norm": 0.5858935713768005,
      "learning_rate": 4.952042079207921e-05,
      "loss": 8.4209,
      "step": 220
    },
    {
      "epoch": 0.07114685639161704,
      "grad_norm": 1.2992138862609863,
      "learning_rate": 4.949832036775106e-05,
      "loss": 8.3929,
      "step": 230
    },
    {
      "epoch": 0.07424019797386126,
      "grad_norm": 1.6960211992263794,
      "learning_rate": 4.9476219943422916e-05,
      "loss": 8.4018,
      "step": 240
    },
    {
      "epoch": 0.07733353955610549,
      "grad_norm": 1.0526018142700195,
      "learning_rate": 4.9454119519094774e-05,
      "loss": 8.374,
      "step": 250
    },
    {
      "epoch": 0.0804268811383497,
      "grad_norm": 0.9247509241104126,
      "learning_rate": 4.9432019094766625e-05,
      "loss": 8.3619,
      "step": 260
    },
    {
      "epoch": 0.08352022272059392,
      "grad_norm": 0.650263249874115,
      "learning_rate": 4.9409918670438475e-05,
      "loss": 8.3572,
      "step": 270
    },
    {
      "epoch": 0.08661356430283815,
      "grad_norm": 0.8367137908935547,
      "learning_rate": 4.9387818246110326e-05,
      "loss": 8.3374,
      "step": 280
    },
    {
      "epoch": 0.08970690588508236,
      "grad_norm": 0.5976635813713074,
      "learning_rate": 4.9365717821782184e-05,
      "loss": 8.3021,
      "step": 290
    },
    {
      "epoch": 0.09280024746732658,
      "grad_norm": 1.133556604385376,
      "learning_rate": 4.9343617397454035e-05,
      "loss": 8.3094,
      "step": 300
    },
    {
      "epoch": 0.0958935890495708,
      "grad_norm": 2.212977409362793,
      "learning_rate": 4.9321516973125886e-05,
      "loss": 8.2983,
      "step": 310
    },
    {
      "epoch": 0.09898693063181502,
      "grad_norm": 0.8591291308403015,
      "learning_rate": 4.929941654879774e-05,
      "loss": 8.254,
      "step": 320
    },
    {
      "epoch": 0.10208027221405924,
      "grad_norm": 0.7119815349578857,
      "learning_rate": 4.9277316124469595e-05,
      "loss": 8.2402,
      "step": 330
    },
    {
      "epoch": 0.10517361379630345,
      "grad_norm": 0.860245406627655,
      "learning_rate": 4.9255215700141446e-05,
      "loss": 8.245,
      "step": 340
    },
    {
      "epoch": 0.10826695537854768,
      "grad_norm": 1.2802064418792725,
      "learning_rate": 4.9233115275813297e-05,
      "loss": 8.2269,
      "step": 350
    },
    {
      "epoch": 0.1113602969607919,
      "grad_norm": 0.6481074690818787,
      "learning_rate": 4.921101485148515e-05,
      "loss": 8.2231,
      "step": 360
    },
    {
      "epoch": 0.11445363854303611,
      "grad_norm": 1.1971083879470825,
      "learning_rate": 4.9188914427157e-05,
      "loss": 8.2189,
      "step": 370
    },
    {
      "epoch": 0.11754698012528034,
      "grad_norm": 1.7580838203430176,
      "learning_rate": 4.9166814002828856e-05,
      "loss": 8.2223,
      "step": 380
    },
    {
      "epoch": 0.12064032170752455,
      "grad_norm": 1.2845189571380615,
      "learning_rate": 4.9144713578500714e-05,
      "loss": 8.2078,
      "step": 390
    },
    {
      "epoch": 0.12373366328976877,
      "grad_norm": 2.1474101543426514,
      "learning_rate": 4.9122613154172565e-05,
      "loss": 8.2149,
      "step": 400
    },
    {
      "epoch": 0.12682700487201298,
      "grad_norm": 0.6936149001121521,
      "learning_rate": 4.9100512729844416e-05,
      "loss": 8.1938,
      "step": 410
    },
    {
      "epoch": 0.1299203464542572,
      "grad_norm": 2.1909961700439453,
      "learning_rate": 4.907841230551627e-05,
      "loss": 8.2125,
      "step": 420
    },
    {
      "epoch": 0.13301368803650143,
      "grad_norm": 0.7333984971046448,
      "learning_rate": 4.9056311881188124e-05,
      "loss": 8.1978,
      "step": 430
    },
    {
      "epoch": 0.13610702961874566,
      "grad_norm": 1.0266189575195312,
      "learning_rate": 4.9034211456859975e-05,
      "loss": 8.2031,
      "step": 440
    },
    {
      "epoch": 0.13920037120098988,
      "grad_norm": 0.7094117403030396,
      "learning_rate": 4.9012111032531826e-05,
      "loss": 8.1962,
      "step": 450
    },
    {
      "epoch": 0.14229371278323408,
      "grad_norm": 1.4269354343414307,
      "learning_rate": 4.899001060820368e-05,
      "loss": 8.2066,
      "step": 460
    },
    {
      "epoch": 0.1453870543654783,
      "grad_norm": 0.6357672810554504,
      "learning_rate": 4.8967910183875535e-05,
      "loss": 8.1813,
      "step": 470
    },
    {
      "epoch": 0.14848039594772253,
      "grad_norm": 1.524726390838623,
      "learning_rate": 4.8945809759547386e-05,
      "loss": 8.1788,
      "step": 480
    },
    {
      "epoch": 0.15157373752996675,
      "grad_norm": 1.4942302703857422,
      "learning_rate": 4.892370933521924e-05,
      "loss": 8.1809,
      "step": 490
    },
    {
      "epoch": 0.15466707911221098,
      "grad_norm": 0.7273604273796082,
      "learning_rate": 4.890160891089109e-05,
      "loss": 8.1683,
      "step": 500
    },
    {
      "epoch": 0.15776042069445517,
      "grad_norm": 1.1210496425628662,
      "learning_rate": 4.8879508486562946e-05,
      "loss": 8.1691,
      "step": 510
    },
    {
      "epoch": 0.1608537622766994,
      "grad_norm": 1.029540777206421,
      "learning_rate": 4.8857408062234796e-05,
      "loss": 8.1916,
      "step": 520
    },
    {
      "epoch": 0.16394710385894362,
      "grad_norm": 1.556471347808838,
      "learning_rate": 4.8835307637906654e-05,
      "loss": 8.1873,
      "step": 530
    },
    {
      "epoch": 0.16704044544118785,
      "grad_norm": 0.7839841246604919,
      "learning_rate": 4.8813207213578505e-05,
      "loss": 8.1909,
      "step": 540
    },
    {
      "epoch": 0.17013378702343207,
      "grad_norm": 2.0269980430603027,
      "learning_rate": 4.8791106789250356e-05,
      "loss": 8.1842,
      "step": 550
    },
    {
      "epoch": 0.1732271286056763,
      "grad_norm": 1.3354445695877075,
      "learning_rate": 4.876900636492221e-05,
      "loss": 8.1838,
      "step": 560
    },
    {
      "epoch": 0.1763204701879205,
      "grad_norm": 0.6603084206581116,
      "learning_rate": 4.8746905940594065e-05,
      "loss": 8.1838,
      "step": 570
    },
    {
      "epoch": 0.17941381177016472,
      "grad_norm": 1.0205258131027222,
      "learning_rate": 4.8724805516265916e-05,
      "loss": 8.1684,
      "step": 580
    },
    {
      "epoch": 0.18250715335240894,
      "grad_norm": 0.6809381246566772,
      "learning_rate": 4.870270509193777e-05,
      "loss": 8.1736,
      "step": 590
    },
    {
      "epoch": 0.18560049493465317,
      "grad_norm": 0.7379045486450195,
      "learning_rate": 4.868060466760962e-05,
      "loss": 8.1663,
      "step": 600
    },
    {
      "epoch": 0.1886938365168974,
      "grad_norm": 1.0289238691329956,
      "learning_rate": 4.8658504243281475e-05,
      "loss": 8.1652,
      "step": 610
    },
    {
      "epoch": 0.1917871780991416,
      "grad_norm": 1.4722477197647095,
      "learning_rate": 4.8636403818953326e-05,
      "loss": 8.1719,
      "step": 620
    },
    {
      "epoch": 0.1948805196813858,
      "grad_norm": 0.985119640827179,
      "learning_rate": 4.861430339462518e-05,
      "loss": 8.1764,
      "step": 630
    },
    {
      "epoch": 0.19797386126363004,
      "grad_norm": 2.193415880203247,
      "learning_rate": 4.859220297029703e-05,
      "loss": 8.1621,
      "step": 640
    },
    {
      "epoch": 0.20106720284587426,
      "grad_norm": 0.7924662828445435,
      "learning_rate": 4.8570102545968886e-05,
      "loss": 8.1579,
      "step": 650
    },
    {
      "epoch": 0.20416054442811848,
      "grad_norm": 0.7754663228988647,
      "learning_rate": 4.854800212164074e-05,
      "loss": 8.1731,
      "step": 660
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 1.1253646612167358,
      "learning_rate": 4.8525901697312595e-05,
      "loss": 8.1679,
      "step": 670
    },
    {
      "epoch": 0.2103472275926069,
      "grad_norm": 1.7341458797454834,
      "learning_rate": 4.8503801272984445e-05,
      "loss": 8.1671,
      "step": 680
    },
    {
      "epoch": 0.21344056917485113,
      "grad_norm": 1.9493780136108398,
      "learning_rate": 4.8481700848656296e-05,
      "loss": 8.1735,
      "step": 690
    },
    {
      "epoch": 0.21653391075709535,
      "grad_norm": 3.367149829864502,
      "learning_rate": 4.845960042432815e-05,
      "loss": 8.1516,
      "step": 700
    },
    {
      "epoch": 0.21962725233933958,
      "grad_norm": 1.8500995635986328,
      "learning_rate": 4.8437500000000005e-05,
      "loss": 8.1614,
      "step": 710
    },
    {
      "epoch": 0.2227205939215838,
      "grad_norm": 0.8552808165550232,
      "learning_rate": 4.8415399575671856e-05,
      "loss": 8.1589,
      "step": 720
    },
    {
      "epoch": 0.225813935503828,
      "grad_norm": 1.2967503070831299,
      "learning_rate": 4.839329915134371e-05,
      "loss": 8.1755,
      "step": 730
    },
    {
      "epoch": 0.22890727708607223,
      "grad_norm": 1.5053879022598267,
      "learning_rate": 4.837119872701556e-05,
      "loss": 8.1735,
      "step": 740
    },
    {
      "epoch": 0.23200061866831645,
      "grad_norm": 0.7907457947731018,
      "learning_rate": 4.8349098302687416e-05,
      "loss": 8.1565,
      "step": 750
    },
    {
      "epoch": 0.23509396025056067,
      "grad_norm": 0.8512012958526611,
      "learning_rate": 4.8326997878359267e-05,
      "loss": 8.1454,
      "step": 760
    },
    {
      "epoch": 0.2381873018328049,
      "grad_norm": 0.8684555292129517,
      "learning_rate": 4.830489745403112e-05,
      "loss": 8.1468,
      "step": 770
    },
    {
      "epoch": 0.2412806434150491,
      "grad_norm": 2.083754301071167,
      "learning_rate": 4.8282797029702975e-05,
      "loss": 8.1667,
      "step": 780
    },
    {
      "epoch": 0.24437398499729332,
      "grad_norm": 0.9785476922988892,
      "learning_rate": 4.8260696605374826e-05,
      "loss": 8.1745,
      "step": 790
    },
    {
      "epoch": 0.24746732657953754,
      "grad_norm": 0.8621866703033447,
      "learning_rate": 4.823859618104668e-05,
      "loss": 8.1734,
      "step": 800
    },
    {
      "epoch": 0.25056066816178174,
      "grad_norm": 1.065608024597168,
      "learning_rate": 4.8216495756718535e-05,
      "loss": 8.1508,
      "step": 810
    },
    {
      "epoch": 0.25365400974402597,
      "grad_norm": 0.7921701669692993,
      "learning_rate": 4.8194395332390386e-05,
      "loss": 8.1613,
      "step": 820
    },
    {
      "epoch": 0.2567473513262702,
      "grad_norm": 0.9583138823509216,
      "learning_rate": 4.817229490806224e-05,
      "loss": 8.1486,
      "step": 830
    },
    {
      "epoch": 0.2598406929085144,
      "grad_norm": 1.781218409538269,
      "learning_rate": 4.815019448373409e-05,
      "loss": 8.1601,
      "step": 840
    },
    {
      "epoch": 0.26293403449075864,
      "grad_norm": 1.411521315574646,
      "learning_rate": 4.8128094059405945e-05,
      "loss": 8.166,
      "step": 850
    },
    {
      "epoch": 0.26602737607300286,
      "grad_norm": 0.6862319707870483,
      "learning_rate": 4.8105993635077796e-05,
      "loss": 8.1452,
      "step": 860
    },
    {
      "epoch": 0.2691207176552471,
      "grad_norm": 0.910425066947937,
      "learning_rate": 4.808389321074965e-05,
      "loss": 8.162,
      "step": 870
    },
    {
      "epoch": 0.2722140592374913,
      "grad_norm": 1.6667001247406006,
      "learning_rate": 4.80617927864215e-05,
      "loss": 8.1382,
      "step": 880
    },
    {
      "epoch": 0.27530740081973554,
      "grad_norm": 1.0555663108825684,
      "learning_rate": 4.803969236209335e-05,
      "loss": 8.1397,
      "step": 890
    },
    {
      "epoch": 0.27840074240197976,
      "grad_norm": 1.4642082452774048,
      "learning_rate": 4.801759193776521e-05,
      "loss": 8.159,
      "step": 900
    },
    {
      "epoch": 0.28149408398422393,
      "grad_norm": 0.7943904399871826,
      "learning_rate": 4.799549151343706e-05,
      "loss": 8.1628,
      "step": 910
    },
    {
      "epoch": 0.28458742556646816,
      "grad_norm": 1.0083155632019043,
      "learning_rate": 4.7973391089108916e-05,
      "loss": 8.1653,
      "step": 920
    },
    {
      "epoch": 0.2876807671487124,
      "grad_norm": 0.8918737173080444,
      "learning_rate": 4.7951290664780766e-05,
      "loss": 8.1491,
      "step": 930
    },
    {
      "epoch": 0.2907741087309566,
      "grad_norm": 1.3468626737594604,
      "learning_rate": 4.792919024045262e-05,
      "loss": 8.1569,
      "step": 940
    },
    {
      "epoch": 0.29386745031320083,
      "grad_norm": 1.7926127910614014,
      "learning_rate": 4.7907089816124475e-05,
      "loss": 8.145,
      "step": 950
    },
    {
      "epoch": 0.29696079189544505,
      "grad_norm": 2.6235406398773193,
      "learning_rate": 4.7884989391796326e-05,
      "loss": 8.1438,
      "step": 960
    },
    {
      "epoch": 0.3000541334776893,
      "grad_norm": 1.613265037536621,
      "learning_rate": 4.786288896746818e-05,
      "loss": 8.1618,
      "step": 970
    },
    {
      "epoch": 0.3031474750599335,
      "grad_norm": 1.0502805709838867,
      "learning_rate": 4.784078854314003e-05,
      "loss": 8.1507,
      "step": 980
    },
    {
      "epoch": 0.3062408166421777,
      "grad_norm": 0.9201352596282959,
      "learning_rate": 4.7818688118811886e-05,
      "loss": 8.1586,
      "step": 990
    },
    {
      "epoch": 0.30933415822442195,
      "grad_norm": 0.9506177306175232,
      "learning_rate": 4.779658769448374e-05,
      "loss": 8.1499,
      "step": 1000
    },
    {
      "epoch": 0.3124274998066662,
      "grad_norm": 0.8562129139900208,
      "learning_rate": 4.777448727015559e-05,
      "loss": 8.1545,
      "step": 1010
    },
    {
      "epoch": 0.31552084138891034,
      "grad_norm": 1.137046456336975,
      "learning_rate": 4.775238684582744e-05,
      "loss": 8.1428,
      "step": 1020
    },
    {
      "epoch": 0.31861418297115457,
      "grad_norm": 4.624128341674805,
      "learning_rate": 4.773028642149929e-05,
      "loss": 8.159,
      "step": 1030
    },
    {
      "epoch": 0.3217075245533988,
      "grad_norm": 0.9169142246246338,
      "learning_rate": 4.770818599717115e-05,
      "loss": 8.1528,
      "step": 1040
    },
    {
      "epoch": 0.324800866135643,
      "grad_norm": 4.3146209716796875,
      "learning_rate": 4.7686085572843005e-05,
      "loss": 8.1421,
      "step": 1050
    },
    {
      "epoch": 0.32789420771788724,
      "grad_norm": 1.1886427402496338,
      "learning_rate": 4.7663985148514856e-05,
      "loss": 8.1575,
      "step": 1060
    },
    {
      "epoch": 0.33098754930013147,
      "grad_norm": 1.0226562023162842,
      "learning_rate": 4.764188472418671e-05,
      "loss": 8.1538,
      "step": 1070
    },
    {
      "epoch": 0.3340808908823757,
      "grad_norm": 0.9351243376731873,
      "learning_rate": 4.761978429985856e-05,
      "loss": 8.1418,
      "step": 1080
    },
    {
      "epoch": 0.3371742324646199,
      "grad_norm": 0.6706920862197876,
      "learning_rate": 4.7597683875530415e-05,
      "loss": 8.1471,
      "step": 1090
    },
    {
      "epoch": 0.34026757404686414,
      "grad_norm": 1.1914315223693848,
      "learning_rate": 4.7575583451202266e-05,
      "loss": 8.1389,
      "step": 1100
    },
    {
      "epoch": 0.34336091562910837,
      "grad_norm": 0.7992492914199829,
      "learning_rate": 4.755348302687412e-05,
      "loss": 8.1572,
      "step": 1110
    },
    {
      "epoch": 0.3464542572113526,
      "grad_norm": 1.4357860088348389,
      "learning_rate": 4.753138260254597e-05,
      "loss": 8.147,
      "step": 1120
    },
    {
      "epoch": 0.34954759879359676,
      "grad_norm": 0.9896734952926636,
      "learning_rate": 4.7509282178217826e-05,
      "loss": 8.1361,
      "step": 1130
    },
    {
      "epoch": 0.352640940375841,
      "grad_norm": 0.7115346193313599,
      "learning_rate": 4.748718175388968e-05,
      "loss": 8.1448,
      "step": 1140
    },
    {
      "epoch": 0.3557342819580852,
      "grad_norm": 1.3110510110855103,
      "learning_rate": 4.746508132956153e-05,
      "loss": 8.1237,
      "step": 1150
    },
    {
      "epoch": 0.35882762354032943,
      "grad_norm": 1.0065312385559082,
      "learning_rate": 4.744298090523338e-05,
      "loss": 8.135,
      "step": 1160
    },
    {
      "epoch": 0.36192096512257366,
      "grad_norm": 1.3407121896743774,
      "learning_rate": 4.742088048090523e-05,
      "loss": 8.1458,
      "step": 1170
    },
    {
      "epoch": 0.3650143067048179,
      "grad_norm": 1.2935097217559814,
      "learning_rate": 4.7398780056577094e-05,
      "loss": 8.1379,
      "step": 1180
    },
    {
      "epoch": 0.3681076482870621,
      "grad_norm": 0.9102542996406555,
      "learning_rate": 4.7376679632248945e-05,
      "loss": 8.1432,
      "step": 1190
    },
    {
      "epoch": 0.37120098986930633,
      "grad_norm": 2.4544589519500732,
      "learning_rate": 4.7354579207920796e-05,
      "loss": 8.1546,
      "step": 1200
    },
    {
      "epoch": 0.37429433145155055,
      "grad_norm": 0.9030328989028931,
      "learning_rate": 4.733247878359265e-05,
      "loss": 8.1316,
      "step": 1210
    },
    {
      "epoch": 0.3773876730337948,
      "grad_norm": 0.8985339403152466,
      "learning_rate": 4.73103783592645e-05,
      "loss": 8.1446,
      "step": 1220
    },
    {
      "epoch": 0.38048101461603895,
      "grad_norm": 1.6634750366210938,
      "learning_rate": 4.7288277934936356e-05,
      "loss": 8.1418,
      "step": 1230
    },
    {
      "epoch": 0.3835743561982832,
      "grad_norm": 3.069864511489868,
      "learning_rate": 4.726617751060821e-05,
      "loss": 8.1398,
      "step": 1240
    },
    {
      "epoch": 0.3866676977805274,
      "grad_norm": 0.7385414838790894,
      "learning_rate": 4.724407708628006e-05,
      "loss": 8.1339,
      "step": 1250
    },
    {
      "epoch": 0.3897610393627716,
      "grad_norm": 0.7566729187965393,
      "learning_rate": 4.722197666195191e-05,
      "loss": 8.1308,
      "step": 1260
    },
    {
      "epoch": 0.39285438094501585,
      "grad_norm": 2.357363224029541,
      "learning_rate": 4.7199876237623766e-05,
      "loss": 8.1348,
      "step": 1270
    },
    {
      "epoch": 0.39594772252726007,
      "grad_norm": 0.7281991839408875,
      "learning_rate": 4.717777581329562e-05,
      "loss": 8.1437,
      "step": 1280
    },
    {
      "epoch": 0.3990410641095043,
      "grad_norm": 0.8826242685317993,
      "learning_rate": 4.715567538896747e-05,
      "loss": 8.1458,
      "step": 1290
    },
    {
      "epoch": 0.4021344056917485,
      "grad_norm": 0.9293636083602905,
      "learning_rate": 4.713357496463932e-05,
      "loss": 8.1498,
      "step": 1300
    },
    {
      "epoch": 0.40522774727399274,
      "grad_norm": 2.375598907470703,
      "learning_rate": 4.711147454031117e-05,
      "loss": 8.1371,
      "step": 1310
    },
    {
      "epoch": 0.40832108885623697,
      "grad_norm": 1.2174696922302246,
      "learning_rate": 4.7089374115983035e-05,
      "loss": 8.1515,
      "step": 1320
    },
    {
      "epoch": 0.4114144304384812,
      "grad_norm": 1.1991961002349854,
      "learning_rate": 4.7067273691654886e-05,
      "loss": 8.1401,
      "step": 1330
    },
    {
      "epoch": 0.41450777202072536,
      "grad_norm": 0.7864306569099426,
      "learning_rate": 4.7045173267326737e-05,
      "loss": 8.1199,
      "step": 1340
    },
    {
      "epoch": 0.4176011136029696,
      "grad_norm": 0.7330999374389648,
      "learning_rate": 4.702307284299859e-05,
      "loss": 8.1305,
      "step": 1350
    },
    {
      "epoch": 0.4206944551852138,
      "grad_norm": 0.8905574083328247,
      "learning_rate": 4.700097241867044e-05,
      "loss": 8.1283,
      "step": 1360
    },
    {
      "epoch": 0.42378779676745804,
      "grad_norm": 1.4793035984039307,
      "learning_rate": 4.6978871994342296e-05,
      "loss": 8.1219,
      "step": 1370
    },
    {
      "epoch": 0.42688113834970226,
      "grad_norm": 0.665165901184082,
      "learning_rate": 4.695677157001415e-05,
      "loss": 8.131,
      "step": 1380
    },
    {
      "epoch": 0.4299744799319465,
      "grad_norm": 1.6748032569885254,
      "learning_rate": 4.6934671145686e-05,
      "loss": 8.1446,
      "step": 1390
    },
    {
      "epoch": 0.4330678215141907,
      "grad_norm": 0.8435137271881104,
      "learning_rate": 4.691257072135785e-05,
      "loss": 8.1182,
      "step": 1400
    },
    {
      "epoch": 0.43616116309643493,
      "grad_norm": 0.8907887935638428,
      "learning_rate": 4.689047029702971e-05,
      "loss": 8.1445,
      "step": 1410
    },
    {
      "epoch": 0.43925450467867916,
      "grad_norm": 0.9058837294578552,
      "learning_rate": 4.686836987270156e-05,
      "loss": 8.1434,
      "step": 1420
    },
    {
      "epoch": 0.4423478462609234,
      "grad_norm": 2.731689214706421,
      "learning_rate": 4.684626944837341e-05,
      "loss": 8.143,
      "step": 1430
    },
    {
      "epoch": 0.4454411878431676,
      "grad_norm": 1.3596062660217285,
      "learning_rate": 4.682416902404526e-05,
      "loss": 8.1403,
      "step": 1440
    },
    {
      "epoch": 0.4485345294254118,
      "grad_norm": 1.5356475114822388,
      "learning_rate": 4.680206859971712e-05,
      "loss": 8.14,
      "step": 1450
    },
    {
      "epoch": 0.451627871007656,
      "grad_norm": 2.6440606117248535,
      "learning_rate": 4.6779968175388975e-05,
      "loss": 8.1299,
      "step": 1460
    },
    {
      "epoch": 0.4547212125899002,
      "grad_norm": 1.2428972721099854,
      "learning_rate": 4.6757867751060826e-05,
      "loss": 8.134,
      "step": 1470
    },
    {
      "epoch": 0.45781455417214445,
      "grad_norm": 2.447598695755005,
      "learning_rate": 4.673576732673268e-05,
      "loss": 8.1356,
      "step": 1480
    },
    {
      "epoch": 0.4609078957543887,
      "grad_norm": 2.3701610565185547,
      "learning_rate": 4.671366690240453e-05,
      "loss": 8.1562,
      "step": 1490
    },
    {
      "epoch": 0.4640012373366329,
      "grad_norm": 0.5955577492713928,
      "learning_rate": 4.669156647807638e-05,
      "loss": 8.1211,
      "step": 1500
    },
    {
      "epoch": 0.4670945789188771,
      "grad_norm": 1.9649672508239746,
      "learning_rate": 4.6669466053748236e-05,
      "loss": 8.1387,
      "step": 1510
    },
    {
      "epoch": 0.47018792050112135,
      "grad_norm": 1.5700713396072388,
      "learning_rate": 4.664736562942009e-05,
      "loss": 8.144,
      "step": 1520
    },
    {
      "epoch": 0.4732812620833656,
      "grad_norm": 1.2700107097625732,
      "learning_rate": 4.662526520509194e-05,
      "loss": 8.1341,
      "step": 1530
    },
    {
      "epoch": 0.4763746036656098,
      "grad_norm": 1.2104922533035278,
      "learning_rate": 4.660316478076379e-05,
      "loss": 8.1475,
      "step": 1540
    },
    {
      "epoch": 0.479467945247854,
      "grad_norm": 1.0661829710006714,
      "learning_rate": 4.658106435643565e-05,
      "loss": 8.1214,
      "step": 1550
    },
    {
      "epoch": 0.4825612868300982,
      "grad_norm": 0.8409711122512817,
      "learning_rate": 4.65589639321075e-05,
      "loss": 8.1108,
      "step": 1560
    },
    {
      "epoch": 0.4856546284123424,
      "grad_norm": 3.221015214920044,
      "learning_rate": 4.653686350777935e-05,
      "loss": 8.1492,
      "step": 1570
    },
    {
      "epoch": 0.48874796999458664,
      "grad_norm": 1.6262084245681763,
      "learning_rate": 4.6514763083451207e-05,
      "loss": 8.1294,
      "step": 1580
    },
    {
      "epoch": 0.49184131157683086,
      "grad_norm": 2.6076228618621826,
      "learning_rate": 4.649266265912306e-05,
      "loss": 8.1244,
      "step": 1590
    },
    {
      "epoch": 0.4949346531590751,
      "grad_norm": 0.7324378490447998,
      "learning_rate": 4.6470562234794915e-05,
      "loss": 8.1377,
      "step": 1600
    },
    {
      "epoch": 0.4980279947413193,
      "grad_norm": 0.8694437742233276,
      "learning_rate": 4.6448461810466766e-05,
      "loss": 8.1318,
      "step": 1610
    },
    {
      "epoch": 0.5011213363235635,
      "grad_norm": 1.190567135810852,
      "learning_rate": 4.642636138613862e-05,
      "loss": 8.1313,
      "step": 1620
    },
    {
      "epoch": 0.5042146779058078,
      "grad_norm": 1.044485092163086,
      "learning_rate": 4.640426096181047e-05,
      "loss": 8.1099,
      "step": 1630
    },
    {
      "epoch": 0.5073080194880519,
      "grad_norm": 1.5426740646362305,
      "learning_rate": 4.638216053748232e-05,
      "loss": 8.1393,
      "step": 1640
    },
    {
      "epoch": 0.5104013610702962,
      "grad_norm": 1.1623259782791138,
      "learning_rate": 4.636006011315418e-05,
      "loss": 8.1356,
      "step": 1650
    },
    {
      "epoch": 0.5134947026525404,
      "grad_norm": 1.1647775173187256,
      "learning_rate": 4.633795968882603e-05,
      "loss": 8.1274,
      "step": 1660
    },
    {
      "epoch": 0.5165880442347847,
      "grad_norm": 1.2641135454177856,
      "learning_rate": 4.631585926449788e-05,
      "loss": 8.1376,
      "step": 1670
    },
    {
      "epoch": 0.5196813858170288,
      "grad_norm": 1.3211780786514282,
      "learning_rate": 4.629375884016973e-05,
      "loss": 8.116,
      "step": 1680
    },
    {
      "epoch": 0.5227747273992731,
      "grad_norm": 1.3174850940704346,
      "learning_rate": 4.627165841584159e-05,
      "loss": 8.1222,
      "step": 1690
    },
    {
      "epoch": 0.5258680689815173,
      "grad_norm": 1.4946551322937012,
      "learning_rate": 4.624955799151344e-05,
      "loss": 8.1274,
      "step": 1700
    },
    {
      "epoch": 0.5289614105637616,
      "grad_norm": 1.0001411437988281,
      "learning_rate": 4.622745756718529e-05,
      "loss": 8.1387,
      "step": 1710
    },
    {
      "epoch": 0.5320547521460057,
      "grad_norm": 1.0746123790740967,
      "learning_rate": 4.620535714285715e-05,
      "loss": 8.1302,
      "step": 1720
    },
    {
      "epoch": 0.5351480937282499,
      "grad_norm": 1.3485393524169922,
      "learning_rate": 4.6183256718529e-05,
      "loss": 8.1223,
      "step": 1730
    },
    {
      "epoch": 0.5382414353104942,
      "grad_norm": 1.9648051261901855,
      "learning_rate": 4.6161156294200856e-05,
      "loss": 8.1313,
      "step": 1740
    },
    {
      "epoch": 0.5413347768927383,
      "grad_norm": 1.2787649631500244,
      "learning_rate": 4.6139055869872707e-05,
      "loss": 8.1354,
      "step": 1750
    },
    {
      "epoch": 0.5444281184749826,
      "grad_norm": 0.7873222231864929,
      "learning_rate": 4.611695544554456e-05,
      "loss": 8.1405,
      "step": 1760
    },
    {
      "epoch": 0.5475214600572268,
      "grad_norm": 2.2081856727600098,
      "learning_rate": 4.609485502121641e-05,
      "loss": 8.1167,
      "step": 1770
    },
    {
      "epoch": 0.5506148016394711,
      "grad_norm": 0.6028438806533813,
      "learning_rate": 4.607275459688826e-05,
      "loss": 8.1369,
      "step": 1780
    },
    {
      "epoch": 0.5537081432217152,
      "grad_norm": 0.9986103773117065,
      "learning_rate": 4.605065417256012e-05,
      "loss": 8.1335,
      "step": 1790
    },
    {
      "epoch": 0.5568014848039595,
      "grad_norm": 0.6742298603057861,
      "learning_rate": 4.602855374823197e-05,
      "loss": 8.1261,
      "step": 1800
    },
    {
      "epoch": 0.5598948263862037,
      "grad_norm": 1.2286006212234497,
      "learning_rate": 4.600645332390382e-05,
      "loss": 8.1172,
      "step": 1810
    },
    {
      "epoch": 0.5629881679684479,
      "grad_norm": 0.8139893412590027,
      "learning_rate": 4.598435289957567e-05,
      "loss": 8.1406,
      "step": 1820
    },
    {
      "epoch": 0.5660815095506921,
      "grad_norm": 1.3433475494384766,
      "learning_rate": 4.596225247524753e-05,
      "loss": 8.1309,
      "step": 1830
    },
    {
      "epoch": 0.5691748511329363,
      "grad_norm": 1.5777674913406372,
      "learning_rate": 4.594015205091938e-05,
      "loss": 8.1259,
      "step": 1840
    },
    {
      "epoch": 0.5722681927151806,
      "grad_norm": 0.9981272220611572,
      "learning_rate": 4.5918051626591236e-05,
      "loss": 8.1216,
      "step": 1850
    },
    {
      "epoch": 0.5753615342974248,
      "grad_norm": 0.880254864692688,
      "learning_rate": 4.589595120226309e-05,
      "loss": 8.1399,
      "step": 1860
    },
    {
      "epoch": 0.578454875879669,
      "grad_norm": 1.2760629653930664,
      "learning_rate": 4.587385077793494e-05,
      "loss": 8.15,
      "step": 1870
    },
    {
      "epoch": 0.5815482174619132,
      "grad_norm": 1.6959831714630127,
      "learning_rate": 4.5851750353606796e-05,
      "loss": 8.1277,
      "step": 1880
    },
    {
      "epoch": 0.5846415590441575,
      "grad_norm": 0.7466061115264893,
      "learning_rate": 4.582964992927865e-05,
      "loss": 8.1268,
      "step": 1890
    },
    {
      "epoch": 0.5877349006264017,
      "grad_norm": 1.1226886510849,
      "learning_rate": 4.58075495049505e-05,
      "loss": 8.1372,
      "step": 1900
    },
    {
      "epoch": 0.5908282422086459,
      "grad_norm": 1.5224499702453613,
      "learning_rate": 4.578544908062235e-05,
      "loss": 8.13,
      "step": 1910
    },
    {
      "epoch": 0.5939215837908901,
      "grad_norm": 1.4019792079925537,
      "learning_rate": 4.57633486562942e-05,
      "loss": 8.1344,
      "step": 1920
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.8918095827102661,
      "learning_rate": 4.574124823196606e-05,
      "loss": 8.1311,
      "step": 1930
    },
    {
      "epoch": 0.6001082669553786,
      "grad_norm": 0.9084849953651428,
      "learning_rate": 4.571914780763791e-05,
      "loss": 8.1366,
      "step": 1940
    },
    {
      "epoch": 0.6032016085376227,
      "grad_norm": 0.7314757704734802,
      "learning_rate": 4.569704738330976e-05,
      "loss": 8.1226,
      "step": 1950
    },
    {
      "epoch": 0.606294950119867,
      "grad_norm": 0.8405730128288269,
      "learning_rate": 4.567494695898161e-05,
      "loss": 8.1186,
      "step": 1960
    },
    {
      "epoch": 0.6093882917021112,
      "grad_norm": 0.7289599776268005,
      "learning_rate": 4.565284653465347e-05,
      "loss": 8.1128,
      "step": 1970
    },
    {
      "epoch": 0.6124816332843555,
      "grad_norm": 2.0056517124176025,
      "learning_rate": 4.5630746110325326e-05,
      "loss": 8.1228,
      "step": 1980
    },
    {
      "epoch": 0.6155749748665996,
      "grad_norm": 1.4175626039505005,
      "learning_rate": 4.5608645685997177e-05,
      "loss": 8.1313,
      "step": 1990
    },
    {
      "epoch": 0.6186683164488439,
      "grad_norm": 1.7403383255004883,
      "learning_rate": 4.558654526166903e-05,
      "loss": 8.1341,
      "step": 2000
    },
    {
      "epoch": 0.6217616580310881,
      "grad_norm": 0.9728478789329529,
      "learning_rate": 4.556444483734088e-05,
      "loss": 8.1278,
      "step": 2010
    },
    {
      "epoch": 0.6248549996133324,
      "grad_norm": 0.9657310843467712,
      "learning_rate": 4.5542344413012736e-05,
      "loss": 8.1455,
      "step": 2020
    },
    {
      "epoch": 0.6279483411955765,
      "grad_norm": 0.7016928791999817,
      "learning_rate": 4.552024398868459e-05,
      "loss": 8.1257,
      "step": 2030
    },
    {
      "epoch": 0.6310416827778207,
      "grad_norm": 1.750995397567749,
      "learning_rate": 4.549814356435644e-05,
      "loss": 8.1391,
      "step": 2040
    },
    {
      "epoch": 0.634135024360065,
      "grad_norm": 3.0268523693084717,
      "learning_rate": 4.547604314002829e-05,
      "loss": 8.1331,
      "step": 2050
    },
    {
      "epoch": 0.6372283659423091,
      "grad_norm": 2.257249116897583,
      "learning_rate": 4.545394271570014e-05,
      "loss": 8.1268,
      "step": 2060
    },
    {
      "epoch": 0.6403217075245534,
      "grad_norm": 0.7347825169563293,
      "learning_rate": 4.5431842291372e-05,
      "loss": 8.1315,
      "step": 2070
    },
    {
      "epoch": 0.6434150491067976,
      "grad_norm": 2.116239070892334,
      "learning_rate": 4.540974186704385e-05,
      "loss": 8.1446,
      "step": 2080
    },
    {
      "epoch": 0.6465083906890419,
      "grad_norm": 2.6621463298797607,
      "learning_rate": 4.53876414427157e-05,
      "loss": 8.1376,
      "step": 2090
    },
    {
      "epoch": 0.649601732271286,
      "grad_norm": 0.8156794309616089,
      "learning_rate": 4.536554101838755e-05,
      "loss": 8.1435,
      "step": 2100
    },
    {
      "epoch": 0.6526950738535303,
      "grad_norm": 1.779827356338501,
      "learning_rate": 4.534344059405941e-05,
      "loss": 8.1286,
      "step": 2110
    },
    {
      "epoch": 0.6557884154357745,
      "grad_norm": 3.6931838989257812,
      "learning_rate": 4.5321340169731266e-05,
      "loss": 8.1238,
      "step": 2120
    },
    {
      "epoch": 0.6588817570180188,
      "grad_norm": 1.4945954084396362,
      "learning_rate": 4.529923974540312e-05,
      "loss": 8.1367,
      "step": 2130
    },
    {
      "epoch": 0.6619750986002629,
      "grad_norm": 1.838985800743103,
      "learning_rate": 4.527713932107497e-05,
      "loss": 8.1363,
      "step": 2140
    },
    {
      "epoch": 0.6650684401825071,
      "grad_norm": 1.0356295108795166,
      "learning_rate": 4.525503889674682e-05,
      "loss": 8.1453,
      "step": 2150
    },
    {
      "epoch": 0.6681617817647514,
      "grad_norm": 2.1280863285064697,
      "learning_rate": 4.5232938472418677e-05,
      "loss": 8.1204,
      "step": 2160
    },
    {
      "epoch": 0.6712551233469956,
      "grad_norm": 2.179682731628418,
      "learning_rate": 4.521083804809053e-05,
      "loss": 8.1256,
      "step": 2170
    },
    {
      "epoch": 0.6743484649292398,
      "grad_norm": 1.5666074752807617,
      "learning_rate": 4.518873762376238e-05,
      "loss": 8.1255,
      "step": 2180
    },
    {
      "epoch": 0.677441806511484,
      "grad_norm": 1.3898165225982666,
      "learning_rate": 4.516663719943423e-05,
      "loss": 8.1194,
      "step": 2190
    },
    {
      "epoch": 0.6805351480937283,
      "grad_norm": 1.6152321100234985,
      "learning_rate": 4.514453677510608e-05,
      "loss": 8.1377,
      "step": 2200
    },
    {
      "epoch": 0.6836284896759725,
      "grad_norm": 0.9872780442237854,
      "learning_rate": 4.512243635077794e-05,
      "loss": 8.1251,
      "step": 2210
    },
    {
      "epoch": 0.6867218312582167,
      "grad_norm": 1.3864965438842773,
      "learning_rate": 4.510033592644979e-05,
      "loss": 8.1347,
      "step": 2220
    },
    {
      "epoch": 0.6898151728404609,
      "grad_norm": 1.6805392503738403,
      "learning_rate": 4.507823550212164e-05,
      "loss": 8.1309,
      "step": 2230
    },
    {
      "epoch": 0.6929085144227052,
      "grad_norm": 0.8944875597953796,
      "learning_rate": 4.505613507779349e-05,
      "loss": 8.1284,
      "step": 2240
    },
    {
      "epoch": 0.6960018560049493,
      "grad_norm": 0.7924079895019531,
      "learning_rate": 4.503403465346535e-05,
      "loss": 8.1483,
      "step": 2250
    },
    {
      "epoch": 0.6990951975871935,
      "grad_norm": 2.8641250133514404,
      "learning_rate": 4.5011934229137206e-05,
      "loss": 8.1206,
      "step": 2260
    },
    {
      "epoch": 0.7021885391694378,
      "grad_norm": 2.6894214153289795,
      "learning_rate": 4.498983380480906e-05,
      "loss": 8.1099,
      "step": 2270
    },
    {
      "epoch": 0.705281880751682,
      "grad_norm": 1.2986316680908203,
      "learning_rate": 4.496773338048091e-05,
      "loss": 8.1174,
      "step": 2280
    },
    {
      "epoch": 0.7083752223339262,
      "grad_norm": 0.7564579248428345,
      "learning_rate": 4.494563295615276e-05,
      "loss": 8.1263,
      "step": 2290
    },
    {
      "epoch": 0.7114685639161704,
      "grad_norm": 2.1276535987854004,
      "learning_rate": 4.492353253182462e-05,
      "loss": 8.1272,
      "step": 2300
    },
    {
      "epoch": 0.7145619054984147,
      "grad_norm": 1.3789678812026978,
      "learning_rate": 4.490143210749647e-05,
      "loss": 8.1159,
      "step": 2310
    },
    {
      "epoch": 0.7176552470806589,
      "grad_norm": 0.9833824634552002,
      "learning_rate": 4.487933168316832e-05,
      "loss": 8.1261,
      "step": 2320
    },
    {
      "epoch": 0.7207485886629031,
      "grad_norm": 1.6973011493682861,
      "learning_rate": 4.485723125884017e-05,
      "loss": 8.1218,
      "step": 2330
    },
    {
      "epoch": 0.7238419302451473,
      "grad_norm": 1.2579853534698486,
      "learning_rate": 4.483513083451202e-05,
      "loss": 8.1356,
      "step": 2340
    },
    {
      "epoch": 0.7269352718273916,
      "grad_norm": 1.0678609609603882,
      "learning_rate": 4.481303041018388e-05,
      "loss": 8.1115,
      "step": 2350
    },
    {
      "epoch": 0.7300286134096358,
      "grad_norm": 1.0080513954162598,
      "learning_rate": 4.479092998585573e-05,
      "loss": 8.1416,
      "step": 2360
    },
    {
      "epoch": 0.7331219549918799,
      "grad_norm": 1.728085994720459,
      "learning_rate": 4.476882956152758e-05,
      "loss": 8.1256,
      "step": 2370
    },
    {
      "epoch": 0.7362152965741242,
      "grad_norm": 1.0013184547424316,
      "learning_rate": 4.474672913719944e-05,
      "loss": 8.1265,
      "step": 2380
    },
    {
      "epoch": 0.7393086381563684,
      "grad_norm": 1.2561790943145752,
      "learning_rate": 4.472462871287129e-05,
      "loss": 8.1165,
      "step": 2390
    },
    {
      "epoch": 0.7424019797386127,
      "grad_norm": 2.669976234436035,
      "learning_rate": 4.470252828854315e-05,
      "loss": 8.117,
      "step": 2400
    },
    {
      "epoch": 0.7454953213208568,
      "grad_norm": 1.0632567405700684,
      "learning_rate": 4.4680427864215e-05,
      "loss": 8.1222,
      "step": 2410
    },
    {
      "epoch": 0.7485886629031011,
      "grad_norm": 2.326310396194458,
      "learning_rate": 4.465832743988685e-05,
      "loss": 8.1339,
      "step": 2420
    },
    {
      "epoch": 0.7516820044853453,
      "grad_norm": 1.9265451431274414,
      "learning_rate": 4.46362270155587e-05,
      "loss": 8.1283,
      "step": 2430
    },
    {
      "epoch": 0.7547753460675896,
      "grad_norm": 1.9307504892349243,
      "learning_rate": 4.461412659123056e-05,
      "loss": 8.1387,
      "step": 2440
    },
    {
      "epoch": 0.7578686876498337,
      "grad_norm": 1.3953375816345215,
      "learning_rate": 4.459202616690241e-05,
      "loss": 8.1301,
      "step": 2450
    },
    {
      "epoch": 0.7609620292320779,
      "grad_norm": 0.7817894220352173,
      "learning_rate": 4.456992574257426e-05,
      "loss": 8.1312,
      "step": 2460
    },
    {
      "epoch": 0.7640553708143222,
      "grad_norm": 0.9156811237335205,
      "learning_rate": 4.454782531824611e-05,
      "loss": 8.1052,
      "step": 2470
    },
    {
      "epoch": 0.7671487123965663,
      "grad_norm": 1.400242567062378,
      "learning_rate": 4.452572489391796e-05,
      "loss": 8.1224,
      "step": 2480
    },
    {
      "epoch": 0.7702420539788106,
      "grad_norm": 1.0398952960968018,
      "learning_rate": 4.450362446958982e-05,
      "loss": 8.1375,
      "step": 2490
    },
    {
      "epoch": 0.7733353955610548,
      "grad_norm": 0.8242036700248718,
      "learning_rate": 4.448152404526167e-05,
      "loss": 8.1193,
      "step": 2500
    },
    {
      "epoch": 0.7764287371432991,
      "grad_norm": 0.8784144520759583,
      "learning_rate": 4.445942362093352e-05,
      "loss": 8.1386,
      "step": 2510
    },
    {
      "epoch": 0.7795220787255432,
      "grad_norm": 0.7130635976791382,
      "learning_rate": 4.443732319660538e-05,
      "loss": 8.122,
      "step": 2520
    },
    {
      "epoch": 0.7826154203077875,
      "grad_norm": 0.9169226288795471,
      "learning_rate": 4.441522277227723e-05,
      "loss": 8.0999,
      "step": 2530
    },
    {
      "epoch": 0.7857087618900317,
      "grad_norm": 0.8026123642921448,
      "learning_rate": 4.439312234794909e-05,
      "loss": 8.1268,
      "step": 2540
    },
    {
      "epoch": 0.788802103472276,
      "grad_norm": 3.0320956707000732,
      "learning_rate": 4.437102192362094e-05,
      "loss": 8.1179,
      "step": 2550
    },
    {
      "epoch": 0.7918954450545201,
      "grad_norm": 1.7157331705093384,
      "learning_rate": 4.434892149929279e-05,
      "loss": 8.1289,
      "step": 2560
    },
    {
      "epoch": 0.7949887866367643,
      "grad_norm": 1.0207972526550293,
      "learning_rate": 4.432682107496464e-05,
      "loss": 8.1293,
      "step": 2570
    },
    {
      "epoch": 0.7980821282190086,
      "grad_norm": 1.2355436086654663,
      "learning_rate": 4.43047206506365e-05,
      "loss": 8.1244,
      "step": 2580
    },
    {
      "epoch": 0.8011754698012528,
      "grad_norm": 0.8576319813728333,
      "learning_rate": 4.428262022630835e-05,
      "loss": 8.1309,
      "step": 2590
    },
    {
      "epoch": 0.804268811383497,
      "grad_norm": 1.5930135250091553,
      "learning_rate": 4.42605198019802e-05,
      "loss": 8.1319,
      "step": 2600
    },
    {
      "epoch": 0.8073621529657412,
      "grad_norm": 1.2188942432403564,
      "learning_rate": 4.423841937765205e-05,
      "loss": 8.1204,
      "step": 2610
    },
    {
      "epoch": 0.8104554945479855,
      "grad_norm": 1.5238261222839355,
      "learning_rate": 4.42163189533239e-05,
      "loss": 8.1038,
      "step": 2620
    },
    {
      "epoch": 0.8135488361302297,
      "grad_norm": 1.3246231079101562,
      "learning_rate": 4.419421852899576e-05,
      "loss": 8.1085,
      "step": 2630
    },
    {
      "epoch": 0.8166421777124739,
      "grad_norm": 2.379167318344116,
      "learning_rate": 4.417211810466761e-05,
      "loss": 8.1276,
      "step": 2640
    },
    {
      "epoch": 0.8197355192947181,
      "grad_norm": 1.0612999200820923,
      "learning_rate": 4.415001768033947e-05,
      "loss": 8.1354,
      "step": 2650
    },
    {
      "epoch": 0.8228288608769624,
      "grad_norm": 1.0679903030395508,
      "learning_rate": 4.412791725601132e-05,
      "loss": 8.1209,
      "step": 2660
    },
    {
      "epoch": 0.8259222024592066,
      "grad_norm": 1.2253437042236328,
      "learning_rate": 4.410581683168317e-05,
      "loss": 8.1256,
      "step": 2670
    },
    {
      "epoch": 0.8290155440414507,
      "grad_norm": 1.4744584560394287,
      "learning_rate": 4.408371640735503e-05,
      "loss": 8.141,
      "step": 2680
    },
    {
      "epoch": 0.832108885623695,
      "grad_norm": 0.7224366664886475,
      "learning_rate": 4.406161598302688e-05,
      "loss": 8.1234,
      "step": 2690
    },
    {
      "epoch": 0.8352022272059392,
      "grad_norm": 1.8724217414855957,
      "learning_rate": 4.403951555869873e-05,
      "loss": 8.1192,
      "step": 2700
    },
    {
      "epoch": 0.8382955687881835,
      "grad_norm": 0.7011707425117493,
      "learning_rate": 4.401741513437058e-05,
      "loss": 8.1254,
      "step": 2710
    },
    {
      "epoch": 0.8413889103704276,
      "grad_norm": 0.8190743923187256,
      "learning_rate": 4.399531471004244e-05,
      "loss": 8.1438,
      "step": 2720
    },
    {
      "epoch": 0.8444822519526719,
      "grad_norm": 1.3879348039627075,
      "learning_rate": 4.397321428571429e-05,
      "loss": 8.1342,
      "step": 2730
    },
    {
      "epoch": 0.8475755935349161,
      "grad_norm": 1.4845677614212036,
      "learning_rate": 4.395111386138614e-05,
      "loss": 8.1168,
      "step": 2740
    },
    {
      "epoch": 0.8506689351171604,
      "grad_norm": 0.7718964219093323,
      "learning_rate": 4.392901343705799e-05,
      "loss": 8.132,
      "step": 2750
    },
    {
      "epoch": 0.8537622766994045,
      "grad_norm": 0.7378916144371033,
      "learning_rate": 4.390691301272984e-05,
      "loss": 8.1368,
      "step": 2760
    },
    {
      "epoch": 0.8568556182816488,
      "grad_norm": 0.7976837754249573,
      "learning_rate": 4.38848125884017e-05,
      "loss": 8.1308,
      "step": 2770
    },
    {
      "epoch": 0.859948959863893,
      "grad_norm": 1.2536346912384033,
      "learning_rate": 4.386271216407356e-05,
      "loss": 8.1242,
      "step": 2780
    },
    {
      "epoch": 0.8630423014461371,
      "grad_norm": 0.9537434577941895,
      "learning_rate": 4.384061173974541e-05,
      "loss": 8.1134,
      "step": 2790
    },
    {
      "epoch": 0.8661356430283814,
      "grad_norm": 0.9349040389060974,
      "learning_rate": 4.381851131541726e-05,
      "loss": 8.1374,
      "step": 2800
    },
    {
      "epoch": 0.8692289846106256,
      "grad_norm": 0.7789999842643738,
      "learning_rate": 4.379641089108911e-05,
      "loss": 8.1113,
      "step": 2810
    },
    {
      "epoch": 0.8723223261928699,
      "grad_norm": 1.2549035549163818,
      "learning_rate": 4.377431046676097e-05,
      "loss": 8.1103,
      "step": 2820
    },
    {
      "epoch": 0.875415667775114,
      "grad_norm": 1.6388391256332397,
      "learning_rate": 4.375221004243282e-05,
      "loss": 8.1508,
      "step": 2830
    },
    {
      "epoch": 0.8785090093573583,
      "grad_norm": 0.7326608300209045,
      "learning_rate": 4.373010961810467e-05,
      "loss": 8.1253,
      "step": 2840
    },
    {
      "epoch": 0.8816023509396025,
      "grad_norm": 0.9195551872253418,
      "learning_rate": 4.370800919377652e-05,
      "loss": 8.1298,
      "step": 2850
    },
    {
      "epoch": 0.8846956925218468,
      "grad_norm": 1.2265956401824951,
      "learning_rate": 4.368590876944838e-05,
      "loss": 8.1217,
      "step": 2860
    },
    {
      "epoch": 0.8877890341040909,
      "grad_norm": 0.8913390040397644,
      "learning_rate": 4.366380834512023e-05,
      "loss": 8.1237,
      "step": 2870
    },
    {
      "epoch": 0.8908823756863352,
      "grad_norm": 1.0213226079940796,
      "learning_rate": 4.364170792079208e-05,
      "loss": 8.1215,
      "step": 2880
    },
    {
      "epoch": 0.8939757172685794,
      "grad_norm": 1.399215817451477,
      "learning_rate": 4.361960749646393e-05,
      "loss": 8.12,
      "step": 2890
    },
    {
      "epoch": 0.8970690588508236,
      "grad_norm": 2.366387128829956,
      "learning_rate": 4.359750707213578e-05,
      "loss": 8.132,
      "step": 2900
    },
    {
      "epoch": 0.9001624004330678,
      "grad_norm": 1.5653551816940308,
      "learning_rate": 4.357540664780764e-05,
      "loss": 8.1367,
      "step": 2910
    },
    {
      "epoch": 0.903255742015312,
      "grad_norm": 2.0236313343048096,
      "learning_rate": 4.35533062234795e-05,
      "loss": 8.1309,
      "step": 2920
    },
    {
      "epoch": 0.9063490835975563,
      "grad_norm": 2.4536983966827393,
      "learning_rate": 4.353120579915135e-05,
      "loss": 8.1306,
      "step": 2930
    },
    {
      "epoch": 0.9094424251798005,
      "grad_norm": 0.7204288840293884,
      "learning_rate": 4.35091053748232e-05,
      "loss": 8.1167,
      "step": 2940
    },
    {
      "epoch": 0.9125357667620447,
      "grad_norm": 0.8028860688209534,
      "learning_rate": 4.348700495049505e-05,
      "loss": 8.1145,
      "step": 2950
    },
    {
      "epoch": 0.9156291083442889,
      "grad_norm": 1.3860328197479248,
      "learning_rate": 4.346490452616691e-05,
      "loss": 8.1142,
      "step": 2960
    },
    {
      "epoch": 0.9187224499265332,
      "grad_norm": 0.9409781694412231,
      "learning_rate": 4.344280410183876e-05,
      "loss": 8.1198,
      "step": 2970
    },
    {
      "epoch": 0.9218157915087773,
      "grad_norm": 2.06516695022583,
      "learning_rate": 4.342070367751061e-05,
      "loss": 8.1193,
      "step": 2980
    },
    {
      "epoch": 0.9249091330910216,
      "grad_norm": 1.0741915702819824,
      "learning_rate": 4.339860325318246e-05,
      "loss": 8.1338,
      "step": 2990
    },
    {
      "epoch": 0.9280024746732658,
      "grad_norm": 1.0689867734909058,
      "learning_rate": 4.337650282885432e-05,
      "loss": 8.13,
      "step": 3000
    },
    {
      "epoch": 0.93109581625551,
      "grad_norm": 0.6213433146476746,
      "learning_rate": 4.335440240452617e-05,
      "loss": 8.1026,
      "step": 3010
    },
    {
      "epoch": 0.9341891578377542,
      "grad_norm": 0.8502789735794067,
      "learning_rate": 4.333230198019802e-05,
      "loss": 8.1249,
      "step": 3020
    },
    {
      "epoch": 0.9372824994199984,
      "grad_norm": 0.8461669087409973,
      "learning_rate": 4.331020155586987e-05,
      "loss": 8.1218,
      "step": 3030
    },
    {
      "epoch": 0.9403758410022427,
      "grad_norm": 1.2638654708862305,
      "learning_rate": 4.328810113154172e-05,
      "loss": 8.1162,
      "step": 3040
    },
    {
      "epoch": 0.9434691825844869,
      "grad_norm": 1.817523717880249,
      "learning_rate": 4.326600070721358e-05,
      "loss": 8.1324,
      "step": 3050
    },
    {
      "epoch": 0.9465625241667311,
      "grad_norm": 1.7943769693374634,
      "learning_rate": 4.324390028288544e-05,
      "loss": 8.1317,
      "step": 3060
    },
    {
      "epoch": 0.9496558657489753,
      "grad_norm": 1.5107828378677368,
      "learning_rate": 4.322179985855729e-05,
      "loss": 8.1342,
      "step": 3070
    },
    {
      "epoch": 0.9527492073312196,
      "grad_norm": 1.071122407913208,
      "learning_rate": 4.319969943422914e-05,
      "loss": 8.1342,
      "step": 3080
    },
    {
      "epoch": 0.9558425489134638,
      "grad_norm": 1.332771897315979,
      "learning_rate": 4.317759900990099e-05,
      "loss": 8.1103,
      "step": 3090
    },
    {
      "epoch": 0.958935890495708,
      "grad_norm": 1.1416230201721191,
      "learning_rate": 4.315549858557285e-05,
      "loss": 8.1236,
      "step": 3100
    },
    {
      "epoch": 0.9620292320779522,
      "grad_norm": 0.8420162200927734,
      "learning_rate": 4.31333981612447e-05,
      "loss": 8.1285,
      "step": 3110
    },
    {
      "epoch": 0.9651225736601964,
      "grad_norm": 2.129894256591797,
      "learning_rate": 4.311129773691655e-05,
      "loss": 8.113,
      "step": 3120
    },
    {
      "epoch": 0.9682159152424407,
      "grad_norm": 0.8761067986488342,
      "learning_rate": 4.30891973125884e-05,
      "loss": 8.1245,
      "step": 3130
    },
    {
      "epoch": 0.9713092568246848,
      "grad_norm": 1.02657949924469,
      "learning_rate": 4.306709688826026e-05,
      "loss": 8.1165,
      "step": 3140
    },
    {
      "epoch": 0.9744025984069291,
      "grad_norm": 0.9194222092628479,
      "learning_rate": 4.304499646393211e-05,
      "loss": 8.1331,
      "step": 3150
    },
    {
      "epoch": 0.9774959399891733,
      "grad_norm": 0.9798198938369751,
      "learning_rate": 4.302289603960396e-05,
      "loss": 8.127,
      "step": 3160
    },
    {
      "epoch": 0.9805892815714176,
      "grad_norm": 0.6078124046325684,
      "learning_rate": 4.300079561527581e-05,
      "loss": 8.1163,
      "step": 3170
    },
    {
      "epoch": 0.9836826231536617,
      "grad_norm": 0.9838354587554932,
      "learning_rate": 4.297869519094767e-05,
      "loss": 8.1217,
      "step": 3180
    },
    {
      "epoch": 0.986775964735906,
      "grad_norm": 0.9231829047203064,
      "learning_rate": 4.295659476661952e-05,
      "loss": 8.116,
      "step": 3190
    },
    {
      "epoch": 0.9898693063181502,
      "grad_norm": 1.1258717775344849,
      "learning_rate": 4.293449434229138e-05,
      "loss": 8.1171,
      "step": 3200
    },
    {
      "epoch": 0.9929626479003943,
      "grad_norm": 1.7347180843353271,
      "learning_rate": 4.291239391796323e-05,
      "loss": 8.1217,
      "step": 3210
    },
    {
      "epoch": 0.9960559894826386,
      "grad_norm": 0.7764060497283936,
      "learning_rate": 4.289029349363508e-05,
      "loss": 8.1162,
      "step": 3220
    },
    {
      "epoch": 0.9991493310648828,
      "grad_norm": 1.1703591346740723,
      "learning_rate": 4.286819306930693e-05,
      "loss": 8.1115,
      "step": 3230
    },
    {
      "epoch": 1.002242672647127,
      "grad_norm": 1.196240782737732,
      "learning_rate": 4.284609264497879e-05,
      "loss": 8.1123,
      "step": 3240
    },
    {
      "epoch": 1.0053360142293712,
      "grad_norm": 0.6723086833953857,
      "learning_rate": 4.282399222065064e-05,
      "loss": 8.129,
      "step": 3250
    },
    {
      "epoch": 1.0084293558116155,
      "grad_norm": 0.9803570508956909,
      "learning_rate": 4.280189179632249e-05,
      "loss": 8.1097,
      "step": 3260
    },
    {
      "epoch": 1.0115226973938598,
      "grad_norm": 0.9006723761558533,
      "learning_rate": 4.277979137199434e-05,
      "loss": 8.1046,
      "step": 3270
    },
    {
      "epoch": 1.0146160389761039,
      "grad_norm": 1.476979374885559,
      "learning_rate": 4.27576909476662e-05,
      "loss": 8.1122,
      "step": 3280
    },
    {
      "epoch": 1.0177093805583481,
      "grad_norm": 0.8841773271560669,
      "learning_rate": 4.273559052333805e-05,
      "loss": 8.1124,
      "step": 3290
    },
    {
      "epoch": 1.0208027221405924,
      "grad_norm": 0.9440211057662964,
      "learning_rate": 4.27134900990099e-05,
      "loss": 8.128,
      "step": 3300
    },
    {
      "epoch": 1.0238960637228367,
      "grad_norm": 0.9217091798782349,
      "learning_rate": 4.269138967468175e-05,
      "loss": 8.1291,
      "step": 3310
    },
    {
      "epoch": 1.0269894053050808,
      "grad_norm": 1.0334994792938232,
      "learning_rate": 4.266928925035361e-05,
      "loss": 8.1276,
      "step": 3320
    },
    {
      "epoch": 1.030082746887325,
      "grad_norm": 0.8725922107696533,
      "learning_rate": 4.264718882602546e-05,
      "loss": 8.1128,
      "step": 3330
    },
    {
      "epoch": 1.0331760884695693,
      "grad_norm": 0.8615626096725464,
      "learning_rate": 4.262508840169732e-05,
      "loss": 8.1265,
      "step": 3340
    },
    {
      "epoch": 1.0362694300518134,
      "grad_norm": 1.0167025327682495,
      "learning_rate": 4.260298797736917e-05,
      "loss": 8.109,
      "step": 3350
    },
    {
      "epoch": 1.0393627716340577,
      "grad_norm": 1.2046356201171875,
      "learning_rate": 4.258088755304102e-05,
      "loss": 8.1251,
      "step": 3360
    },
    {
      "epoch": 1.042456113216302,
      "grad_norm": 0.7053648829460144,
      "learning_rate": 4.255878712871287e-05,
      "loss": 8.1268,
      "step": 3370
    },
    {
      "epoch": 1.0455494547985462,
      "grad_norm": 0.7233771681785583,
      "learning_rate": 4.253668670438473e-05,
      "loss": 8.1078,
      "step": 3380
    },
    {
      "epoch": 1.0486427963807903,
      "grad_norm": 0.6681994795799255,
      "learning_rate": 4.251458628005658e-05,
      "loss": 8.1095,
      "step": 3390
    },
    {
      "epoch": 1.0517361379630346,
      "grad_norm": 0.8064314126968384,
      "learning_rate": 4.249248585572843e-05,
      "loss": 8.122,
      "step": 3400
    },
    {
      "epoch": 1.0548294795452788,
      "grad_norm": 1.7897396087646484,
      "learning_rate": 4.247038543140028e-05,
      "loss": 8.1182,
      "step": 3410
    },
    {
      "epoch": 1.057922821127523,
      "grad_norm": 0.9416254162788391,
      "learning_rate": 4.244828500707214e-05,
      "loss": 8.1054,
      "step": 3420
    },
    {
      "epoch": 1.0610161627097672,
      "grad_norm": 1.1845312118530273,
      "learning_rate": 4.242618458274399e-05,
      "loss": 8.1231,
      "step": 3430
    },
    {
      "epoch": 1.0641095042920115,
      "grad_norm": 1.0002448558807373,
      "learning_rate": 4.240408415841584e-05,
      "loss": 8.1166,
      "step": 3440
    },
    {
      "epoch": 1.0672028458742557,
      "grad_norm": 1.26829993724823,
      "learning_rate": 4.23819837340877e-05,
      "loss": 8.1306,
      "step": 3450
    },
    {
      "epoch": 1.0702961874564998,
      "grad_norm": 0.8291583061218262,
      "learning_rate": 4.235988330975955e-05,
      "loss": 8.1312,
      "step": 3460
    },
    {
      "epoch": 1.073389529038744,
      "grad_norm": 1.0712348222732544,
      "learning_rate": 4.23377828854314e-05,
      "loss": 8.1141,
      "step": 3470
    },
    {
      "epoch": 1.0764828706209884,
      "grad_norm": 0.7974317669868469,
      "learning_rate": 4.231568246110326e-05,
      "loss": 8.1273,
      "step": 3480
    },
    {
      "epoch": 1.0795762122032326,
      "grad_norm": 0.7962043285369873,
      "learning_rate": 4.229358203677511e-05,
      "loss": 8.142,
      "step": 3490
    },
    {
      "epoch": 1.0826695537854767,
      "grad_norm": 0.7399154305458069,
      "learning_rate": 4.227148161244696e-05,
      "loss": 8.1102,
      "step": 3500
    },
    {
      "epoch": 1.085762895367721,
      "grad_norm": 0.807029128074646,
      "learning_rate": 4.224938118811881e-05,
      "loss": 8.1096,
      "step": 3510
    },
    {
      "epoch": 1.0888562369499653,
      "grad_norm": 0.8599138259887695,
      "learning_rate": 4.222728076379067e-05,
      "loss": 8.119,
      "step": 3520
    },
    {
      "epoch": 1.0919495785322093,
      "grad_norm": 0.8368381857872009,
      "learning_rate": 4.220518033946252e-05,
      "loss": 8.1338,
      "step": 3530
    },
    {
      "epoch": 1.0950429201144536,
      "grad_norm": 1.4200211763381958,
      "learning_rate": 4.218307991513437e-05,
      "loss": 8.1225,
      "step": 3540
    },
    {
      "epoch": 1.0981362616966979,
      "grad_norm": 0.8795577883720398,
      "learning_rate": 4.216097949080622e-05,
      "loss": 8.1274,
      "step": 3550
    },
    {
      "epoch": 1.1012296032789421,
      "grad_norm": 0.6865503787994385,
      "learning_rate": 4.213887906647808e-05,
      "loss": 8.1112,
      "step": 3560
    },
    {
      "epoch": 1.1043229448611862,
      "grad_norm": 1.285470724105835,
      "learning_rate": 4.211677864214993e-05,
      "loss": 8.1272,
      "step": 3570
    },
    {
      "epoch": 1.1074162864434305,
      "grad_norm": 1.0266700983047485,
      "learning_rate": 4.209467821782179e-05,
      "loss": 8.1126,
      "step": 3580
    },
    {
      "epoch": 1.1105096280256748,
      "grad_norm": 1.6995761394500732,
      "learning_rate": 4.207257779349364e-05,
      "loss": 8.1124,
      "step": 3590
    },
    {
      "epoch": 1.113602969607919,
      "grad_norm": 0.841712236404419,
      "learning_rate": 4.205047736916549e-05,
      "loss": 8.125,
      "step": 3600
    },
    {
      "epoch": 1.116696311190163,
      "grad_norm": 0.7503789663314819,
      "learning_rate": 4.202837694483734e-05,
      "loss": 8.1223,
      "step": 3610
    },
    {
      "epoch": 1.1197896527724074,
      "grad_norm": 0.9557420611381531,
      "learning_rate": 4.20062765205092e-05,
      "loss": 8.1153,
      "step": 3620
    },
    {
      "epoch": 1.1228829943546517,
      "grad_norm": 0.7093141674995422,
      "learning_rate": 4.198417609618105e-05,
      "loss": 8.1388,
      "step": 3630
    },
    {
      "epoch": 1.1259763359368957,
      "grad_norm": 0.7262153625488281,
      "learning_rate": 4.19620756718529e-05,
      "loss": 8.1167,
      "step": 3640
    },
    {
      "epoch": 1.12906967751914,
      "grad_norm": 0.7020951509475708,
      "learning_rate": 4.193997524752475e-05,
      "loss": 8.1244,
      "step": 3650
    },
    {
      "epoch": 1.1321630191013843,
      "grad_norm": 1.3000520467758179,
      "learning_rate": 4.191787482319661e-05,
      "loss": 8.1277,
      "step": 3660
    },
    {
      "epoch": 1.1352563606836286,
      "grad_norm": 0.5666297674179077,
      "learning_rate": 4.189577439886846e-05,
      "loss": 8.1104,
      "step": 3670
    },
    {
      "epoch": 1.1383497022658726,
      "grad_norm": 1.1555589437484741,
      "learning_rate": 4.187367397454031e-05,
      "loss": 8.1115,
      "step": 3680
    },
    {
      "epoch": 1.141443043848117,
      "grad_norm": 0.8192570209503174,
      "learning_rate": 4.185157355021216e-05,
      "loss": 8.1068,
      "step": 3690
    },
    {
      "epoch": 1.1445363854303612,
      "grad_norm": 0.8242233395576477,
      "learning_rate": 4.182947312588402e-05,
      "loss": 8.1175,
      "step": 3700
    },
    {
      "epoch": 1.1476297270126055,
      "grad_norm": 1.2689039707183838,
      "learning_rate": 4.180737270155587e-05,
      "loss": 8.1058,
      "step": 3710
    },
    {
      "epoch": 1.1507230685948495,
      "grad_norm": 1.6071594953536987,
      "learning_rate": 4.178527227722773e-05,
      "loss": 8.1172,
      "step": 3720
    },
    {
      "epoch": 1.1538164101770938,
      "grad_norm": 1.1766440868377686,
      "learning_rate": 4.176317185289958e-05,
      "loss": 8.1127,
      "step": 3730
    },
    {
      "epoch": 1.156909751759338,
      "grad_norm": 1.276888370513916,
      "learning_rate": 4.174107142857143e-05,
      "loss": 8.1253,
      "step": 3740
    },
    {
      "epoch": 1.1600030933415821,
      "grad_norm": 0.7621655464172363,
      "learning_rate": 4.171897100424328e-05,
      "loss": 8.11,
      "step": 3750
    },
    {
      "epoch": 1.1630964349238264,
      "grad_norm": 0.8809208869934082,
      "learning_rate": 4.169687057991514e-05,
      "loss": 8.1232,
      "step": 3760
    },
    {
      "epoch": 1.1661897765060707,
      "grad_norm": 0.8856102824211121,
      "learning_rate": 4.167477015558699e-05,
      "loss": 8.1147,
      "step": 3770
    },
    {
      "epoch": 1.169283118088315,
      "grad_norm": 0.7884340882301331,
      "learning_rate": 4.165266973125884e-05,
      "loss": 8.1138,
      "step": 3780
    },
    {
      "epoch": 1.172376459670559,
      "grad_norm": 0.6883849501609802,
      "learning_rate": 4.163056930693069e-05,
      "loss": 8.1246,
      "step": 3790
    },
    {
      "epoch": 1.1754698012528033,
      "grad_norm": 0.6409220695495605,
      "learning_rate": 4.160846888260255e-05,
      "loss": 8.1282,
      "step": 3800
    },
    {
      "epoch": 1.1785631428350476,
      "grad_norm": 0.958209216594696,
      "learning_rate": 4.15863684582744e-05,
      "loss": 8.1206,
      "step": 3810
    },
    {
      "epoch": 1.1816564844172919,
      "grad_norm": 0.7926703095436096,
      "learning_rate": 4.156426803394625e-05,
      "loss": 8.1229,
      "step": 3820
    },
    {
      "epoch": 1.184749825999536,
      "grad_norm": 0.8342359662055969,
      "learning_rate": 4.15421676096181e-05,
      "loss": 8.101,
      "step": 3830
    },
    {
      "epoch": 1.1878431675817802,
      "grad_norm": 1.3700296878814697,
      "learning_rate": 4.152006718528996e-05,
      "loss": 8.1235,
      "step": 3840
    },
    {
      "epoch": 1.1909365091640245,
      "grad_norm": 1.2245920896530151,
      "learning_rate": 4.149796676096182e-05,
      "loss": 8.1154,
      "step": 3850
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 0.8439896106719971,
      "learning_rate": 4.147586633663367e-05,
      "loss": 8.1216,
      "step": 3860
    },
    {
      "epoch": 1.1971231923285128,
      "grad_norm": 1.4020590782165527,
      "learning_rate": 4.145376591230552e-05,
      "loss": 8.1103,
      "step": 3870
    },
    {
      "epoch": 1.200216533910757,
      "grad_norm": 0.6946353912353516,
      "learning_rate": 4.143166548797737e-05,
      "loss": 8.1273,
      "step": 3880
    },
    {
      "epoch": 1.2033098754930014,
      "grad_norm": 1.4112876653671265,
      "learning_rate": 4.140956506364922e-05,
      "loss": 8.1156,
      "step": 3890
    },
    {
      "epoch": 1.2064032170752454,
      "grad_norm": 1.538412094116211,
      "learning_rate": 4.138746463932108e-05,
      "loss": 8.1164,
      "step": 3900
    },
    {
      "epoch": 1.2094965586574897,
      "grad_norm": 0.8160387277603149,
      "learning_rate": 4.136536421499293e-05,
      "loss": 8.1171,
      "step": 3910
    },
    {
      "epoch": 1.212589900239734,
      "grad_norm": 0.8632400035858154,
      "learning_rate": 4.134326379066478e-05,
      "loss": 8.1156,
      "step": 3920
    },
    {
      "epoch": 1.2156832418219783,
      "grad_norm": 1.040489673614502,
      "learning_rate": 4.132116336633663e-05,
      "loss": 8.1033,
      "step": 3930
    },
    {
      "epoch": 1.2187765834042223,
      "grad_norm": 1.193398356437683,
      "learning_rate": 4.129906294200849e-05,
      "loss": 8.128,
      "step": 3940
    },
    {
      "epoch": 1.2218699249864666,
      "grad_norm": 0.8503872752189636,
      "learning_rate": 4.127696251768034e-05,
      "loss": 8.1126,
      "step": 3950
    },
    {
      "epoch": 1.224963266568711,
      "grad_norm": 2.0096020698547363,
      "learning_rate": 4.125486209335219e-05,
      "loss": 8.1267,
      "step": 3960
    },
    {
      "epoch": 1.228056608150955,
      "grad_norm": 2.299232244491577,
      "learning_rate": 4.123276166902404e-05,
      "loss": 8.1184,
      "step": 3970
    },
    {
      "epoch": 1.2311499497331992,
      "grad_norm": 0.7290074229240417,
      "learning_rate": 4.12106612446959e-05,
      "loss": 8.106,
      "step": 3980
    },
    {
      "epoch": 1.2342432913154435,
      "grad_norm": 1.5019081830978394,
      "learning_rate": 4.118856082036776e-05,
      "loss": 8.1194,
      "step": 3990
    },
    {
      "epoch": 1.2373366328976878,
      "grad_norm": 0.9979866743087769,
      "learning_rate": 4.116646039603961e-05,
      "loss": 8.1205,
      "step": 4000
    },
    {
      "epoch": 1.2404299744799319,
      "grad_norm": 0.748677670955658,
      "learning_rate": 4.114435997171146e-05,
      "loss": 8.1205,
      "step": 4010
    },
    {
      "epoch": 1.2435233160621761,
      "grad_norm": 0.5963840484619141,
      "learning_rate": 4.112225954738331e-05,
      "loss": 8.1159,
      "step": 4020
    },
    {
      "epoch": 1.2466166576444204,
      "grad_norm": 1.075423002243042,
      "learning_rate": 4.110015912305516e-05,
      "loss": 8.1272,
      "step": 4030
    },
    {
      "epoch": 1.2497099992266647,
      "grad_norm": 0.8496195673942566,
      "learning_rate": 4.107805869872702e-05,
      "loss": 8.1193,
      "step": 4040
    },
    {
      "epoch": 1.2528033408089088,
      "grad_norm": 0.7476891279220581,
      "learning_rate": 4.105595827439887e-05,
      "loss": 8.1056,
      "step": 4050
    },
    {
      "epoch": 1.255896682391153,
      "grad_norm": 1.019012212753296,
      "learning_rate": 4.103385785007072e-05,
      "loss": 8.1105,
      "step": 4060
    },
    {
      "epoch": 1.2589900239733973,
      "grad_norm": 0.9022393226623535,
      "learning_rate": 4.101175742574257e-05,
      "loss": 8.1065,
      "step": 4070
    },
    {
      "epoch": 1.2620833655556414,
      "grad_norm": 0.8938788771629333,
      "learning_rate": 4.098965700141443e-05,
      "loss": 8.1397,
      "step": 4080
    },
    {
      "epoch": 1.2651767071378857,
      "grad_norm": 0.9741756319999695,
      "learning_rate": 4.096755657708628e-05,
      "loss": 8.1337,
      "step": 4090
    },
    {
      "epoch": 1.26827004872013,
      "grad_norm": 1.0345206260681152,
      "learning_rate": 4.094545615275813e-05,
      "loss": 8.1068,
      "step": 4100
    },
    {
      "epoch": 1.2713633903023742,
      "grad_norm": 0.7918864488601685,
      "learning_rate": 4.092335572842998e-05,
      "loss": 8.104,
      "step": 4110
    },
    {
      "epoch": 1.2744567318846183,
      "grad_norm": 0.8275113701820374,
      "learning_rate": 4.090125530410184e-05,
      "loss": 8.1012,
      "step": 4120
    },
    {
      "epoch": 1.2775500734668626,
      "grad_norm": 0.8178954124450684,
      "learning_rate": 4.08791548797737e-05,
      "loss": 8.1174,
      "step": 4130
    },
    {
      "epoch": 1.2806434150491068,
      "grad_norm": 0.8959318399429321,
      "learning_rate": 4.085705445544555e-05,
      "loss": 8.1188,
      "step": 4140
    },
    {
      "epoch": 1.2837367566313511,
      "grad_norm": 1.0194681882858276,
      "learning_rate": 4.08349540311174e-05,
      "loss": 8.1197,
      "step": 4150
    },
    {
      "epoch": 1.2868300982135952,
      "grad_norm": 0.8763304948806763,
      "learning_rate": 4.081285360678925e-05,
      "loss": 8.1239,
      "step": 4160
    },
    {
      "epoch": 1.2899234397958395,
      "grad_norm": 0.9870306253433228,
      "learning_rate": 4.07907531824611e-05,
      "loss": 8.1268,
      "step": 4170
    },
    {
      "epoch": 1.2930167813780837,
      "grad_norm": 1.064742922782898,
      "learning_rate": 4.076865275813296e-05,
      "loss": 8.1194,
      "step": 4180
    },
    {
      "epoch": 1.2961101229603278,
      "grad_norm": 0.6587199568748474,
      "learning_rate": 4.074655233380481e-05,
      "loss": 8.1078,
      "step": 4190
    },
    {
      "epoch": 1.299203464542572,
      "grad_norm": 0.9830871224403381,
      "learning_rate": 4.072445190947666e-05,
      "loss": 8.1107,
      "step": 4200
    },
    {
      "epoch": 1.3022968061248164,
      "grad_norm": 1.1235051155090332,
      "learning_rate": 4.070235148514851e-05,
      "loss": 8.1202,
      "step": 4210
    },
    {
      "epoch": 1.3053901477070606,
      "grad_norm": 0.8844292759895325,
      "learning_rate": 4.068025106082037e-05,
      "loss": 8.125,
      "step": 4220
    },
    {
      "epoch": 1.3084834892893047,
      "grad_norm": 0.8180374503135681,
      "learning_rate": 4.0660360678925034e-05,
      "loss": 8.1132,
      "step": 4230
    },
    {
      "epoch": 1.311576830871549,
      "grad_norm": 0.7194439172744751,
      "learning_rate": 4.0638260254596885e-05,
      "loss": 8.1183,
      "step": 4240
    },
    {
      "epoch": 1.3146701724537933,
      "grad_norm": 0.6383553743362427,
      "learning_rate": 4.061615983026874e-05,
      "loss": 8.1124,
      "step": 4250
    },
    {
      "epoch": 1.3177635140360375,
      "grad_norm": 1.5439708232879639,
      "learning_rate": 4.05940594059406e-05,
      "loss": 8.1049,
      "step": 4260
    },
    {
      "epoch": 1.3208568556182816,
      "grad_norm": 0.8215919137001038,
      "learning_rate": 4.057195898161245e-05,
      "loss": 8.1187,
      "step": 4270
    },
    {
      "epoch": 1.3239501972005259,
      "grad_norm": 1.3486407995224,
      "learning_rate": 4.05498585572843e-05,
      "loss": 8.1113,
      "step": 4280
    },
    {
      "epoch": 1.3270435387827701,
      "grad_norm": 0.7948507070541382,
      "learning_rate": 4.052775813295615e-05,
      "loss": 8.1193,
      "step": 4290
    },
    {
      "epoch": 1.3301368803650142,
      "grad_norm": 0.8348109126091003,
      "learning_rate": 4.050565770862801e-05,
      "loss": 8.1097,
      "step": 4300
    },
    {
      "epoch": 1.3332302219472585,
      "grad_norm": 1.6734665632247925,
      "learning_rate": 4.048355728429986e-05,
      "loss": 8.1013,
      "step": 4310
    },
    {
      "epoch": 1.3363235635295028,
      "grad_norm": 1.6595321893692017,
      "learning_rate": 4.046145685997171e-05,
      "loss": 8.1157,
      "step": 4320
    },
    {
      "epoch": 1.339416905111747,
      "grad_norm": 1.155346155166626,
      "learning_rate": 4.0439356435643564e-05,
      "loss": 8.1204,
      "step": 4330
    },
    {
      "epoch": 1.342510246693991,
      "grad_norm": 1.04776930809021,
      "learning_rate": 4.041725601131542e-05,
      "loss": 8.1105,
      "step": 4340
    },
    {
      "epoch": 1.3456035882762354,
      "grad_norm": 0.9083101153373718,
      "learning_rate": 4.039515558698727e-05,
      "loss": 8.1142,
      "step": 4350
    },
    {
      "epoch": 1.3486969298584797,
      "grad_norm": 0.7897149920463562,
      "learning_rate": 4.037305516265912e-05,
      "loss": 8.1239,
      "step": 4360
    },
    {
      "epoch": 1.351790271440724,
      "grad_norm": 0.9427816867828369,
      "learning_rate": 4.0350954738330974e-05,
      "loss": 8.1215,
      "step": 4370
    },
    {
      "epoch": 1.354883613022968,
      "grad_norm": 0.8597368597984314,
      "learning_rate": 4.0328854314002825e-05,
      "loss": 8.129,
      "step": 4380
    },
    {
      "epoch": 1.3579769546052123,
      "grad_norm": 0.7657512426376343,
      "learning_rate": 4.030675388967468e-05,
      "loss": 8.1156,
      "step": 4390
    },
    {
      "epoch": 1.3610702961874566,
      "grad_norm": 1.1744600534439087,
      "learning_rate": 4.028465346534654e-05,
      "loss": 8.1072,
      "step": 4400
    },
    {
      "epoch": 1.3641636377697006,
      "grad_norm": 0.7139284014701843,
      "learning_rate": 4.026255304101839e-05,
      "loss": 8.1136,
      "step": 4410
    },
    {
      "epoch": 1.367256979351945,
      "grad_norm": 0.9807798266410828,
      "learning_rate": 4.024045261669024e-05,
      "loss": 8.124,
      "step": 4420
    },
    {
      "epoch": 1.3703503209341892,
      "grad_norm": 0.5589386224746704,
      "learning_rate": 4.0218352192362093e-05,
      "loss": 8.1331,
      "step": 4430
    },
    {
      "epoch": 1.3734436625164332,
      "grad_norm": 0.8512092232704163,
      "learning_rate": 4.019625176803395e-05,
      "loss": 8.1161,
      "step": 4440
    },
    {
      "epoch": 1.3765370040986775,
      "grad_norm": 0.8046604990959167,
      "learning_rate": 4.01741513437058e-05,
      "loss": 8.1099,
      "step": 4450
    },
    {
      "epoch": 1.3796303456809218,
      "grad_norm": 0.9923661947250366,
      "learning_rate": 4.015205091937765e-05,
      "loss": 8.1189,
      "step": 4460
    },
    {
      "epoch": 1.382723687263166,
      "grad_norm": 1.1229900121688843,
      "learning_rate": 4.0129950495049504e-05,
      "loss": 8.117,
      "step": 4470
    },
    {
      "epoch": 1.3858170288454104,
      "grad_norm": 1.0397707223892212,
      "learning_rate": 4.010785007072136e-05,
      "loss": 8.1115,
      "step": 4480
    },
    {
      "epoch": 1.3889103704276544,
      "grad_norm": 1.102938175201416,
      "learning_rate": 4.008574964639321e-05,
      "loss": 8.1159,
      "step": 4490
    },
    {
      "epoch": 1.3920037120098987,
      "grad_norm": 0.8214883208274841,
      "learning_rate": 4.0063649222065064e-05,
      "loss": 8.1035,
      "step": 4500
    },
    {
      "epoch": 1.395097053592143,
      "grad_norm": 0.5585210919380188,
      "learning_rate": 4.0041548797736915e-05,
      "loss": 8.1115,
      "step": 4510
    },
    {
      "epoch": 1.398190395174387,
      "grad_norm": 0.7796432971954346,
      "learning_rate": 4.0019448373408765e-05,
      "loss": 8.1078,
      "step": 4520
    },
    {
      "epoch": 1.4012837367566313,
      "grad_norm": 1.1832889318466187,
      "learning_rate": 3.999734794908063e-05,
      "loss": 8.1274,
      "step": 4530
    },
    {
      "epoch": 1.4043770783388756,
      "grad_norm": 0.934551477432251,
      "learning_rate": 3.997524752475248e-05,
      "loss": 8.1287,
      "step": 4540
    },
    {
      "epoch": 1.4074704199211197,
      "grad_norm": 0.9180008172988892,
      "learning_rate": 3.995314710042433e-05,
      "loss": 8.105,
      "step": 4550
    },
    {
      "epoch": 1.410563761503364,
      "grad_norm": 1.7560917139053345,
      "learning_rate": 3.993104667609618e-05,
      "loss": 8.1259,
      "step": 4560
    },
    {
      "epoch": 1.4136571030856082,
      "grad_norm": 0.662970244884491,
      "learning_rate": 3.9908946251768034e-05,
      "loss": 8.129,
      "step": 4570
    },
    {
      "epoch": 1.4167504446678525,
      "grad_norm": 0.7053908109664917,
      "learning_rate": 3.988684582743989e-05,
      "loss": 8.1133,
      "step": 4580
    },
    {
      "epoch": 1.4198437862500968,
      "grad_norm": 0.7108381986618042,
      "learning_rate": 3.986474540311174e-05,
      "loss": 8.1014,
      "step": 4590
    },
    {
      "epoch": 1.4229371278323408,
      "grad_norm": 0.5228309035301208,
      "learning_rate": 3.984264497878359e-05,
      "loss": 8.0972,
      "step": 4600
    },
    {
      "epoch": 1.4260304694145851,
      "grad_norm": 1.6444404125213623,
      "learning_rate": 3.9820544554455444e-05,
      "loss": 8.1057,
      "step": 4610
    },
    {
      "epoch": 1.4291238109968294,
      "grad_norm": 1.3708868026733398,
      "learning_rate": 3.97984441301273e-05,
      "loss": 8.1136,
      "step": 4620
    },
    {
      "epoch": 1.4322171525790734,
      "grad_norm": 0.8128147125244141,
      "learning_rate": 3.977634370579915e-05,
      "loss": 8.1201,
      "step": 4630
    },
    {
      "epoch": 1.4353104941613177,
      "grad_norm": 0.6589893102645874,
      "learning_rate": 3.9754243281471004e-05,
      "loss": 8.1182,
      "step": 4640
    },
    {
      "epoch": 1.438403835743562,
      "grad_norm": 0.6600621342658997,
      "learning_rate": 3.9732142857142855e-05,
      "loss": 8.121,
      "step": 4650
    },
    {
      "epoch": 1.441497177325806,
      "grad_norm": 0.7272864580154419,
      "learning_rate": 3.971004243281471e-05,
      "loss": 8.1113,
      "step": 4660
    },
    {
      "epoch": 1.4445905189080503,
      "grad_norm": 0.5461433529853821,
      "learning_rate": 3.968794200848657e-05,
      "loss": 8.1205,
      "step": 4670
    },
    {
      "epoch": 1.4476838604902946,
      "grad_norm": 1.0180320739746094,
      "learning_rate": 3.966584158415842e-05,
      "loss": 8.1157,
      "step": 4680
    },
    {
      "epoch": 1.450777202072539,
      "grad_norm": 0.6926457285881042,
      "learning_rate": 3.964374115983027e-05,
      "loss": 8.108,
      "step": 4690
    },
    {
      "epoch": 1.4538705436547832,
      "grad_norm": 0.7072666883468628,
      "learning_rate": 3.962164073550212e-05,
      "loss": 8.1183,
      "step": 4700
    },
    {
      "epoch": 1.4569638852370272,
      "grad_norm": 0.7059903740882874,
      "learning_rate": 3.9599540311173974e-05,
      "loss": 8.1171,
      "step": 4710
    },
    {
      "epoch": 1.4600572268192715,
      "grad_norm": 0.957748293876648,
      "learning_rate": 3.957743988684583e-05,
      "loss": 8.1209,
      "step": 4720
    },
    {
      "epoch": 1.4631505684015158,
      "grad_norm": 1.2396856546401978,
      "learning_rate": 3.955533946251768e-05,
      "loss": 8.1166,
      "step": 4730
    },
    {
      "epoch": 1.4662439099837599,
      "grad_norm": 0.7316197752952576,
      "learning_rate": 3.9533239038189534e-05,
      "loss": 8.1132,
      "step": 4740
    },
    {
      "epoch": 1.4693372515660041,
      "grad_norm": 1.0521752834320068,
      "learning_rate": 3.9511138613861385e-05,
      "loss": 8.1125,
      "step": 4750
    },
    {
      "epoch": 1.4724305931482484,
      "grad_norm": 0.9110093116760254,
      "learning_rate": 3.948903818953324e-05,
      "loss": 8.1194,
      "step": 4760
    },
    {
      "epoch": 1.4755239347304925,
      "grad_norm": 1.4379953145980835,
      "learning_rate": 3.946693776520509e-05,
      "loss": 8.1168,
      "step": 4770
    },
    {
      "epoch": 1.4786172763127368,
      "grad_norm": 2.6326723098754883,
      "learning_rate": 3.9444837340876944e-05,
      "loss": 8.1228,
      "step": 4780
    },
    {
      "epoch": 1.481710617894981,
      "grad_norm": 0.9093007445335388,
      "learning_rate": 3.9422736916548795e-05,
      "loss": 8.1172,
      "step": 4790
    },
    {
      "epoch": 1.4848039594772253,
      "grad_norm": 1.3808073997497559,
      "learning_rate": 3.940063649222065e-05,
      "loss": 8.1097,
      "step": 4800
    },
    {
      "epoch": 1.4878973010594696,
      "grad_norm": 1.156030297279358,
      "learning_rate": 3.937853606789251e-05,
      "loss": 8.1163,
      "step": 4810
    },
    {
      "epoch": 1.4909906426417137,
      "grad_norm": 0.951022207736969,
      "learning_rate": 3.935643564356436e-05,
      "loss": 8.1125,
      "step": 4820
    },
    {
      "epoch": 1.494083984223958,
      "grad_norm": 1.0353246927261353,
      "learning_rate": 3.933433521923621e-05,
      "loss": 8.1266,
      "step": 4830
    },
    {
      "epoch": 1.4971773258062022,
      "grad_norm": 1.5673549175262451,
      "learning_rate": 3.9312234794908063e-05,
      "loss": 8.1071,
      "step": 4840
    },
    {
      "epoch": 1.5002706673884463,
      "grad_norm": 0.8361724019050598,
      "learning_rate": 3.9290134370579914e-05,
      "loss": 8.1019,
      "step": 4850
    },
    {
      "epoch": 1.5033640089706906,
      "grad_norm": 1.9008229970932007,
      "learning_rate": 3.926803394625177e-05,
      "loss": 8.1018,
      "step": 4860
    },
    {
      "epoch": 1.5064573505529348,
      "grad_norm": 1.5485745668411255,
      "learning_rate": 3.924593352192362e-05,
      "loss": 8.1134,
      "step": 4870
    },
    {
      "epoch": 1.509550692135179,
      "grad_norm": 1.1073501110076904,
      "learning_rate": 3.9223833097595474e-05,
      "loss": 8.1189,
      "step": 4880
    },
    {
      "epoch": 1.5126440337174234,
      "grad_norm": 0.9330412149429321,
      "learning_rate": 3.9201732673267325e-05,
      "loss": 8.1145,
      "step": 4890
    },
    {
      "epoch": 1.5157373752996675,
      "grad_norm": 1.2551467418670654,
      "learning_rate": 3.917963224893918e-05,
      "loss": 8.1187,
      "step": 4900
    },
    {
      "epoch": 1.5188307168819117,
      "grad_norm": 0.8344804048538208,
      "learning_rate": 3.9157531824611034e-05,
      "loss": 8.1093,
      "step": 4910
    },
    {
      "epoch": 1.521924058464156,
      "grad_norm": 0.6430570483207703,
      "learning_rate": 3.9135431400282885e-05,
      "loss": 8.1202,
      "step": 4920
    },
    {
      "epoch": 1.5250174000464,
      "grad_norm": 0.8347640037536621,
      "learning_rate": 3.911333097595474e-05,
      "loss": 8.0982,
      "step": 4930
    },
    {
      "epoch": 1.5281107416286444,
      "grad_norm": 0.781800389289856,
      "learning_rate": 3.909123055162659e-05,
      "loss": 8.1216,
      "step": 4940
    },
    {
      "epoch": 1.5312040832108886,
      "grad_norm": 0.928343653678894,
      "learning_rate": 3.906913012729845e-05,
      "loss": 8.1129,
      "step": 4950
    },
    {
      "epoch": 1.5342974247931327,
      "grad_norm": 1.2262492179870605,
      "learning_rate": 3.90470297029703e-05,
      "loss": 8.1268,
      "step": 4960
    },
    {
      "epoch": 1.537390766375377,
      "grad_norm": 1.014028549194336,
      "learning_rate": 3.902492927864215e-05,
      "loss": 8.1158,
      "step": 4970
    },
    {
      "epoch": 1.5404841079576213,
      "grad_norm": 0.7858170866966248,
      "learning_rate": 3.9002828854314004e-05,
      "loss": 8.1221,
      "step": 4980
    },
    {
      "epoch": 1.5435774495398653,
      "grad_norm": 0.6599904894828796,
      "learning_rate": 3.8980728429985855e-05,
      "loss": 8.1171,
      "step": 4990
    },
    {
      "epoch": 1.5466707911221098,
      "grad_norm": 1.3178737163543701,
      "learning_rate": 3.895862800565771e-05,
      "loss": 8.1041,
      "step": 5000
    },
    {
      "epoch": 1.5497641327043539,
      "grad_norm": 1.6083639860153198,
      "learning_rate": 3.8936527581329563e-05,
      "loss": 8.1254,
      "step": 5010
    },
    {
      "epoch": 1.5528574742865981,
      "grad_norm": 1.5805290937423706,
      "learning_rate": 3.8914427157001414e-05,
      "loss": 8.0958,
      "step": 5020
    },
    {
      "epoch": 1.5559508158688424,
      "grad_norm": 1.2158139944076538,
      "learning_rate": 3.8892326732673265e-05,
      "loss": 8.1145,
      "step": 5030
    },
    {
      "epoch": 1.5590441574510865,
      "grad_norm": 0.7423880696296692,
      "learning_rate": 3.887022630834512e-05,
      "loss": 8.1075,
      "step": 5040
    },
    {
      "epoch": 1.5621374990333308,
      "grad_norm": 1.2053617238998413,
      "learning_rate": 3.8848125884016974e-05,
      "loss": 8.1019,
      "step": 5050
    },
    {
      "epoch": 1.565230840615575,
      "grad_norm": 0.5607873201370239,
      "learning_rate": 3.882602545968883e-05,
      "loss": 8.1265,
      "step": 5060
    },
    {
      "epoch": 1.568324182197819,
      "grad_norm": 0.7237718105316162,
      "learning_rate": 3.880392503536068e-05,
      "loss": 8.0937,
      "step": 5070
    },
    {
      "epoch": 1.5714175237800634,
      "grad_norm": 1.6400437355041504,
      "learning_rate": 3.8781824611032534e-05,
      "loss": 8.1132,
      "step": 5080
    },
    {
      "epoch": 1.5745108653623077,
      "grad_norm": 0.618736982345581,
      "learning_rate": 3.875972418670439e-05,
      "loss": 8.1222,
      "step": 5090
    },
    {
      "epoch": 1.5776042069445517,
      "grad_norm": 1.4394365549087524,
      "learning_rate": 3.873762376237624e-05,
      "loss": 8.1201,
      "step": 5100
    },
    {
      "epoch": 1.5806975485267962,
      "grad_norm": 1.4025369882583618,
      "learning_rate": 3.871552333804809e-05,
      "loss": 8.1226,
      "step": 5110
    },
    {
      "epoch": 1.5837908901090403,
      "grad_norm": 0.745320737361908,
      "learning_rate": 3.8693422913719944e-05,
      "loss": 8.1075,
      "step": 5120
    },
    {
      "epoch": 1.5868842316912846,
      "grad_norm": 0.6909487247467041,
      "learning_rate": 3.8671322489391795e-05,
      "loss": 8.1062,
      "step": 5130
    },
    {
      "epoch": 1.5899775732735288,
      "grad_norm": 1.0641508102416992,
      "learning_rate": 3.864922206506365e-05,
      "loss": 8.1132,
      "step": 5140
    },
    {
      "epoch": 1.593070914855773,
      "grad_norm": 1.2837423086166382,
      "learning_rate": 3.8627121640735504e-05,
      "loss": 8.1281,
      "step": 5150
    },
    {
      "epoch": 1.5961642564380172,
      "grad_norm": 0.9686927795410156,
      "learning_rate": 3.8605021216407355e-05,
      "loss": 8.111,
      "step": 5160
    },
    {
      "epoch": 1.5992575980202615,
      "grad_norm": 0.5163270235061646,
      "learning_rate": 3.8582920792079206e-05,
      "loss": 8.1001,
      "step": 5170
    },
    {
      "epoch": 1.6023509396025055,
      "grad_norm": 0.7700867652893066,
      "learning_rate": 3.856082036775106e-05,
      "loss": 8.1198,
      "step": 5180
    },
    {
      "epoch": 1.6054442811847498,
      "grad_norm": 1.0453859567642212,
      "learning_rate": 3.8538719943422914e-05,
      "loss": 8.114,
      "step": 5190
    },
    {
      "epoch": 1.608537622766994,
      "grad_norm": 1.1160624027252197,
      "learning_rate": 3.851661951909477e-05,
      "loss": 8.1098,
      "step": 5200
    },
    {
      "epoch": 1.6116309643492381,
      "grad_norm": 0.7706289887428284,
      "learning_rate": 3.849451909476662e-05,
      "loss": 8.1194,
      "step": 5210
    },
    {
      "epoch": 1.6147243059314826,
      "grad_norm": 1.5528191328048706,
      "learning_rate": 3.8472418670438474e-05,
      "loss": 8.0974,
      "step": 5220
    },
    {
      "epoch": 1.6178176475137267,
      "grad_norm": 1.085829257965088,
      "learning_rate": 3.845031824611033e-05,
      "loss": 8.1216,
      "step": 5230
    },
    {
      "epoch": 1.620910989095971,
      "grad_norm": 1.0217398405075073,
      "learning_rate": 3.842821782178218e-05,
      "loss": 8.1287,
      "step": 5240
    },
    {
      "epoch": 1.6240043306782153,
      "grad_norm": 1.269791603088379,
      "learning_rate": 3.8406117397454033e-05,
      "loss": 8.1177,
      "step": 5250
    },
    {
      "epoch": 1.6270976722604593,
      "grad_norm": 0.6634871363639832,
      "learning_rate": 3.8384016973125884e-05,
      "loss": 8.1193,
      "step": 5260
    },
    {
      "epoch": 1.6301910138427036,
      "grad_norm": 0.7631880044937134,
      "learning_rate": 3.8361916548797735e-05,
      "loss": 8.1177,
      "step": 5270
    },
    {
      "epoch": 1.6332843554249479,
      "grad_norm": 1.3291696310043335,
      "learning_rate": 3.833981612446959e-05,
      "loss": 8.1398,
      "step": 5280
    },
    {
      "epoch": 1.636377697007192,
      "grad_norm": 0.8691796660423279,
      "learning_rate": 3.8317715700141444e-05,
      "loss": 8.1067,
      "step": 5290
    },
    {
      "epoch": 1.6394710385894362,
      "grad_norm": 0.931352436542511,
      "learning_rate": 3.8295615275813295e-05,
      "loss": 8.1137,
      "step": 5300
    },
    {
      "epoch": 1.6425643801716805,
      "grad_norm": 0.9599955081939697,
      "learning_rate": 3.8273514851485146e-05,
      "loss": 8.1128,
      "step": 5310
    },
    {
      "epoch": 1.6456577217539246,
      "grad_norm": 0.7459596395492554,
      "learning_rate": 3.8251414427157004e-05,
      "loss": 8.109,
      "step": 5320
    },
    {
      "epoch": 1.648751063336169,
      "grad_norm": 0.8625908493995667,
      "learning_rate": 3.822931400282886e-05,
      "loss": 8.0995,
      "step": 5330
    },
    {
      "epoch": 1.6518444049184131,
      "grad_norm": 1.3661296367645264,
      "learning_rate": 3.820721357850071e-05,
      "loss": 8.1041,
      "step": 5340
    },
    {
      "epoch": 1.6549377465006572,
      "grad_norm": 1.1211706399917603,
      "learning_rate": 3.818511315417256e-05,
      "loss": 8.1119,
      "step": 5350
    },
    {
      "epoch": 1.6580310880829017,
      "grad_norm": 0.6514363288879395,
      "learning_rate": 3.8163012729844414e-05,
      "loss": 8.1057,
      "step": 5360
    },
    {
      "epoch": 1.6611244296651457,
      "grad_norm": 0.8911955952644348,
      "learning_rate": 3.814091230551627e-05,
      "loss": 8.1198,
      "step": 5370
    },
    {
      "epoch": 1.66421777124739,
      "grad_norm": 1.1355024576187134,
      "learning_rate": 3.811881188118812e-05,
      "loss": 8.1219,
      "step": 5380
    },
    {
      "epoch": 1.6673111128296343,
      "grad_norm": 0.9677493572235107,
      "learning_rate": 3.8096711456859974e-05,
      "loss": 8.1104,
      "step": 5390
    },
    {
      "epoch": 1.6704044544118783,
      "grad_norm": 0.7535940408706665,
      "learning_rate": 3.8074611032531825e-05,
      "loss": 8.1196,
      "step": 5400
    },
    {
      "epoch": 1.6734977959941226,
      "grad_norm": 0.5847275257110596,
      "learning_rate": 3.8052510608203676e-05,
      "loss": 8.1004,
      "step": 5410
    },
    {
      "epoch": 1.676591137576367,
      "grad_norm": 0.885806143283844,
      "learning_rate": 3.8030410183875533e-05,
      "loss": 8.0886,
      "step": 5420
    },
    {
      "epoch": 1.679684479158611,
      "grad_norm": 1.359310269355774,
      "learning_rate": 3.8008309759547384e-05,
      "loss": 8.0951,
      "step": 5430
    },
    {
      "epoch": 1.6827778207408555,
      "grad_norm": 2.5270769596099854,
      "learning_rate": 3.7986209335219235e-05,
      "loss": 8.1231,
      "step": 5440
    },
    {
      "epoch": 1.6858711623230995,
      "grad_norm": 1.284864068031311,
      "learning_rate": 3.7964108910891086e-05,
      "loss": 8.1244,
      "step": 5450
    },
    {
      "epoch": 1.6889645039053436,
      "grad_norm": 1.3578400611877441,
      "learning_rate": 3.7942008486562944e-05,
      "loss": 8.1189,
      "step": 5460
    },
    {
      "epoch": 1.692057845487588,
      "grad_norm": 2.665391683578491,
      "learning_rate": 3.79199080622348e-05,
      "loss": 8.1234,
      "step": 5470
    },
    {
      "epoch": 1.6951511870698321,
      "grad_norm": 0.6308792233467102,
      "learning_rate": 3.789780763790665e-05,
      "loss": 8.1219,
      "step": 5480
    },
    {
      "epoch": 1.6982445286520764,
      "grad_norm": 1.0213325023651123,
      "learning_rate": 3.7875707213578504e-05,
      "loss": 8.1212,
      "step": 5490
    },
    {
      "epoch": 1.7013378702343207,
      "grad_norm": 1.4457077980041504,
      "learning_rate": 3.7853606789250355e-05,
      "loss": 8.1045,
      "step": 5500
    },
    {
      "epoch": 1.7044312118165648,
      "grad_norm": 0.6500237584114075,
      "learning_rate": 3.783150636492221e-05,
      "loss": 8.1071,
      "step": 5510
    },
    {
      "epoch": 1.707524553398809,
      "grad_norm": 0.9573272466659546,
      "learning_rate": 3.780940594059406e-05,
      "loss": 8.1274,
      "step": 5520
    },
    {
      "epoch": 1.7106178949810533,
      "grad_norm": 0.8521197438240051,
      "learning_rate": 3.7787305516265914e-05,
      "loss": 8.1037,
      "step": 5530
    },
    {
      "epoch": 1.7137112365632974,
      "grad_norm": 0.679905891418457,
      "learning_rate": 3.7765205091937765e-05,
      "loss": 8.1103,
      "step": 5540
    },
    {
      "epoch": 1.7168045781455419,
      "grad_norm": 0.9739455580711365,
      "learning_rate": 3.7743104667609616e-05,
      "loss": 8.1163,
      "step": 5550
    },
    {
      "epoch": 1.719897919727786,
      "grad_norm": 1.4107309579849243,
      "learning_rate": 3.7721004243281474e-05,
      "loss": 8.1068,
      "step": 5560
    },
    {
      "epoch": 1.72299126131003,
      "grad_norm": 0.9260146617889404,
      "learning_rate": 3.7698903818953325e-05,
      "loss": 8.1167,
      "step": 5570
    },
    {
      "epoch": 1.7260846028922745,
      "grad_norm": 0.6201887130737305,
      "learning_rate": 3.7676803394625176e-05,
      "loss": 8.1159,
      "step": 5580
    },
    {
      "epoch": 1.7291779444745186,
      "grad_norm": 0.917417049407959,
      "learning_rate": 3.7654702970297027e-05,
      "loss": 8.106,
      "step": 5590
    },
    {
      "epoch": 1.7322712860567628,
      "grad_norm": 1.094301462173462,
      "learning_rate": 3.7632602545968884e-05,
      "loss": 8.1308,
      "step": 5600
    },
    {
      "epoch": 1.7353646276390071,
      "grad_norm": 0.8865593075752258,
      "learning_rate": 3.7612712164073554e-05,
      "loss": 8.1141,
      "step": 5610
    },
    {
      "epoch": 1.7384579692212512,
      "grad_norm": 2.1721909046173096,
      "learning_rate": 3.7590611739745405e-05,
      "loss": 8.1198,
      "step": 5620
    },
    {
      "epoch": 1.7415513108034955,
      "grad_norm": 2.3732712268829346,
      "learning_rate": 3.756851131541726e-05,
      "loss": 8.1197,
      "step": 5630
    },
    {
      "epoch": 1.7446446523857397,
      "grad_norm": 1.7584190368652344,
      "learning_rate": 3.7546410891089114e-05,
      "loss": 8.1233,
      "step": 5640
    },
    {
      "epoch": 1.7477379939679838,
      "grad_norm": 0.9037502408027649,
      "learning_rate": 3.7524310466760965e-05,
      "loss": 8.1121,
      "step": 5650
    },
    {
      "epoch": 1.7508313355502283,
      "grad_norm": 1.0689423084259033,
      "learning_rate": 3.7502210042432816e-05,
      "loss": 8.1178,
      "step": 5660
    },
    {
      "epoch": 1.7539246771324724,
      "grad_norm": 0.8617199063301086,
      "learning_rate": 3.748010961810467e-05,
      "loss": 8.1235,
      "step": 5670
    },
    {
      "epoch": 1.7570180187147164,
      "grad_norm": 0.687280535697937,
      "learning_rate": 3.7458009193776524e-05,
      "loss": 8.1259,
      "step": 5680
    },
    {
      "epoch": 1.760111360296961,
      "grad_norm": 0.6172989010810852,
      "learning_rate": 3.7435908769448375e-05,
      "loss": 8.1163,
      "step": 5690
    },
    {
      "epoch": 1.763204701879205,
      "grad_norm": 0.7951784133911133,
      "learning_rate": 3.7413808345120226e-05,
      "loss": 8.1104,
      "step": 5700
    },
    {
      "epoch": 1.7662980434614493,
      "grad_norm": 1.39143705368042,
      "learning_rate": 3.739170792079208e-05,
      "loss": 8.1237,
      "step": 5710
    },
    {
      "epoch": 1.7693913850436935,
      "grad_norm": 0.688704252243042,
      "learning_rate": 3.7369607496463935e-05,
      "loss": 8.1073,
      "step": 5720
    },
    {
      "epoch": 1.7724847266259376,
      "grad_norm": 0.890242338180542,
      "learning_rate": 3.7347507072135786e-05,
      "loss": 8.1066,
      "step": 5730
    },
    {
      "epoch": 1.7755780682081819,
      "grad_norm": 0.5613893866539001,
      "learning_rate": 3.732540664780764e-05,
      "loss": 8.1179,
      "step": 5740
    },
    {
      "epoch": 1.7786714097904262,
      "grad_norm": 1.1916487216949463,
      "learning_rate": 3.7303306223479495e-05,
      "loss": 8.101,
      "step": 5750
    },
    {
      "epoch": 1.7817647513726702,
      "grad_norm": 0.8538577556610107,
      "learning_rate": 3.7281205799151345e-05,
      "loss": 8.1059,
      "step": 5760
    },
    {
      "epoch": 1.7848580929549147,
      "grad_norm": 0.7381499409675598,
      "learning_rate": 3.72591053748232e-05,
      "loss": 8.1159,
      "step": 5770
    },
    {
      "epoch": 1.7879514345371588,
      "grad_norm": 0.6222342848777771,
      "learning_rate": 3.7237004950495054e-05,
      "loss": 8.1096,
      "step": 5780
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 0.6624096035957336,
      "learning_rate": 3.7214904526166905e-05,
      "loss": 8.1123,
      "step": 5790
    },
    {
      "epoch": 1.7941381177016473,
      "grad_norm": 1.1382503509521484,
      "learning_rate": 3.7192804101838756e-05,
      "loss": 8.111,
      "step": 5800
    },
    {
      "epoch": 1.7972314592838914,
      "grad_norm": 0.8287107348442078,
      "learning_rate": 3.717070367751061e-05,
      "loss": 8.1129,
      "step": 5810
    },
    {
      "epoch": 1.8003248008661357,
      "grad_norm": 0.6545336842536926,
      "learning_rate": 3.7148603253182465e-05,
      "loss": 8.1124,
      "step": 5820
    },
    {
      "epoch": 1.80341814244838,
      "grad_norm": 1.1369034051895142,
      "learning_rate": 3.7126502828854316e-05,
      "loss": 8.1055,
      "step": 5830
    },
    {
      "epoch": 1.806511484030624,
      "grad_norm": 1.0850688219070435,
      "learning_rate": 3.7104402404526167e-05,
      "loss": 8.1012,
      "step": 5840
    },
    {
      "epoch": 1.8096048256128683,
      "grad_norm": 0.6212559938430786,
      "learning_rate": 3.708230198019802e-05,
      "loss": 8.1106,
      "step": 5850
    },
    {
      "epoch": 1.8126981671951126,
      "grad_norm": 1.202318549156189,
      "learning_rate": 3.7060201555869875e-05,
      "loss": 8.1227,
      "step": 5860
    },
    {
      "epoch": 1.8157915087773566,
      "grad_norm": 1.0532689094543457,
      "learning_rate": 3.7038101131541726e-05,
      "loss": 8.0981,
      "step": 5870
    },
    {
      "epoch": 1.8188848503596011,
      "grad_norm": 1.1592659950256348,
      "learning_rate": 3.7016000707213584e-05,
      "loss": 8.1151,
      "step": 5880
    },
    {
      "epoch": 1.8219781919418452,
      "grad_norm": 0.48596423864364624,
      "learning_rate": 3.6993900282885435e-05,
      "loss": 8.1205,
      "step": 5890
    },
    {
      "epoch": 1.8250715335240892,
      "grad_norm": 0.6388185024261475,
      "learning_rate": 3.6971799858557286e-05,
      "loss": 8.1025,
      "step": 5900
    },
    {
      "epoch": 1.8281648751063337,
      "grad_norm": 0.7377865314483643,
      "learning_rate": 3.6949699434229143e-05,
      "loss": 8.0985,
      "step": 5910
    },
    {
      "epoch": 1.8312582166885778,
      "grad_norm": 0.7783490419387817,
      "learning_rate": 3.6927599009900994e-05,
      "loss": 8.1144,
      "step": 5920
    },
    {
      "epoch": 1.834351558270822,
      "grad_norm": 0.6981886029243469,
      "learning_rate": 3.6905498585572845e-05,
      "loss": 8.1129,
      "step": 5930
    },
    {
      "epoch": 1.8374448998530664,
      "grad_norm": 1.4069945812225342,
      "learning_rate": 3.6883398161244696e-05,
      "loss": 8.121,
      "step": 5940
    },
    {
      "epoch": 1.8405382414353104,
      "grad_norm": 0.6732660531997681,
      "learning_rate": 3.686129773691655e-05,
      "loss": 8.1234,
      "step": 5950
    },
    {
      "epoch": 1.8436315830175547,
      "grad_norm": 0.8745042681694031,
      "learning_rate": 3.6839197312588405e-05,
      "loss": 8.1142,
      "step": 5960
    },
    {
      "epoch": 1.846724924599799,
      "grad_norm": 0.6202366352081299,
      "learning_rate": 3.6817096888260256e-05,
      "loss": 8.1145,
      "step": 5970
    },
    {
      "epoch": 1.849818266182043,
      "grad_norm": 0.8213227391242981,
      "learning_rate": 3.679499646393211e-05,
      "loss": 8.1017,
      "step": 5980
    },
    {
      "epoch": 1.8529116077642873,
      "grad_norm": 0.5994757413864136,
      "learning_rate": 3.677289603960396e-05,
      "loss": 8.1097,
      "step": 5990
    },
    {
      "epoch": 1.8560049493465316,
      "grad_norm": 1.21660315990448,
      "learning_rate": 3.6750795615275816e-05,
      "loss": 8.0982,
      "step": 6000
    },
    {
      "epoch": 1.8590982909287757,
      "grad_norm": 0.48843103647232056,
      "learning_rate": 3.672869519094767e-05,
      "loss": 8.1246,
      "step": 6010
    },
    {
      "epoch": 1.8621916325110202,
      "grad_norm": 0.7663419246673584,
      "learning_rate": 3.6706594766619524e-05,
      "loss": 8.1105,
      "step": 6020
    },
    {
      "epoch": 1.8652849740932642,
      "grad_norm": 0.4496191740036011,
      "learning_rate": 3.6684494342291375e-05,
      "loss": 8.1142,
      "step": 6030
    },
    {
      "epoch": 1.8683783156755085,
      "grad_norm": 0.770578920841217,
      "learning_rate": 3.6662393917963226e-05,
      "loss": 8.1236,
      "step": 6040
    },
    {
      "epoch": 1.8714716572577528,
      "grad_norm": 1.0419385433197021,
      "learning_rate": 3.6640293493635084e-05,
      "loss": 8.1019,
      "step": 6050
    },
    {
      "epoch": 1.8745649988399968,
      "grad_norm": 0.9988660216331482,
      "learning_rate": 3.6618193069306935e-05,
      "loss": 8.119,
      "step": 6060
    },
    {
      "epoch": 1.8776583404222411,
      "grad_norm": 0.7319515347480774,
      "learning_rate": 3.6596092644978786e-05,
      "loss": 8.1195,
      "step": 6070
    },
    {
      "epoch": 1.8807516820044854,
      "grad_norm": 0.826828122138977,
      "learning_rate": 3.657399222065064e-05,
      "loss": 8.1205,
      "step": 6080
    },
    {
      "epoch": 1.8838450235867295,
      "grad_norm": 0.4903888702392578,
      "learning_rate": 3.655189179632249e-05,
      "loss": 8.107,
      "step": 6090
    },
    {
      "epoch": 1.8869383651689737,
      "grad_norm": 0.757673442363739,
      "learning_rate": 3.6529791371994345e-05,
      "loss": 8.0938,
      "step": 6100
    },
    {
      "epoch": 1.890031706751218,
      "grad_norm": 0.9070502519607544,
      "learning_rate": 3.6507690947666196e-05,
      "loss": 8.1064,
      "step": 6110
    },
    {
      "epoch": 1.893125048333462,
      "grad_norm": 0.5928594470024109,
      "learning_rate": 3.648559052333805e-05,
      "loss": 8.1188,
      "step": 6120
    },
    {
      "epoch": 1.8962183899157066,
      "grad_norm": 0.6357729434967041,
      "learning_rate": 3.64634900990099e-05,
      "loss": 8.0994,
      "step": 6130
    },
    {
      "epoch": 1.8993117314979506,
      "grad_norm": 0.7452264428138733,
      "learning_rate": 3.6441389674681756e-05,
      "loss": 8.1156,
      "step": 6140
    },
    {
      "epoch": 1.902405073080195,
      "grad_norm": 0.7934433817863464,
      "learning_rate": 3.6419289250353614e-05,
      "loss": 8.1179,
      "step": 6150
    },
    {
      "epoch": 1.9054984146624392,
      "grad_norm": 1.0771790742874146,
      "learning_rate": 3.6397188826025465e-05,
      "loss": 8.099,
      "step": 6160
    },
    {
      "epoch": 1.9085917562446832,
      "grad_norm": 0.8761550784111023,
      "learning_rate": 3.6375088401697315e-05,
      "loss": 8.1045,
      "step": 6170
    },
    {
      "epoch": 1.9116850978269275,
      "grad_norm": 0.6440709829330444,
      "learning_rate": 3.6352987977369166e-05,
      "loss": 8.1169,
      "step": 6180
    },
    {
      "epoch": 1.9147784394091718,
      "grad_norm": 0.9463909268379211,
      "learning_rate": 3.6330887553041024e-05,
      "loss": 8.1157,
      "step": 6190
    },
    {
      "epoch": 1.9178717809914159,
      "grad_norm": 0.8317248225212097,
      "learning_rate": 3.6308787128712875e-05,
      "loss": 8.1227,
      "step": 6200
    },
    {
      "epoch": 1.9209651225736601,
      "grad_norm": 0.7556623816490173,
      "learning_rate": 3.6286686704384726e-05,
      "loss": 8.1101,
      "step": 6210
    },
    {
      "epoch": 1.9240584641559044,
      "grad_norm": 1.0234439373016357,
      "learning_rate": 3.626458628005658e-05,
      "loss": 8.1006,
      "step": 6220
    },
    {
      "epoch": 1.9271518057381485,
      "grad_norm": 0.7974225878715515,
      "learning_rate": 3.624248585572843e-05,
      "loss": 8.1144,
      "step": 6230
    },
    {
      "epoch": 1.930245147320393,
      "grad_norm": 1.123907446861267,
      "learning_rate": 3.6220385431400286e-05,
      "loss": 8.1249,
      "step": 6240
    },
    {
      "epoch": 1.933338488902637,
      "grad_norm": 1.000357985496521,
      "learning_rate": 3.6198285007072137e-05,
      "loss": 8.0901,
      "step": 6250
    },
    {
      "epoch": 1.9364318304848813,
      "grad_norm": 1.2129359245300293,
      "learning_rate": 3.617618458274399e-05,
      "loss": 8.1148,
      "step": 6260
    },
    {
      "epoch": 1.9395251720671256,
      "grad_norm": 0.7437799572944641,
      "learning_rate": 3.615408415841584e-05,
      "loss": 8.1119,
      "step": 6270
    },
    {
      "epoch": 1.9426185136493697,
      "grad_norm": 1.0087193250656128,
      "learning_rate": 3.6131983734087696e-05,
      "loss": 8.1252,
      "step": 6280
    },
    {
      "epoch": 1.945711855231614,
      "grad_norm": 1.2354352474212646,
      "learning_rate": 3.6109883309759554e-05,
      "loss": 8.1075,
      "step": 6290
    },
    {
      "epoch": 1.9488051968138582,
      "grad_norm": 1.126196265220642,
      "learning_rate": 3.6087782885431405e-05,
      "loss": 8.1118,
      "step": 6300
    },
    {
      "epoch": 1.9518985383961023,
      "grad_norm": 0.8466517329216003,
      "learning_rate": 3.6065682461103256e-05,
      "loss": 8.1117,
      "step": 6310
    },
    {
      "epoch": 1.9549918799783466,
      "grad_norm": 0.7501287460327148,
      "learning_rate": 3.604358203677511e-05,
      "loss": 8.0991,
      "step": 6320
    },
    {
      "epoch": 1.9580852215605908,
      "grad_norm": 1.0328483581542969,
      "learning_rate": 3.6021481612446964e-05,
      "loss": 8.1006,
      "step": 6330
    },
    {
      "epoch": 1.961178563142835,
      "grad_norm": 1.1136817932128906,
      "learning_rate": 3.5999381188118815e-05,
      "loss": 8.1184,
      "step": 6340
    },
    {
      "epoch": 1.9642719047250794,
      "grad_norm": 1.0115580558776855,
      "learning_rate": 3.5977280763790666e-05,
      "loss": 8.1086,
      "step": 6350
    },
    {
      "epoch": 1.9673652463073235,
      "grad_norm": 0.8790600895881653,
      "learning_rate": 3.595518033946252e-05,
      "loss": 8.1123,
      "step": 6360
    },
    {
      "epoch": 1.9704585878895677,
      "grad_norm": 0.6049520969390869,
      "learning_rate": 3.593307991513437e-05,
      "loss": 8.1193,
      "step": 6370
    },
    {
      "epoch": 1.973551929471812,
      "grad_norm": 1.1578625440597534,
      "learning_rate": 3.5910979490806226e-05,
      "loss": 8.1196,
      "step": 6380
    },
    {
      "epoch": 1.976645271054056,
      "grad_norm": 0.6569232940673828,
      "learning_rate": 3.588887906647808e-05,
      "loss": 8.11,
      "step": 6390
    },
    {
      "epoch": 1.9797386126363004,
      "grad_norm": 0.8159201741218567,
      "learning_rate": 3.586677864214993e-05,
      "loss": 8.1017,
      "step": 6400
    },
    {
      "epoch": 1.9828319542185446,
      "grad_norm": 0.8416740298271179,
      "learning_rate": 3.5844678217821786e-05,
      "loss": 8.1226,
      "step": 6410
    },
    {
      "epoch": 1.9859252958007887,
      "grad_norm": 0.7170424461364746,
      "learning_rate": 3.5822577793493636e-05,
      "loss": 8.1122,
      "step": 6420
    },
    {
      "epoch": 1.989018637383033,
      "grad_norm": 0.48569872975349426,
      "learning_rate": 3.5800477369165494e-05,
      "loss": 8.1162,
      "step": 6430
    },
    {
      "epoch": 1.9921119789652773,
      "grad_norm": 0.9472678303718567,
      "learning_rate": 3.5778376944837345e-05,
      "loss": 8.1127,
      "step": 6440
    },
    {
      "epoch": 1.9952053205475213,
      "grad_norm": 0.8315709829330444,
      "learning_rate": 3.5756276520509196e-05,
      "loss": 8.1198,
      "step": 6450
    },
    {
      "epoch": 1.9982986621297658,
      "grad_norm": 0.8758796453475952,
      "learning_rate": 3.573417609618105e-05,
      "loss": 8.1063,
      "step": 6460
    },
    {
      "epoch": 2.00139200371201,
      "grad_norm": 1.0007539987564087,
      "learning_rate": 3.5712075671852905e-05,
      "loss": 8.1037,
      "step": 6470
    },
    {
      "epoch": 2.004485345294254,
      "grad_norm": 0.8656543493270874,
      "learning_rate": 3.5689975247524756e-05,
      "loss": 8.1169,
      "step": 6480
    },
    {
      "epoch": 2.0075786868764984,
      "grad_norm": 0.5615689754486084,
      "learning_rate": 3.566787482319661e-05,
      "loss": 8.1057,
      "step": 6490
    },
    {
      "epoch": 2.0106720284587425,
      "grad_norm": 0.669521689414978,
      "learning_rate": 3.564577439886846e-05,
      "loss": 8.1122,
      "step": 6500
    },
    {
      "epoch": 2.013765370040987,
      "grad_norm": 0.8987202644348145,
      "learning_rate": 3.562367397454031e-05,
      "loss": 8.1,
      "step": 6510
    },
    {
      "epoch": 2.016858711623231,
      "grad_norm": 0.8017703890800476,
      "learning_rate": 3.5601573550212166e-05,
      "loss": 8.1146,
      "step": 6520
    },
    {
      "epoch": 2.019952053205475,
      "grad_norm": 1.0373128652572632,
      "learning_rate": 3.557947312588402e-05,
      "loss": 8.1044,
      "step": 6530
    },
    {
      "epoch": 2.0230453947877196,
      "grad_norm": 1.2457104921340942,
      "learning_rate": 3.555737270155587e-05,
      "loss": 8.1045,
      "step": 6540
    },
    {
      "epoch": 2.0261387363699637,
      "grad_norm": 0.6558496952056885,
      "learning_rate": 3.5535272277227726e-05,
      "loss": 8.1138,
      "step": 6550
    },
    {
      "epoch": 2.0292320779522077,
      "grad_norm": 0.5642615556716919,
      "learning_rate": 3.551317185289958e-05,
      "loss": 8.107,
      "step": 6560
    },
    {
      "epoch": 2.0323254195344522,
      "grad_norm": 0.7868329882621765,
      "learning_rate": 3.5491071428571435e-05,
      "loss": 8.1063,
      "step": 6570
    },
    {
      "epoch": 2.0354187611166963,
      "grad_norm": 0.7656639814376831,
      "learning_rate": 3.5468971004243285e-05,
      "loss": 8.1114,
      "step": 6580
    },
    {
      "epoch": 2.0385121026989403,
      "grad_norm": 0.6155396103858948,
      "learning_rate": 3.5446870579915136e-05,
      "loss": 8.1103,
      "step": 6590
    },
    {
      "epoch": 2.041605444281185,
      "grad_norm": 0.8541555404663086,
      "learning_rate": 3.542477015558699e-05,
      "loss": 8.1207,
      "step": 6600
    },
    {
      "epoch": 2.044698785863429,
      "grad_norm": 0.9370625019073486,
      "learning_rate": 3.5402669731258845e-05,
      "loss": 8.1017,
      "step": 6610
    },
    {
      "epoch": 2.0477921274456734,
      "grad_norm": 0.7069550156593323,
      "learning_rate": 3.5380569306930696e-05,
      "loss": 8.1038,
      "step": 6620
    },
    {
      "epoch": 2.0508854690279175,
      "grad_norm": 0.9374760389328003,
      "learning_rate": 3.535846888260255e-05,
      "loss": 8.1098,
      "step": 6630
    },
    {
      "epoch": 2.0539788106101615,
      "grad_norm": 0.8458296060562134,
      "learning_rate": 3.53363684582744e-05,
      "loss": 8.1001,
      "step": 6640
    },
    {
      "epoch": 2.057072152192406,
      "grad_norm": 0.8135137557983398,
      "learning_rate": 3.531426803394625e-05,
      "loss": 8.1168,
      "step": 6650
    },
    {
      "epoch": 2.06016549377465,
      "grad_norm": 0.9663774967193604,
      "learning_rate": 3.5292167609618107e-05,
      "loss": 8.1215,
      "step": 6660
    },
    {
      "epoch": 2.063258835356894,
      "grad_norm": 1.0900077819824219,
      "learning_rate": 3.527006718528996e-05,
      "loss": 8.0952,
      "step": 6670
    },
    {
      "epoch": 2.0663521769391386,
      "grad_norm": 0.9920023679733276,
      "learning_rate": 3.5247966760961815e-05,
      "loss": 8.0943,
      "step": 6680
    },
    {
      "epoch": 2.0694455185213827,
      "grad_norm": 1.369418740272522,
      "learning_rate": 3.5225866336633666e-05,
      "loss": 8.1113,
      "step": 6690
    },
    {
      "epoch": 2.0725388601036268,
      "grad_norm": 1.2087135314941406,
      "learning_rate": 3.520376591230552e-05,
      "loss": 8.1073,
      "step": 6700
    },
    {
      "epoch": 2.0756322016858713,
      "grad_norm": 1.1333025693893433,
      "learning_rate": 3.5181665487977375e-05,
      "loss": 8.107,
      "step": 6710
    },
    {
      "epoch": 2.0787255432681153,
      "grad_norm": 0.5594799518585205,
      "learning_rate": 3.5159565063649226e-05,
      "loss": 8.0972,
      "step": 6720
    },
    {
      "epoch": 2.0818188848503594,
      "grad_norm": 0.7939359545707703,
      "learning_rate": 3.513746463932108e-05,
      "loss": 8.0981,
      "step": 6730
    },
    {
      "epoch": 2.084912226432604,
      "grad_norm": 0.7826473712921143,
      "learning_rate": 3.511536421499293e-05,
      "loss": 8.128,
      "step": 6740
    },
    {
      "epoch": 2.088005568014848,
      "grad_norm": 0.6012619137763977,
      "learning_rate": 3.5093263790664785e-05,
      "loss": 8.095,
      "step": 6750
    },
    {
      "epoch": 2.0910989095970924,
      "grad_norm": 0.8687459230422974,
      "learning_rate": 3.5071163366336636e-05,
      "loss": 8.1158,
      "step": 6760
    },
    {
      "epoch": 2.0941922511793365,
      "grad_norm": 1.0301141738891602,
      "learning_rate": 3.504906294200849e-05,
      "loss": 8.1157,
      "step": 6770
    },
    {
      "epoch": 2.0972855927615806,
      "grad_norm": 0.6876010894775391,
      "learning_rate": 3.502696251768034e-05,
      "loss": 8.1098,
      "step": 6780
    },
    {
      "epoch": 2.100378934343825,
      "grad_norm": 0.6319012641906738,
      "learning_rate": 3.500486209335219e-05,
      "loss": 8.1159,
      "step": 6790
    },
    {
      "epoch": 2.103472275926069,
      "grad_norm": 0.6446672677993774,
      "learning_rate": 3.498276166902405e-05,
      "loss": 8.1058,
      "step": 6800
    },
    {
      "epoch": 2.106565617508313,
      "grad_norm": 0.9121010899543762,
      "learning_rate": 3.4960661244695905e-05,
      "loss": 8.1111,
      "step": 6810
    },
    {
      "epoch": 2.1096589590905577,
      "grad_norm": 0.7679120302200317,
      "learning_rate": 3.4938560820367756e-05,
      "loss": 8.1159,
      "step": 6820
    },
    {
      "epoch": 2.1127523006728017,
      "grad_norm": 0.7495049238204956,
      "learning_rate": 3.4916460396039607e-05,
      "loss": 8.109,
      "step": 6830
    },
    {
      "epoch": 2.115845642255046,
      "grad_norm": 0.7522687911987305,
      "learning_rate": 3.489435997171146e-05,
      "loss": 8.1117,
      "step": 6840
    },
    {
      "epoch": 2.1189389838372903,
      "grad_norm": 0.8810567259788513,
      "learning_rate": 3.4872259547383315e-05,
      "loss": 8.1076,
      "step": 6850
    },
    {
      "epoch": 2.1220323254195343,
      "grad_norm": 0.7428545951843262,
      "learning_rate": 3.4850159123055166e-05,
      "loss": 8.1087,
      "step": 6860
    },
    {
      "epoch": 2.125125667001779,
      "grad_norm": 0.703344464302063,
      "learning_rate": 3.482805869872702e-05,
      "loss": 8.1083,
      "step": 6870
    },
    {
      "epoch": 2.128219008584023,
      "grad_norm": 1.0406299829483032,
      "learning_rate": 3.480595827439887e-05,
      "loss": 8.1129,
      "step": 6880
    },
    {
      "epoch": 2.131312350166267,
      "grad_norm": 0.752870500087738,
      "learning_rate": 3.4783857850070726e-05,
      "loss": 8.1042,
      "step": 6890
    },
    {
      "epoch": 2.1344056917485115,
      "grad_norm": 0.6443193554878235,
      "learning_rate": 3.476175742574258e-05,
      "loss": 8.1156,
      "step": 6900
    },
    {
      "epoch": 2.1374990333307555,
      "grad_norm": 0.6838558316230774,
      "learning_rate": 3.473965700141443e-05,
      "loss": 8.1162,
      "step": 6910
    },
    {
      "epoch": 2.1405923749129996,
      "grad_norm": 0.6239073872566223,
      "learning_rate": 3.471755657708628e-05,
      "loss": 8.1017,
      "step": 6920
    },
    {
      "epoch": 2.143685716495244,
      "grad_norm": 0.6637186408042908,
      "learning_rate": 3.469545615275813e-05,
      "loss": 8.1147,
      "step": 6930
    },
    {
      "epoch": 2.146779058077488,
      "grad_norm": 0.5597219467163086,
      "learning_rate": 3.467335572842999e-05,
      "loss": 8.1049,
      "step": 6940
    },
    {
      "epoch": 2.149872399659732,
      "grad_norm": 0.6399242877960205,
      "learning_rate": 3.4651255304101845e-05,
      "loss": 8.1031,
      "step": 6950
    },
    {
      "epoch": 2.1529657412419767,
      "grad_norm": 1.1416271924972534,
      "learning_rate": 3.4629154879773696e-05,
      "loss": 8.107,
      "step": 6960
    },
    {
      "epoch": 2.1560590828242208,
      "grad_norm": 0.9163448810577393,
      "learning_rate": 3.460705445544555e-05,
      "loss": 8.0966,
      "step": 6970
    },
    {
      "epoch": 2.1591524244064653,
      "grad_norm": 0.7773393988609314,
      "learning_rate": 3.45849540311174e-05,
      "loss": 8.104,
      "step": 6980
    },
    {
      "epoch": 2.1622457659887093,
      "grad_norm": 0.5174259543418884,
      "learning_rate": 3.4562853606789255e-05,
      "loss": 8.1139,
      "step": 6990
    },
    {
      "epoch": 2.1653391075709534,
      "grad_norm": 1.4004563093185425,
      "learning_rate": 3.4540753182461106e-05,
      "loss": 8.112,
      "step": 7000
    },
    {
      "epoch": 2.168432449153198,
      "grad_norm": 1.0067520141601562,
      "learning_rate": 3.451865275813296e-05,
      "loss": 8.1162,
      "step": 7010
    },
    {
      "epoch": 2.171525790735442,
      "grad_norm": 0.8072924017906189,
      "learning_rate": 3.449655233380481e-05,
      "loss": 8.1182,
      "step": 7020
    },
    {
      "epoch": 2.174619132317686,
      "grad_norm": 0.5735112428665161,
      "learning_rate": 3.4474451909476666e-05,
      "loss": 8.0978,
      "step": 7030
    },
    {
      "epoch": 2.1777124738999305,
      "grad_norm": 1.1015483140945435,
      "learning_rate": 3.445235148514852e-05,
      "loss": 8.1214,
      "step": 7040
    },
    {
      "epoch": 2.1808058154821746,
      "grad_norm": 0.9034416079521179,
      "learning_rate": 3.443025106082037e-05,
      "loss": 8.1156,
      "step": 7050
    },
    {
      "epoch": 2.1838991570644186,
      "grad_norm": 0.7700001001358032,
      "learning_rate": 3.440815063649222e-05,
      "loss": 8.1123,
      "step": 7060
    },
    {
      "epoch": 2.186992498646663,
      "grad_norm": 0.8236660361289978,
      "learning_rate": 3.438605021216407e-05,
      "loss": 8.0989,
      "step": 7070
    },
    {
      "epoch": 2.190085840228907,
      "grad_norm": 0.5504794716835022,
      "learning_rate": 3.436394978783593e-05,
      "loss": 8.0951,
      "step": 7080
    },
    {
      "epoch": 2.1931791818111517,
      "grad_norm": 1.2962919473648071,
      "learning_rate": 3.4341849363507785e-05,
      "loss": 8.1137,
      "step": 7090
    },
    {
      "epoch": 2.1962725233933957,
      "grad_norm": 1.162819266319275,
      "learning_rate": 3.4319748939179636e-05,
      "loss": 8.0999,
      "step": 7100
    },
    {
      "epoch": 2.19936586497564,
      "grad_norm": 0.6030256152153015,
      "learning_rate": 3.429764851485149e-05,
      "loss": 8.1104,
      "step": 7110
    },
    {
      "epoch": 2.2024592065578843,
      "grad_norm": 0.9995957016944885,
      "learning_rate": 3.427554809052334e-05,
      "loss": 8.0951,
      "step": 7120
    },
    {
      "epoch": 2.2055525481401284,
      "grad_norm": 0.9046604037284851,
      "learning_rate": 3.4253447666195196e-05,
      "loss": 8.1152,
      "step": 7130
    },
    {
      "epoch": 2.2086458897223724,
      "grad_norm": 0.7797784805297852,
      "learning_rate": 3.423134724186705e-05,
      "loss": 8.0983,
      "step": 7140
    },
    {
      "epoch": 2.211739231304617,
      "grad_norm": 0.7534307241439819,
      "learning_rate": 3.42092468175389e-05,
      "loss": 8.1089,
      "step": 7150
    },
    {
      "epoch": 2.214832572886861,
      "grad_norm": 1.0686620473861694,
      "learning_rate": 3.418714639321075e-05,
      "loss": 8.0987,
      "step": 7160
    },
    {
      "epoch": 2.217925914469105,
      "grad_norm": 0.9020822644233704,
      "learning_rate": 3.4165045968882606e-05,
      "loss": 8.1097,
      "step": 7170
    },
    {
      "epoch": 2.2210192560513495,
      "grad_norm": 1.0647459030151367,
      "learning_rate": 3.414294554455446e-05,
      "loss": 8.1077,
      "step": 7180
    },
    {
      "epoch": 2.2241125976335936,
      "grad_norm": 0.8828995227813721,
      "learning_rate": 3.412084512022631e-05,
      "loss": 8.0981,
      "step": 7190
    },
    {
      "epoch": 2.227205939215838,
      "grad_norm": 1.0085227489471436,
      "learning_rate": 3.409874469589816e-05,
      "loss": 8.1115,
      "step": 7200
    },
    {
      "epoch": 2.230299280798082,
      "grad_norm": 1.5114398002624512,
      "learning_rate": 3.407664427157002e-05,
      "loss": 8.0966,
      "step": 7210
    },
    {
      "epoch": 2.233392622380326,
      "grad_norm": 0.7035461068153381,
      "learning_rate": 3.405454384724187e-05,
      "loss": 8.1034,
      "step": 7220
    },
    {
      "epoch": 2.2364859639625707,
      "grad_norm": 0.9180713891983032,
      "learning_rate": 3.4032443422913726e-05,
      "loss": 8.1241,
      "step": 7230
    },
    {
      "epoch": 2.2395793055448148,
      "grad_norm": 0.8001329898834229,
      "learning_rate": 3.4010342998585577e-05,
      "loss": 8.1032,
      "step": 7240
    },
    {
      "epoch": 2.242672647127059,
      "grad_norm": 0.6048594117164612,
      "learning_rate": 3.398824257425743e-05,
      "loss": 8.0986,
      "step": 7250
    },
    {
      "epoch": 2.2457659887093033,
      "grad_norm": 0.6743848323822021,
      "learning_rate": 3.396614214992928e-05,
      "loss": 8.1076,
      "step": 7260
    },
    {
      "epoch": 2.2488593302915474,
      "grad_norm": 0.6874709129333496,
      "learning_rate": 3.3944041725601136e-05,
      "loss": 8.1171,
      "step": 7270
    },
    {
      "epoch": 2.2519526718737914,
      "grad_norm": 1.0765892267227173,
      "learning_rate": 3.392194130127299e-05,
      "loss": 8.1083,
      "step": 7280
    },
    {
      "epoch": 2.255046013456036,
      "grad_norm": 0.8502073884010315,
      "learning_rate": 3.389984087694484e-05,
      "loss": 8.1095,
      "step": 7290
    },
    {
      "epoch": 2.25813935503828,
      "grad_norm": 1.6911770105361938,
      "learning_rate": 3.387774045261669e-05,
      "loss": 8.1235,
      "step": 7300
    },
    {
      "epoch": 2.2612326966205245,
      "grad_norm": 2.313249111175537,
      "learning_rate": 3.385564002828855e-05,
      "loss": 8.0983,
      "step": 7310
    },
    {
      "epoch": 2.2643260382027686,
      "grad_norm": 1.0674614906311035,
      "learning_rate": 3.38335396039604e-05,
      "loss": 8.1068,
      "step": 7320
    },
    {
      "epoch": 2.2674193797850126,
      "grad_norm": 1.7403515577316284,
      "learning_rate": 3.381143917963225e-05,
      "loss": 8.1267,
      "step": 7330
    },
    {
      "epoch": 2.270512721367257,
      "grad_norm": 2.1093876361846924,
      "learning_rate": 3.37893387553041e-05,
      "loss": 8.0998,
      "step": 7340
    },
    {
      "epoch": 2.273606062949501,
      "grad_norm": 1.8482344150543213,
      "learning_rate": 3.376723833097596e-05,
      "loss": 8.1192,
      "step": 7350
    },
    {
      "epoch": 2.2766994045317452,
      "grad_norm": 2.4280407428741455,
      "learning_rate": 3.374513790664781e-05,
      "loss": 8.1001,
      "step": 7360
    },
    {
      "epoch": 2.2797927461139897,
      "grad_norm": 1.7819243669509888,
      "learning_rate": 3.3723037482319666e-05,
      "loss": 8.1028,
      "step": 7370
    },
    {
      "epoch": 2.282886087696234,
      "grad_norm": 1.0094128847122192,
      "learning_rate": 3.370093705799152e-05,
      "loss": 8.1176,
      "step": 7380
    },
    {
      "epoch": 2.285979429278478,
      "grad_norm": 0.8377143144607544,
      "learning_rate": 3.367883663366337e-05,
      "loss": 8.1097,
      "step": 7390
    },
    {
      "epoch": 2.2890727708607224,
      "grad_norm": 0.5556678175926208,
      "learning_rate": 3.365673620933522e-05,
      "loss": 8.1112,
      "step": 7400
    },
    {
      "epoch": 2.2921661124429664,
      "grad_norm": 0.6424263715744019,
      "learning_rate": 3.3634635785007076e-05,
      "loss": 8.1178,
      "step": 7410
    },
    {
      "epoch": 2.295259454025211,
      "grad_norm": 0.7304377555847168,
      "learning_rate": 3.361253536067893e-05,
      "loss": 8.1072,
      "step": 7420
    },
    {
      "epoch": 2.298352795607455,
      "grad_norm": 0.6802080869674683,
      "learning_rate": 3.359043493635078e-05,
      "loss": 8.0901,
      "step": 7430
    },
    {
      "epoch": 2.301446137189699,
      "grad_norm": 0.982870876789093,
      "learning_rate": 3.356833451202263e-05,
      "loss": 8.1184,
      "step": 7440
    },
    {
      "epoch": 2.3045394787719435,
      "grad_norm": 0.7983734607696533,
      "learning_rate": 3.354623408769448e-05,
      "loss": 8.1126,
      "step": 7450
    },
    {
      "epoch": 2.3076328203541876,
      "grad_norm": 0.6771348714828491,
      "learning_rate": 3.352413366336634e-05,
      "loss": 8.0988,
      "step": 7460
    },
    {
      "epoch": 2.3107261619364317,
      "grad_norm": 0.7076992988586426,
      "learning_rate": 3.350203323903819e-05,
      "loss": 8.108,
      "step": 7470
    },
    {
      "epoch": 2.313819503518676,
      "grad_norm": 1.0335328578948975,
      "learning_rate": 3.347993281471005e-05,
      "loss": 8.0945,
      "step": 7480
    },
    {
      "epoch": 2.31691284510092,
      "grad_norm": 0.9147815108299255,
      "learning_rate": 3.34578323903819e-05,
      "loss": 8.1067,
      "step": 7490
    },
    {
      "epoch": 2.3200061866831643,
      "grad_norm": 0.6106426119804382,
      "learning_rate": 3.343573196605375e-05,
      "loss": 8.1179,
      "step": 7500
    },
    {
      "epoch": 2.3230995282654088,
      "grad_norm": 0.7255522012710571,
      "learning_rate": 3.3413631541725606e-05,
      "loss": 8.1008,
      "step": 7510
    },
    {
      "epoch": 2.326192869847653,
      "grad_norm": 0.6398424506187439,
      "learning_rate": 3.339153111739746e-05,
      "loss": 8.1036,
      "step": 7520
    },
    {
      "epoch": 2.3292862114298973,
      "grad_norm": 0.7270287871360779,
      "learning_rate": 3.336943069306931e-05,
      "loss": 8.1115,
      "step": 7530
    },
    {
      "epoch": 2.3323795530121414,
      "grad_norm": 1.3434518575668335,
      "learning_rate": 3.334733026874116e-05,
      "loss": 8.0963,
      "step": 7540
    },
    {
      "epoch": 2.3354728945943855,
      "grad_norm": 0.8692969679832458,
      "learning_rate": 3.332522984441302e-05,
      "loss": 8.1117,
      "step": 7550
    },
    {
      "epoch": 2.33856623617663,
      "grad_norm": 0.6519240736961365,
      "learning_rate": 3.330312942008487e-05,
      "loss": 8.1044,
      "step": 7560
    },
    {
      "epoch": 2.341659577758874,
      "grad_norm": 0.8035667538642883,
      "learning_rate": 3.328102899575672e-05,
      "loss": 8.1098,
      "step": 7570
    },
    {
      "epoch": 2.344752919341118,
      "grad_norm": 0.7302557229995728,
      "learning_rate": 3.325892857142857e-05,
      "loss": 8.1276,
      "step": 7580
    },
    {
      "epoch": 2.3478462609233626,
      "grad_norm": 0.8394455313682556,
      "learning_rate": 3.323682814710042e-05,
      "loss": 8.097,
      "step": 7590
    },
    {
      "epoch": 2.3509396025056066,
      "grad_norm": 0.8157474398612976,
      "learning_rate": 3.321472772277228e-05,
      "loss": 8.1098,
      "step": 7600
    },
    {
      "epoch": 2.3540329440878507,
      "grad_norm": 0.6307093501091003,
      "learning_rate": 3.3192627298444136e-05,
      "loss": 8.1105,
      "step": 7610
    },
    {
      "epoch": 2.357126285670095,
      "grad_norm": 0.7378803491592407,
      "learning_rate": 3.317052687411599e-05,
      "loss": 8.0956,
      "step": 7620
    },
    {
      "epoch": 2.3602196272523392,
      "grad_norm": 0.6132755875587463,
      "learning_rate": 3.314842644978784e-05,
      "loss": 8.1186,
      "step": 7630
    },
    {
      "epoch": 2.3633129688345837,
      "grad_norm": 1.782410979270935,
      "learning_rate": 3.312632602545969e-05,
      "loss": 8.1124,
      "step": 7640
    },
    {
      "epoch": 2.366406310416828,
      "grad_norm": 1.033473253250122,
      "learning_rate": 3.3104225601131547e-05,
      "loss": 8.1078,
      "step": 7650
    },
    {
      "epoch": 2.369499651999072,
      "grad_norm": 1.1419384479522705,
      "learning_rate": 3.30821251768034e-05,
      "loss": 8.1032,
      "step": 7660
    },
    {
      "epoch": 2.3725929935813164,
      "grad_norm": 0.9021451473236084,
      "learning_rate": 3.306002475247525e-05,
      "loss": 8.1203,
      "step": 7670
    },
    {
      "epoch": 2.3756863351635604,
      "grad_norm": 1.115128993988037,
      "learning_rate": 3.30379243281471e-05,
      "loss": 8.1416,
      "step": 7680
    },
    {
      "epoch": 2.3787796767458045,
      "grad_norm": 1.12799072265625,
      "learning_rate": 3.301582390381896e-05,
      "loss": 8.1044,
      "step": 7690
    },
    {
      "epoch": 2.381873018328049,
      "grad_norm": 1.0197303295135498,
      "learning_rate": 3.299372347949081e-05,
      "loss": 8.1003,
      "step": 7700
    },
    {
      "epoch": 2.384966359910293,
      "grad_norm": 0.834733784198761,
      "learning_rate": 3.297162305516266e-05,
      "loss": 8.1109,
      "step": 7710
    },
    {
      "epoch": 2.388059701492537,
      "grad_norm": 0.926567792892456,
      "learning_rate": 3.294952263083451e-05,
      "loss": 8.118,
      "step": 7720
    },
    {
      "epoch": 2.3911530430747816,
      "grad_norm": 1.0582120418548584,
      "learning_rate": 3.292742220650636e-05,
      "loss": 8.1065,
      "step": 7730
    },
    {
      "epoch": 2.3942463846570257,
      "grad_norm": 0.5994278788566589,
      "learning_rate": 3.290532178217822e-05,
      "loss": 8.1055,
      "step": 7740
    },
    {
      "epoch": 2.39733972623927,
      "grad_norm": 1.3775322437286377,
      "learning_rate": 3.2883221357850076e-05,
      "loss": 8.1136,
      "step": 7750
    },
    {
      "epoch": 2.400433067821514,
      "grad_norm": 1.0181688070297241,
      "learning_rate": 3.286112093352193e-05,
      "loss": 8.1103,
      "step": 7760
    },
    {
      "epoch": 2.4035264094037583,
      "grad_norm": 0.7116814255714417,
      "learning_rate": 3.283902050919378e-05,
      "loss": 8.1153,
      "step": 7770
    },
    {
      "epoch": 2.406619750986003,
      "grad_norm": 0.8103683590888977,
      "learning_rate": 3.281692008486563e-05,
      "loss": 8.1021,
      "step": 7780
    },
    {
      "epoch": 2.409713092568247,
      "grad_norm": 0.902049720287323,
      "learning_rate": 3.279481966053749e-05,
      "loss": 8.1123,
      "step": 7790
    },
    {
      "epoch": 2.412806434150491,
      "grad_norm": 1.3356956243515015,
      "learning_rate": 3.277271923620934e-05,
      "loss": 8.1081,
      "step": 7800
    },
    {
      "epoch": 2.4158997757327354,
      "grad_norm": 1.5610414743423462,
      "learning_rate": 3.275061881188119e-05,
      "loss": 8.1147,
      "step": 7810
    },
    {
      "epoch": 2.4189931173149795,
      "grad_norm": 0.9040111303329468,
      "learning_rate": 3.272851838755304e-05,
      "loss": 8.1084,
      "step": 7820
    },
    {
      "epoch": 2.4220864588972235,
      "grad_norm": 0.8800963163375854,
      "learning_rate": 3.27064179632249e-05,
      "loss": 8.0994,
      "step": 7830
    },
    {
      "epoch": 2.425179800479468,
      "grad_norm": 1.0713331699371338,
      "learning_rate": 3.268431753889675e-05,
      "loss": 8.105,
      "step": 7840
    },
    {
      "epoch": 2.428273142061712,
      "grad_norm": 0.6551599502563477,
      "learning_rate": 3.26622171145686e-05,
      "loss": 8.1157,
      "step": 7850
    },
    {
      "epoch": 2.4313664836439566,
      "grad_norm": 1.2914223670959473,
      "learning_rate": 3.264011669024045e-05,
      "loss": 8.1127,
      "step": 7860
    },
    {
      "epoch": 2.4344598252262006,
      "grad_norm": 1.1945642232894897,
      "learning_rate": 3.26180162659123e-05,
      "loss": 8.1118,
      "step": 7870
    },
    {
      "epoch": 2.4375531668084447,
      "grad_norm": 1.3108043670654297,
      "learning_rate": 3.2595915841584166e-05,
      "loss": 8.0951,
      "step": 7880
    },
    {
      "epoch": 2.440646508390689,
      "grad_norm": 1.1306266784667969,
      "learning_rate": 3.257381541725602e-05,
      "loss": 8.1114,
      "step": 7890
    },
    {
      "epoch": 2.4437398499729333,
      "grad_norm": 0.9698882102966309,
      "learning_rate": 3.255171499292787e-05,
      "loss": 8.1097,
      "step": 7900
    },
    {
      "epoch": 2.4468331915551773,
      "grad_norm": 0.9795451760292053,
      "learning_rate": 3.252961456859972e-05,
      "loss": 8.1275,
      "step": 7910
    },
    {
      "epoch": 2.449926533137422,
      "grad_norm": 0.7491248250007629,
      "learning_rate": 3.250751414427157e-05,
      "loss": 8.1055,
      "step": 7920
    },
    {
      "epoch": 2.453019874719666,
      "grad_norm": 1.0926135778427124,
      "learning_rate": 3.248541371994343e-05,
      "loss": 8.12,
      "step": 7930
    },
    {
      "epoch": 2.45611321630191,
      "grad_norm": 0.6629694700241089,
      "learning_rate": 3.246331329561528e-05,
      "loss": 8.1012,
      "step": 7940
    },
    {
      "epoch": 2.4592065578841544,
      "grad_norm": 0.6353484988212585,
      "learning_rate": 3.244121287128713e-05,
      "loss": 8.1127,
      "step": 7950
    },
    {
      "epoch": 2.4622998994663985,
      "grad_norm": 0.7175458669662476,
      "learning_rate": 3.241911244695898e-05,
      "loss": 8.1063,
      "step": 7960
    },
    {
      "epoch": 2.465393241048643,
      "grad_norm": 1.1927061080932617,
      "learning_rate": 3.239701202263084e-05,
      "loss": 8.1297,
      "step": 7970
    },
    {
      "epoch": 2.468486582630887,
      "grad_norm": 0.7836518883705139,
      "learning_rate": 3.237491159830269e-05,
      "loss": 8.1122,
      "step": 7980
    },
    {
      "epoch": 2.471579924213131,
      "grad_norm": 0.5218590497970581,
      "learning_rate": 3.235281117397454e-05,
      "loss": 8.1046,
      "step": 7990
    },
    {
      "epoch": 2.4746732657953756,
      "grad_norm": 0.792202889919281,
      "learning_rate": 3.233071074964639e-05,
      "loss": 8.1042,
      "step": 8000
    },
    {
      "epoch": 2.4777666073776197,
      "grad_norm": 0.702460765838623,
      "learning_rate": 3.230861032531825e-05,
      "loss": 8.1055,
      "step": 8010
    },
    {
      "epoch": 2.4808599489598637,
      "grad_norm": 0.8208922147750854,
      "learning_rate": 3.2286509900990106e-05,
      "loss": 8.1083,
      "step": 8020
    },
    {
      "epoch": 2.4839532905421082,
      "grad_norm": 0.8189626932144165,
      "learning_rate": 3.226440947666196e-05,
      "loss": 8.1138,
      "step": 8030
    },
    {
      "epoch": 2.4870466321243523,
      "grad_norm": 0.7804985642433167,
      "learning_rate": 3.224230905233381e-05,
      "loss": 8.1128,
      "step": 8040
    },
    {
      "epoch": 2.4901399737065963,
      "grad_norm": NaN,
      "learning_rate": 3.222241867043848e-05,
      "loss": 8.1188,
      "step": 8050
    },
    {
      "epoch": 2.493233315288841,
      "grad_norm": 0.7697494029998779,
      "learning_rate": 3.220031824611033e-05,
      "loss": 8.1176,
      "step": 8060
    },
    {
      "epoch": 2.496326656871085,
      "grad_norm": 0.8394335508346558,
      "learning_rate": 3.217821782178218e-05,
      "loss": 8.1076,
      "step": 8070
    },
    {
      "epoch": 2.4994199984533294,
      "grad_norm": 0.7376328110694885,
      "learning_rate": 3.215611739745403e-05,
      "loss": 8.1141,
      "step": 8080
    },
    {
      "epoch": 2.5025133400355735,
      "grad_norm": 0.7855545282363892,
      "learning_rate": 3.213401697312589e-05,
      "loss": 8.1062,
      "step": 8090
    },
    {
      "epoch": 2.5056066816178175,
      "grad_norm": 0.6221078634262085,
      "learning_rate": 3.211191654879774e-05,
      "loss": 8.1123,
      "step": 8100
    },
    {
      "epoch": 2.508700023200062,
      "grad_norm": 0.7163888216018677,
      "learning_rate": 3.208981612446959e-05,
      "loss": 8.1165,
      "step": 8110
    },
    {
      "epoch": 2.511793364782306,
      "grad_norm": 1.189736008644104,
      "learning_rate": 3.206771570014144e-05,
      "loss": 8.1213,
      "step": 8120
    },
    {
      "epoch": 2.51488670636455,
      "grad_norm": 0.9522821307182312,
      "learning_rate": 3.204561527581329e-05,
      "loss": 8.0977,
      "step": 8130
    },
    {
      "epoch": 2.5179800479467946,
      "grad_norm": 0.7873066663742065,
      "learning_rate": 3.202351485148515e-05,
      "loss": 8.1067,
      "step": 8140
    },
    {
      "epoch": 2.5210733895290387,
      "grad_norm": 0.7745828628540039,
      "learning_rate": 3.2001414427157e-05,
      "loss": 8.0973,
      "step": 8150
    },
    {
      "epoch": 2.5241667311112828,
      "grad_norm": 0.7857110500335693,
      "learning_rate": 3.197931400282886e-05,
      "loss": 8.1026,
      "step": 8160
    },
    {
      "epoch": 2.5272600726935273,
      "grad_norm": 0.5915932059288025,
      "learning_rate": 3.195721357850071e-05,
      "loss": 8.1088,
      "step": 8170
    },
    {
      "epoch": 2.5303534142757713,
      "grad_norm": 1.2239477634429932,
      "learning_rate": 3.193511315417256e-05,
      "loss": 8.1229,
      "step": 8180
    },
    {
      "epoch": 2.533446755858016,
      "grad_norm": 1.1027289628982544,
      "learning_rate": 3.191301272984442e-05,
      "loss": 8.0968,
      "step": 8190
    },
    {
      "epoch": 2.53654009744026,
      "grad_norm": 0.84235680103302,
      "learning_rate": 3.189091230551627e-05,
      "loss": 8.0915,
      "step": 8200
    },
    {
      "epoch": 2.539633439022504,
      "grad_norm": 1.046024203300476,
      "learning_rate": 3.186881188118812e-05,
      "loss": 8.1074,
      "step": 8210
    },
    {
      "epoch": 2.5427267806047484,
      "grad_norm": 0.8541591763496399,
      "learning_rate": 3.184671145685997e-05,
      "loss": 8.1011,
      "step": 8220
    },
    {
      "epoch": 2.5458201221869925,
      "grad_norm": 0.7309040427207947,
      "learning_rate": 3.182461103253183e-05,
      "loss": 8.1174,
      "step": 8230
    },
    {
      "epoch": 2.5489134637692366,
      "grad_norm": 0.9606079459190369,
      "learning_rate": 3.180251060820368e-05,
      "loss": 8.0986,
      "step": 8240
    },
    {
      "epoch": 2.552006805351481,
      "grad_norm": 0.7950613498687744,
      "learning_rate": 3.178041018387553e-05,
      "loss": 8.0949,
      "step": 8250
    },
    {
      "epoch": 2.555100146933725,
      "grad_norm": 0.5533953309059143,
      "learning_rate": 3.175830975954738e-05,
      "loss": 8.0918,
      "step": 8260
    },
    {
      "epoch": 2.558193488515969,
      "grad_norm": 0.6102727651596069,
      "learning_rate": 3.173620933521923e-05,
      "loss": 8.1015,
      "step": 8270
    },
    {
      "epoch": 2.5612868300982137,
      "grad_norm": 0.5869930386543274,
      "learning_rate": 3.171410891089109e-05,
      "loss": 8.0907,
      "step": 8280
    },
    {
      "epoch": 2.5643801716804577,
      "grad_norm": 0.6928898692131042,
      "learning_rate": 3.169200848656295e-05,
      "loss": 8.1008,
      "step": 8290
    },
    {
      "epoch": 2.5674735132627022,
      "grad_norm": 0.9670068621635437,
      "learning_rate": 3.16699080622348e-05,
      "loss": 8.1147,
      "step": 8300
    },
    {
      "epoch": 2.5705668548449463,
      "grad_norm": 0.9791125655174255,
      "learning_rate": 3.164780763790665e-05,
      "loss": 8.1094,
      "step": 8310
    },
    {
      "epoch": 2.5736601964271903,
      "grad_norm": 0.8992400765419006,
      "learning_rate": 3.16257072135785e-05,
      "loss": 8.0969,
      "step": 8320
    },
    {
      "epoch": 2.576753538009435,
      "grad_norm": 0.8083611726760864,
      "learning_rate": 3.160360678925036e-05,
      "loss": 8.1059,
      "step": 8330
    },
    {
      "epoch": 2.579846879591679,
      "grad_norm": 1.174232840538025,
      "learning_rate": 3.158150636492221e-05,
      "loss": 8.1054,
      "step": 8340
    },
    {
      "epoch": 2.582940221173923,
      "grad_norm": 0.889126181602478,
      "learning_rate": 3.155940594059406e-05,
      "loss": 8.1165,
      "step": 8350
    },
    {
      "epoch": 2.5860335627561675,
      "grad_norm": 0.7267313599586487,
      "learning_rate": 3.153730551626591e-05,
      "loss": 8.1073,
      "step": 8360
    },
    {
      "epoch": 2.5891269043384115,
      "grad_norm": 1.0108970403671265,
      "learning_rate": 3.151520509193777e-05,
      "loss": 8.1125,
      "step": 8370
    },
    {
      "epoch": 2.5922202459206556,
      "grad_norm": 0.6237018704414368,
      "learning_rate": 3.149310466760962e-05,
      "loss": 8.1142,
      "step": 8380
    },
    {
      "epoch": 2.5953135875029,
      "grad_norm": 0.8178033232688904,
      "learning_rate": 3.147100424328147e-05,
      "loss": 8.1088,
      "step": 8390
    },
    {
      "epoch": 2.598406929085144,
      "grad_norm": 0.9345575571060181,
      "learning_rate": 3.144890381895332e-05,
      "loss": 8.1038,
      "step": 8400
    },
    {
      "epoch": 2.6015002706673886,
      "grad_norm": 0.8388963937759399,
      "learning_rate": 3.142680339462517e-05,
      "loss": 8.0982,
      "step": 8410
    },
    {
      "epoch": 2.6045936122496327,
      "grad_norm": 0.9106768369674683,
      "learning_rate": 3.140470297029703e-05,
      "loss": 8.0855,
      "step": 8420
    },
    {
      "epoch": 2.6076869538318768,
      "grad_norm": 0.5219316482543945,
      "learning_rate": 3.138260254596889e-05,
      "loss": 8.1185,
      "step": 8430
    },
    {
      "epoch": 2.6107802954141213,
      "grad_norm": 0.7657433152198792,
      "learning_rate": 3.136050212164074e-05,
      "loss": 8.1072,
      "step": 8440
    },
    {
      "epoch": 2.6138736369963653,
      "grad_norm": 1.067562222480774,
      "learning_rate": 3.133840169731259e-05,
      "loss": 8.1014,
      "step": 8450
    },
    {
      "epoch": 2.6169669785786094,
      "grad_norm": 0.8140122294425964,
      "learning_rate": 3.131630127298444e-05,
      "loss": 8.1128,
      "step": 8460
    },
    {
      "epoch": 2.620060320160854,
      "grad_norm": 0.804144024848938,
      "learning_rate": 3.12942008486563e-05,
      "loss": 8.1205,
      "step": 8470
    },
    {
      "epoch": 2.623153661743098,
      "grad_norm": 0.7900106906890869,
      "learning_rate": 3.127210042432815e-05,
      "loss": 8.0945,
      "step": 8480
    },
    {
      "epoch": 2.626247003325342,
      "grad_norm": 0.9645407795906067,
      "learning_rate": 3.125e-05,
      "loss": 8.1026,
      "step": 8490
    },
    {
      "epoch": 2.6293403449075865,
      "grad_norm": 0.7588421702384949,
      "learning_rate": 3.122789957567185e-05,
      "loss": 8.1119,
      "step": 8500
    },
    {
      "epoch": 2.6324336864898306,
      "grad_norm": 0.8346685171127319,
      "learning_rate": 3.120579915134371e-05,
      "loss": 8.0961,
      "step": 8510
    },
    {
      "epoch": 2.635527028072075,
      "grad_norm": 1.0086463689804077,
      "learning_rate": 3.118369872701556e-05,
      "loss": 8.1164,
      "step": 8520
    },
    {
      "epoch": 2.638620369654319,
      "grad_norm": 1.4347095489501953,
      "learning_rate": 3.116159830268741e-05,
      "loss": 8.1053,
      "step": 8530
    },
    {
      "epoch": 2.641713711236563,
      "grad_norm": 0.5833174586296082,
      "learning_rate": 3.113949787835926e-05,
      "loss": 8.0913,
      "step": 8540
    },
    {
      "epoch": 2.6448070528188077,
      "grad_norm": 0.8336417078971863,
      "learning_rate": 3.111739745403111e-05,
      "loss": 8.1015,
      "step": 8550
    },
    {
      "epoch": 2.6479003944010517,
      "grad_norm": 0.734184741973877,
      "learning_rate": 3.109529702970298e-05,
      "loss": 8.1216,
      "step": 8560
    },
    {
      "epoch": 2.650993735983296,
      "grad_norm": 0.5296395421028137,
      "learning_rate": 3.107319660537483e-05,
      "loss": 8.1094,
      "step": 8570
    },
    {
      "epoch": 2.6540870775655403,
      "grad_norm": 0.4169941842556,
      "learning_rate": 3.105109618104668e-05,
      "loss": 8.11,
      "step": 8580
    },
    {
      "epoch": 2.6571804191477844,
      "grad_norm": 0.7747315168380737,
      "learning_rate": 3.102899575671853e-05,
      "loss": 8.1138,
      "step": 8590
    },
    {
      "epoch": 2.6602737607300284,
      "grad_norm": 0.6991413235664368,
      "learning_rate": 3.100689533239038e-05,
      "loss": 8.0857,
      "step": 8600
    },
    {
      "epoch": 2.663367102312273,
      "grad_norm": 0.9075675010681152,
      "learning_rate": 3.098479490806224e-05,
      "loss": 8.1037,
      "step": 8610
    },
    {
      "epoch": 2.666460443894517,
      "grad_norm": 1.363296389579773,
      "learning_rate": 3.096269448373409e-05,
      "loss": 8.0972,
      "step": 8620
    },
    {
      "epoch": 2.6695537854767615,
      "grad_norm": 0.905543327331543,
      "learning_rate": 3.094059405940594e-05,
      "loss": 8.1,
      "step": 8630
    },
    {
      "epoch": 2.6726471270590055,
      "grad_norm": 0.5974816679954529,
      "learning_rate": 3.091849363507779e-05,
      "loss": 8.0904,
      "step": 8640
    },
    {
      "epoch": 2.6757404686412496,
      "grad_norm": 0.7393059730529785,
      "learning_rate": 3.089639321074965e-05,
      "loss": 8.1147,
      "step": 8650
    },
    {
      "epoch": 2.678833810223494,
      "grad_norm": 1.3015791177749634,
      "learning_rate": 3.08742927864215e-05,
      "loss": 8.1212,
      "step": 8660
    },
    {
      "epoch": 2.681927151805738,
      "grad_norm": 0.637285053730011,
      "learning_rate": 3.085219236209335e-05,
      "loss": 8.1117,
      "step": 8670
    },
    {
      "epoch": 2.685020493387982,
      "grad_norm": 0.6907744407653809,
      "learning_rate": 3.08300919377652e-05,
      "loss": 8.0938,
      "step": 8680
    },
    {
      "epoch": 2.6881138349702267,
      "grad_norm": 0.8641349673271179,
      "learning_rate": 3.0807991513437053e-05,
      "loss": 8.107,
      "step": 8690
    },
    {
      "epoch": 2.6912071765524708,
      "grad_norm": 0.6408829092979431,
      "learning_rate": 3.078589108910892e-05,
      "loss": 8.0934,
      "step": 8700
    },
    {
      "epoch": 2.694300518134715,
      "grad_norm": 0.7601438164710999,
      "learning_rate": 3.076379066478077e-05,
      "loss": 8.11,
      "step": 8710
    },
    {
      "epoch": 2.6973938597169593,
      "grad_norm": 1.263532280921936,
      "learning_rate": 3.074169024045262e-05,
      "loss": 8.1236,
      "step": 8720
    },
    {
      "epoch": 2.7004872012992034,
      "grad_norm": 0.7138783931732178,
      "learning_rate": 3.071958981612447e-05,
      "loss": 8.1209,
      "step": 8730
    },
    {
      "epoch": 2.703580542881448,
      "grad_norm": 1.109820008277893,
      "learning_rate": 3.069748939179632e-05,
      "loss": 8.1003,
      "step": 8740
    },
    {
      "epoch": 2.706673884463692,
      "grad_norm": 1.1382349729537964,
      "learning_rate": 3.067538896746818e-05,
      "loss": 8.0829,
      "step": 8750
    },
    {
      "epoch": 2.709767226045936,
      "grad_norm": 0.6802732348442078,
      "learning_rate": 3.065328854314003e-05,
      "loss": 8.1164,
      "step": 8760
    },
    {
      "epoch": 2.7128605676281805,
      "grad_norm": 0.8587148785591125,
      "learning_rate": 3.063118811881188e-05,
      "loss": 8.1103,
      "step": 8770
    },
    {
      "epoch": 2.7159539092104246,
      "grad_norm": 0.6482588052749634,
      "learning_rate": 3.060908769448373e-05,
      "loss": 8.1161,
      "step": 8780
    },
    {
      "epoch": 2.7190472507926686,
      "grad_norm": 0.9727070927619934,
      "learning_rate": 3.058698727015559e-05,
      "loss": 8.1261,
      "step": 8790
    },
    {
      "epoch": 2.722140592374913,
      "grad_norm": 0.9787530899047852,
      "learning_rate": 3.056488684582744e-05,
      "loss": 8.1158,
      "step": 8800
    },
    {
      "epoch": 2.725233933957157,
      "grad_norm": 1.090343952178955,
      "learning_rate": 3.054278642149929e-05,
      "loss": 8.0904,
      "step": 8810
    },
    {
      "epoch": 2.7283272755394012,
      "grad_norm": 1.2675690650939941,
      "learning_rate": 3.052068599717114e-05,
      "loss": 8.1049,
      "step": 8820
    },
    {
      "epoch": 2.7314206171216457,
      "grad_norm": 1.2774320840835571,
      "learning_rate": 3.0498585572843004e-05,
      "loss": 8.1215,
      "step": 8830
    },
    {
      "epoch": 2.73451395870389,
      "grad_norm": 0.5210721492767334,
      "learning_rate": 3.0476485148514855e-05,
      "loss": 8.0958,
      "step": 8840
    },
    {
      "epoch": 2.7376073002861343,
      "grad_norm": 0.6900545954704285,
      "learning_rate": 3.045438472418671e-05,
      "loss": 8.116,
      "step": 8850
    },
    {
      "epoch": 2.7407006418683784,
      "grad_norm": 0.772678017616272,
      "learning_rate": 3.043228429985856e-05,
      "loss": 8.0975,
      "step": 8860
    },
    {
      "epoch": 2.7437939834506224,
      "grad_norm": 0.8305589556694031,
      "learning_rate": 3.041018387553041e-05,
      "loss": 8.1115,
      "step": 8870
    },
    {
      "epoch": 2.7468873250328665,
      "grad_norm": 0.7291635870933533,
      "learning_rate": 3.0388083451202265e-05,
      "loss": 8.1119,
      "step": 8880
    },
    {
      "epoch": 2.749980666615111,
      "grad_norm": 0.6218641996383667,
      "learning_rate": 3.0365983026874116e-05,
      "loss": 8.0935,
      "step": 8890
    },
    {
      "epoch": 2.753074008197355,
      "grad_norm": 0.593895435333252,
      "learning_rate": 3.034388260254597e-05,
      "loss": 8.0982,
      "step": 8900
    },
    {
      "epoch": 2.7561673497795995,
      "grad_norm": 0.730521023273468,
      "learning_rate": 3.032178217821782e-05,
      "loss": 8.105,
      "step": 8910
    },
    {
      "epoch": 2.7592606913618436,
      "grad_norm": 0.812873899936676,
      "learning_rate": 3.0299681753889676e-05,
      "loss": 8.1115,
      "step": 8920
    },
    {
      "epoch": 2.7623540329440877,
      "grad_norm": 0.6386334300041199,
      "learning_rate": 3.0277581329561527e-05,
      "loss": 8.1087,
      "step": 8930
    },
    {
      "epoch": 2.765447374526332,
      "grad_norm": 0.7302212715148926,
      "learning_rate": 3.025548090523338e-05,
      "loss": 8.1043,
      "step": 8940
    },
    {
      "epoch": 2.768540716108576,
      "grad_norm": 1.023606300354004,
      "learning_rate": 3.0233380480905232e-05,
      "loss": 8.0989,
      "step": 8950
    },
    {
      "epoch": 2.7716340576908207,
      "grad_norm": 0.5995789170265198,
      "learning_rate": 3.021128005657709e-05,
      "loss": 8.0908,
      "step": 8960
    },
    {
      "epoch": 2.7747273992730648,
      "grad_norm": 0.9647005796432495,
      "learning_rate": 3.0189179632248944e-05,
      "loss": 8.1252,
      "step": 8970
    },
    {
      "epoch": 2.777820740855309,
      "grad_norm": 1.0539631843566895,
      "learning_rate": 3.0167079207920795e-05,
      "loss": 8.1174,
      "step": 8980
    },
    {
      "epoch": 2.780914082437553,
      "grad_norm": 0.6901851296424866,
      "learning_rate": 3.014497878359265e-05,
      "loss": 8.1152,
      "step": 8990
    },
    {
      "epoch": 2.7840074240197974,
      "grad_norm": 0.6861261129379272,
      "learning_rate": 3.01228783592645e-05,
      "loss": 8.1167,
      "step": 9000
    },
    {
      "epoch": 2.7871007656020415,
      "grad_norm": 0.8329610228538513,
      "learning_rate": 3.010077793493635e-05,
      "loss": 8.1064,
      "step": 9010
    },
    {
      "epoch": 2.790194107184286,
      "grad_norm": 0.9155378341674805,
      "learning_rate": 3.0078677510608206e-05,
      "loss": 8.1052,
      "step": 9020
    },
    {
      "epoch": 2.79328744876653,
      "grad_norm": 1.3295714855194092,
      "learning_rate": 3.0056577086280057e-05,
      "loss": 8.107,
      "step": 9030
    },
    {
      "epoch": 2.796380790348774,
      "grad_norm": 0.7985524535179138,
      "learning_rate": 3.003447666195191e-05,
      "loss": 8.1109,
      "step": 9040
    },
    {
      "epoch": 2.7994741319310186,
      "grad_norm": 0.7160539031028748,
      "learning_rate": 3.0012376237623762e-05,
      "loss": 8.1112,
      "step": 9050
    },
    {
      "epoch": 2.8025674735132626,
      "grad_norm": 0.9186545610427856,
      "learning_rate": 2.9990275813295616e-05,
      "loss": 8.1152,
      "step": 9060
    },
    {
      "epoch": 2.805660815095507,
      "grad_norm": 0.7360554337501526,
      "learning_rate": 2.9968175388967467e-05,
      "loss": 8.1115,
      "step": 9070
    },
    {
      "epoch": 2.808754156677751,
      "grad_norm": 0.6553969383239746,
      "learning_rate": 2.994607496463932e-05,
      "loss": 8.1118,
      "step": 9080
    },
    {
      "epoch": 2.8118474982599952,
      "grad_norm": 0.5571410655975342,
      "learning_rate": 2.9923974540311172e-05,
      "loss": 8.1047,
      "step": 9090
    },
    {
      "epoch": 2.8149408398422393,
      "grad_norm": 0.6194122433662415,
      "learning_rate": 2.990187411598303e-05,
      "loss": 8.1029,
      "step": 9100
    },
    {
      "epoch": 2.818034181424484,
      "grad_norm": 0.5988227128982544,
      "learning_rate": 2.9879773691654885e-05,
      "loss": 8.0884,
      "step": 9110
    },
    {
      "epoch": 2.821127523006728,
      "grad_norm": 0.6157711744308472,
      "learning_rate": 2.9857673267326735e-05,
      "loss": 8.1192,
      "step": 9120
    },
    {
      "epoch": 2.8242208645889724,
      "grad_norm": 0.8260595798492432,
      "learning_rate": 2.983557284299859e-05,
      "loss": 8.1044,
      "step": 9130
    },
    {
      "epoch": 2.8273142061712164,
      "grad_norm": 0.7173621654510498,
      "learning_rate": 2.981347241867044e-05,
      "loss": 8.1026,
      "step": 9140
    },
    {
      "epoch": 2.8304075477534605,
      "grad_norm": 1.073249340057373,
      "learning_rate": 2.9791371994342292e-05,
      "loss": 8.0891,
      "step": 9150
    },
    {
      "epoch": 2.833500889335705,
      "grad_norm": 0.6968765258789062,
      "learning_rate": 2.9769271570014146e-05,
      "loss": 8.0983,
      "step": 9160
    },
    {
      "epoch": 2.836594230917949,
      "grad_norm": 0.6450964212417603,
      "learning_rate": 2.9747171145685997e-05,
      "loss": 8.0982,
      "step": 9170
    },
    {
      "epoch": 2.8396875725001935,
      "grad_norm": 0.7721308469772339,
      "learning_rate": 2.972507072135785e-05,
      "loss": 8.1229,
      "step": 9180
    },
    {
      "epoch": 2.8427809140824376,
      "grad_norm": 0.7427786588668823,
      "learning_rate": 2.9702970297029702e-05,
      "loss": 8.1041,
      "step": 9190
    },
    {
      "epoch": 2.8458742556646817,
      "grad_norm": 0.6238769888877869,
      "learning_rate": 2.9680869872701557e-05,
      "loss": 8.1088,
      "step": 9200
    },
    {
      "epoch": 2.8489675972469257,
      "grad_norm": 0.8641448616981506,
      "learning_rate": 2.9658769448373408e-05,
      "loss": 8.1061,
      "step": 9210
    },
    {
      "epoch": 2.8520609388291702,
      "grad_norm": 0.9350304007530212,
      "learning_rate": 2.9636669024045262e-05,
      "loss": 8.116,
      "step": 9220
    },
    {
      "epoch": 2.8551542804114143,
      "grad_norm": 1.1103359460830688,
      "learning_rate": 2.961456859971712e-05,
      "loss": 8.1294,
      "step": 9230
    },
    {
      "epoch": 2.858247621993659,
      "grad_norm": 0.7019891738891602,
      "learning_rate": 2.959246817538897e-05,
      "loss": 8.1149,
      "step": 9240
    },
    {
      "epoch": 2.861340963575903,
      "grad_norm": 0.7045094966888428,
      "learning_rate": 2.9570367751060825e-05,
      "loss": 8.107,
      "step": 9250
    },
    {
      "epoch": 2.864434305158147,
      "grad_norm": 0.7139843106269836,
      "learning_rate": 2.9548267326732676e-05,
      "loss": 8.0928,
      "step": 9260
    },
    {
      "epoch": 2.8675276467403914,
      "grad_norm": 0.6711528897285461,
      "learning_rate": 2.9526166902404527e-05,
      "loss": 8.1008,
      "step": 9270
    },
    {
      "epoch": 2.8706209883226355,
      "grad_norm": 0.6146473288536072,
      "learning_rate": 2.950406647807638e-05,
      "loss": 8.1174,
      "step": 9280
    },
    {
      "epoch": 2.87371432990488,
      "grad_norm": 1.5169636011123657,
      "learning_rate": 2.9481966053748232e-05,
      "loss": 8.1184,
      "step": 9290
    },
    {
      "epoch": 2.876807671487124,
      "grad_norm": 0.872775137424469,
      "learning_rate": 2.9459865629420086e-05,
      "loss": 8.1163,
      "step": 9300
    },
    {
      "epoch": 2.879901013069368,
      "grad_norm": 1.0280133485794067,
      "learning_rate": 2.9437765205091937e-05,
      "loss": 8.1069,
      "step": 9310
    },
    {
      "epoch": 2.882994354651612,
      "grad_norm": 0.6314733028411865,
      "learning_rate": 2.941566478076379e-05,
      "loss": 8.1043,
      "step": 9320
    },
    {
      "epoch": 2.8860876962338566,
      "grad_norm": 0.677172064781189,
      "learning_rate": 2.9393564356435643e-05,
      "loss": 8.1198,
      "step": 9330
    },
    {
      "epoch": 2.8891810378161007,
      "grad_norm": 0.9150173664093018,
      "learning_rate": 2.9371463932107497e-05,
      "loss": 8.1165,
      "step": 9340
    },
    {
      "epoch": 2.892274379398345,
      "grad_norm": 0.7769370675086975,
      "learning_rate": 2.9349363507779348e-05,
      "loss": 8.1093,
      "step": 9350
    },
    {
      "epoch": 2.8953677209805893,
      "grad_norm": 0.5955384373664856,
      "learning_rate": 2.9327263083451206e-05,
      "loss": 8.0871,
      "step": 9360
    },
    {
      "epoch": 2.8984610625628333,
      "grad_norm": 0.9734750390052795,
      "learning_rate": 2.930516265912306e-05,
      "loss": 8.1023,
      "step": 9370
    },
    {
      "epoch": 2.901554404145078,
      "grad_norm": 0.7921080589294434,
      "learning_rate": 2.928306223479491e-05,
      "loss": 8.0942,
      "step": 9380
    },
    {
      "epoch": 2.904647745727322,
      "grad_norm": 0.7161248326301575,
      "learning_rate": 2.9260961810466765e-05,
      "loss": 8.1005,
      "step": 9390
    },
    {
      "epoch": 2.9077410873095664,
      "grad_norm": 0.7211789488792419,
      "learning_rate": 2.9238861386138616e-05,
      "loss": 8.1121,
      "step": 9400
    },
    {
      "epoch": 2.9108344288918104,
      "grad_norm": 0.6164160370826721,
      "learning_rate": 2.9216760961810467e-05,
      "loss": 8.0947,
      "step": 9410
    },
    {
      "epoch": 2.9139277704740545,
      "grad_norm": 0.7180536389350891,
      "learning_rate": 2.919466053748232e-05,
      "loss": 8.1094,
      "step": 9420
    },
    {
      "epoch": 2.9170211120562985,
      "grad_norm": 0.5280080437660217,
      "learning_rate": 2.9172560113154172e-05,
      "loss": 8.1158,
      "step": 9430
    },
    {
      "epoch": 2.920114453638543,
      "grad_norm": 0.7820165753364563,
      "learning_rate": 2.9150459688826027e-05,
      "loss": 8.1146,
      "step": 9440
    },
    {
      "epoch": 2.923207795220787,
      "grad_norm": 1.354386806488037,
      "learning_rate": 2.9128359264497878e-05,
      "loss": 8.1099,
      "step": 9450
    },
    {
      "epoch": 2.9263011368030316,
      "grad_norm": 0.6691245436668396,
      "learning_rate": 2.9106258840169732e-05,
      "loss": 8.1101,
      "step": 9460
    },
    {
      "epoch": 2.9293944783852757,
      "grad_norm": 0.9536430239677429,
      "learning_rate": 2.9084158415841583e-05,
      "loss": 8.106,
      "step": 9470
    },
    {
      "epoch": 2.9324878199675197,
      "grad_norm": 0.6749069094657898,
      "learning_rate": 2.9062057991513437e-05,
      "loss": 8.13,
      "step": 9480
    },
    {
      "epoch": 2.9355811615497642,
      "grad_norm": 0.8197126984596252,
      "learning_rate": 2.9039957567185288e-05,
      "loss": 8.1024,
      "step": 9490
    },
    {
      "epoch": 2.9386745031320083,
      "grad_norm": 0.8701388835906982,
      "learning_rate": 2.9017857142857146e-05,
      "loss": 8.1054,
      "step": 9500
    },
    {
      "epoch": 2.941767844714253,
      "grad_norm": 0.8511384129524231,
      "learning_rate": 2.8995756718529e-05,
      "loss": 8.1021,
      "step": 9510
    },
    {
      "epoch": 2.944861186296497,
      "grad_norm": 0.6534299254417419,
      "learning_rate": 2.897365629420085e-05,
      "loss": 8.1098,
      "step": 9520
    },
    {
      "epoch": 2.947954527878741,
      "grad_norm": 0.7964046001434326,
      "learning_rate": 2.8951555869872705e-05,
      "loss": 8.0982,
      "step": 9530
    },
    {
      "epoch": 2.951047869460985,
      "grad_norm": 0.7299574017524719,
      "learning_rate": 2.8929455445544556e-05,
      "loss": 8.1013,
      "step": 9540
    },
    {
      "epoch": 2.9541412110432295,
      "grad_norm": 0.9422321915626526,
      "learning_rate": 2.8907355021216407e-05,
      "loss": 8.1146,
      "step": 9550
    },
    {
      "epoch": 2.9572345526254735,
      "grad_norm": 0.7960830926895142,
      "learning_rate": 2.8885254596888262e-05,
      "loss": 8.1241,
      "step": 9560
    },
    {
      "epoch": 2.960327894207718,
      "grad_norm": 0.5942158102989197,
      "learning_rate": 2.8863154172560113e-05,
      "loss": 8.1029,
      "step": 9570
    },
    {
      "epoch": 2.963421235789962,
      "grad_norm": 0.879065990447998,
      "learning_rate": 2.8841053748231967e-05,
      "loss": 8.096,
      "step": 9580
    },
    {
      "epoch": 2.966514577372206,
      "grad_norm": 0.7971692681312561,
      "learning_rate": 2.8818953323903818e-05,
      "loss": 8.1248,
      "step": 9590
    },
    {
      "epoch": 2.9696079189544506,
      "grad_norm": 1.0028879642486572,
      "learning_rate": 2.8796852899575672e-05,
      "loss": 8.1105,
      "step": 9600
    },
    {
      "epoch": 2.9727012605366947,
      "grad_norm": 0.6132903695106506,
      "learning_rate": 2.8774752475247523e-05,
      "loss": 8.1125,
      "step": 9610
    },
    {
      "epoch": 2.975794602118939,
      "grad_norm": 0.8034408092498779,
      "learning_rate": 2.8752652050919378e-05,
      "loss": 8.109,
      "step": 9620
    },
    {
      "epoch": 2.9788879437011833,
      "grad_norm": 0.7330671548843384,
      "learning_rate": 2.8730551626591235e-05,
      "loss": 8.1117,
      "step": 9630
    },
    {
      "epoch": 2.9819812852834273,
      "grad_norm": 1.3551102876663208,
      "learning_rate": 2.8708451202263086e-05,
      "loss": 8.0958,
      "step": 9640
    },
    {
      "epoch": 2.9850746268656714,
      "grad_norm": 0.8165993690490723,
      "learning_rate": 2.868635077793494e-05,
      "loss": 8.1108,
      "step": 9650
    },
    {
      "epoch": 2.988167968447916,
      "grad_norm": 0.8879673480987549,
      "learning_rate": 2.866425035360679e-05,
      "loss": 8.1201,
      "step": 9660
    },
    {
      "epoch": 2.99126131003016,
      "grad_norm": 0.7033306360244751,
      "learning_rate": 2.8642149929278646e-05,
      "loss": 8.1094,
      "step": 9670
    },
    {
      "epoch": 2.9943546516124044,
      "grad_norm": 1.238077998161316,
      "learning_rate": 2.8620049504950497e-05,
      "loss": 8.0959,
      "step": 9680
    },
    {
      "epoch": 2.9974479931946485,
      "grad_norm": 0.6576830744743347,
      "learning_rate": 2.8597949080622348e-05,
      "loss": 8.1195,
      "step": 9690
    },
    {
      "epoch": 3.0005413347768926,
      "grad_norm": 0.8184911012649536,
      "learning_rate": 2.8575848656294202e-05,
      "loss": 8.0814,
      "step": 9700
    },
    {
      "epoch": 3.003634676359137,
      "grad_norm": 0.6919069290161133,
      "learning_rate": 2.8553748231966053e-05,
      "loss": 8.114,
      "step": 9710
    },
    {
      "epoch": 3.006728017941381,
      "grad_norm": 0.8699197173118591,
      "learning_rate": 2.8531647807637907e-05,
      "loss": 8.1104,
      "step": 9720
    },
    {
      "epoch": 3.0098213595236256,
      "grad_norm": 0.7390061020851135,
      "learning_rate": 2.8509547383309758e-05,
      "loss": 8.1011,
      "step": 9730
    },
    {
      "epoch": 3.0129147011058697,
      "grad_norm": 0.8009206056594849,
      "learning_rate": 2.8487446958981613e-05,
      "loss": 8.1083,
      "step": 9740
    },
    {
      "epoch": 3.0160080426881137,
      "grad_norm": 0.7796114087104797,
      "learning_rate": 2.8465346534653464e-05,
      "loss": 8.0899,
      "step": 9750
    },
    {
      "epoch": 3.0191013842703582,
      "grad_norm": 0.7079329490661621,
      "learning_rate": 2.844324611032532e-05,
      "loss": 8.1022,
      "step": 9760
    },
    {
      "epoch": 3.0221947258526023,
      "grad_norm": 0.6712028980255127,
      "learning_rate": 2.8421145685997176e-05,
      "loss": 8.0885,
      "step": 9770
    },
    {
      "epoch": 3.0252880674348464,
      "grad_norm": 0.7117624878883362,
      "learning_rate": 2.8399045261669027e-05,
      "loss": 8.1149,
      "step": 9780
    },
    {
      "epoch": 3.028381409017091,
      "grad_norm": 0.6632695198059082,
      "learning_rate": 2.837694483734088e-05,
      "loss": 8.1138,
      "step": 9790
    },
    {
      "epoch": 3.031474750599335,
      "grad_norm": 0.8488229513168335,
      "learning_rate": 2.8354844413012732e-05,
      "loss": 8.1052,
      "step": 9800
    },
    {
      "epoch": 3.034568092181579,
      "grad_norm": 0.5635408759117126,
      "learning_rate": 2.8332743988684586e-05,
      "loss": 8.0935,
      "step": 9810
    },
    {
      "epoch": 3.0376614337638235,
      "grad_norm": 0.7147914171218872,
      "learning_rate": 2.8310643564356437e-05,
      "loss": 8.1116,
      "step": 9820
    },
    {
      "epoch": 3.0407547753460675,
      "grad_norm": 0.6911981701850891,
      "learning_rate": 2.8288543140028288e-05,
      "loss": 8.1079,
      "step": 9830
    },
    {
      "epoch": 3.043848116928312,
      "grad_norm": 0.5810366868972778,
      "learning_rate": 2.8266442715700142e-05,
      "loss": 8.1064,
      "step": 9840
    },
    {
      "epoch": 3.046941458510556,
      "grad_norm": 0.6798393130302429,
      "learning_rate": 2.8244342291371993e-05,
      "loss": 8.0978,
      "step": 9850
    },
    {
      "epoch": 3.0500348000928,
      "grad_norm": 0.8289865851402283,
      "learning_rate": 2.8222241867043848e-05,
      "loss": 8.1073,
      "step": 9860
    },
    {
      "epoch": 3.0531281416750446,
      "grad_norm": 0.6055102944374084,
      "learning_rate": 2.82001414427157e-05,
      "loss": 8.0968,
      "step": 9870
    },
    {
      "epoch": 3.0562214832572887,
      "grad_norm": 0.7936039566993713,
      "learning_rate": 2.8178041018387553e-05,
      "loss": 8.1044,
      "step": 9880
    },
    {
      "epoch": 3.0593148248395328,
      "grad_norm": 0.9882813692092896,
      "learning_rate": 2.8155940594059404e-05,
      "loss": 8.1015,
      "step": 9890
    },
    {
      "epoch": 3.0624081664217773,
      "grad_norm": 0.7666711807250977,
      "learning_rate": 2.813384016973126e-05,
      "loss": 8.0974,
      "step": 9900
    },
    {
      "epoch": 3.0655015080040213,
      "grad_norm": 1.0095900297164917,
      "learning_rate": 2.8111739745403116e-05,
      "loss": 8.1136,
      "step": 9910
    },
    {
      "epoch": 3.0685948495862654,
      "grad_norm": 1.0002144575119019,
      "learning_rate": 2.8089639321074967e-05,
      "loss": 8.0965,
      "step": 9920
    },
    {
      "epoch": 3.07168819116851,
      "grad_norm": 1.022759199142456,
      "learning_rate": 2.806753889674682e-05,
      "loss": 8.1026,
      "step": 9930
    },
    {
      "epoch": 3.074781532750754,
      "grad_norm": 0.903852105140686,
      "learning_rate": 2.8045438472418672e-05,
      "loss": 8.1113,
      "step": 9940
    },
    {
      "epoch": 3.0778748743329984,
      "grad_norm": 0.9230719208717346,
      "learning_rate": 2.8023338048090526e-05,
      "loss": 8.1115,
      "step": 9950
    },
    {
      "epoch": 3.0809682159152425,
      "grad_norm": 0.5162269473075867,
      "learning_rate": 2.8001237623762377e-05,
      "loss": 8.1062,
      "step": 9960
    },
    {
      "epoch": 3.0840615574974866,
      "grad_norm": 0.7275529503822327,
      "learning_rate": 2.797913719943423e-05,
      "loss": 8.1093,
      "step": 9970
    },
    {
      "epoch": 3.087154899079731,
      "grad_norm": 0.6984078884124756,
      "learning_rate": 2.7957036775106083e-05,
      "loss": 8.115,
      "step": 9980
    },
    {
      "epoch": 3.090248240661975,
      "grad_norm": 0.5768193602561951,
      "learning_rate": 2.7934936350777934e-05,
      "loss": 8.0994,
      "step": 9990
    },
    {
      "epoch": 3.093341582244219,
      "grad_norm": 0.6336302161216736,
      "learning_rate": 2.7912835926449788e-05,
      "loss": 8.1033,
      "step": 10000
    },
    {
      "epoch": 3.0964349238264637,
      "grad_norm": 0.8876256346702576,
      "learning_rate": 2.789073550212164e-05,
      "loss": 8.1141,
      "step": 10010
    },
    {
      "epoch": 3.0995282654087077,
      "grad_norm": 0.8742994666099548,
      "learning_rate": 2.7868635077793493e-05,
      "loss": 8.109,
      "step": 10020
    },
    {
      "epoch": 3.102621606990952,
      "grad_norm": 0.8469825387001038,
      "learning_rate": 2.784653465346535e-05,
      "loss": 8.1036,
      "step": 10030
    },
    {
      "epoch": 3.1057149485731963,
      "grad_norm": 0.5281784534454346,
      "learning_rate": 2.7824434229137202e-05,
      "loss": 8.1059,
      "step": 10040
    },
    {
      "epoch": 3.1088082901554404,
      "grad_norm": 0.7395058870315552,
      "learning_rate": 2.7802333804809056e-05,
      "loss": 8.0962,
      "step": 10050
    },
    {
      "epoch": 3.111901631737685,
      "grad_norm": 0.8850139379501343,
      "learning_rate": 2.7780233380480907e-05,
      "loss": 8.1022,
      "step": 10060
    },
    {
      "epoch": 3.114994973319929,
      "grad_norm": 0.5404447317123413,
      "learning_rate": 2.775813295615276e-05,
      "loss": 8.0916,
      "step": 10070
    },
    {
      "epoch": 3.118088314902173,
      "grad_norm": 0.7866524457931519,
      "learning_rate": 2.7736032531824612e-05,
      "loss": 8.1133,
      "step": 10080
    },
    {
      "epoch": 3.1211816564844175,
      "grad_norm": 0.883952260017395,
      "learning_rate": 2.7713932107496467e-05,
      "loss": 8.1186,
      "step": 10090
    },
    {
      "epoch": 3.1242749980666615,
      "grad_norm": 0.8230529427528381,
      "learning_rate": 2.7691831683168318e-05,
      "loss": 8.0949,
      "step": 10100
    },
    {
      "epoch": 3.1273683396489056,
      "grad_norm": 0.7071940898895264,
      "learning_rate": 2.766973125884017e-05,
      "loss": 8.1082,
      "step": 10110
    },
    {
      "epoch": 3.13046168123115,
      "grad_norm": 0.7847083210945129,
      "learning_rate": 2.7647630834512023e-05,
      "loss": 8.1168,
      "step": 10120
    },
    {
      "epoch": 3.133555022813394,
      "grad_norm": 0.8094547390937805,
      "learning_rate": 2.7625530410183874e-05,
      "loss": 8.0932,
      "step": 10130
    },
    {
      "epoch": 3.136648364395638,
      "grad_norm": 1.084108591079712,
      "learning_rate": 2.7603429985855728e-05,
      "loss": 8.1063,
      "step": 10140
    },
    {
      "epoch": 3.1397417059778827,
      "grad_norm": 1.3549492359161377,
      "learning_rate": 2.758132956152758e-05,
      "loss": 8.0973,
      "step": 10150
    },
    {
      "epoch": 3.1428350475601268,
      "grad_norm": 1.0578364133834839,
      "learning_rate": 2.7559229137199437e-05,
      "loss": 8.0985,
      "step": 10160
    },
    {
      "epoch": 3.1459283891423713,
      "grad_norm": 0.8843377828598022,
      "learning_rate": 2.753712871287129e-05,
      "loss": 8.102,
      "step": 10170
    },
    {
      "epoch": 3.1490217307246153,
      "grad_norm": 0.7174914479255676,
      "learning_rate": 2.7515028288543142e-05,
      "loss": 8.0851,
      "step": 10180
    },
    {
      "epoch": 3.1521150723068594,
      "grad_norm": 1.5227183103561401,
      "learning_rate": 2.7492927864214997e-05,
      "loss": 8.1142,
      "step": 10190
    },
    {
      "epoch": 3.155208413889104,
      "grad_norm": 1.1208131313323975,
      "learning_rate": 2.7470827439886847e-05,
      "loss": 8.1087,
      "step": 10200
    },
    {
      "epoch": 3.158301755471348,
      "grad_norm": 0.5456786751747131,
      "learning_rate": 2.7448727015558702e-05,
      "loss": 8.1058,
      "step": 10210
    },
    {
      "epoch": 3.161395097053592,
      "grad_norm": 0.8475409150123596,
      "learning_rate": 2.7426626591230553e-05,
      "loss": 8.1081,
      "step": 10220
    },
    {
      "epoch": 3.1644884386358365,
      "grad_norm": 0.8401370644569397,
      "learning_rate": 2.7404526166902407e-05,
      "loss": 8.0968,
      "step": 10230
    },
    {
      "epoch": 3.1675817802180806,
      "grad_norm": 1.2489978075027466,
      "learning_rate": 2.7382425742574258e-05,
      "loss": 8.0884,
      "step": 10240
    },
    {
      "epoch": 3.1706751218003246,
      "grad_norm": 0.7131091356277466,
      "learning_rate": 2.736032531824611e-05,
      "loss": 8.107,
      "step": 10250
    },
    {
      "epoch": 3.173768463382569,
      "grad_norm": 1.099993348121643,
      "learning_rate": 2.7338224893917963e-05,
      "loss": 8.0937,
      "step": 10260
    },
    {
      "epoch": 3.176861804964813,
      "grad_norm": 0.8480978608131409,
      "learning_rate": 2.7316124469589814e-05,
      "loss": 8.0946,
      "step": 10270
    },
    {
      "epoch": 3.1799551465470577,
      "grad_norm": 0.9558651447296143,
      "learning_rate": 2.729402404526167e-05,
      "loss": 8.1071,
      "step": 10280
    },
    {
      "epoch": 3.1830484881293017,
      "grad_norm": 0.6734650731086731,
      "learning_rate": 2.727192362093352e-05,
      "loss": 8.1006,
      "step": 10290
    },
    {
      "epoch": 3.186141829711546,
      "grad_norm": 0.9376435875892639,
      "learning_rate": 2.7249823196605377e-05,
      "loss": 8.0917,
      "step": 10300
    },
    {
      "epoch": 3.1892351712937903,
      "grad_norm": 0.8801725506782532,
      "learning_rate": 2.722772277227723e-05,
      "loss": 8.1017,
      "step": 10310
    },
    {
      "epoch": 3.1923285128760344,
      "grad_norm": 0.8488035202026367,
      "learning_rate": 2.7205622347949083e-05,
      "loss": 8.1161,
      "step": 10320
    },
    {
      "epoch": 3.1954218544582784,
      "grad_norm": 0.6034402847290039,
      "learning_rate": 2.7183521923620937e-05,
      "loss": 8.1087,
      "step": 10330
    },
    {
      "epoch": 3.198515196040523,
      "grad_norm": 0.6377551555633545,
      "learning_rate": 2.7161421499292788e-05,
      "loss": 8.106,
      "step": 10340
    },
    {
      "epoch": 3.201608537622767,
      "grad_norm": 0.8840941786766052,
      "learning_rate": 2.7139321074964642e-05,
      "loss": 8.1055,
      "step": 10350
    },
    {
      "epoch": 3.204701879205011,
      "grad_norm": 0.8724914193153381,
      "learning_rate": 2.7117220650636493e-05,
      "loss": 8.1064,
      "step": 10360
    },
    {
      "epoch": 3.2077952207872555,
      "grad_norm": 0.7110289931297302,
      "learning_rate": 2.7095120226308347e-05,
      "loss": 8.1003,
      "step": 10370
    },
    {
      "epoch": 3.2108885623694996,
      "grad_norm": 0.4816248118877411,
      "learning_rate": 2.70730198019802e-05,
      "loss": 8.1008,
      "step": 10380
    },
    {
      "epoch": 3.213981903951744,
      "grad_norm": 0.5340366959571838,
      "learning_rate": 2.705091937765205e-05,
      "loss": 8.1041,
      "step": 10390
    },
    {
      "epoch": 3.217075245533988,
      "grad_norm": 1.170303463935852,
      "learning_rate": 2.7028818953323904e-05,
      "loss": 8.1148,
      "step": 10400
    },
    {
      "epoch": 3.220168587116232,
      "grad_norm": 1.3181408643722534,
      "learning_rate": 2.7006718528995755e-05,
      "loss": 8.1034,
      "step": 10410
    },
    {
      "epoch": 3.2232619286984767,
      "grad_norm": 0.5492451190948486,
      "learning_rate": 2.698461810466761e-05,
      "loss": 8.1052,
      "step": 10420
    },
    {
      "epoch": 3.2263552702807208,
      "grad_norm": 0.4860824942588806,
      "learning_rate": 2.6962517680339467e-05,
      "loss": 8.0985,
      "step": 10430
    },
    {
      "epoch": 3.229448611862965,
      "grad_norm": 1.2488993406295776,
      "learning_rate": 2.6940417256011318e-05,
      "loss": 8.0925,
      "step": 10440
    },
    {
      "epoch": 3.2325419534452093,
      "grad_norm": 0.5049091577529907,
      "learning_rate": 2.6918316831683172e-05,
      "loss": 8.1049,
      "step": 10450
    },
    {
      "epoch": 3.2356352950274534,
      "grad_norm": 0.8086821436882019,
      "learning_rate": 2.6896216407355023e-05,
      "loss": 8.1021,
      "step": 10460
    },
    {
      "epoch": 3.2387286366096975,
      "grad_norm": 0.767903208732605,
      "learning_rate": 2.6874115983026877e-05,
      "loss": 8.0928,
      "step": 10470
    },
    {
      "epoch": 3.241821978191942,
      "grad_norm": 0.9076136946678162,
      "learning_rate": 2.6852015558698728e-05,
      "loss": 8.1073,
      "step": 10480
    },
    {
      "epoch": 3.244915319774186,
      "grad_norm": 0.7727258801460266,
      "learning_rate": 2.6829915134370582e-05,
      "loss": 8.1061,
      "step": 10490
    },
    {
      "epoch": 3.2480086613564305,
      "grad_norm": 0.9299774169921875,
      "learning_rate": 2.6807814710042433e-05,
      "loss": 8.1005,
      "step": 10500
    },
    {
      "epoch": 3.2511020029386746,
      "grad_norm": 0.8597429990768433,
      "learning_rate": 2.6785714285714288e-05,
      "loss": 8.101,
      "step": 10510
    },
    {
      "epoch": 3.2541953445209186,
      "grad_norm": 0.8112443685531616,
      "learning_rate": 2.676361386138614e-05,
      "loss": 8.1108,
      "step": 10520
    },
    {
      "epoch": 3.257288686103163,
      "grad_norm": 0.8010808825492859,
      "learning_rate": 2.674151343705799e-05,
      "loss": 8.1069,
      "step": 10530
    },
    {
      "epoch": 3.260382027685407,
      "grad_norm": 0.9142884612083435,
      "learning_rate": 2.6719413012729844e-05,
      "loss": 8.1093,
      "step": 10540
    },
    {
      "epoch": 3.2634753692676512,
      "grad_norm": 0.6666103005409241,
      "learning_rate": 2.6697312588401695e-05,
      "loss": 8.1138,
      "step": 10550
    },
    {
      "epoch": 3.2665687108498958,
      "grad_norm": 0.6035496592521667,
      "learning_rate": 2.6675212164073553e-05,
      "loss": 8.1176,
      "step": 10560
    },
    {
      "epoch": 3.26966205243214,
      "grad_norm": 1.1168321371078491,
      "learning_rate": 2.6653111739745407e-05,
      "loss": 8.111,
      "step": 10570
    },
    {
      "epoch": 3.272755394014384,
      "grad_norm": 0.9762133359909058,
      "learning_rate": 2.6631011315417258e-05,
      "loss": 8.1169,
      "step": 10580
    },
    {
      "epoch": 3.2758487355966284,
      "grad_norm": 0.6811995506286621,
      "learning_rate": 2.6608910891089112e-05,
      "loss": 8.1221,
      "step": 10590
    },
    {
      "epoch": 3.2789420771788724,
      "grad_norm": 0.6053406596183777,
      "learning_rate": 2.6586810466760963e-05,
      "loss": 8.1143,
      "step": 10600
    },
    {
      "epoch": 3.282035418761117,
      "grad_norm": 0.599897027015686,
      "learning_rate": 2.6564710042432817e-05,
      "loss": 8.1007,
      "step": 10610
    },
    {
      "epoch": 3.285128760343361,
      "grad_norm": 0.8028992414474487,
      "learning_rate": 2.654260961810467e-05,
      "loss": 8.1068,
      "step": 10620
    },
    {
      "epoch": 3.288222101925605,
      "grad_norm": 0.7862556576728821,
      "learning_rate": 2.6520509193776523e-05,
      "loss": 8.113,
      "step": 10630
    },
    {
      "epoch": 3.2913154435078495,
      "grad_norm": 0.7700480818748474,
      "learning_rate": 2.6498408769448374e-05,
      "loss": 8.103,
      "step": 10640
    },
    {
      "epoch": 3.2944087850900936,
      "grad_norm": 0.8608717918395996,
      "learning_rate": 2.6476308345120228e-05,
      "loss": 8.1089,
      "step": 10650
    },
    {
      "epoch": 3.2975021266723377,
      "grad_norm": 0.6745821237564087,
      "learning_rate": 2.645420792079208e-05,
      "loss": 8.0897,
      "step": 10660
    },
    {
      "epoch": 3.300595468254582,
      "grad_norm": 1.000858187675476,
      "learning_rate": 2.643210749646393e-05,
      "loss": 8.1162,
      "step": 10670
    },
    {
      "epoch": 3.3036888098368262,
      "grad_norm": 0.9222114682197571,
      "learning_rate": 2.6410007072135784e-05,
      "loss": 8.0992,
      "step": 10680
    },
    {
      "epoch": 3.3067821514190703,
      "grad_norm": 0.8219615817070007,
      "learning_rate": 2.6387906647807635e-05,
      "loss": 8.114,
      "step": 10690
    },
    {
      "epoch": 3.309875493001315,
      "grad_norm": 0.8828024864196777,
      "learning_rate": 2.6365806223479493e-05,
      "loss": 8.0951,
      "step": 10700
    },
    {
      "epoch": 3.312968834583559,
      "grad_norm": 0.9151893854141235,
      "learning_rate": 2.6343705799151347e-05,
      "loss": 8.1073,
      "step": 10710
    },
    {
      "epoch": 3.3160621761658033,
      "grad_norm": 0.8627604842185974,
      "learning_rate": 2.6321605374823198e-05,
      "loss": 8.1088,
      "step": 10720
    },
    {
      "epoch": 3.3191555177480474,
      "grad_norm": 0.5789752006530762,
      "learning_rate": 2.6299504950495053e-05,
      "loss": 8.1024,
      "step": 10730
    },
    {
      "epoch": 3.3222488593302915,
      "grad_norm": 1.0330649614334106,
      "learning_rate": 2.6277404526166903e-05,
      "loss": 8.1149,
      "step": 10740
    },
    {
      "epoch": 3.3253422009125355,
      "grad_norm": 0.996377170085907,
      "learning_rate": 2.6255304101838758e-05,
      "loss": 8.0982,
      "step": 10750
    },
    {
      "epoch": 3.32843554249478,
      "grad_norm": 0.801764726638794,
      "learning_rate": 2.623320367751061e-05,
      "loss": 8.0958,
      "step": 10760
    },
    {
      "epoch": 3.331528884077024,
      "grad_norm": 0.7370763421058655,
      "learning_rate": 2.6211103253182463e-05,
      "loss": 8.1014,
      "step": 10770
    },
    {
      "epoch": 3.3346222256592686,
      "grad_norm": 0.5048869848251343,
      "learning_rate": 2.6189002828854314e-05,
      "loss": 8.1054,
      "step": 10780
    },
    {
      "epoch": 3.3377155672415126,
      "grad_norm": 0.9257993698120117,
      "learning_rate": 2.616690240452617e-05,
      "loss": 8.0954,
      "step": 10790
    },
    {
      "epoch": 3.3408089088237567,
      "grad_norm": 0.7626732587814331,
      "learning_rate": 2.614480198019802e-05,
      "loss": 8.0915,
      "step": 10800
    },
    {
      "epoch": 3.343902250406001,
      "grad_norm": 0.6530042290687561,
      "learning_rate": 2.612270155586987e-05,
      "loss": 8.1037,
      "step": 10810
    },
    {
      "epoch": 3.3469955919882453,
      "grad_norm": 0.5945021510124207,
      "learning_rate": 2.6100601131541725e-05,
      "loss": 8.09,
      "step": 10820
    },
    {
      "epoch": 3.3500889335704898,
      "grad_norm": 0.6831685900688171,
      "learning_rate": 2.6078500707213582e-05,
      "loss": 8.0981,
      "step": 10830
    },
    {
      "epoch": 3.353182275152734,
      "grad_norm": 0.9660640954971313,
      "learning_rate": 2.6056400282885433e-05,
      "loss": 8.1146,
      "step": 10840
    },
    {
      "epoch": 3.356275616734978,
      "grad_norm": 0.9918832778930664,
      "learning_rate": 2.6034299858557288e-05,
      "loss": 8.1199,
      "step": 10850
    },
    {
      "epoch": 3.359368958317222,
      "grad_norm": 0.7425703406333923,
      "learning_rate": 2.601219943422914e-05,
      "loss": 8.0921,
      "step": 10860
    },
    {
      "epoch": 3.3624622998994664,
      "grad_norm": 0.8416071534156799,
      "learning_rate": 2.5990099009900993e-05,
      "loss": 8.0981,
      "step": 10870
    },
    {
      "epoch": 3.3655556414817105,
      "grad_norm": 1.227294683456421,
      "learning_rate": 2.5967998585572844e-05,
      "loss": 8.1318,
      "step": 10880
    },
    {
      "epoch": 3.368648983063955,
      "grad_norm": 1.0093448162078857,
      "learning_rate": 2.5945898161244698e-05,
      "loss": 8.1006,
      "step": 10890
    },
    {
      "epoch": 3.371742324646199,
      "grad_norm": 1.0424818992614746,
      "learning_rate": 2.592379773691655e-05,
      "loss": 8.0962,
      "step": 10900
    },
    {
      "epoch": 3.374835666228443,
      "grad_norm": 0.9508131742477417,
      "learning_rate": 2.5901697312588403e-05,
      "loss": 8.0845,
      "step": 10910
    },
    {
      "epoch": 3.3779290078106876,
      "grad_norm": 1.014140009880066,
      "learning_rate": 2.5879596888260254e-05,
      "loss": 8.0861,
      "step": 10920
    },
    {
      "epoch": 3.3810223493929317,
      "grad_norm": 0.6049110293388367,
      "learning_rate": 2.585749646393211e-05,
      "loss": 8.1038,
      "step": 10930
    },
    {
      "epoch": 3.384115690975176,
      "grad_norm": 0.7292504906654358,
      "learning_rate": 2.583539603960396e-05,
      "loss": 8.0941,
      "step": 10940
    },
    {
      "epoch": 3.3872090325574202,
      "grad_norm": 0.7692973613739014,
      "learning_rate": 2.581329561527581e-05,
      "loss": 8.0976,
      "step": 10950
    },
    {
      "epoch": 3.3903023741396643,
      "grad_norm": 0.6634624004364014,
      "learning_rate": 2.579119519094767e-05,
      "loss": 8.1048,
      "step": 10960
    },
    {
      "epoch": 3.3933957157219083,
      "grad_norm": 0.832862377166748,
      "learning_rate": 2.5769094766619523e-05,
      "loss": 8.1058,
      "step": 10970
    },
    {
      "epoch": 3.396489057304153,
      "grad_norm": 0.792634904384613,
      "learning_rate": 2.5746994342291374e-05,
      "loss": 8.1259,
      "step": 10980
    },
    {
      "epoch": 3.399582398886397,
      "grad_norm": 0.9821207523345947,
      "learning_rate": 2.5724893917963228e-05,
      "loss": 8.1043,
      "step": 10990
    },
    {
      "epoch": 3.4026757404686414,
      "grad_norm": 1.0299824476242065,
      "learning_rate": 2.570279349363508e-05,
      "loss": 8.1012,
      "step": 11000
    },
    {
      "epoch": 3.4057690820508855,
      "grad_norm": 1.0243135690689087,
      "learning_rate": 2.5680693069306933e-05,
      "loss": 8.0962,
      "step": 11010
    },
    {
      "epoch": 3.4088624236331295,
      "grad_norm": 0.6784717440605164,
      "learning_rate": 2.5658592644978784e-05,
      "loss": 8.1037,
      "step": 11020
    },
    {
      "epoch": 3.411955765215374,
      "grad_norm": 0.5318592190742493,
      "learning_rate": 2.563649222065064e-05,
      "loss": 8.1074,
      "step": 11030
    },
    {
      "epoch": 3.415049106797618,
      "grad_norm": 0.7155341506004333,
      "learning_rate": 2.561439179632249e-05,
      "loss": 8.121,
      "step": 11040
    },
    {
      "epoch": 3.4181424483798626,
      "grad_norm": 1.128713607788086,
      "learning_rate": 2.5592291371994344e-05,
      "loss": 8.0937,
      "step": 11050
    },
    {
      "epoch": 3.4212357899621066,
      "grad_norm": 0.853648841381073,
      "learning_rate": 2.5570190947666195e-05,
      "loss": 8.1003,
      "step": 11060
    },
    {
      "epoch": 3.4243291315443507,
      "grad_norm": 0.9434267282485962,
      "learning_rate": 2.554809052333805e-05,
      "loss": 8.1092,
      "step": 11070
    },
    {
      "epoch": 3.4274224731265948,
      "grad_norm": 0.789575457572937,
      "learning_rate": 2.55259900990099e-05,
      "loss": 8.1039,
      "step": 11080
    },
    {
      "epoch": 3.4305158147088393,
      "grad_norm": 0.6753472685813904,
      "learning_rate": 2.550388967468175e-05,
      "loss": 8.1001,
      "step": 11090
    },
    {
      "epoch": 3.4336091562910833,
      "grad_norm": 0.4465691149234772,
      "learning_rate": 2.5481789250353612e-05,
      "loss": 8.1063,
      "step": 11100
    },
    {
      "epoch": 3.436702497873328,
      "grad_norm": 0.8238527178764343,
      "learning_rate": 2.5459688826025463e-05,
      "loss": 8.0911,
      "step": 11110
    },
    {
      "epoch": 3.439795839455572,
      "grad_norm": 0.7504106760025024,
      "learning_rate": 2.5437588401697314e-05,
      "loss": 8.0931,
      "step": 11120
    },
    {
      "epoch": 3.442889181037816,
      "grad_norm": 0.9832870364189148,
      "learning_rate": 2.5415487977369168e-05,
      "loss": 8.1082,
      "step": 11130
    },
    {
      "epoch": 3.4459825226200604,
      "grad_norm": 1.1161816120147705,
      "learning_rate": 2.539338755304102e-05,
      "loss": 8.0966,
      "step": 11140
    },
    {
      "epoch": 3.4490758642023045,
      "grad_norm": 0.7530856728553772,
      "learning_rate": 2.5371287128712873e-05,
      "loss": 8.1109,
      "step": 11150
    },
    {
      "epoch": 3.452169205784549,
      "grad_norm": 0.6870070695877075,
      "learning_rate": 2.5349186704384724e-05,
      "loss": 8.1096,
      "step": 11160
    },
    {
      "epoch": 3.455262547366793,
      "grad_norm": 0.7706400752067566,
      "learning_rate": 2.532708628005658e-05,
      "loss": 8.0892,
      "step": 11170
    },
    {
      "epoch": 3.458355888949037,
      "grad_norm": 0.7305397391319275,
      "learning_rate": 2.530498585572843e-05,
      "loss": 8.1077,
      "step": 11180
    },
    {
      "epoch": 3.461449230531281,
      "grad_norm": 0.8493666648864746,
      "learning_rate": 2.5282885431400284e-05,
      "loss": 8.0998,
      "step": 11190
    },
    {
      "epoch": 3.4645425721135257,
      "grad_norm": 0.7701996564865112,
      "learning_rate": 2.5260785007072135e-05,
      "loss": 8.1022,
      "step": 11200
    },
    {
      "epoch": 3.4676359136957697,
      "grad_norm": 0.5574349164962769,
      "learning_rate": 2.523868458274399e-05,
      "loss": 8.1004,
      "step": 11210
    },
    {
      "epoch": 3.4707292552780142,
      "grad_norm": 0.6364221572875977,
      "learning_rate": 2.521658415841584e-05,
      "loss": 8.0852,
      "step": 11220
    },
    {
      "epoch": 3.4738225968602583,
      "grad_norm": 0.5897513628005981,
      "learning_rate": 2.5194483734087698e-05,
      "loss": 8.1052,
      "step": 11230
    },
    {
      "epoch": 3.4769159384425024,
      "grad_norm": 0.7260732054710388,
      "learning_rate": 2.5172383309759552e-05,
      "loss": 8.0914,
      "step": 11240
    },
    {
      "epoch": 3.480009280024747,
      "grad_norm": 0.7340152859687805,
      "learning_rate": 2.5150282885431403e-05,
      "loss": 8.1152,
      "step": 11250
    },
    {
      "epoch": 3.483102621606991,
      "grad_norm": 0.7422891855239868,
      "learning_rate": 2.5128182461103254e-05,
      "loss": 8.1078,
      "step": 11260
    },
    {
      "epoch": 3.4861959631892354,
      "grad_norm": 0.7885480523109436,
      "learning_rate": 2.510608203677511e-05,
      "loss": 8.1149,
      "step": 11270
    },
    {
      "epoch": 3.4892893047714795,
      "grad_norm": 0.6894049048423767,
      "learning_rate": 2.508398161244696e-05,
      "loss": 8.1159,
      "step": 11280
    },
    {
      "epoch": 3.4923826463537235,
      "grad_norm": 0.7820294499397278,
      "learning_rate": 2.5061881188118814e-05,
      "loss": 8.101,
      "step": 11290
    },
    {
      "epoch": 3.4954759879359676,
      "grad_norm": 0.6289247274398804,
      "learning_rate": 2.5039780763790665e-05,
      "loss": 8.111,
      "step": 11300
    },
    {
      "epoch": 3.498569329518212,
      "grad_norm": 0.8931630849838257,
      "learning_rate": 2.501768033946252e-05,
      "loss": 8.0927,
      "step": 11310
    },
    {
      "epoch": 3.501662671100456,
      "grad_norm": 0.6462466716766357,
      "learning_rate": 2.499557991513437e-05,
      "loss": 8.1134,
      "step": 11320
    },
    {
      "epoch": 3.5047560126827006,
      "grad_norm": 0.9078208208084106,
      "learning_rate": 2.4973479490806224e-05,
      "loss": 8.1185,
      "step": 11330
    },
    {
      "epoch": 3.5078493542649447,
      "grad_norm": 0.5990978479385376,
      "learning_rate": 2.495137906647808e-05,
      "loss": 8.0992,
      "step": 11340
    },
    {
      "epoch": 3.5109426958471888,
      "grad_norm": 0.7442519068717957,
      "learning_rate": 2.492927864214993e-05,
      "loss": 8.1061,
      "step": 11350
    },
    {
      "epoch": 3.5140360374294333,
      "grad_norm": 0.5777914524078369,
      "learning_rate": 2.4907178217821784e-05,
      "loss": 8.104,
      "step": 11360
    },
    {
      "epoch": 3.5171293790116773,
      "grad_norm": 0.6656046509742737,
      "learning_rate": 2.4885077793493635e-05,
      "loss": 8.1165,
      "step": 11370
    },
    {
      "epoch": 3.520222720593922,
      "grad_norm": 0.8046016097068787,
      "learning_rate": 2.486297736916549e-05,
      "loss": 8.0854,
      "step": 11380
    },
    {
      "epoch": 3.523316062176166,
      "grad_norm": 0.9777056574821472,
      "learning_rate": 2.484087694483734e-05,
      "loss": 8.1062,
      "step": 11390
    },
    {
      "epoch": 3.52640940375841,
      "grad_norm": 0.805345892906189,
      "learning_rate": 2.4818776520509195e-05,
      "loss": 8.1052,
      "step": 11400
    },
    {
      "epoch": 3.529502745340654,
      "grad_norm": 0.9235185384750366,
      "learning_rate": 2.479667609618105e-05,
      "loss": 8.1143,
      "step": 11410
    },
    {
      "epoch": 3.5325960869228985,
      "grad_norm": 0.5825061202049255,
      "learning_rate": 2.47745756718529e-05,
      "loss": 8.1107,
      "step": 11420
    },
    {
      "epoch": 3.5356894285051426,
      "grad_norm": 1.2757313251495361,
      "learning_rate": 2.4752475247524754e-05,
      "loss": 8.1102,
      "step": 11430
    },
    {
      "epoch": 3.538782770087387,
      "grad_norm": 0.6582181453704834,
      "learning_rate": 2.4730374823196605e-05,
      "loss": 8.1118,
      "step": 11440
    },
    {
      "epoch": 3.541876111669631,
      "grad_norm": 0.552203357219696,
      "learning_rate": 2.470827439886846e-05,
      "loss": 8.0976,
      "step": 11450
    },
    {
      "epoch": 3.544969453251875,
      "grad_norm": 0.5782486796379089,
      "learning_rate": 2.4686173974540314e-05,
      "loss": 8.0789,
      "step": 11460
    },
    {
      "epoch": 3.5480627948341197,
      "grad_norm": 0.8015296459197998,
      "learning_rate": 2.4664073550212165e-05,
      "loss": 8.0959,
      "step": 11470
    },
    {
      "epoch": 3.5511561364163637,
      "grad_norm": 0.8656583428382874,
      "learning_rate": 2.464197312588402e-05,
      "loss": 8.1116,
      "step": 11480
    },
    {
      "epoch": 3.5542494779986082,
      "grad_norm": 0.6272944808006287,
      "learning_rate": 2.461987270155587e-05,
      "loss": 8.1099,
      "step": 11490
    },
    {
      "epoch": 3.5573428195808523,
      "grad_norm": 0.8610741496086121,
      "learning_rate": 2.4597772277227724e-05,
      "loss": 8.1071,
      "step": 11500
    },
    {
      "epoch": 3.5604361611630964,
      "grad_norm": 0.6074814200401306,
      "learning_rate": 2.4575671852899575e-05,
      "loss": 8.0986,
      "step": 11510
    },
    {
      "epoch": 3.5635295027453404,
      "grad_norm": 1.0561699867248535,
      "learning_rate": 2.455357142857143e-05,
      "loss": 8.1117,
      "step": 11520
    },
    {
      "epoch": 3.566622844327585,
      "grad_norm": 0.65511155128479,
      "learning_rate": 2.4531471004243284e-05,
      "loss": 8.1183,
      "step": 11530
    },
    {
      "epoch": 3.569716185909829,
      "grad_norm": 0.7973815202713013,
      "learning_rate": 2.4509370579915135e-05,
      "loss": 8.1179,
      "step": 11540
    },
    {
      "epoch": 3.5728095274920735,
      "grad_norm": 0.8005743622779846,
      "learning_rate": 2.448727015558699e-05,
      "loss": 8.1032,
      "step": 11550
    },
    {
      "epoch": 3.5759028690743175,
      "grad_norm": 0.834827721118927,
      "learning_rate": 2.446516973125884e-05,
      "loss": 8.1008,
      "step": 11560
    },
    {
      "epoch": 3.5789962106565616,
      "grad_norm": 0.7012044787406921,
      "learning_rate": 2.4443069306930694e-05,
      "loss": 8.1085,
      "step": 11570
    },
    {
      "epoch": 3.582089552238806,
      "grad_norm": 0.8557447791099548,
      "learning_rate": 2.4420968882602545e-05,
      "loss": 8.1132,
      "step": 11580
    },
    {
      "epoch": 3.58518289382105,
      "grad_norm": 0.7214196920394897,
      "learning_rate": 2.43988684582744e-05,
      "loss": 8.0994,
      "step": 11590
    },
    {
      "epoch": 3.5882762354032947,
      "grad_norm": 1.0588473081588745,
      "learning_rate": 2.4376768033946254e-05,
      "loss": 8.1149,
      "step": 11600
    },
    {
      "epoch": 3.5913695769855387,
      "grad_norm": 0.7677791714668274,
      "learning_rate": 2.4354667609618105e-05,
      "loss": 8.0965,
      "step": 11610
    },
    {
      "epoch": 3.5944629185677828,
      "grad_norm": 1.0300345420837402,
      "learning_rate": 2.433256718528996e-05,
      "loss": 8.0952,
      "step": 11620
    },
    {
      "epoch": 3.597556260150027,
      "grad_norm": 1.1303727626800537,
      "learning_rate": 2.431046676096181e-05,
      "loss": 8.1065,
      "step": 11630
    },
    {
      "epoch": 3.6006496017322713,
      "grad_norm": 0.6124145984649658,
      "learning_rate": 2.4288366336633665e-05,
      "loss": 8.0975,
      "step": 11640
    },
    {
      "epoch": 3.6037429433145154,
      "grad_norm": 0.6149651408195496,
      "learning_rate": 2.4266265912305516e-05,
      "loss": 8.1108,
      "step": 11650
    },
    {
      "epoch": 3.60683628489676,
      "grad_norm": 0.7164033055305481,
      "learning_rate": 2.4244165487977373e-05,
      "loss": 8.1043,
      "step": 11660
    },
    {
      "epoch": 3.609929626479004,
      "grad_norm": 0.7561760544776917,
      "learning_rate": 2.4222065063649224e-05,
      "loss": 8.1125,
      "step": 11670
    },
    {
      "epoch": 3.613022968061248,
      "grad_norm": 0.9271063208580017,
      "learning_rate": 2.4199964639321075e-05,
      "loss": 8.1092,
      "step": 11680
    },
    {
      "epoch": 3.6161163096434925,
      "grad_norm": 0.9286230206489563,
      "learning_rate": 2.417786421499293e-05,
      "loss": 8.0978,
      "step": 11690
    },
    {
      "epoch": 3.6192096512257366,
      "grad_norm": 0.6517167091369629,
      "learning_rate": 2.415576379066478e-05,
      "loss": 8.099,
      "step": 11700
    },
    {
      "epoch": 3.622302992807981,
      "grad_norm": 0.6297550201416016,
      "learning_rate": 2.4133663366336635e-05,
      "loss": 8.0938,
      "step": 11710
    },
    {
      "epoch": 3.625396334390225,
      "grad_norm": 0.6664562225341797,
      "learning_rate": 2.4111562942008486e-05,
      "loss": 8.117,
      "step": 11720
    },
    {
      "epoch": 3.628489675972469,
      "grad_norm": 0.6474733948707581,
      "learning_rate": 2.4089462517680343e-05,
      "loss": 8.0969,
      "step": 11730
    },
    {
      "epoch": 3.6315830175547132,
      "grad_norm": 0.7206547260284424,
      "learning_rate": 2.4067362093352194e-05,
      "loss": 8.1016,
      "step": 11740
    },
    {
      "epoch": 3.6346763591369577,
      "grad_norm": 0.5733475685119629,
      "learning_rate": 2.4045261669024045e-05,
      "loss": 8.1002,
      "step": 11750
    },
    {
      "epoch": 3.637769700719202,
      "grad_norm": 0.7435771822929382,
      "learning_rate": 2.40231612446959e-05,
      "loss": 8.093,
      "step": 11760
    },
    {
      "epoch": 3.6408630423014463,
      "grad_norm": 0.7430015802383423,
      "learning_rate": 2.400106082036775e-05,
      "loss": 8.0996,
      "step": 11770
    },
    {
      "epoch": 3.6439563838836904,
      "grad_norm": 0.6231874227523804,
      "learning_rate": 2.3978960396039605e-05,
      "loss": 8.0972,
      "step": 11780
    },
    {
      "epoch": 3.6470497254659344,
      "grad_norm": 0.8493922352790833,
      "learning_rate": 2.3956859971711456e-05,
      "loss": 8.0946,
      "step": 11790
    },
    {
      "epoch": 3.650143067048179,
      "grad_norm": 0.765673041343689,
      "learning_rate": 2.3934759547383314e-05,
      "loss": 8.1169,
      "step": 11800
    },
    {
      "epoch": 3.653236408630423,
      "grad_norm": 0.7977613210678101,
      "learning_rate": 2.3912659123055165e-05,
      "loss": 8.0974,
      "step": 11810
    },
    {
      "epoch": 3.6563297502126675,
      "grad_norm": 1.1814916133880615,
      "learning_rate": 2.3890558698727015e-05,
      "loss": 8.1026,
      "step": 11820
    },
    {
      "epoch": 3.6594230917949115,
      "grad_norm": 0.7055432200431824,
      "learning_rate": 2.386845827439887e-05,
      "loss": 8.1038,
      "step": 11830
    },
    {
      "epoch": 3.6625164333771556,
      "grad_norm": 0.8180091977119446,
      "learning_rate": 2.384635785007072e-05,
      "loss": 8.1143,
      "step": 11840
    },
    {
      "epoch": 3.6656097749593997,
      "grad_norm": 0.8891481161117554,
      "learning_rate": 2.3824257425742575e-05,
      "loss": 8.1103,
      "step": 11850
    },
    {
      "epoch": 3.668703116541644,
      "grad_norm": 0.6856452822685242,
      "learning_rate": 2.380215700141443e-05,
      "loss": 8.1028,
      "step": 11860
    },
    {
      "epoch": 3.671796458123888,
      "grad_norm": 1.027034044265747,
      "learning_rate": 2.3780056577086284e-05,
      "loss": 8.1073,
      "step": 11870
    },
    {
      "epoch": 3.6748897997061327,
      "grad_norm": 0.5551108717918396,
      "learning_rate": 2.3757956152758135e-05,
      "loss": 8.0919,
      "step": 11880
    },
    {
      "epoch": 3.677983141288377,
      "grad_norm": 0.9678500890731812,
      "learning_rate": 2.3735855728429986e-05,
      "loss": 8.0962,
      "step": 11890
    },
    {
      "epoch": 3.681076482870621,
      "grad_norm": 0.7704302668571472,
      "learning_rate": 2.371375530410184e-05,
      "loss": 8.1009,
      "step": 11900
    },
    {
      "epoch": 3.6841698244528653,
      "grad_norm": 0.7322105169296265,
      "learning_rate": 2.369165487977369e-05,
      "loss": 8.099,
      "step": 11910
    },
    {
      "epoch": 3.6872631660351094,
      "grad_norm": 0.7687269449234009,
      "learning_rate": 2.3669554455445545e-05,
      "loss": 8.1092,
      "step": 11920
    },
    {
      "epoch": 3.690356507617354,
      "grad_norm": 0.6603623628616333,
      "learning_rate": 2.36474540311174e-05,
      "loss": 8.1068,
      "step": 11930
    },
    {
      "epoch": 3.693449849199598,
      "grad_norm": 0.8556596040725708,
      "learning_rate": 2.3625353606789254e-05,
      "loss": 8.0915,
      "step": 11940
    },
    {
      "epoch": 3.696543190781842,
      "grad_norm": 0.7415850758552551,
      "learning_rate": 2.3603253182461105e-05,
      "loss": 8.1204,
      "step": 11950
    },
    {
      "epoch": 3.699636532364086,
      "grad_norm": 0.7488853335380554,
      "learning_rate": 2.3581152758132956e-05,
      "loss": 8.0988,
      "step": 11960
    },
    {
      "epoch": 3.7027298739463306,
      "grad_norm": 0.7889118790626526,
      "learning_rate": 2.355905233380481e-05,
      "loss": 8.1141,
      "step": 11970
    },
    {
      "epoch": 3.7058232155285746,
      "grad_norm": 0.7262313961982727,
      "learning_rate": 2.353695190947666e-05,
      "loss": 8.1159,
      "step": 11980
    },
    {
      "epoch": 3.708916557110819,
      "grad_norm": 1.0776149034500122,
      "learning_rate": 2.3514851485148515e-05,
      "loss": 8.1086,
      "step": 11990
    },
    {
      "epoch": 3.712009898693063,
      "grad_norm": 1.2664437294006348,
      "learning_rate": 2.349275106082037e-05,
      "loss": 8.1028,
      "step": 12000
    },
    {
      "epoch": 3.7151032402753073,
      "grad_norm": 0.754374086856842,
      "learning_rate": 2.3470650636492224e-05,
      "loss": 8.1099,
      "step": 12010
    },
    {
      "epoch": 3.7181965818575518,
      "grad_norm": 0.5984283685684204,
      "learning_rate": 2.3448550212164075e-05,
      "loss": 8.1078,
      "step": 12020
    },
    {
      "epoch": 3.721289923439796,
      "grad_norm": 0.9657517671585083,
      "learning_rate": 2.3426449787835926e-05,
      "loss": 8.1038,
      "step": 12030
    },
    {
      "epoch": 3.7243832650220403,
      "grad_norm": 1.1449147462844849,
      "learning_rate": 2.340434936350778e-05,
      "loss": 8.101,
      "step": 12040
    },
    {
      "epoch": 3.7274766066042844,
      "grad_norm": 0.8093847632408142,
      "learning_rate": 2.338224893917963e-05,
      "loss": 8.0983,
      "step": 12050
    },
    {
      "epoch": 3.7305699481865284,
      "grad_norm": 0.6851373314857483,
      "learning_rate": 2.336014851485149e-05,
      "loss": 8.1021,
      "step": 12060
    },
    {
      "epoch": 3.7336632897687725,
      "grad_norm": 0.5898375511169434,
      "learning_rate": 2.333804809052334e-05,
      "loss": 8.1009,
      "step": 12070
    },
    {
      "epoch": 3.736756631351017,
      "grad_norm": 0.6686884164810181,
      "learning_rate": 2.3315947666195194e-05,
      "loss": 8.115,
      "step": 12080
    },
    {
      "epoch": 3.739849972933261,
      "grad_norm": 0.7025202512741089,
      "learning_rate": 2.3293847241867045e-05,
      "loss": 8.1148,
      "step": 12090
    },
    {
      "epoch": 3.7429433145155055,
      "grad_norm": 0.775547981262207,
      "learning_rate": 2.3271746817538896e-05,
      "loss": 8.1124,
      "step": 12100
    },
    {
      "epoch": 3.7460366560977496,
      "grad_norm": 0.788159966468811,
      "learning_rate": 2.324964639321075e-05,
      "loss": 8.0981,
      "step": 12110
    },
    {
      "epoch": 3.7491299976799937,
      "grad_norm": 0.8167747259140015,
      "learning_rate": 2.32275459688826e-05,
      "loss": 8.1112,
      "step": 12120
    },
    {
      "epoch": 3.752223339262238,
      "grad_norm": 1.0406444072723389,
      "learning_rate": 2.320544554455446e-05,
      "loss": 8.108,
      "step": 12130
    },
    {
      "epoch": 3.7553166808444822,
      "grad_norm": 0.7632429599761963,
      "learning_rate": 2.318334512022631e-05,
      "loss": 8.0723,
      "step": 12140
    },
    {
      "epoch": 3.7584100224267267,
      "grad_norm": 0.9803694486618042,
      "learning_rate": 2.3161244695898164e-05,
      "loss": 8.1042,
      "step": 12150
    },
    {
      "epoch": 3.761503364008971,
      "grad_norm": 0.6924657225608826,
      "learning_rate": 2.3139144271570015e-05,
      "loss": 8.1127,
      "step": 12160
    },
    {
      "epoch": 3.764596705591215,
      "grad_norm": 0.8107100129127502,
      "learning_rate": 2.3117043847241866e-05,
      "loss": 8.1137,
      "step": 12170
    },
    {
      "epoch": 3.767690047173459,
      "grad_norm": 0.712113618850708,
      "learning_rate": 2.309494342291372e-05,
      "loss": 8.1106,
      "step": 12180
    },
    {
      "epoch": 3.7707833887557034,
      "grad_norm": 0.8182639479637146,
      "learning_rate": 2.307284299858557e-05,
      "loss": 8.0995,
      "step": 12190
    },
    {
      "epoch": 3.7738767303379475,
      "grad_norm": 0.7950517535209656,
      "learning_rate": 2.305074257425743e-05,
      "loss": 8.1092,
      "step": 12200
    },
    {
      "epoch": 3.776970071920192,
      "grad_norm": 0.7742359638214111,
      "learning_rate": 2.302864214992928e-05,
      "loss": 8.1086,
      "step": 12210
    },
    {
      "epoch": 3.780063413502436,
      "grad_norm": 0.8650312423706055,
      "learning_rate": 2.300654172560113e-05,
      "loss": 8.1138,
      "step": 12220
    },
    {
      "epoch": 3.78315675508468,
      "grad_norm": 0.9999988079071045,
      "learning_rate": 2.2984441301272985e-05,
      "loss": 8.1145,
      "step": 12230
    },
    {
      "epoch": 3.7862500966669246,
      "grad_norm": 0.9465503096580505,
      "learning_rate": 2.2962340876944836e-05,
      "loss": 8.108,
      "step": 12240
    },
    {
      "epoch": 3.7893434382491686,
      "grad_norm": 0.8597802519798279,
      "learning_rate": 2.294024045261669e-05,
      "loss": 8.1096,
      "step": 12250
    },
    {
      "epoch": 3.792436779831413,
      "grad_norm": 0.5820393562316895,
      "learning_rate": 2.2918140028288545e-05,
      "loss": 8.0988,
      "step": 12260
    },
    {
      "epoch": 3.795530121413657,
      "grad_norm": 1.0791078805923462,
      "learning_rate": 2.28960396039604e-05,
      "loss": 8.1055,
      "step": 12270
    },
    {
      "epoch": 3.7986234629959013,
      "grad_norm": 1.310927391052246,
      "learning_rate": 2.287393917963225e-05,
      "loss": 8.1045,
      "step": 12280
    },
    {
      "epoch": 3.8017168045781453,
      "grad_norm": 0.9056339859962463,
      "learning_rate": 2.28518387553041e-05,
      "loss": 8.1033,
      "step": 12290
    },
    {
      "epoch": 3.80481014616039,
      "grad_norm": 1.0938446521759033,
      "learning_rate": 2.2829738330975956e-05,
      "loss": 8.101,
      "step": 12300
    },
    {
      "epoch": 3.807903487742634,
      "grad_norm": 0.9294905662536621,
      "learning_rate": 2.2807637906647807e-05,
      "loss": 8.1022,
      "step": 12310
    },
    {
      "epoch": 3.8109968293248784,
      "grad_norm": 0.610062837600708,
      "learning_rate": 2.278553748231966e-05,
      "loss": 8.0918,
      "step": 12320
    },
    {
      "epoch": 3.8140901709071224,
      "grad_norm": 0.5966021418571472,
      "learning_rate": 2.2763437057991515e-05,
      "loss": 8.099,
      "step": 12330
    },
    {
      "epoch": 3.8171835124893665,
      "grad_norm": 0.8067981600761414,
      "learning_rate": 2.274133663366337e-05,
      "loss": 8.1105,
      "step": 12340
    },
    {
      "epoch": 3.820276854071611,
      "grad_norm": 0.7344384789466858,
      "learning_rate": 2.271923620933522e-05,
      "loss": 8.1098,
      "step": 12350
    },
    {
      "epoch": 3.823370195653855,
      "grad_norm": 0.8760346174240112,
      "learning_rate": 2.269713578500707e-05,
      "loss": 8.0858,
      "step": 12360
    },
    {
      "epoch": 3.8264635372360996,
      "grad_norm": 0.7347402572631836,
      "learning_rate": 2.2675035360678926e-05,
      "loss": 8.1139,
      "step": 12370
    },
    {
      "epoch": 3.8295568788183436,
      "grad_norm": 0.7172052264213562,
      "learning_rate": 2.2652934936350777e-05,
      "loss": 8.1113,
      "step": 12380
    },
    {
      "epoch": 3.8326502204005877,
      "grad_norm": 1.0694178342819214,
      "learning_rate": 2.263083451202263e-05,
      "loss": 8.0983,
      "step": 12390
    },
    {
      "epoch": 3.8357435619828317,
      "grad_norm": 0.8179268836975098,
      "learning_rate": 2.2608734087694485e-05,
      "loss": 8.1077,
      "step": 12400
    },
    {
      "epoch": 3.8388369035650762,
      "grad_norm": 0.8312594294548035,
      "learning_rate": 2.258663366336634e-05,
      "loss": 8.0965,
      "step": 12410
    },
    {
      "epoch": 3.8419302451473203,
      "grad_norm": 0.7831716537475586,
      "learning_rate": 2.256453323903819e-05,
      "loss": 8.0982,
      "step": 12420
    },
    {
      "epoch": 3.845023586729565,
      "grad_norm": 0.7636587619781494,
      "learning_rate": 2.254243281471004e-05,
      "loss": 8.1061,
      "step": 12430
    },
    {
      "epoch": 3.848116928311809,
      "grad_norm": 1.0535497665405273,
      "learning_rate": 2.2520332390381896e-05,
      "loss": 8.0984,
      "step": 12440
    },
    {
      "epoch": 3.851210269894053,
      "grad_norm": 0.7738524079322815,
      "learning_rate": 2.2498231966053747e-05,
      "loss": 8.1057,
      "step": 12450
    },
    {
      "epoch": 3.8543036114762974,
      "grad_norm": 0.5989582538604736,
      "learning_rate": 2.2476131541725605e-05,
      "loss": 8.0974,
      "step": 12460
    },
    {
      "epoch": 3.8573969530585415,
      "grad_norm": 0.6576058864593506,
      "learning_rate": 2.2454031117397456e-05,
      "loss": 8.1131,
      "step": 12470
    },
    {
      "epoch": 3.860490294640786,
      "grad_norm": 0.4653606116771698,
      "learning_rate": 2.243193069306931e-05,
      "loss": 8.1033,
      "step": 12480
    },
    {
      "epoch": 3.86358363622303,
      "grad_norm": 0.6948969960212708,
      "learning_rate": 2.240983026874116e-05,
      "loss": 8.1121,
      "step": 12490
    },
    {
      "epoch": 3.866676977805274,
      "grad_norm": 0.6448131203651428,
      "learning_rate": 2.2387729844413012e-05,
      "loss": 8.0964,
      "step": 12500
    },
    {
      "epoch": 3.869770319387518,
      "grad_norm": 0.8628362417221069,
      "learning_rate": 2.2365629420084866e-05,
      "loss": 8.103,
      "step": 12510
    },
    {
      "epoch": 3.8728636609697626,
      "grad_norm": 0.5982326865196228,
      "learning_rate": 2.2343528995756717e-05,
      "loss": 8.0971,
      "step": 12520
    },
    {
      "epoch": 3.8759570025520067,
      "grad_norm": 0.782917857170105,
      "learning_rate": 2.2321428571428575e-05,
      "loss": 8.1147,
      "step": 12530
    },
    {
      "epoch": 3.879050344134251,
      "grad_norm": 0.48247870802879333,
      "learning_rate": 2.2299328147100426e-05,
      "loss": 8.1083,
      "step": 12540
    },
    {
      "epoch": 3.8821436857164953,
      "grad_norm": 0.8832669854164124,
      "learning_rate": 2.227722772277228e-05,
      "loss": 8.0962,
      "step": 12550
    },
    {
      "epoch": 3.8852370272987393,
      "grad_norm": 0.6849269866943359,
      "learning_rate": 2.225512729844413e-05,
      "loss": 8.0999,
      "step": 12560
    },
    {
      "epoch": 3.888330368880984,
      "grad_norm": 0.8913506865501404,
      "learning_rate": 2.2233026874115982e-05,
      "loss": 8.1014,
      "step": 12570
    },
    {
      "epoch": 3.891423710463228,
      "grad_norm": 0.8575028777122498,
      "learning_rate": 2.2210926449787836e-05,
      "loss": 8.1107,
      "step": 12580
    },
    {
      "epoch": 3.8945170520454724,
      "grad_norm": 0.5796117186546326,
      "learning_rate": 2.2188826025459687e-05,
      "loss": 8.0998,
      "step": 12590
    },
    {
      "epoch": 3.8976103936277164,
      "grad_norm": 0.5379560589790344,
      "learning_rate": 2.2166725601131545e-05,
      "loss": 8.1051,
      "step": 12600
    },
    {
      "epoch": 3.9007037352099605,
      "grad_norm": 0.5631392598152161,
      "learning_rate": 2.2144625176803396e-05,
      "loss": 8.0846,
      "step": 12610
    },
    {
      "epoch": 3.9037970767922046,
      "grad_norm": 0.7395532727241516,
      "learning_rate": 2.212252475247525e-05,
      "loss": 8.108,
      "step": 12620
    },
    {
      "epoch": 3.906890418374449,
      "grad_norm": 0.5533865690231323,
      "learning_rate": 2.21004243281471e-05,
      "loss": 8.1031,
      "step": 12630
    },
    {
      "epoch": 3.909983759956693,
      "grad_norm": 0.5846147537231445,
      "learning_rate": 2.2078323903818952e-05,
      "loss": 8.1095,
      "step": 12640
    },
    {
      "epoch": 3.9130771015389376,
      "grad_norm": 0.730043888092041,
      "learning_rate": 2.2056223479490806e-05,
      "loss": 8.1061,
      "step": 12650
    },
    {
      "epoch": 3.9161704431211817,
      "grad_norm": 0.8328079581260681,
      "learning_rate": 2.203412305516266e-05,
      "loss": 8.1053,
      "step": 12660
    },
    {
      "epoch": 3.9192637847034257,
      "grad_norm": 0.6462491154670715,
      "learning_rate": 2.2012022630834515e-05,
      "loss": 8.101,
      "step": 12670
    },
    {
      "epoch": 3.9223571262856702,
      "grad_norm": 0.9627012610435486,
      "learning_rate": 2.1989922206506366e-05,
      "loss": 8.1076,
      "step": 12680
    },
    {
      "epoch": 3.9254504678679143,
      "grad_norm": 1.363150715827942,
      "learning_rate": 2.196782178217822e-05,
      "loss": 8.1047,
      "step": 12690
    },
    {
      "epoch": 3.928543809450159,
      "grad_norm": 1.0151373147964478,
      "learning_rate": 2.194572135785007e-05,
      "loss": 8.103,
      "step": 12700
    },
    {
      "epoch": 3.931637151032403,
      "grad_norm": 0.5576894283294678,
      "learning_rate": 2.1923620933521922e-05,
      "loss": 8.1015,
      "step": 12710
    },
    {
      "epoch": 3.934730492614647,
      "grad_norm": 0.6014869213104248,
      "learning_rate": 2.1901520509193777e-05,
      "loss": 8.1058,
      "step": 12720
    },
    {
      "epoch": 3.937823834196891,
      "grad_norm": 0.710477888584137,
      "learning_rate": 2.187942008486563e-05,
      "loss": 8.1019,
      "step": 12730
    },
    {
      "epoch": 3.9409171757791355,
      "grad_norm": 0.6685858964920044,
      "learning_rate": 2.1857319660537485e-05,
      "loss": 8.1046,
      "step": 12740
    },
    {
      "epoch": 3.9440105173613795,
      "grad_norm": 0.9516357183456421,
      "learning_rate": 2.1835219236209336e-05,
      "loss": 8.1147,
      "step": 12750
    },
    {
      "epoch": 3.947103858943624,
      "grad_norm": 0.9290387630462646,
      "learning_rate": 2.181311881188119e-05,
      "loss": 8.0899,
      "step": 12760
    },
    {
      "epoch": 3.950197200525868,
      "grad_norm": 0.776019811630249,
      "learning_rate": 2.179101838755304e-05,
      "loss": 8.1029,
      "step": 12770
    },
    {
      "epoch": 3.953290542108112,
      "grad_norm": 0.6510490775108337,
      "learning_rate": 2.1768917963224892e-05,
      "loss": 8.0982,
      "step": 12780
    },
    {
      "epoch": 3.9563838836903567,
      "grad_norm": 1.0956438779830933,
      "learning_rate": 2.1746817538896747e-05,
      "loss": 8.0889,
      "step": 12790
    },
    {
      "epoch": 3.9594772252726007,
      "grad_norm": 0.848426342010498,
      "learning_rate": 2.17247171145686e-05,
      "loss": 8.1117,
      "step": 12800
    },
    {
      "epoch": 3.962570566854845,
      "grad_norm": 0.6948767304420471,
      "learning_rate": 2.1702616690240455e-05,
      "loss": 8.1012,
      "step": 12810
    },
    {
      "epoch": 3.9656639084370893,
      "grad_norm": 0.7312864661216736,
      "learning_rate": 2.1680516265912306e-05,
      "loss": 8.1157,
      "step": 12820
    },
    {
      "epoch": 3.9687572500193333,
      "grad_norm": 0.72239750623703,
      "learning_rate": 2.165841584158416e-05,
      "loss": 8.1124,
      "step": 12830
    },
    {
      "epoch": 3.9718505916015774,
      "grad_norm": 0.8603538870811462,
      "learning_rate": 2.163631541725601e-05,
      "loss": 8.103,
      "step": 12840
    },
    {
      "epoch": 3.974943933183822,
      "grad_norm": 0.572148859500885,
      "learning_rate": 2.1614214992927863e-05,
      "loss": 8.0941,
      "step": 12850
    },
    {
      "epoch": 3.978037274766066,
      "grad_norm": 0.8024046421051025,
      "learning_rate": 2.159211456859972e-05,
      "loss": 8.1006,
      "step": 12860
    },
    {
      "epoch": 3.9811306163483104,
      "grad_norm": 0.8555017113685608,
      "learning_rate": 2.157001414427157e-05,
      "loss": 8.107,
      "step": 12870
    },
    {
      "epoch": 3.9842239579305545,
      "grad_norm": 0.9062358736991882,
      "learning_rate": 2.1547913719943426e-05,
      "loss": 8.0931,
      "step": 12880
    },
    {
      "epoch": 3.9873172995127986,
      "grad_norm": 0.7969098091125488,
      "learning_rate": 2.1525813295615277e-05,
      "loss": 8.1041,
      "step": 12890
    },
    {
      "epoch": 3.990410641095043,
      "grad_norm": 0.6713497638702393,
      "learning_rate": 2.150371287128713e-05,
      "loss": 8.1218,
      "step": 12900
    },
    {
      "epoch": 3.993503982677287,
      "grad_norm": 0.9727796912193298,
      "learning_rate": 2.1481612446958982e-05,
      "loss": 8.1045,
      "step": 12910
    },
    {
      "epoch": 3.9965973242595316,
      "grad_norm": 0.8432744145393372,
      "learning_rate": 2.1459512022630833e-05,
      "loss": 8.1154,
      "step": 12920
    },
    {
      "epoch": 3.9996906658417757,
      "grad_norm": 0.6530187726020813,
      "learning_rate": 2.143741159830269e-05,
      "loss": 8.0942,
      "step": 12930
    }
  ],
  "logging_steps": 10,
  "max_steps": 22624,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 7,
  "save_steps": 500,
  "total_flos": 5.366534882471117e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
